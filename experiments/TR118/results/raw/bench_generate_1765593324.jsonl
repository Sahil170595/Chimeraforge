{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [668.7055999937002, 48.172700000577606], "ttft_ms": [629.1819000034593, 5.40039999759756], "tokens_processed": [8, 8], "throughput_tok_s": [11.96341110359382, 166.06916365294197], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2059.0436000056798, 257.6714000024367, 1.4623000024585053], "resource_metrics": {"samples": 22, "duration_s": 2.9680585861206055, "gpu_memory_mean_mb": 516.6558948863636, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.2727272727272727, "gpu_power_mean_watts": 29.247454545454545, "gpu_power_peak_watts": 31.216, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1112.25, "cpu_memory_peak_mb": 1377.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593332.7602108}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [45.470600001863204, 48.569700004009064], "ttft_ms": [5.732099998567719, 5.122599999594968], "tokens_processed": [8, 8], "throughput_tok_s": [175.93785874108087, 164.7117441396521], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.084100001316983, 5.420399997092318, 5.2973999991081655], "resource_metrics": {"samples": 2, "duration_s": 0.10747075080871582, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.343, "gpu_power_peak_watts": 29.343, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1384.01171875, "cpu_memory_peak_mb": 1384.01171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593332.9778147}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [55.140700002084486, 45.35580000083428], "ttft_ms": [12.556200003018603, 5.981300004350487], "tokens_processed": [8, 8], "throughput_tok_s": [145.08339574393463, 176.38317480571055], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.391200000303797, 5.559899997024331, 5.200700004934333], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.343, "gpu_power_peak_watts": 29.343, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1384.0703125, "cpu_memory_peak_mb": 1384.0703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593333.1092713}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [45.41169999720296, 46.18499999924097], "ttft_ms": [5.3662999998778105, 5.595700000412762], "tokens_processed": [8, 8], "throughput_tok_s": [176.16605413346656, 173.2164122579079], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.249699997482821, 6.35350000084145, 5.397100001573563], "resource_metrics": {"samples": 2, "duration_s": 0.11680293083190918, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 30.994, "gpu_power_peak_watts": 30.994, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1384.103515625, "cpu_memory_peak_mb": 1384.10546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593333.3354173}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [43.89090000040596, 45.65129999537021], "ttft_ms": [5.463199995574541, 5.365000004530884], "tokens_processed": [8, 8], "throughput_tok_s": [182.270128886079, 175.24144987790777], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.311699999670964, 11.797700004535727, 12.101699998311233], "resource_metrics": {"samples": 2, "duration_s": 0.12086772918701172, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 30.994, "gpu_power_peak_watts": 30.994, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1384.109375, "cpu_memory_peak_mb": 1384.109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593333.5673172}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [1865.4563000018243, 54.52129999321187], "ttft_ms": [1817.4480000016047, 6.661100000201259], "tokens_processed": [8, 8], "throughput_tok_s": [4.288494991810945, 146.73164434809942], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1782.7583999969647, 9.769099997356534, 10.62149999779649], "resource_metrics": {"samples": 36, "duration_s": 3.740609645843506, "gpu_memory_mean_mb": 563.9639756944445, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.6944444444444444, "gpu_power_mean_watts": 29.699888888888886, "gpu_power_peak_watts": 33.686, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1406.782009548611, "cpu_memory_peak_mb": 1439.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593337.4182856}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [64.4248000025982, 85.1494000016828], "ttft_ms": [5.830599999171682, 11.888799999724142], "tokens_processed": [8, 8], "throughput_tok_s": [124.17578323374487, 93.95251170110295], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [13.942500001576263, 6.685300002573058, 5.511199997272342], "resource_metrics": {"samples": 2, "duration_s": 0.11641335487365723, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.862, "gpu_power_peak_watts": 28.862, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1439.8515625, "cpu_memory_peak_mb": 1439.8515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593337.6446958}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [62.31660000048578, 53.13029999524588], "ttft_ms": [7.355599998845719, 8.198099996661767], "tokens_processed": [8, 8], "throughput_tok_s": [128.37670861275546, 150.5732134152422], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.028399999602698, 6.038599996827543, 6.029099997249432], "resource_metrics": {"samples": 2, "duration_s": 0.11002373695373535, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 29.388, "gpu_power_peak_watts": 29.388, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1439.8515625, "cpu_memory_peak_mb": 1439.8515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593337.8635037}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [62.45850000414066, 65.805400001409], "ttft_ms": [6.091700000979472, 6.255000000237487], "tokens_processed": [8, 8], "throughput_tok_s": [128.0850484636942, 121.57057019376384], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.717799999634735, 9.370700005092658, 7.484000001568347], "resource_metrics": {"samples": 2, "duration_s": 0.12229347229003906, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 29.388, "gpu_power_peak_watts": 29.388, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1439.85546875, "cpu_memory_peak_mb": 1439.85546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593338.0958915}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [93.86549999908311, 89.5818000062718], "ttft_ms": [12.20869999815477, 11.46580000204267], "tokens_processed": [8, 8], "throughput_tok_s": [85.22833202910702, 89.30385412483231], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.115299999190029, 5.911699998250697, 14.396300000953488], "resource_metrics": {"samples": 2, "duration_s": 0.12033414840698242, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 27.249, "gpu_power_peak_watts": 27.249, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1439.92578125, "cpu_memory_peak_mb": 1439.92578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593338.3286316}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [326.1062999954447, 27.05229999992298], "ttft_ms": [302.50969999906374, 3.2535999998799525], "tokens_processed": [8, 8], "throughput_tok_s": [24.531878102666983, 295.7234689849949], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [357.0620000027702, 3.8526999996975064, 3.527399996528402], "resource_metrics": {"samples": 3, "duration_s": 0.6908409595489502, "gpu_memory_mean_mb": 565.3528645833334, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 22.436333333333334, "gpu_power_peak_watts": 27.249, "gpu_temperature_mean_c": 45.333333333333336, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1440.8802083333333, "cpu_memory_peak_mb": 1441.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.1305118}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.025800002680626, 32.01529999932973], "ttft_ms": [3.907500002242159, 3.188499998941552], "tokens_processed": [8, 8], "throughput_tok_s": [257.84991843268506, 249.8805258787982], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9890000043669716, 3.063500000280328, 3.300600001239218], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.52, "gpu_power_peak_watts": 13.52, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1441.77734375, "cpu_memory_peak_mb": 1441.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.254997}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.598200002103113, 26.22640000481624], "ttft_ms": [3.660399997897912, 3.3051999998860992], "tokens_processed": [8, 8], "throughput_tok_s": [289.8739772662841, 305.0361467273767], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.700300003401935, 3.43099999736296, 3.523000006680377], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.52, "gpu_power_peak_watts": 13.52, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1441.7890625, "cpu_memory_peak_mb": 1441.7890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.375843}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.982599998940714, 39.73440000117989], "ttft_ms": [4.28739999915706, 4.507899997406639], "tokens_processed": [8, 8], "throughput_tok_s": [210.6227588480807, 201.3368768563875], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.209300001093652, 4.698199998529162, 3.6579000006895512], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.52, "gpu_power_peak_watts": 13.52, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1441.80859375, "cpu_memory_peak_mb": 1441.80859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.5016997}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [41.308700005174614, 44.38980000122683], "ttft_ms": [4.582199995638803, 4.44710000010673], "tokens_processed": [8, 8], "throughput_tok_s": [193.66380445276332, 180.22158243062367], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.372499999590218, 3.8108000007923692, 4.593999998178333], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.52, "gpu_power_peak_watts": 13.52, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1441.80859375, "cpu_memory_peak_mb": 1441.80859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.6285837}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "degraded", "error": "2 degraded: exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "latencies_ms": [], "ttft_ms": [], "tokens_processed": [], "throughput_tok_s": [], "predicted_tokens": [], "outputs": [], "degraded_count": 2, "degraded_reasons": ["exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n"], "warmup_latencies_ms": [0.0, 0.0, 0.0], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.239, "gpu_power_peak_watts": 13.239, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1441.984375, "cpu_memory_peak_mb": 1441.984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.7536635}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "degraded", "error": "2 degraded: exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "latencies_ms": [], "ttft_ms": [], "tokens_processed": [], "throughput_tok_s": [], "predicted_tokens": [], "outputs": [], "degraded_count": 2, "degraded_reasons": ["exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n"], "warmup_latencies_ms": [0.0, 0.0, 0.0], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 570.01953125, "gpu_memory_peak_mb": 570.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.239, "gpu_power_peak_watts": 13.239, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1442.984375, "cpu_memory_peak_mb": 1442.984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593339.87815}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "degraded", "error": "2 degraded: exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "latencies_ms": [], "ttft_ms": [], "tokens_processed": [], "throughput_tok_s": [], "predicted_tokens": [], "outputs": [], "degraded_count": 2, "degraded_reasons": ["exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n"], "warmup_latencies_ms": [0.0, 0.0, 0.0], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 570.01953125, "gpu_memory_peak_mb": 570.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.239, "gpu_power_peak_watts": 13.239, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1443.015625, "cpu_memory_peak_mb": 1443.015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593340.0029233}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "degraded", "error": "2 degraded: exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "latencies_ms": [], "ttft_ms": [], "tokens_processed": [], "throughput_tok_s": [], "predicted_tokens": [], "outputs": [], "degraded_count": 2, "degraded_reasons": ["exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n"], "warmup_latencies_ms": [0.0, 0.0, 0.0], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 570.01953125, "gpu_memory_peak_mb": 570.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 13.239, "gpu_power_peak_watts": 13.239, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1443.0234375, "cpu_memory_peak_mb": 1443.0234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593340.1271799}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "degraded", "error": "2 degraded: exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "latencies_ms": [], "ttft_ms": [], "tokens_processed": [], "throughput_tok_s": [], "predicted_tokens": [], "outputs": [], "degraded_count": 2, "degraded_reasons": ["exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n", "exception:RuntimeError:Traceback (most recent call last):\n  File \"C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr118\\run_benchmark.py\", line 403, in _infer_torch_logits\n    out = model(input_ids=input_ids_t, attention_mask=attention_mask_t, use_cache=False)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 375, in __call__\n    return super().__call__(*args, **kwargs)\n           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 736, in compile_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1784, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py\", line 1027, in forward\n    @auto_docstring\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py\", line 929, in _fn\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\aot_autograd.py\", line 1241, in forward\n    return compiled_fn(full_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 384, in runtime_wrapper\n    all_outs = call_func_at_runtime_with_args(\n        compiled_fn, args, disable_amp=disable_amp, steal_args=True\n    )\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\utils.py\", line 126, in call_func_at_runtime_with_args\n    out = normalize_as_list(f(args))\n                            ~^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 750, in inner_fn\n    outs = compiled_fn(args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_functorch\\_aot_autograd\\runtime_wrappers.py\", line 556, in wrapper\n    return compiled_fn(runtime_args)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 387, in deferred_cudagraphify\n    return fn(inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\utils.py\", line 2716, in run\n    out = model(new_inputs)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2149, in _run\n    return self.run(new_inputs, function_id)\n           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2012, in run\n    out = self._run(new_inputs, function_id)\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2131, in _run\n    return self.execute_node(child, new_inputs)\n           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 2247, in execute_node\n    return node.run(new_inputs)\n           ~~~~~~~~^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1107, in run\n    self._copy_inputs_and_remove_from_src(self.reconstructed_inputs, new_inputs)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\sahil\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\_inductor\\cudagraph_trees.py\", line 1070, in _copy_inputs_and_remove_from_src\n    torch._foreach_copy_(dst_tensors, src_tensors)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (27) must match the size of tensor b (35) at non-singleton dimension 1\n"], "warmup_latencies_ms": [0.0, 0.0, 0.0], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 570.01953125, "gpu_memory_peak_mb": 570.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 2.0, "gpu_power_mean_watts": 11.931, "gpu_power_peak_watts": 11.931, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1443.11328125, "cpu_memory_peak_mb": 1443.11328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593340.2499704}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2132.01389999449, 128.04660000256263], "ttft_ms": [2019.4990000018151, 14.590300001145806], "tokens_processed": [32, 32], "throughput_tok_s": [15.009283007058583, 249.90901749331553], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2401.2010999940685, 23.44920000177808, 16.437300000688992], "resource_metrics": {"samples": 45, "duration_s": 4.732853889465332, "gpu_memory_mean_mb": 587.61953125, "gpu_memory_peak_mb": 604.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.35555555555555557, "gpu_power_mean_watts": 5.675022222222222, "gpu_power_peak_watts": 11.931, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1447.2083333333333, "cpu_memory_peak_mb": 1505.1875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593345.0919762}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [124.41659999603871, 114.67849999462487], "ttft_ms": [14.272299995354842, 14.23110000177985], "tokens_processed": [32, 32], "throughput_tok_s": [257.200405741829, 279.0409710756583], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [14.792199996009003, 14.363900001626462, 14.570699997420888], "resource_metrics": {"samples": 3, "duration_s": 0.2145061492919922, "gpu_memory_mean_mb": 604.01953125, "gpu_memory_peak_mb": 604.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 1.971, "gpu_power_peak_watts": 1.971, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1505.1953125, "cpu_memory_peak_mb": 1505.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593345.4146516}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [89.49740000389284, 88.53249999810942], "ttft_ms": [10.475200004293583, 10.044300004665274], "tokens_processed": [32, 32], "throughput_tok_s": [357.55228641958433, 361.4491853351408], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.32169999962207, 12.006800003291573, 10.091399999510031], "resource_metrics": {"samples": 2, "duration_s": 0.11873698234558105, "gpu_memory_mean_mb": 604.01953125, "gpu_memory_peak_mb": 604.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 1.971, "gpu_power_peak_watts": 1.971, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1505.296875, "cpu_memory_peak_mb": 1505.296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593345.6408484}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [91.45859999989625, 91.84119999554241], "ttft_ms": [9.789600000658538, 10.553300002356991], "tokens_processed": [32, 32], "throughput_tok_s": [349.8850846179178, 348.4275031418704], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [17.44759999564849, 10.147099994355813, 10.173599999689031], "resource_metrics": {"samples": 3, "duration_s": 0.21194982528686523, "gpu_memory_mean_mb": 604.01953125, "gpu_memory_peak_mb": 604.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 3.36, "gpu_power_peak_watts": 3.36, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1505.3125, "cpu_memory_peak_mb": 1505.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593345.9608054}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [90.07290000590729, 93.90510000230279], "ttft_ms": [9.979299997212365, 10.79079999908572], "tokens_processed": [32, 32], "throughput_tok_s": [355.267788623452, 340.76956415801993], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.153399998671375, 10.103600005095359, 10.123800006113015], "resource_metrics": {"samples": 2, "duration_s": 0.11916995048522949, "gpu_memory_mean_mb": 604.01953125, "gpu_memory_peak_mb": 604.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 3.36, "gpu_power_peak_watts": 3.36, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1505.421875, "cpu_memory_peak_mb": 1505.421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593346.1897888}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [409.80470000067726, 78.85620000160998], "ttft_ms": [344.35369999846444, 8.832100000290666], "tokens_processed": [32, 32], "throughput_tok_s": [78.08597607579199, 405.8019534208682], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [349.86429999844404, 8.322300003783312, 8.277900000393856], "resource_metrics": {"samples": 4, "duration_s": 0.8249108791351318, "gpu_memory_mean_mb": 617.51953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 40.0, "gpu_power_mean_watts": 12.3825, "gpu_power_peak_watts": 13.135, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1511.3134765625, "cpu_memory_peak_mb": 1527.328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593347.1275122}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [42.86839999986114, 43.607300001895055], "ttft_ms": [8.520399998815265, 4.356899997219443], "tokens_processed": [32, 32], "throughput_tok_s": [746.4705937264665, 733.822089388918], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.316599996760488, 8.319900000060443, 8.764600002905354], "resource_metrics": {"samples": 2, "duration_s": 0.10792350769042969, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 14.044, "gpu_power_peak_watts": 14.044, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1527.3359375, "cpu_memory_peak_mb": 1527.3359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593347.344221}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.701799998932984, 50.56400000466965], "ttft_ms": [6.932600001164246, 5.1848000002792105], "tokens_processed": [32, 32], "throughput_tok_s": [631.1413007166104, 632.8613242038755], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.4393999962485395, 4.4082000022172, 7.731300000159536], "resource_metrics": {"samples": 2, "duration_s": 0.11723828315734863, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 14.044, "gpu_power_peak_watts": 14.044, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1527.3359375, "cpu_memory_peak_mb": 1527.3359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593347.5693786}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [62.641700002131984, 55.122399993706495], "ttft_ms": [8.358500002941582, 7.416600004944485], "tokens_processed": [32, 32], "throughput_tok_s": [510.84181940960883, 580.526247109225], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.293099999718834, 7.293200003914535, 7.431300000462215], "resource_metrics": {"samples": 2, "duration_s": 0.11761593818664551, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 17.937, "gpu_power_peak_watts": 17.937, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1527.33984375, "cpu_memory_peak_mb": 1527.33984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593347.7997246}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [39.60930000175722, 39.99219999968773], "ttft_ms": [4.529899997578468, 4.851900004723575], "tokens_processed": [32, 32], "throughput_tok_s": [807.8910760498256, 800.156030432181], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.028999999922235, 5.936399997153785, 4.740199998195749], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 17.937, "gpu_power_peak_watts": 17.937, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1527.33984375, "cpu_memory_peak_mb": 1527.33984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1890.025799999421, "compile_ms": 577.9997999998159, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765593347.915222}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.5828000036417507, 3.3173999981954694], "ttft_ms": [0.341799997841008, 0.35770000249613076], "tokens_processed": [8, 8], "throughput_tok_s": [2232.8904744524866, 2411.527100847554], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.2705999983591028, 0.5590000000665896, 0.39200000173877925], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 17.937, "gpu_power_peak_watts": 17.937, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1534.42578125, "cpu_memory_peak_mb": 1534.42578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.0423486}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.35099999938393, 3.723800000443589], "ttft_ms": [0.3941000031773001, 0.38160000258358195], "tokens_processed": [8, 8], "throughput_tok_s": [2387.3470610178374, 2148.343090135619], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8875999992596917, 0.4019999978481792, 0.3971999976783991], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 78.0, "gpu_power_mean_watts": 17.937, "gpu_power_peak_watts": 17.937, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1534.43359375, "cpu_memory_peak_mb": 1534.43359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.1661403}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.4981000024126843, 2.499599999282509], "ttft_ms": [0.31060000037541613, 0.4186999940429814], "tokens_processed": [8, 8], "throughput_tok_s": [3202.4338466328563, 3200.5120828517915], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5340999996406026, 0.3671000013127923, 0.43870000081369653], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.661, "gpu_power_peak_watts": 20.661, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1534.47265625, "cpu_memory_peak_mb": 1534.47265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.2914972}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.534600000421051, 3.7674000050174072], "ttft_ms": [0.3995000006398186, 0.5042000047978945], "tokens_processed": [8, 8], "throughput_tok_s": [2263.3395572475015, 2123.4803815219075], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.705400001606904, 0.40550000267103314, 0.42859999666688964], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.661, "gpu_power_peak_watts": 20.661, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1534.5, "cpu_memory_peak_mb": 1534.5, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.4183805}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.1821000011404976, 3.035699999600183], "ttft_ms": [0.3662000017357059, 0.3571999986888841], "tokens_processed": [8, 8], "throughput_tok_s": [2514.063039229665, 2635.3065194365845], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9132999985013157, 0.393300004361663, 0.3620999996201135], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.661, "gpu_power_peak_watts": 20.661, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1534.53515625, "cpu_memory_peak_mb": 1534.53515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.5427945}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.594899993913714, 3.4850000010919757], "ttft_ms": [0.4122000027564354, 0.4177000009804033], "tokens_processed": [8, 8], "throughput_tok_s": [2225.37484034167, 2295.5523665691], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9245000037481077, 0.4669999980251305, 0.4707000043708831], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.661, "gpu_power_peak_watts": 20.661, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1535.73828125, "cpu_memory_peak_mb": 1535.73828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.6666083}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.3261999962851405, 4.3804999950225465], "ttft_ms": [0.5164999965927564, 0.6406000029528514], "tokens_processed": [8, 8], "throughput_tok_s": [1849.1979119942469, 1826.2755413971468], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5281000048853457, 0.34389999927952886, 0.5189000003156252], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.718, "gpu_power_peak_watts": 20.718, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1535.75, "cpu_memory_peak_mb": 1535.75, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.791475}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.830100002232939, 3.418000000237953], "ttft_ms": [0.5335000023478642, 0.4065999964950606], "tokens_processed": [8, 8], "throughput_tok_s": [2088.7183090091694, 2340.5500290939317], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9324999991804361, 0.5441000030259602, 0.41590000182623044], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.718, "gpu_power_peak_watts": 20.718, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1535.75, "cpu_memory_peak_mb": 1535.75, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593348.9181855}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.9605000017909333, 3.1725000008009374], "ttft_ms": [0.3555000002961606, 0.4369000016595237], "tokens_processed": [8, 8], "throughput_tok_s": [2702.246240554115, 2521.6706061403625], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5799999998998828, 0.38180000410648063, 0.34279999817954376], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.718, "gpu_power_peak_watts": 20.718, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1535.7578125, "cpu_memory_peak_mb": 1535.7578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.0450568}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.5635999956866726, 3.3957999985432252], "ttft_ms": [0.41710000368766487, 0.39770000148564577], "tokens_processed": [8, 8], "throughput_tok_s": [2244.9208692566726, 2355.851346790725], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.953200004005339, 0.4117999997106381, 0.3971999976783991], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.718, "gpu_power_peak_watts": 20.718, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1535.76171875, "cpu_memory_peak_mb": 1535.76171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.1687734}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.44190000416711, 3.199200000381097], "ttft_ms": [0.39489999471697956, 0.41030000284081325], "tokens_processed": [8, 8], "throughput_tok_s": [1801.0310886095829, 2500.625155991191], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8683999951463193, 0.553100006072782, 0.4084999964106828], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1546.8984375, "cpu_memory_peak_mb": 1546.8984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.295394}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.059099999722093, 3.8264999966486357], "ttft_ms": [0.5022999976063147, 0.5250000031082891], "tokens_processed": [8, 8], "throughput_tok_s": [1970.8802445240867, 2090.6833939648873], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.832600002468098, 0.5396000051405281, 0.5077000023447908], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1546.9375, "cpu_memory_peak_mb": 1546.9375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.4202049}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.1586999941500835, 3.9894000001368113], "ttft_ms": [0.5174999969312921, 0.509900004544761], "tokens_processed": [8, 8], "throughput_tok_s": [1923.6780751805506, 2005.3140822493735], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0955999969155528, 0.5323000004864298, 0.5072999992989935], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1546.9609375, "cpu_memory_peak_mb": 1546.9609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.5465145}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.148499996517785, 3.9543999955640174], "ttft_ms": [0.48460000107297674, 0.4760000010719523], "tokens_processed": [8, 8], "throughput_tok_s": [1928.4078598807114, 2023.0629195261663], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9353999994345941, 0.5010000022593886, 0.5145000031916425], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1546.98828125, "cpu_memory_peak_mb": 1546.98828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.6696367}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.525900003500283, 4.434200003743172], "ttft_ms": [0.4549000004772097, 0.7534999967901967], "tokens_processed": [8, 8], "throughput_tok_s": [1767.6042320450927, 1804.1585840166715], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7989000005181879, 0.5256999938865192, 0.502099996083416], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1547.02734375, "cpu_memory_peak_mb": 1547.02734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.7961109}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.6574999980512075, 4.610700001649093], "ttft_ms": [0.6083999978727661, 0.5883000048925169], "tokens_processed": [8, 8], "throughput_tok_s": [1414.0521436598665, 1735.0944535837657], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.367299999401439, 0.7065000027068891, 0.7075999965309165], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1550.12109375, "cpu_memory_peak_mb": 1550.12109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593349.9201112}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.9474999939557165, 3.5329000020283274], "ttft_ms": [0.4647999958251603, 0.49530000251252204], "tokens_processed": [8, 8], "throughput_tok_s": [2026.5991164659506, 2264.42865504458], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7467000032193027, 0.5345000026864, 0.473000000056345], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1550.16796875, "cpu_memory_peak_mb": 1550.16796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.0448468}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.09409999701893, 4.581100001814775], "ttft_ms": [0.4827999946428463, 0.5438000007416122], "tokens_processed": [8, 8], "throughput_tok_s": [1954.0314124777376, 1746.305471792986], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7497999977204017, 0.5466000002343208, 0.4712000009021722], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.811, "gpu_power_peak_watts": 20.811, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1550.203125, "cpu_memory_peak_mb": 1550.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.1712494}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.099300004076213, 4.574600003252272], "ttft_ms": [0.3702000030898489, 0.555899998289533], "tokens_processed": [8, 8], "throughput_tok_s": [1568.8427810885933, 1748.7867779286648], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7453000071109273, 0.5103000003146008, 0.39319999632425606], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1550.21875, "cpu_memory_peak_mb": 1550.21875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.2973168}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.6381000027176924, 3.5974999991594814], "ttft_ms": [0.43870000081369653, 0.5175999976927415], "tokens_processed": [8, 8], "throughput_tok_s": [2198.949999731709, 2223.766505036586], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7251999995787628, 0.5208000002312474, 0.4220000046188943], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1550.234375, "cpu_memory_peak_mb": 1550.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.4215589}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.649200001556892, 8.12430000223685], "ttft_ms": [0.9696999986772425, 1.13489999785088], "tokens_processed": [32, 32], "throughput_tok_s": [2746.969748628512, 3938.8008802222334], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.5488000028417446, 0.9552999981679022, 0.8104000007733703], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1573.953125, "cpu_memory_peak_mb": 1573.953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.5450387}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.428000000596512, 6.24460000108229], "ttft_ms": [0.4709999993792735, 0.7442999994964339], "tokens_processed": [32, 32], "throughput_tok_s": [4978.220285785693, 5124.427504476489], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.422399996954482, 0.8063000059337355, 0.5338999981177039], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1574.12890625, "cpu_memory_peak_mb": 1574.12890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.670736}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.982899998954963, 8.50660000287462], "ttft_ms": [1.003699995635543, 1.3480999987223186], "tokens_processed": [32, 32], "throughput_tok_s": [3562.3239715150744, 3761.7849656956123], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3327999986358918, 0.9114000058616512, 0.9279000005335547], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1574.12890625, "cpu_memory_peak_mb": 1574.12890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.795958}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.777799997711554, 8.080200001131743], "ttft_ms": [0.8854000043356791, 1.1085000005550683], "tokens_processed": [32, 32], "throughput_tok_s": [4114.273960427794, 3960.298011870741], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4245999991544522, 0.9518000006210059, 0.9872000009636395], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1574.1328125, "cpu_memory_peak_mb": 1574.1328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593350.9216747}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.32040000386769, 8.03299999824958], "ttft_ms": [0.7641000047442503, 1.2626999960048124], "tokens_processed": [32, 32], "throughput_tok_s": [3845.9689420130026, 3983.5677837635903], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4102999994065613, 0.9904999969876371, 0.8199000003514811], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1574.2578125, "cpu_memory_peak_mb": 1574.2578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.0495622}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.43219999782741, 12.98059999680845], "ttft_ms": [0.9813000069698319, 1.5874000018811785], "tokens_processed": [32, 32], "throughput_tok_s": [2217.264173502114, 2465.2173249208713], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.502199997252319, 1.4876999994157813, 1.1890999958268367], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.785, "gpu_power_peak_watts": 6.785, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1580.56640625, "cpu_memory_peak_mb": 1580.56640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.1704788}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.09999999991851, 13.00700000138022], "ttft_ms": [0.9452999947825447, 1.716600003419444], "tokens_processed": [32, 32], "throughput_tok_s": [2644.6280991913645, 2460.2137308068245], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7851000011432916, 1.5504999973927625, 1.018900002236478], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1603.35546875, "cpu_memory_peak_mb": 1603.35546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.2964246}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.641699994972441, 8.360799998627044], "ttft_ms": [0.7632000051671639, 1.4563999939127825], "tokens_processed": [32, 32], "throughput_tok_s": [3318.916790263755, 3827.384939868772], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.762000007147435, 0.9793000062927604, 0.7548000066890381], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1603.37890625, "cpu_memory_peak_mb": 1603.37890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.4213202}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.83879999734927, 12.977500002307352], "ttft_ms": [1.2376999948173761, 1.5891999937593937], "tokens_processed": [32, 32], "throughput_tok_s": [2702.9766536443612, 2465.806202605318], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.979500004381407, 1.4037000000826083, 1.0653999997884966], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1603.41015625, "cpu_memory_peak_mb": 1603.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.5459187}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.144699998316355, 9.402100004081149], "ttft_ms": [1.145499998528976, 1.572800007124897], "tokens_processed": [32, 32], "throughput_tok_s": [2634.8942340639314, 3403.494962413698], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9436000002315268, 1.036599998769816, 0.9400999988429248], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 630.01953125, "gpu_memory_peak_mb": 630.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1603.41015625, "cpu_memory_peak_mb": 1603.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 118.30999999801861, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.6703515}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [42.83250000298722, 39.245399995706975], "ttft_ms": [5.246699998679105, 4.682500002672896], "tokens_processed": [8, 8], "throughput_tok_s": [186.7740617391482, 203.84554625192033], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [98.52200000023004, 4.818099994736258, 5.701699999917764], "resource_metrics": {"samples": 2, "duration_s": 0.11420226097106934, "gpu_memory_mean_mb": 642.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1681.376953125, "cpu_memory_peak_mb": 1743.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593351.893377}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.516399995889515, 37.73080000246409], "ttft_ms": [4.608499999449123, 4.833599996345583], "tokens_processed": [8, 8], "throughput_tok_s": [219.0796464301115, 212.02836938197817], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.803100004210137, 6.390900001861155, 4.820900001504924], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1747.05859375, "cpu_memory_peak_mb": 1747.05859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.0105186}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.87249999732012, 35.79200000240235], "ttft_ms": [4.418600001372397, 4.342599997471552], "tokens_processed": [8, 8], "throughput_tok_s": [211.23506503574058, 223.51363431669208], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.2217000047676265, 5.814700001792517, 4.442999997991137], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.181, "gpu_power_peak_watts": 4.181, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1747.0625, "cpu_memory_peak_mb": 1747.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.1353714}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.30560000136029, 31.796799994481262], "ttft_ms": [4.058099999383558, 3.643699994427152], "tokens_processed": [8, 8], "throughput_tok_s": [255.54533373110192, 251.59764508971034], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.108699999051169, 6.0540000049513765, 4.01179999607848], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 2.174, "gpu_power_peak_watts": 2.174, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1747.2421875, "cpu_memory_peak_mb": 1747.2421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.2641816}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.178800001740456, 33.28009999677306], "ttft_ms": [4.323900000599679, 4.401099999086], "tokens_processed": [8, 8], "throughput_tok_s": [199.10997838794233, 240.3838931005527], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.400699996040203, 6.757700000889599, 4.189600003883243], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 2.174, "gpu_power_peak_watts": 2.174, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1747.2578125, "cpu_memory_peak_mb": 1747.2578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.3858647}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.337199997215066, 23.67209999647457], "ttft_ms": [2.608800001326017, 3.221099999791477], "tokens_processed": [8, 8], "throughput_tok_s": [303.75286669979846, 337.95058322630524], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.71339999785414, 3.0687000034959055, 2.766899997368455], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 2.174, "gpu_power_peak_watts": 2.174, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1748.49609375, "cpu_memory_peak_mb": 1748.49609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.511666}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.46469999611145, 22.98960000189254], "ttft_ms": [3.2719000009819865, 3.120699999271892], "tokens_processed": [8, 8], "throughput_tok_s": [340.93766386639305, 347.9834359598004], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.8480000002891757, 6.233000000065658, 3.8600000043516047], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 2.174, "gpu_power_peak_watts": 2.174, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1748.50390625, "cpu_memory_peak_mb": 1748.50390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.6357894}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.118800004653167, 22.321500000543892], "ttft_ms": [3.6899999977322295, 2.786600001854822], "tokens_processed": [8, 8], "throughput_tok_s": [274.73659624440586, 358.39885311493714], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.019699998025317, 3.7496000004466623, 3.293399997346569], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 3.514, "gpu_power_peak_watts": 3.514, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1748.55078125, "cpu_memory_peak_mb": 1748.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.7614658}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.70820000214735, 25.78969999740366], "ttft_ms": [3.256700001657009, 2.6104999997187406], "tokens_processed": [8, 8], "throughput_tok_s": [337.4359925795888, 310.2013594886869], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9572000023326837, 5.183499997656327, 3.125199997157324], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 3.514, "gpu_power_peak_watts": 3.514, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1748.55859375, "cpu_memory_peak_mb": 1748.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593352.884443}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.07920000021113, 22.47460000216961], "ttft_ms": [2.8176999985589646, 2.4052000007941388], "tokens_processed": [8, 8], "throughput_tok_s": [346.6324655935568, 355.9573918658268], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.3597999936318956, 2.9930999953649007, 3.1394000034197234], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 3.514, "gpu_power_peak_watts": 3.514, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1748.5625, "cpu_memory_peak_mb": 1748.5625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.0109642}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.007099997717887, 25.260399997932836], "ttft_ms": [3.5673000020324253, 3.275700000813231], "tokens_processed": [8, 8], "throughput_tok_s": [296.21840185269815, 316.70123991127116], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.496999998285901, 5.458500003442168, 3.0985999983386137], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 49.0, "gpu_power_mean_watts": 3.514, "gpu_power_peak_watts": 3.514, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1754.3828125, "cpu_memory_peak_mb": 1754.3828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.1352797}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.37859999574721, 26.368300001195166], "ttft_ms": [3.1006999997771345, 3.3822999976109713], "tokens_processed": [8, 8], "throughput_tok_s": [315.22621426479753, 303.39460638863306], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.6421000040718354, 3.1263999990187585, 3.0790999953751452], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 9.281, "gpu_power_peak_watts": 9.281, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1760.1328125, "cpu_memory_peak_mb": 1760.1328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.260525}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.800299998081755, 26.986199998646043], "ttft_ms": [3.5014000022783875, 4.601300002832431], "tokens_processed": [8, 8], "throughput_tok_s": [310.07391389227246, 296.4478140828044], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.780999999435153, 3.362299998116214, 2.8395999979693443], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 9.281, "gpu_power_peak_watts": 9.281, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1760.1875, "cpu_memory_peak_mb": 1760.1875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.3880384}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.53749999788124, 28.71929999673739], "ttft_ms": [3.2725000055506825, 3.302999997686129], "tokens_processed": [8, 8], "throughput_tok_s": [313.26480668286763, 278.55832143920037], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.181900003459305, 3.4856999991461635, 3.3613000050536357], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 9.281, "gpu_power_peak_watts": 9.281, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1760.1875, "cpu_memory_peak_mb": 1760.1875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.5108624}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.26439999928698, 24.825699998473283], "ttft_ms": [2.9473000031430274, 3.113300001132302], "tokens_processed": [8, 8], "throughput_tok_s": [316.6510979966189, 322.2467040402478], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.674600004160311, 3.372999999555759, 3.22059999598423], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 9.281, "gpu_power_peak_watts": 9.281, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1760.2109375, "cpu_memory_peak_mb": 1760.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.6360557}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.87239999655867, 32.685899997886736], "ttft_ms": [4.556800005957484, 4.010499993455596], "tokens_processed": [8, 8], "throughput_tok_s": [211.23562279461908, 244.75385412417066], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.906399997707922, 4.107399996428285, 5.1192999962950125], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.299, "gpu_power_peak_watts": 12.299, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1761.75, "cpu_memory_peak_mb": 1761.75, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.7592537}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.26429999672109, 33.15129999828059], "ttft_ms": [4.709599998022895, 3.9850999964983203], "tokens_processed": [8, 8], "throughput_tok_s": [233.479160548021, 241.31783671876892], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7921999974059872, 4.194000001007225, 4.6252000029198825], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.299, "gpu_power_peak_watts": 12.299, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1763.26171875, "cpu_memory_peak_mb": 1763.26171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593353.8850713}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.67939999891678, 29.988800000865012], "ttft_ms": [3.688600001623854, 4.035099998873193], "tokens_processed": [8, 8], "throughput_tok_s": [260.7612926029343, 266.7662593958159], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.35299999662675, 3.6217999950167723, 3.614599998400081], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.299, "gpu_power_peak_watts": 12.299, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1763.26953125, "cpu_memory_peak_mb": 1763.26953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.0075684}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.79090000008, 31.728299996757414], "ttft_ms": [3.90289999631932, 4.9663999961921945], "tokens_processed": [8, 8], "throughput_tok_s": [268.5383791687568, 252.14083328818714], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.613600001903251, 5.247599998256192, 3.4988999977940693], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.299, "gpu_power_peak_watts": 12.299, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1763.27734375, "cpu_memory_peak_mb": 1763.27734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.1363637}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.753000000317115, 31.53800000291085], "ttft_ms": [5.469499999890104, 3.8961999962339178], "tokens_processed": [8, 8], "throughput_tok_s": [217.66930590512268, 253.6622486924227], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.581000001053326, 3.3933000013348646, 3.787200002989266], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 652.01953125, "gpu_memory_peak_mb": 652.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 37.0, "gpu_power_mean_watts": 12.584, "gpu_power_peak_watts": 12.584, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1763.3046875, "cpu_memory_peak_mb": 1763.3046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.2600543}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.686700000369456, 52.710099997057114], "ttft_ms": [6.250200000067707, 6.917600003362168], "tokens_processed": [32, 32], "throughput_tok_s": [631.3293230722606, 607.0942760834566], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.30779999907827, 6.005999995977618, 5.967200006125495], "resource_metrics": {"samples": 2, "duration_s": 0.11779069900512695, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 37.0, "gpu_power_mean_watts": 12.584, "gpu_power_peak_watts": 12.584, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1779.85546875, "cpu_memory_peak_mb": 1787.6640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.485665}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.681100005225744, 53.39079999976093], "ttft_ms": [6.038899999111891, 6.124100000306498], "tokens_processed": [32, 32], "throughput_tok_s": [619.1818671964086, 599.3541958566511], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.522399999084882, 8.694900003320072, 6.236400004127063], "resource_metrics": {"samples": 2, "duration_s": 0.1102910041809082, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 37.0, "gpu_power_mean_watts": 12.875, "gpu_power_peak_watts": 13.166, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1787.671875, "cpu_memory_peak_mb": 1787.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.7071612}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [56.99989999993704, 54.025400000682566], "ttft_ms": [6.8405000056372955, 7.156100000429433], "tokens_processed": [32, 32], "throughput_tok_s": [561.4044936927143, 592.3139856363064], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.7132000005804, 7.069499995850492, 7.430699995893519], "resource_metrics": {"samples": 2, "duration_s": 0.1199955940246582, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 37.0, "gpu_power_mean_watts": 13.166, "gpu_power_peak_watts": 13.166, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1787.671875, "cpu_memory_peak_mb": 1787.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593354.9386003}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.64090000430588, 55.49880000035046], "ttft_ms": [6.139799996162765, 9.060299998964183], "tokens_processed": [32, 32], "throughput_tok_s": [644.6297306701591, 576.5890433630625], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.678799996734597, 6.0398999994504265, 5.8110000027227215], "resource_metrics": {"samples": 2, "duration_s": 0.11083745956420898, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 37.0, "gpu_power_mean_watts": 13.166, "gpu_power_peak_watts": 13.166, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1787.671875, "cpu_memory_peak_mb": 1787.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.1598716}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.543499996885657, 27.686800000083167], "ttft_ms": [3.2691999949747697, 3.7005999984103255], "tokens_processed": [32, 32], "throughput_tok_s": [1121.095871336433, 1155.785428431739], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.676899999845773, 3.6041999992448837, 3.791900002397597], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 13.855, "gpu_power_peak_watts": 13.855, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1787.68359375, "cpu_memory_peak_mb": 1787.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.29265}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.258100003702566, 46.88429999805521], "ttft_ms": [4.934500000672415, 6.024100002832711], "tokens_processed": [32, 32], "throughput_tok_s": [636.71328597067, 682.5312524944892], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.038499996473547, 3.488899994408712, 4.331899996032007], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 700.01953125, "gpu_memory_peak_mb": 700.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 13.855, "gpu_power_peak_watts": 13.855, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1794.8125, "cpu_memory_peak_mb": 1794.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.4153798}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [46.80060000100639, 48.62989999674028], "ttft_ms": [4.523199997493066, 7.223000000522006], "tokens_processed": [32, 32], "throughput_tok_s": [683.7519176957535, 658.031375802644], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.5156999951577745, 5.313800000294577, 4.495900000620168], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 700.01953125, "gpu_memory_peak_mb": 700.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 13.855, "gpu_power_peak_watts": 13.855, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1817.53515625, "cpu_memory_peak_mb": 1817.53515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.541429}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.12000000331318, 35.855200003425125], "ttft_ms": [4.821499998797663, 3.4083999998983927], "tokens_processed": [32, 32], "throughput_tok_s": [651.4657980016608, 892.4786362073883], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.778399998031091, 4.7591999973519705, 4.568800002743956], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 700.01953125, "gpu_memory_peak_mb": 700.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 13.855, "gpu_power_peak_watts": 13.855, "gpu_temperature_mean_c": 45.0, "gpu_temperature_peak_c": 45, "cpu_memory_mean_mb": 1817.53515625, "cpu_memory_peak_mb": 1817.53515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.6820247}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.9217000004719, 37.08389999519568], "ttft_ms": [5.202699998335447, 5.0142000036430545], "tokens_processed": [32, 32], "throughput_tok_s": [890.8264363763301, 862.908162414031], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.793100001639687, 9.447299999010283, 6.254399995668791], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 700.01953125, "gpu_memory_peak_mb": 700.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 20.348, "gpu_power_peak_watts": 20.348, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1817.55078125, "cpu_memory_peak_mb": 1817.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.8038838}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.7503999985056, 36.2706999949296], "ttft_ms": [4.601299995556474, 4.7301000013248995], "tokens_processed": [32, 32], "throughput_tok_s": [895.0948800947019, 882.2548228866106], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.22369999816874, 5.0791000030585565, 4.755100002512336], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 700.01953125, "gpu_memory_peak_mb": 700.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 77.0, "gpu_power_mean_watts": 20.348, "gpu_power_peak_watts": 20.348, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1817.55078125, "cpu_memory_peak_mb": 1817.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1864.2247000025236, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765593355.926307}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.756700000551064, 6.437599993660115], "ttft_ms": [0.752799998736009, 0.7008999964455143], "tokens_processed": [8, 8], "throughput_tok_s": [1184.0099455869781, 1242.699143761428], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [27.2530000002007, 1.0572999963187613, 0.7896000024629757], "resource_metrics": {"samples": 3, "duration_s": 0.21852922439575195, "gpu_memory_mean_mb": 780.0325520833334, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 51.666666666666664, "gpu_power_mean_watts": 20.232666666666663, "gpu_power_peak_watts": 20.348, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1941.5833333333333, "cpu_memory_peak_mb": 2088.3515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32"}, "started_at": 1765593356.2520857}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.223300006240606, 6.610300006286707], "ttft_ms": [0.5776000034529716, 0.7918999981484376], "tokens_processed": [8, 8], "throughput_tok_s": [1285.4916189124344, 1210.2325147711333], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3991999949212186, 0.825099996291101, 0.7293999951798469], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 20.002, "gpu_power_peak_watts": 20.002, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2095.79296875, "cpu_memory_peak_mb": 2095.79296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593356.3811555}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.862199996248819, 9.300999998231418], "ttft_ms": [0.7287000044016168, 0.7048999977996573], "tokens_processed": [8, 8], "throughput_tok_s": [1165.806884726932, 860.1225676294158], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9788000024855137, 0.9501999957137741, 0.9449999997741543], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 20.002, "gpu_power_peak_watts": 20.002, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2095.8046875, "cpu_memory_peak_mb": 2095.8046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593356.5058975}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.627900006307755, 9.937800001353025], "ttft_ms": [0.9891999943647534, 1.175000004877802], "tokens_processed": [8, 8], "throughput_tok_s": [830.918475966594, 805.0071443288057], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.355899999907706, 1.2692999953287654, 1.7458000002079643], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 20.002, "gpu_power_peak_watts": 20.002, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2095.91015625, "cpu_memory_peak_mb": 2095.91015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593356.6305275}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.7493000024114735, 6.348000002617482], "ttft_ms": [1.006899998174049, 0.676800002111122], "tokens_processed": [8, 8], "throughput_tok_s": [1185.3081056023086, 1260.2394449750072], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1654000045382418, 0.6028999996487983, 1.0254000007989816], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.763, "gpu_power_peak_watts": 29.763, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2095.94921875, "cpu_memory_peak_mb": 2095.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593356.7594724}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.499200000893325, 7.53390000318177], "ttft_ms": [0.8355999962077476, 1.010599997243844], "tokens_processed": [8, 8], "throughput_tok_s": [1066.7804564549576, 1061.8670272530007], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6684000001987442, 0.8135999960359186, 0.7943000018713064], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.763, "gpu_power_peak_watts": 29.763, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2096.5390625, "cpu_memory_peak_mb": 2096.5390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593356.881017}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.371399995463435, 6.98279999778606], "ttft_ms": [0.9035999974003062, 0.8040999964578077], "tokens_processed": [8, 8], "throughput_tok_s": [1085.2755249916465, 1145.6722235401915], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0979999972041696, 0.6819999980507419, 1.0148000001208857], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.763, "gpu_power_peak_watts": 29.763, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2096.5625, "cpu_memory_peak_mb": 2096.5625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.00835}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.855399999243673, 7.367600002908148], "ttft_ms": [0.7809999951859936, 0.8299000037368387], "tokens_processed": [8, 8], "throughput_tok_s": [1018.4077196285675, 1085.8352783596056], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2476999982027337, 1.075799998943694, 1.0346999988541938], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 29.763, "gpu_power_peak_watts": 29.763, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2096.5625, "cpu_memory_peak_mb": 2096.5625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.1309218}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.929099996341392, 7.128099998226389], "ttft_ms": [0.7722000009380281, 0.7863999999244697], "tokens_processed": [8, 8], "throughput_tok_s": [1154.551096711557, 1122.3187107350566], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.86569999641506, 0.8443999977316707, 0.7731000005151145], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.396, "gpu_power_peak_watts": 30.396, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2096.578125, "cpu_memory_peak_mb": 2096.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.2592301}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.884200003696606, 8.180100005120039], "ttft_ms": [0.774600004660897, 0.800499998149462], "tokens_processed": [8, 8], "throughput_tok_s": [1162.0812869620645, 977.9831536280349], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4508999956888147, 1.1720999973476864, 1.0715000025811605], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 931.0390625, "gpu_memory_peak_mb": 931.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.396, "gpu_power_peak_watts": 30.396, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2096.5859375, "cpu_memory_peak_mb": 2096.5859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.3816955}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.955199999036267, 9.757200001331512], "ttft_ms": [0.9357000017189421, 1.3736999972024933], "tokens_processed": [8, 8], "throughput_tok_s": [893.3357156580464, 819.9073503575087], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4824000027147122, 1.3591000024462119, 0.9145000003627501], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.396, "gpu_power_peak_watts": 30.396, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.1328125, "cpu_memory_peak_mb": 2098.1328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.5092459}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.23270000203047, 10.271999999531545], "ttft_ms": [1.0041999994427897, 1.0813999979291111], "tokens_processed": [8, 8], "throughput_tok_s": [866.4854266076692, 778.816199412465], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0652999955927953, 1.0456000018166378, 0.9355999936815351], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.396, "gpu_power_peak_watts": 30.396, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.1328125, "cpu_memory_peak_mb": 2098.1328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.639625}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.37740000517806, 9.814400000323076], "ttft_ms": [1.0820000024978071, 1.4127999966149218], "tokens_processed": [8, 8], "throughput_tok_s": [853.1149354386631, 815.1287903220423], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.669700002821628, 1.392500002111774, 1.6600000017206185], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.946, "gpu_power_peak_watts": 31.946, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.1328125, "cpu_memory_peak_mb": 2098.1328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.757376}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.446099997148849, 9.296399999584537], "ttft_ms": [1.0517000046093017, 1.3107999984640628], "tokens_processed": [8, 8], "throughput_tok_s": [846.9103653798572, 860.5481692222286], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4026999997440726, 1.4418999999179505, 0.9036999981617555], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.946, "gpu_power_peak_watts": 31.946, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.140625, "cpu_memory_peak_mb": 2098.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593357.8848658}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.156799995456822, 9.681499999715015], "ttft_ms": [1.1062000048696063, 1.4044999988982454], "tokens_processed": [8, 8], "throughput_tok_s": [787.6496537864714, 826.3182358348901], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.3383000007015653, 1.7810999997891486, 1.4624000032199547], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.946, "gpu_power_peak_watts": 31.946, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.140625, "cpu_memory_peak_mb": 2098.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.0117917}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.629999997036066, 10.948999995889608], "ttft_ms": [1.3330000001587905, 1.6334999963873997], "tokens_processed": [8, 8], "throughput_tok_s": [752.5870180837835, 730.660334551402], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.17980000161333, 1.825200000894256, 1.2279000002308749], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 935.0390625, "gpu_memory_peak_mb": 935.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.946, "gpu_power_peak_watts": 31.946, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2099.6875, "cpu_memory_peak_mb": 2099.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.135831}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.251599993556738, 10.821200005011633], "ttft_ms": [1.2195999952382408, 1.3139000002411194], "tokens_processed": [8, 8], "throughput_tok_s": [711.0099900975172, 739.2895424070299], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4986999997054227, 1.0504999954719096, 1.6811999958008528], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 935.0390625, "gpu_memory_peak_mb": 935.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.28, "gpu_power_peak_watts": 30.28, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2099.6875, "cpu_memory_peak_mb": 2099.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.264173}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.027700006205123, 15.767900003993418], "ttft_ms": [1.2300000016693957, 2.088100001856219], "tokens_processed": [8, 8], "throughput_tok_s": [614.0761605033568, 507.35988926704886], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.1564999988186173, 1.7947000014828518, 1.4351999998325482], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 935.0390625, "gpu_memory_peak_mb": 935.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.28, "gpu_power_peak_watts": 30.28, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2099.6875, "cpu_memory_peak_mb": 2099.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.3863058}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.020599999232218, 11.580899998079985], "ttft_ms": [1.8313999971724115, 1.5130000028875656], "tokens_processed": [8, 8], "throughput_tok_s": [665.5241835275259, 690.7925982718383], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4399000030825846, 1.2758000011672266, 1.2014000021736138], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 935.0390625, "gpu_memory_peak_mb": 935.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.28, "gpu_power_peak_watts": 30.28, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2099.6875, "cpu_memory_peak_mb": 2099.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.5137515}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.475700001232326, 14.975700003560632], "ttft_ms": [1.7944000064744614, 1.72049999673618], "tokens_processed": [8, 8], "throughput_tok_s": [641.246583294707, 534.1987351574828], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.9668999995919876, 1.3005000000703149, 1.2777000010828488], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 935.0390625, "gpu_memory_peak_mb": 935.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.28, "gpu_power_peak_watts": 30.28, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2099.6875, "cpu_memory_peak_mb": 2099.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.6383073}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.11980000580661, 27.028899996366818], "ttft_ms": [3.776300000026822, 2.5085999950533733], "tokens_processed": [32, 32], "throughput_tok_s": [966.189409186943, 1183.9179546448945], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.291199995961506, 4.407899999932852, 4.890800002613105], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 927.0390625, "gpu_memory_peak_mb": 927.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 29.732, "gpu_power_peak_watts": 29.732, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2095.19140625, "cpu_memory_peak_mb": 2095.19140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.7634537}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.531300004629884, 21.87009999761358], "ttft_ms": [2.4783000044408254, 3.3133999968413264], "tokens_processed": [32, 32], "throughput_tok_s": [1420.2465012415805, 1463.184896433568], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.44479999714531, 2.071399998385459, 2.8234999990672804], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 29.732, "gpu_power_peak_watts": 29.732, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.21875, "cpu_memory_peak_mb": 2112.21875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593358.88531}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.78669999941485, 21.830199999385513], "ttft_ms": [2.4153000049409457, 2.7387000009184703], "tokens_processed": [32, 32], "throughput_tok_s": [1404.3279632777778, 1465.8592225861764], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.567000003589783, 2.365000000281725, 3.431800003454555], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 29.732, "gpu_power_peak_watts": 29.732, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.234375, "cpu_memory_peak_mb": 2112.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.012946}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.341900003084447, 21.258599997963756], "ttft_ms": [2.9760000034002587, 3.24209999962477], "tokens_processed": [32, 32], "throughput_tok_s": [1432.2864212793982, 1505.2731601829425], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.620699997758493, 2.5242999981855974, 2.795300002617296], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 29.732, "gpu_power_peak_watts": 29.732, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.234375, "cpu_memory_peak_mb": 2112.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.137791}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.489499995368533, 22.011400003975723], "ttft_ms": [2.477900001395028, 2.5976999968406744], "tokens_processed": [32, 32], "throughput_tok_s": [1489.099327899519, 1453.7921256358127], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.927200003294274, 2.2729000047547743, 3.598499999498017], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 30.121, "gpu_power_peak_watts": 30.121, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2112.30859375, "cpu_memory_peak_mb": 2112.30859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.2604046}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.95290000399109, 26.967299992975313], "ttft_ms": [2.9151000053389, 2.8022999977110885], "tokens_processed": [32, 32], "throughput_tok_s": [1068.3439665520248, 1186.6223169666837], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.233000000065658, 3.3097999985329807, 4.0430000008200295], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 30.121, "gpu_power_peak_watts": 30.121, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.453125, "cpu_memory_peak_mb": 2118.453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.3847551}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.30019999702927, 29.393299999355804], "ttft_ms": [5.643200005579274, 3.204500004358124], "tokens_processed": [32, 32], "throughput_tok_s": [960.9551895440491, 1088.68347550977], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.103600004280452, 3.2451000006403774, 4.038300001411699], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 30.121, "gpu_power_peak_watts": 30.121, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.5, "cpu_memory_peak_mb": 2118.5, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.5089054}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.02999999828171, 28.55999999883352], "ttft_ms": [3.3409999959985726, 3.7530999979935586], "tokens_processed": [32, 32], "throughput_tok_s": [1031.260070956236, 1120.4481793174714], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.432400001154747, 3.966100004618056, 4.085499997017905], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 30.121, "gpu_power_peak_watts": 30.121, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.51953125, "cpu_memory_peak_mb": 2118.51953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.6333315}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.500299999606796, 30.499300002702512], "ttft_ms": [3.963799994380679, 4.010800003015902], "tokens_processed": [32, 32], "throughput_tok_s": [1015.8633409967347, 1049.20440787705], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.0900000025867485, 3.2359000033466145, 5.488399998284876], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 31.003, "gpu_power_peak_watts": 31.003, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.52734375, "cpu_memory_peak_mb": 2118.52734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.755932}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.45860000565881, 29.828999999153893], "ttft_ms": [3.303799996501766, 3.4518000029493123], "tokens_processed": [32, 32], "throughput_tok_s": [1050.6063966845102, 1072.7815213687247], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.910100000619423, 4.063399996084627, 4.346499998064246], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 31.003, "gpu_power_peak_watts": 31.003, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.52734375, "cpu_memory_peak_mb": 2118.52734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 193.12100000388455, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593359.8782756}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.606900002225302, 6.3926000002538785], "ttft_ms": [0.6638000049861148, 0.7525000037276186], "tokens_processed": [8, 8], "throughput_tok_s": [1210.8553175173654, 1251.446985527373], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0284999991417862, 0.9226999973179772, 0.647900000330992], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 31.003, "gpu_power_peak_watts": 31.003, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2102.3515625, "cpu_memory_peak_mb": 2102.3515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16"}, "started_at": 1765593360.0006688}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.559399998281151, 7.95510000170907], "ttft_ms": [1.2213999943924136, 1.1876999997184612], "tokens_processed": [8, 8], "throughput_tok_s": [636.9731038978662, 1005.6441777326852], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8932000020868145, 1.5916000047582202, 1.6418000013800338], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 31.003, "gpu_power_peak_watts": 31.003, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.52734375, "cpu_memory_peak_mb": 2107.52734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.1251507}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.2789000003249384, 10.259700000460725], "ttft_ms": [0.7355999987339601, 0.8885999995982274], "tokens_processed": [8, 8], "throughput_tok_s": [1274.1085221274416, 779.7498951860921], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4569999984814785, 0.8968000038294122, 0.7270000060088933], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 34.023, "gpu_power_peak_watts": 34.023, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.55078125, "cpu_memory_peak_mb": 2107.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.2503517}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.05739999486832, 6.821999995736405], "ttft_ms": [0.9151999984169379, 1.071499995305203], "tokens_processed": [8, 8], "throughput_tok_s": [992.8761145152436, 1172.6766351509555], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.675699997576885, 0.8834000036586076, 1.0953000019071624], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 34.023, "gpu_power_peak_watts": 34.023, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.55859375, "cpu_memory_peak_mb": 2107.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.3751245}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.452200002968311, 7.173700003477279], "ttft_ms": [0.7324000034714118, 0.6773999994038604], "tokens_processed": [8, 8], "throughput_tok_s": [1239.8871696971, 1115.1846322152016], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4767000029678456, 1.0782000026665628, 0.8814999964670278], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 34.023, "gpu_power_peak_watts": 34.023, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.57421875, "cpu_memory_peak_mb": 2107.57421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.498321}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.15910000144504, 9.92720000067493], "ttft_ms": [0.7962000017869286, 0.8111999995890073], "tokens_processed": [8, 8], "throughput_tok_s": [1117.4588982393354, 805.8667095914353], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4869000006001443, 1.1675999994622543, 1.0813999979291111], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 34.023, "gpu_power_peak_watts": 34.023, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2108.1484375, "cpu_memory_peak_mb": 2108.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.6224473}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.276399999682326, 8.012999998754822], "ttft_ms": [0.7781000022077933, 0.7945000033942051], "tokens_processed": [8, 8], "throughput_tok_s": [1099.4447804339047, 998.3776364960886], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3560000006691553, 1.1668000006466173, 1.0219000032520853], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 30.399, "gpu_power_peak_watts": 30.399, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2108.1484375, "cpu_memory_peak_mb": 2108.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.746601}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.674799999629613, 9.276500000851229], "ttft_ms": [1.441000000340864, 1.8016999965766445], "tokens_processed": [8, 8], "throughput_tok_s": [826.8904783877981, 862.394221879578], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.490199996624142, 0.9950000021490268, 1.8742000029305927], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 30.399, "gpu_power_peak_watts": 30.399, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2108.1484375, "cpu_memory_peak_mb": 2108.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.8724022}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.397699999273755, 7.165399998484645], "ttft_ms": [0.7407999946735799, 1.0753000024124049], "tokens_processed": [8, 8], "throughput_tok_s": [1081.4171973431437, 1116.4764007161998], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6627000004518777, 0.7733999955235049, 0.603599997702986], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 30.399, "gpu_power_peak_watts": 30.399, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2108.1484375, "cpu_memory_peak_mb": 2108.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593360.9959395}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.768600001232699, 7.309300002816599], "ttft_ms": [1.4456999997491948, 0.7721000001765788], "tokens_processed": [8, 8], "throughput_tok_s": [679.7749944056253, 1094.4960525518509], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.80460000410676, 1.7286000002059154, 1.302099997701589], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 937.0390625, "gpu_memory_peak_mb": 937.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 30.399, "gpu_power_peak_watts": 30.399, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2108.1484375, "cpu_memory_peak_mb": 2108.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.1223686}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.432100003527012, 9.661399999458808], "ttft_ms": [1.244400002178736, 1.0858000023290515], "tokens_processed": [8, 8], "throughput_tok_s": [766.8638143130589, 828.0373445306194], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.329499999177642, 1.5316999997594394, 1.0392000040155835], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.931, "gpu_power_peak_watts": 29.931, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2109.68359375, "cpu_memory_peak_mb": 2109.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.2462757}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.558900001342408, 9.788399998797104], "ttft_ms": [1.2960999956703745, 1.1467000003904104], "tokens_processed": [8, 8], "throughput_tok_s": [757.6546798419265, 817.2939398658739], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.127799998561386, 1.1213999969186261, 1.2272000021766871], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.931, "gpu_power_peak_watts": 29.931, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2109.68359375, "cpu_memory_peak_mb": 2109.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.372772}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.214600002451334, 9.95130000228528], "ttft_ms": [1.0122000021510758, 1.3370000015129335], "tokens_processed": [8, 8], "throughput_tok_s": [868.1874414376948, 803.9150661886215], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.571400003740564, 1.5079000004334375, 1.2318000008235686], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.931, "gpu_power_peak_watts": 29.931, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2109.68359375, "cpu_memory_peak_mb": 2109.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.4979556}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [16.011499996238854, 17.93580000230577], "ttft_ms": [0.8317999963765033, 2.0948000019416213], "tokens_processed": [8, 8], "throughput_tok_s": [499.64088323262797, 446.03530363694654], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.401900004770141, 1.9726999962585978, 1.6362000023946166], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.931, "gpu_power_peak_watts": 29.931, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2109.68359375, "cpu_memory_peak_mb": 2109.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.620088}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.778399999253452, 10.882100003072992], "ttft_ms": [1.774299998942297, 1.4899000016157515], "tokens_processed": [8, 8], "throughput_tok_s": [742.2251911743957, 735.1522222494632], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6285999954561703, 0.8728999964660034, 1.126700000895653], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.756, "gpu_power_peak_watts": 29.756, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2109.68359375, "cpu_memory_peak_mb": 2109.68359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.7430785}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.250599997059908, 11.686300000292249], "ttft_ms": [1.4240000018617138, 1.5046000044094399], "tokens_processed": [8, 8], "throughput_tok_s": [653.0292395409177, 684.5622651994162], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0951000042259693, 1.6897999958018772, 1.6300999996019527], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 941.0390625, "gpu_memory_peak_mb": 941.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.756, "gpu_power_peak_watts": 29.756, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.21484375, "cpu_memory_peak_mb": 2111.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.867829}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.210000000777654, 11.81180000276072], "ttft_ms": [1.2805000005755574, 1.3249000039650127], "tokens_processed": [8, 8], "throughput_tok_s": [655.2006551589255, 677.2888127237335], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.7819000024464913, 1.6109999996842816, 1.203799998620525], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 941.0390625, "gpu_memory_peak_mb": 941.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.756, "gpu_power_peak_watts": 29.756, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.21484375, "cpu_memory_peak_mb": 2111.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593361.9907267}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.701599996944424, 11.85809999878984], "ttft_ms": [1.7745000004651956, 1.6351000012946315], "tokens_processed": [8, 8], "throughput_tok_s": [629.8419098321889, 674.6443360079969], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.015199999732431, 1.2520999953267165, 1.0665000008884817], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 941.0390625, "gpu_memory_peak_mb": 941.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.756, "gpu_power_peak_watts": 29.756, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.21875, "cpu_memory_peak_mb": 2111.21875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.116559}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.46350000333041, 18.90850000199862], "ttft_ms": [1.2643000009120442, 2.730200001678895], "tokens_processed": [8, 8], "throughput_tok_s": [553.1164654584227, 423.0901445992228], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.869300005841069, 1.739100000122562, 1.2948000003234483], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 941.0390625, "gpu_memory_peak_mb": 941.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 29.889, "gpu_power_peak_watts": 29.889, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.3203125, "cpu_memory_peak_mb": 2111.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.243752}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [19.77410000108648, 17.639599995163735], "ttft_ms": [1.6002000047592446, 2.4905000027501956], "tokens_processed": [8, 8], "throughput_tok_s": [404.5696137655035, 453.5250233675008], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.6049000007333234, 1.2597999957506545, 1.2885999967693351], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 941.0390625, "gpu_memory_peak_mb": 941.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 29.889, "gpu_power_peak_watts": 29.889, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.3203125, "cpu_memory_peak_mb": 2111.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.3639455}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.23639999626903, 21.465199999511242], "ttft_ms": [2.7894000013475306, 2.9030000005150214], "tokens_processed": [32, 32], "throughput_tok_s": [1506.8467351162155, 1490.7850847291725], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.330500003765337, 2.6836000033654273, 2.7618000021902844], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 933.0390625, "gpu_memory_peak_mb": 933.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 29.889, "gpu_power_peak_watts": 29.889, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2106.6171875, "cpu_memory_peak_mb": 2106.6171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.4898536}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.52440000203205, 26.57510000426555], "ttft_ms": [4.274600003554951, 4.667299996071961], "tokens_processed": [32, 32], "throughput_tok_s": [900.7893165871781, 1204.1346973243262], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.982299997413065, 2.3354000004474074, 3.252500006055925], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 29.889, "gpu_power_peak_watts": 29.889, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2123.6015625, "cpu_memory_peak_mb": 2123.6015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.6131935}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.876300001167692, 23.303999994823243], "ttft_ms": [2.970699999423232, 3.196699995896779], "tokens_processed": [32, 32], "throughput_tok_s": [1462.7702124350067, 1373.1548235113494], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.55369999690447, 3.171100004692562, 2.345100001548417], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 30.018, "gpu_power_peak_watts": 30.018, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2123.6015625, "cpu_memory_peak_mb": 2123.6015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.7411535}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.007300002267584, 22.541499994986225], "ttft_ms": [2.8918999960296787, 3.083900002820883], "tokens_processed": [32, 32], "throughput_tok_s": [1332.9279009708494, 1419.603842118651], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.7670000023790635, 2.610299998195842, 2.7735000039683655], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 30.018, "gpu_power_peak_watts": 30.018, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2123.6015625, "cpu_memory_peak_mb": 2123.6015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593362.8763766}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.016700002306607, 21.970699999656063], "ttft_ms": [2.8894999995827675, 2.4637000024085864], "tokens_processed": [32, 32], "throughput_tok_s": [1066.0732191593675, 1456.4852280765265], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.592099998262711, 2.791600003547501, 2.729899999394547], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 949.0390625, "gpu_memory_peak_mb": 949.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 30.018, "gpu_power_peak_watts": 30.018, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2123.609375, "cpu_memory_peak_mb": 2123.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.001202}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.572899999446236, 31.05900000082329], "ttft_ms": [6.565800002135802, 4.033900004287716], "tokens_processed": [32, 32], "throughput_tok_s": [1013.5274238527742, 1030.2971763144908], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.222500000149012, 4.039599996758625, 7.01169999956619], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 30.018, "gpu_power_peak_watts": 30.018, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2129.84375, "cpu_memory_peak_mb": 2129.84375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.1269507}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.347900003311224, 28.45339999475982], "ttft_ms": [3.8561999972444028, 4.3484999987413175], "tokens_processed": [32, 32], "throughput_tok_s": [1020.8020312882169, 1124.6459124706835], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.826900000101887, 4.245399999490473, 3.524600004311651], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.4, "gpu_power_peak_watts": 21.4, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2129.84375, "cpu_memory_peak_mb": 2129.84375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.2509863}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.853300000482704, 32.83250000094995], "ttft_ms": [6.455499998992309, 3.8555999999516644], "tokens_processed": [32, 32], "throughput_tok_s": [823.6108644465834, 974.6440264699349], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.061899999622256, 4.128199994738679, 4.191200001514517], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.4, "gpu_power_peak_watts": 21.4, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2129.84375, "cpu_memory_peak_mb": 2129.84375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.3762407}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.638899999961723, 29.778500000247732], "ttft_ms": [4.511300001468044, 3.311499996925704], "tokens_processed": [32, 32], "throughput_tok_s": [1011.4131654399715, 1074.6008025835347], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.452499997976702, 3.695999999763444, 3.678600005514454], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.4, "gpu_power_peak_watts": 21.4, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2129.85546875, "cpu_memory_peak_mb": 2129.85546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.5009227}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.326500000432134, 29.747099993983284], "ttft_ms": [4.299100000935141, 3.4080999976140447], "tokens_processed": [32, 32], "throughput_tok_s": [1021.4993695292668, 1075.7351138925271], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.208099999639671, 3.4527999960118905, 4.20999999914784], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.4, "gpu_power_peak_watts": 21.4, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2129.85546875, "cpu_memory_peak_mb": 2129.85546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.481000000378117, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.6232007}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.015199997113086, 6.797900001402013], "ttft_ms": [0.9034999966388568, 0.8239000017056242], "tokens_processed": [8, 8], "throughput_tok_s": [1140.3808876856228, 1176.8340220288712], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.869699997769203, 0.6520999959320761, 0.9926999991876073], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 939.0390625, "gpu_memory_peak_mb": 939.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.288, "gpu_power_peak_watts": 21.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.62890625, "cpu_memory_peak_mb": 2112.62890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8"}, "started_at": 1765593363.7469616}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.515000002167653, 11.627400002907962], "ttft_ms": [1.206699998874683, 1.6830000022309832], "tokens_processed": [8, 8], "throughput_tok_s": [639.2329203846875, 688.0299979358442], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.17759999941336, 1.3774999970337376, 1.4236999995773658], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.288, "gpu_power_peak_watts": 21.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.62109375, "cpu_memory_peak_mb": 2118.62109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593363.888431}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.53550000465475, 6.691899994621053], "ttft_ms": [1.2390999982017092, 0.6656999976257794], "tokens_processed": [8, 8], "throughput_tok_s": [937.2620228032663, 1195.4751276065688], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0011000015074387, 1.5599999969708733, 1.4837999988230877], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.288, "gpu_power_peak_watts": 21.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.62890625, "cpu_memory_peak_mb": 2118.62890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.0125842}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.805800000322051, 7.104599993908778], "ttft_ms": [1.3571000017691404, 0.9587000022293068], "tokens_processed": [8, 8], "throughput_tok_s": [815.8436843232838, 1126.0310231200779], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.868599996669218, 1.27269999939017, 1.38639999931911], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.288, "gpu_power_peak_watts": 21.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.63671875, "cpu_memory_peak_mb": 2118.63671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.1361034}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.652800000272691, 11.386400001356378], "ttft_ms": [1.4720999970450066, 1.3771000012638979], "tokens_processed": [8, 8], "throughput_tok_s": [585.9603890659948, 702.592566486951], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2565000035683624, 1.3063999940641224, 1.4762999999220483], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 20.143, "gpu_power_peak_watts": 20.143, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.63671875, "cpu_memory_peak_mb": 2118.63671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.2594562}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.982099997519981, 12.782699996023439], "ttft_ms": [1.8190000046161003, 1.633899999433197], "tokens_processed": [8, 8], "throughput_tok_s": [572.1601191107893, 625.8458700031069], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.6112999985343777, 2.02240000362508, 1.810599998862017], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 20.143, "gpu_power_peak_watts": 20.143, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2119.2109375, "cpu_memory_peak_mb": 2119.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.391623}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.108699997246731, 12.516599999798927], "ttft_ms": [1.6235000002779998, 1.5343999984906986], "tokens_processed": [8, 8], "throughput_tok_s": [610.2817214277746, 639.1512072071102], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.300100000866223, 2.1718999996664934, 1.7208999997819774], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 20.143, "gpu_power_peak_watts": 20.143, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2119.2109375, "cpu_memory_peak_mb": 2119.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.5096161}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.315700001840014, 12.632300000404939], "ttft_ms": [1.542500001960434, 1.5010000061010942], "tokens_processed": [8, 8], "throughput_tok_s": [600.7945507104042, 633.297182598858], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9945000021834858, 1.5493999962927774, 1.5788999953656457], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 20.143, "gpu_power_peak_watts": 20.143, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2119.2109375, "cpu_memory_peak_mb": 2119.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.6348596}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.678799998364411, 12.680800005909987], "ttft_ms": [1.8056999979307875, 1.5552999975625426], "tokens_processed": [8, 8], "throughput_tok_s": [545.0036788355588, 630.8750233637891], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2728999974788167, 2.118799995514564, 1.70829999842681], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 14.005, "gpu_power_peak_watts": 14.005, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2119.30859375, "cpu_memory_peak_mb": 2119.30859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.7567449}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [16.772100003436208, 16.567300001042895], "ttft_ms": [1.702000001387205, 1.8951999954879284], "tokens_processed": [8, 8], "throughput_tok_s": [476.9826079239325, 482.87892411536023], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2470999974757433, 2.237499997136183, 1.6636000000289641], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 943.0390625, "gpu_memory_peak_mb": 943.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 14.005, "gpu_power_peak_watts": 14.005, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2119.30859375, "cpu_memory_peak_mb": 2119.30859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593364.880471}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.291099998052232, 19.700600001669955], "ttft_ms": [2.7153999981237575, 2.587299997685477], "tokens_processed": [8, 8], "throughput_tok_s": [375.7438554481385, 406.07900263554745], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.994599996076431, 2.307499999005813, 2.9657000050065108], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 945.0390625, "gpu_memory_peak_mb": 945.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 14.005, "gpu_power_peak_watts": 14.005, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2120.84765625, "cpu_memory_peak_mb": 2120.84765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.0073984}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [19.606999994721264, 21.39539999916451], "ttft_ms": [2.333399999770336, 3.5256999981356785], "tokens_processed": [8, 8], "throughput_tok_s": [408.0175448642738, 373.9121493551137], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.6826000030268915, 2.6214000026811846, 2.3122999991755933], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 945.0390625, "gpu_memory_peak_mb": 945.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 14.005, "gpu_power_peak_watts": 14.005, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2120.85546875, "cpu_memory_peak_mb": 2120.85546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.129764}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [19.458299997495487, 20.106600000872277], "ttft_ms": [2.365499996813014, 2.5591999947209843], "tokens_processed": [8, 8], "throughput_tok_s": [411.13560799400227, 397.87930329607883], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.898700000718236, 2.4274000024888664, 2.340700004424434], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 945.0390625, "gpu_memory_peak_mb": 945.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.675, "gpu_power_peak_watts": 13.675, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2120.8671875, "cpu_memory_peak_mb": 2120.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.2534554}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.179000003437977, 20.20000000629807], "ttft_ms": [2.5894999998854473, 2.459299998008646], "tokens_processed": [8, 8], "throughput_tok_s": [345.13999736025784, 396.0396038369166], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.5449999995762482, 2.824800001690164, 2.8921999983140267], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 945.0390625, "gpu_memory_peak_mb": 945.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.675, "gpu_power_peak_watts": 13.675, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2120.8671875, "cpu_memory_peak_mb": 2120.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.379581}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.144299996900372, 20.018299997900613], "ttft_ms": [2.671600006578956, 2.6877999989665113], "tokens_processed": [8, 8], "throughput_tok_s": [397.13467339301786, 399.63433462576677], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.9763000056846067, 2.402300000539981, 2.327400005015079], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 945.0390625, "gpu_memory_peak_mb": 945.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.675, "gpu_power_peak_watts": 13.675, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2120.88671875, "cpu_memory_peak_mb": 2120.88671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.5034523}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.16310000262456, 26.014700000814628], "ttft_ms": [3.215299999283161, 2.9037999993306585], "tokens_processed": [8, 8], "throughput_tok_s": [331.08334605787553, 307.51844148690884], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.087699999217875, 3.1460000027436763, 3.3063000009860843], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 947.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.675, "gpu_power_peak_watts": 13.675, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2122.41796875, "cpu_memory_peak_mb": 2122.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.6279356}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.8243999958504, 24.310199994943105], "ttft_ms": [3.2176000022445805, 3.286400002252776], "tokens_processed": [8, 8], "throughput_tok_s": [322.2635794354452, 329.07997472929566], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.465399997367058, 2.8377999988151714, 2.8333000009297393], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 947.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.282, "gpu_power_peak_watts": 13.282, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2122.41796875, "cpu_memory_peak_mb": 2122.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.7683756}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.222400003869552, 27.12510000128532], "ttft_ms": [3.3559000003151596, 3.4742999996524304], "tokens_processed": [8, 8], "throughput_tok_s": [283.4627812979451, 294.92978826330307], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.053199998452328, 3.4072999987984076, 3.6994999973103404], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 947.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.282, "gpu_power_peak_watts": 13.282, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2122.41796875, "cpu_memory_peak_mb": 2122.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593365.8902981}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.688699999591336, 25.954700002330355], "ttft_ms": [3.1369999996968545, 2.9145000007702038], "tokens_processed": [8, 8], "throughput_tok_s": [311.4209749861716, 308.22933801129335], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.6810000019613653, 2.9451000009430572, 3.3354999977746047], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 947.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.282, "gpu_power_peak_watts": 13.282, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2122.41796875, "cpu_memory_peak_mb": 2122.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.0157409}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.067100003070664, 25.103699997998774], "ttft_ms": [2.919400001701433, 3.181599997333251], "tokens_processed": [8, 8], "throughput_tok_s": [319.1434190241399, 318.6781231705982], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.5722999964491464, 2.9694999975617975, 2.9361999986576848], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 947.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 13.282, "gpu_power_peak_watts": 13.282, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2122.41796875, "cpu_memory_peak_mb": 2122.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.1388106}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [45.65980000188574, 50.58460000145715], "ttft_ms": [6.34379999974044, 5.288700005621649], "tokens_processed": [32, 32], "throughput_tok_s": [700.8353080538768, 632.6035987054994], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.962600000610109, 5.520099999557715, 5.7373000017832965], "resource_metrics": {"samples": 2, "duration_s": 0.1120150089263916, "gpu_memory_mean_mb": 947.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 12.92, "gpu_power_peak_watts": 12.92, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2126.345703125, "cpu_memory_peak_mb": 2134.75390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.3698695}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [46.1219000062556, 52.60650000127498], "ttft_ms": [5.731599994760472, 7.504400004108902], "tokens_processed": [32, 32], "throughput_tok_s": [693.8135678638516, 608.2898500988365], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.5197999976808205, 7.222899999760557, 5.702900001779199], "resource_metrics": {"samples": 2, "duration_s": 0.11446166038513184, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 12.92, "gpu_power_peak_watts": 12.92, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2134.76171875, "cpu_memory_peak_mb": 2134.76171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.5950205}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.42590000241762, 23.042599998007063], "ttft_ms": [3.279700002167374, 2.769400001852773], "tokens_processed": [32, 32], "throughput_tok_s": [1426.9215503748007, 1388.7321744407166], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.827899996598717, 3.4482999981264584, 2.4868000036804006], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 13.12, "gpu_power_peak_watts": 13.12, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2134.765625, "cpu_memory_peak_mb": 2134.765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.7139962}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.108799998648465, 36.81700000015553], "ttft_ms": [3.793500000028871, 3.3968999996432103], "tokens_processed": [32, 32], "throughput_tok_s": [938.1743128244904, 869.1637015472422], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.064599994511809, 2.9448999994201586, 3.4834000034607016], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 13.12, "gpu_power_peak_watts": 13.12, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2134.7734375, "cpu_memory_peak_mb": 2134.7734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.8407543}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.33069999824511, 22.39170000393642], "ttft_ms": [3.7548999971477315, 2.5624999980209395], "tokens_processed": [32, 32], "throughput_tok_s": [1315.2108242799447, 1429.1009612657572], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.38590000703698, 3.4011999960057437, 2.5858000008156523], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 13.12, "gpu_power_peak_watts": 13.12, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2134.7734375, "cpu_memory_peak_mb": 2134.7734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593366.9610143}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.87939999770606, 30.261100000643637], "ttft_ms": [3.958799999963958, 3.727100003743544], "tokens_processed": [32, 32], "throughput_tok_s": [1003.7830072806457, 1057.46321182374], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.61240000044927, 4.5003999985056, 3.3027000026777387], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 13.12, "gpu_power_peak_watts": 13.12, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 2140.9140625, "cpu_memory_peak_mb": 2140.9140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593367.0848603}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.557699996279553, 29.264299999340437], "ttft_ms": [3.444800000579562, 3.2256999984383583], "tokens_processed": [32, 32], "throughput_tok_s": [1047.19923305406, 1093.4825025960374], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.407900000340305, 4.475499998079613, 4.042499997012783], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.858, "gpu_power_peak_watts": 18.858, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2140.9453125, "cpu_memory_peak_mb": 2140.9453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593367.209471}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.383199999050703, 29.990600000019185], "ttft_ms": [3.510000002279412, 3.260900004534051], "tokens_processed": [32, 32], "throughput_tok_s": [1053.2136180849882, 1067.0009936439928], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.071399999200366, 3.97549999615876, 3.64879999688128], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.858, "gpu_power_peak_watts": 18.858, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2140.9453125, "cpu_memory_peak_mb": 2140.9453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593367.3481886}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.76189999835333, 29.883899995184038], "ttft_ms": [6.640400002652314, 3.2426999969175085], "tokens_processed": [32, 32], "throughput_tok_s": [947.8139560143456, 1070.8107042640681], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.24239999888232, 4.066200002853293, 6.975999996939208], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.858, "gpu_power_peak_watts": 18.858, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2140.96484375, "cpu_memory_peak_mb": 2140.96484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593367.471829}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.15689999505412, 41.33509999519447], "ttft_ms": [6.637400001636706, 4.703999999037478], "tokens_processed": [32, 32], "throughput_tok_s": [796.8742608105019, 774.1604593606947], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.107299996074289, 5.3556999991997145, 6.914899997354951], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.858, "gpu_power_peak_watts": 18.858, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2140.96484375, "cpu_memory_peak_mb": 2140.96484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.110400005360134, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765593367.5976524}
