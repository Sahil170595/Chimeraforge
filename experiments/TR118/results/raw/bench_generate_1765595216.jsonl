{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [692.5945999973919, 95.15749999991385], "ttft_ms": [646.2221999972826, 11.864400003105402], "tokens_processed": [8, 8], "throughput_tok_s": [11.55076866038246, 84.07114520670723], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2035.7910000020638, 259.3208999969647, 1.7434999972465448], "resource_metrics": {"samples": 24, "duration_s": 3.107100486755371, "gpu_memory_mean_mb": 517.4361979166666, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.59475, "gpu_power_peak_watts": 30.643, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1116.9161783854167, "cpu_memory_peak_mb": 1384.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595224.3984113}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [54.60589999711374, 49.43449999700533], "ttft_ms": [6.860000001324806, 5.6324000033782795], "tokens_processed": [8, 8], "throughput_tok_s": [146.50431547548615, 161.83030071073094], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.031100001768209, 5.991699996229727, 6.224999997357372], "resource_metrics": {"samples": 2, "duration_s": 0.1119232177734375, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.7495, "gpu_power_peak_watts": 30.856, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1384.296875, "cpu_memory_peak_mb": 1384.296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595224.6210985}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [46.86719999881461, 49.1420999969705], "ttft_ms": [6.800600000133272, 7.375200002570637], "tokens_processed": [8, 8], "throughput_tok_s": [170.69507033068626, 162.7932058355907], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.563400001847185, 11.232199998630676, 5.993500002659857], "resource_metrics": {"samples": 2, "duration_s": 0.10856246948242188, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.856, "gpu_power_peak_watts": 30.856, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1384.34375, "cpu_memory_peak_mb": 1384.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595224.8362849}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [43.990099999064114, 44.33219999918947], "ttft_ms": [5.428200005553663, 5.336599999282043], "tokens_processed": [8, 8], "throughput_tok_s": [181.85910011957688, 180.4557409771287], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.167499999515712, 7.361999996646773, 5.471700002090074], "resource_metrics": {"samples": 2, "duration_s": 0.10936498641967773, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.213500000000003, "gpu_power_peak_watts": 31.571, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1384.373046875, "cpu_memory_peak_mb": 1384.375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595225.0549176}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.02210000081686, 46.26290000305744], "ttft_ms": [6.385599997884128, 5.502500003785826], "tokens_processed": [8, 8], "throughput_tok_s": [150.88048190993476, 172.92474097973306], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.6828999950084835, 11.648400002741255, 6.194699999468867], "resource_metrics": {"samples": 2, "duration_s": 0.10839509963989258, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.571, "gpu_power_peak_watts": 31.571, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1384.39453125, "cpu_memory_peak_mb": 1384.39453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595225.271787}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [729.2344000015873, 47.981600000639446], "ttft_ms": [685.9875999944052, 5.895700000110082], "tokens_processed": [8, 8], "throughput_tok_s": [10.970409514392884, 166.730580053466], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [815.951500000665, 6.0031000029994175, 5.686699994839728], "resource_metrics": {"samples": 15, "duration_s": 1.5152628421783447, "gpu_memory_mean_mb": 563.8861979166667, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.253533333333333, "gpu_power_peak_watts": 31.725, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1403.89296875, "cpu_memory_peak_mb": 1425.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595226.912987}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [47.56660000566626, 48.37060000136262], "ttft_ms": [5.967500001133885, 5.858700002136175], "tokens_processed": [8, 8], "throughput_tok_s": [168.18523920244496, 165.38972019728175], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.152100002509542, 5.918700000620447, 6.456099996285047], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.62, "gpu_power_peak_watts": 30.62, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1427.5859375, "cpu_memory_peak_mb": 1427.5859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595227.0375075}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [46.47149999800604, 48.10620000353083], "ttft_ms": [5.8902000018861145, 5.524599997443147], "tokens_processed": [8, 8], "throughput_tok_s": [172.14852114399702, 166.29873071273198], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.051699994713999, 6.171599998197053, 6.199299998115748], "resource_metrics": {"samples": 2, "duration_s": 0.11490941047668457, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.851, "gpu_power_peak_watts": 30.851, "gpu_temperature_mean_c": 52.0, "gpu_temperature_peak_c": 52, "cpu_memory_mean_mb": 1427.671875, "cpu_memory_peak_mb": 1427.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595227.2646098}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [72.60369999858085, 65.05409999954281], "ttft_ms": [10.153099996387027, 7.969700003741309], "tokens_processed": [8, 8], "throughput_tok_s": [110.18722186550234, 122.97457039688847], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.053800004650839, 6.064000001060776, 9.754599996085744], "resource_metrics": {"samples": 2, "duration_s": 0.10721421241760254, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.851, "gpu_power_peak_watts": 30.851, "gpu_temperature_mean_c": 52.0, "gpu_temperature_peak_c": 52, "cpu_memory_mean_mb": 1427.755859375, "cpu_memory_peak_mb": 1427.7578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595227.4824312}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [64.47050000133459, 68.6513000036939], "ttft_ms": [6.622799999604467, 8.075500001723412], "tokens_processed": [8, 8], "throughput_tok_s": [124.08776106644736, 116.53093240141914], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.743100002291612, 5.964600000879727, 8.004399998753797], "resource_metrics": {"samples": 2, "duration_s": 0.11075305938720703, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.695, "gpu_power_peak_watts": 31.695, "gpu_temperature_mean_c": 52.0, "gpu_temperature_peak_c": 52, "cpu_memory_mean_mb": 1427.771484375, "cpu_memory_peak_mb": 1427.7734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595227.7005556}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [660.3318999987096, 83.49940000334755], "ttft_ms": [618.839000002481, 5.749800002377015], "tokens_processed": [8, 8], "throughput_tok_s": [12.115119684533845, 95.80907167811115], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [759.453400001803, 5.747900002461392, 5.895499998587184], "resource_metrics": {"samples": 14, "duration_s": 1.4766042232513428, "gpu_memory_mean_mb": 565.8766741071429, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 3.5, "gpu_power_mean_watts": 31.47942857142857, "gpu_power_peak_watts": 32.117, "gpu_temperature_mean_c": 51.142857142857146, "gpu_temperature_peak_c": 52, "cpu_memory_mean_mb": 1431.1810825892858, "cpu_memory_peak_mb": 1434.30859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595229.2856102}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [56.13689999881899, 71.16599999426398], "ttft_ms": [6.34390000050189, 7.4430000022402965], "tokens_processed": [8, 8], "throughput_tok_s": [142.50875983832924, 112.41323104635366], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [13.92709999345243, 8.086699999694247, 6.530900005600415], "resource_metrics": {"samples": 2, "duration_s": 0.11114501953125, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.635, "gpu_power_peak_watts": 30.635, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1439.51171875, "cpu_memory_peak_mb": 1439.51171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595229.5065076}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [75.44819999748142, 73.94519999797922], "ttft_ms": [12.036999993142672, 9.457100000872742], "tokens_processed": [8, 8], "throughput_tok_s": [106.033013382255, 108.18822587833456], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.214400000928435, 7.73250000202097, 6.92939999862574], "resource_metrics": {"samples": 2, "duration_s": 0.11479926109313965, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 27.999, "gpu_power_peak_watts": 27.999, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1439.59375, "cpu_memory_peak_mb": 1439.59375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595229.7379847}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [74.32859999971697, 70.04800000140676], "ttft_ms": [10.193600006459747, 7.952699997986201], "tokens_processed": [8, 8], "throughput_tok_s": [107.6301719665171, 114.20740063726784], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [13.270399998873472, 10.083499997563194, 6.846700001915451], "resource_metrics": {"samples": 2, "duration_s": 0.12022852897644043, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 27.999, "gpu_power_peak_watts": 27.999, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 1439.61328125, "cpu_memory_peak_mb": 1439.61328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595229.9683263}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [73.4121999994386, 69.1341999990982], "ttft_ms": [9.023699996760115, 8.56750000093598], "tokens_processed": [8, 8], "throughput_tok_s": [108.97371281695928, 115.71696786980038], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.907500003057066, 10.440300000482239, 15.664899998228066], "resource_metrics": {"samples": 2, "duration_s": 0.11295747756958008, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 19.636, "gpu_power_peak_watts": 19.636, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1439.640625, "cpu_memory_peak_mb": 1439.640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595230.1906886}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [974.2745000039577, 113.93239999597427], "ttft_ms": [895.9703000000445, 14.794700000493322], "tokens_processed": [8, 8], "throughput_tok_s": [8.211238208500276, 70.21707609321558], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1012.0176000054926, 17.655400006333366, 14.104100002441555], "resource_metrics": {"samples": 20, "duration_s": 2.0685794353485107, "gpu_memory_mean_mb": 579.81953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 3.2, "gpu_power_mean_watts": 10.3494, "gpu_power_peak_watts": 19.636, "gpu_temperature_mean_c": 49.6, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1433.6435546875, "cpu_memory_peak_mb": 1443.46484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595232.370633}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [101.57389999949373, 123.99440000444883], "ttft_ms": [9.066799997526687, 13.921199999458622], "tokens_processed": [8, 8], "throughput_tok_s": [78.7603902187459, 64.5190427931662], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.860100006335415, 12.945000002218876, 9.198100000503473], "resource_metrics": {"samples": 3, "duration_s": 0.2186415195465088, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 2.564, "gpu_power_peak_watts": 2.727, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1450.21875, "cpu_memory_peak_mb": 1450.21875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595232.6968184}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [98.22649999841815, 84.49010000185808], "ttft_ms": [13.786100003926549, 10.128499998245388], "tokens_processed": [8, 8], "throughput_tok_s": [81.4444167320309, 94.68564955922724], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.851199996774085, 14.689699994050898, 13.833400000294205], "resource_metrics": {"samples": 3, "duration_s": 0.21741628646850586, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 48.0, "gpu_power_mean_watts": 2.727, "gpu_power_peak_watts": 2.727, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1450.23828125, "cpu_memory_peak_mb": 1450.23828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595233.0274303}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [81.39759999903617, 83.08589999796823], "ttft_ms": [8.884299997589551, 9.468399999605026], "tokens_processed": [8, 8], "throughput_tok_s": [98.28299606001562, 96.28589207309099], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.886399999028072, 9.386399993672967, 9.286599997722078], "resource_metrics": {"samples": 2, "duration_s": 0.11087775230407715, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 48.0, "gpu_power_mean_watts": 3.925, "gpu_power_peak_watts": 3.925, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1450.25, "cpu_memory_peak_mb": 1450.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595233.252725}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [77.63230000273325, 68.73849999828963], "ttft_ms": [9.03660000039963, 7.237599995278288], "tokens_processed": [8, 8], "throughput_tok_s": [103.04989031264486, 116.38310408576065], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [16.400799999246374, 9.182499998132698, 9.219200001098216], "resource_metrics": {"samples": 2, "duration_s": 0.10931015014648438, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 48.0, "gpu_power_mean_watts": 3.925, "gpu_power_peak_watts": 3.925, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1450.28125, "cpu_memory_peak_mb": 1450.28125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595233.4737625}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [118.80160000146134, 107.42459999892162], "ttft_ms": [14.883600000757724, 12.476500000047963], "tokens_processed": [32, 32], "throughput_tok_s": [269.3566416580785, 297.883352605653], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [19.41619999706745, 16.29490000050282, 14.37889999942854], "resource_metrics": {"samples": 3, "duration_s": 0.5023183822631836, "gpu_memory_mean_mb": 626.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 40.0, "gpu_power_mean_watts": 6.180666666666667, "gpu_power_peak_watts": 8.306, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1447.2877604166667, "cpu_memory_peak_mb": 1453.2265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595234.083827}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [88.71199999703094, 91.68990000034682], "ttft_ms": [11.107599995739292, 9.932099994330201], "tokens_processed": [32, 32], "throughput_tok_s": [360.71782849074526, 349.0024528315437], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.720199999923352, 12.398999999277294, 9.477799998421688], "resource_metrics": {"samples": 2, "duration_s": 0.11930060386657715, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 40.0, "gpu_power_mean_watts": 8.306, "gpu_power_peak_watts": 8.306, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1453.2265625, "cpu_memory_peak_mb": 1453.2265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595234.313151}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [106.81130000011763, 88.574499997776], "ttft_ms": [14.315499996882863, 10.025600000517443], "tokens_processed": [32, 32], "throughput_tok_s": [299.5937695727396, 361.27779440813646], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [15.728100006526802, 13.783900001726579, 13.374499998462852], "resource_metrics": {"samples": 3, "duration_s": 0.21117377281188965, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 36.333333333333336, "gpu_power_mean_watts": 9.782333333333332, "gpu_power_peak_watts": 12.735, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1453.23046875, "cpu_memory_peak_mb": 1453.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595234.641774}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [93.10199999890756, 98.59200000209967], "ttft_ms": [10.758399999758694, 13.907099993957672], "tokens_processed": [32, 32], "throughput_tok_s": [343.7090502929634, 324.56994481619716], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.739199995237868, 12.5837000014144, 9.78179999947315], "resource_metrics": {"samples": 3, "duration_s": 0.21835589408874512, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 29.0, "gpu_power_mean_watts": 12.735, "gpu_power_peak_watts": 12.735, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1453.2278645833333, "cpu_memory_peak_mb": 1453.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595234.968036}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [114.30169999948703, 99.40859999915119], "ttft_ms": [16.058799999882467, 12.923000002047047], "tokens_processed": [32, 32], "throughput_tok_s": [279.96084047869465, 321.9037387134839], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.891300000774208, 12.471100002585445, 14.375700004165992], "resource_metrics": {"samples": 3, "duration_s": 0.2144620418548584, "gpu_memory_mean_mb": 644.01953125, "gpu_memory_peak_mb": 644.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 29.0, "gpu_power_mean_watts": 13.774, "gpu_power_peak_watts": 13.774, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1453.23828125, "cpu_memory_peak_mb": 1453.23828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595235.2942026}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [105.50969999894733, 112.69599999650382], "ttft_ms": [11.71279999834951, 12.971999996807426], "tokens_processed": [32, 32], "throughput_tok_s": [303.2896501489367, 283.9497409046704], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [13.532400000258349, 11.423399999330286, 11.35680000152206], "resource_metrics": {"samples": 3, "duration_s": 0.21048998832702637, "gpu_memory_mean_mb": 661.3528645833334, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 33.333333333333336, "gpu_power_mean_watts": 13.623666666666665, "gpu_power_peak_watts": 13.774, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1480.9986979166667, "cpu_memory_peak_mb": 1494.87890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595235.6150713}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [108.94059999554884, 103.30419999809237], "ttft_ms": [11.754099999961909, 12.127900001360103], "tokens_processed": [32, 32], "throughput_tok_s": [293.7380554293576, 309.76475303609067], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [16.758999998273794, 12.121899999328889, 11.629000000539236], "resource_metrics": {"samples": 3, "duration_s": 0.21913671493530273, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 42.0, "gpu_power_mean_watts": 13.323, "gpu_power_peak_watts": 13.323, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1494.8815104166667, "cpu_memory_peak_mb": 1494.8828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595235.9421804}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [69.8839999968186, 71.0687999962829], "ttft_ms": [7.796700003382284, 8.967000001575798], "tokens_processed": [32, 32], "throughput_tok_s": [457.9016656381542, 450.26790942964686], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.301699999719858, 8.695199998328462, 7.58059999498073], "resource_metrics": {"samples": 2, "duration_s": 0.1190803050994873, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 42.0, "gpu_power_mean_watts": 13.607, "gpu_power_peak_watts": 13.607, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1494.890625, "cpu_memory_peak_mb": 1494.890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595236.169538}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [70.65590000274824, 107.02789999777451], "ttft_ms": [7.426799995300826, 9.055599999555852], "tokens_processed": [32, 32], "throughput_tok_s": [452.89919169885775, 298.98746028526574], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [14.181500002450775, 11.933699999644887, 8.591600002546329], "resource_metrics": {"samples": 2, "duration_s": 0.12132000923156738, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 42.0, "gpu_power_mean_watts": 13.607, "gpu_power_peak_watts": 13.607, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1494.921875, "cpu_memory_peak_mb": 1494.921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595236.4013784}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [73.99209999857703, 71.45109999692068], "ttft_ms": [7.528100002673455, 8.201399999961723], "tokens_processed": [32, 32], "throughput_tok_s": [432.47860245371334, 447.8587453710174], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.19409999914933, 9.458599997742567, 7.978699999512173], "resource_metrics": {"samples": 2, "duration_s": 0.11123514175415039, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 29.0, "gpu_power_mean_watts": 15.552999999999999, "gpu_power_peak_watts": 17.499, "gpu_temperature_mean_c": 49.5, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1494.92578125, "cpu_memory_peak_mb": 1494.92578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1970.515000000887, "compile_ms": 591.2533000009716, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765595236.6216147}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.241800001182128, 3.192500000295695], "ttft_ms": [0.3834000017377548, 0.37319999682949856], "tokens_processed": [8, 8], "throughput_tok_s": [1885.9917954100893, 2505.8731399401804], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0871000015176833, 0.44319999869912863, 0.4181000040262006], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 17.499, "gpu_power_peak_watts": 17.499, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1501.8203125, "cpu_memory_peak_mb": 1501.8203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595236.7530231}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.521900001738686, 2.553899998019915], "ttft_ms": [0.3366999953868799, 0.2902999985963106], "tokens_processed": [8, 8], "throughput_tok_s": [3172.2114257046355, 3132.4640769813013], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5904999998165295, 0.32929999724728987, 0.32839999767020345], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 17.499, "gpu_power_peak_watts": 17.499, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1501.82421875, "cpu_memory_peak_mb": 1501.82421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595236.878001}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.5020000068470836, 3.4402000019326806], "ttft_ms": [0.4470999992918223, 0.41400000191060826], "tokens_processed": [8, 8], "throughput_tok_s": [2284.408904728287, 2325.446193682244], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.797100001364015, 0.5540999991353601, 0.44750000233761966], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 17.499, "gpu_power_peak_watts": 17.499, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1501.82421875, "cpu_memory_peak_mb": 1501.82421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.0034497}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.394599996681791, 3.732299999683164], "ttft_ms": [0.3887999992002733, 0.4006999952252954], "tokens_processed": [8, 8], "throughput_tok_s": [2356.684147711062, 2143.4504194944466], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.777699999161996, 0.45900000259280205, 0.4134999981033616], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 21.504, "gpu_power_peak_watts": 21.504, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1501.82421875, "cpu_memory_peak_mb": 1501.82421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.1277418}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.137100000458304, 2.7056000035372563], "ttft_ms": [0.43929999810643494, 0.4428000029292889], "tokens_processed": [8, 8], "throughput_tok_s": [2550.125912094376, 2956.8302740763356], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7836999939172529, 0.42519999988144264, 0.432399996498134], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 21.504, "gpu_power_peak_watts": 21.504, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1501.82421875, "cpu_memory_peak_mb": 1501.82421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.2541175}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.801800005021505, 3.6728000050061382], "ttft_ms": [0.4621999978553504, 0.4447999963304028], "tokens_processed": [8, 8], "throughput_tok_s": [2104.2663973468925, 2178.174686641188], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0412999981781468, 0.4549000004772097, 0.4394999996293336], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 21.504, "gpu_power_peak_watts": 21.504, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1502.98046875, "cpu_memory_peak_mb": 1502.98046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.3869653}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.815800002485048, 3.4860000014305115], "ttft_ms": [0.4400000034365803, 0.4575000057229772], "tokens_processed": [8, 8], "throughput_tok_s": [2096.5459391975423, 2294.893860217191], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9076999995158985, 0.4647999958251603, 0.4398000019136816], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 21.504, "gpu_power_peak_watts": 21.504, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1503.01171875, "cpu_memory_peak_mb": 1503.01171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.5048034}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.921599996625446, 2.8898000018671155], "ttft_ms": [0.36749999708263204, 0.378400000045076], "tokens_processed": [8, 8], "throughput_tok_s": [2738.225632954645, 2768.357670022547], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.621700004558079, 0.38079999649198726, 0.3708000003825873], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.509, "gpu_power_peak_watts": 21.509, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1503.01953125, "cpu_memory_peak_mb": 1503.01953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.6297293}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.161199998634402, 4.167200000665616], "ttft_ms": [0.43140000343555585, 0.5381000009947456], "tokens_processed": [8, 8], "throughput_tok_s": [1922.522349953233, 1919.7542711466165], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8101999992504716, 0.45059999683871865, 0.43280000681988895], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.509, "gpu_power_peak_watts": 21.509, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1503.171875, "cpu_memory_peak_mb": 1503.171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.756707}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.6173999978927895, 3.5706999988178723], "ttft_ms": [0.436700000136625, 0.4138000003877096], "tokens_processed": [8, 8], "throughput_tok_s": [2211.53314664128, 2240.4570539805936], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8458000011160038, 0.46609999844804406, 0.4402999984449707], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.509, "gpu_power_peak_watts": 21.509, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1503.17578125, "cpu_memory_peak_mb": 1503.17578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595237.8811429}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.288599997584242, 4.262899994500913], "ttft_ms": [0.4307999988668598, 0.5210000017541461], "tokens_processed": [8, 8], "throughput_tok_s": [1512.6876685047625, 1876.6567384456355], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9202000039513223, 0.5588999993051402, 0.4728999992948957], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.509, "gpu_power_peak_watts": 21.509, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1514.29296875, "cpu_memory_peak_mb": 1514.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.0050027}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.5360999972908758, 3.483699998469092], "ttft_ms": [0.4293999954825267, 0.44920000073034316], "tokens_processed": [8, 8], "throughput_tok_s": [2262.3794593278094, 2296.408991450352], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9399999980814755, 0.4704999955720268, 0.4087999986950308], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1514.29296875, "cpu_memory_peak_mb": 1514.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.129952}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.64300000021467, 4.298400002880953], "ttft_ms": [0.5792000010842457, 0.5094000007375143], "tokens_processed": [8, 8], "throughput_tok_s": [1723.0239068770447, 1861.1576388046929], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9955999994417652, 0.7125999982235953, 0.6434999959310517], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1514.34765625, "cpu_memory_peak_mb": 1514.34765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.2547991}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.634400000213645, 4.516900000453461], "ttft_ms": [0.6582000060006976, 0.6083000043872744], "tokens_processed": [8, 8], "throughput_tok_s": [1726.2213014912827, 1771.126214704081], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0095999969053082, 0.6233000021893531, 0.5011000030208379], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1514.34765625, "cpu_memory_peak_mb": 1514.34765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.3788636}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.3603999982005917, 3.387600001587998], "ttft_ms": [0.4065000030095689, 0.46189999557100236], "tokens_processed": [8, 8], "throughput_tok_s": [2380.6689692547898, 2361.553901360803], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7025999948382378, 0.47930000437190756, 0.41000000055646524], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1514.34765625, "cpu_memory_peak_mb": 1514.34765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.5059507}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.948900004616007, 3.397100001166109], "ttft_ms": [0.4427999956533313, 0.4512000014074147], "tokens_processed": [8, 8], "throughput_tok_s": [2025.8806226160502, 2354.949809323797], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.123000001825858, 0.574199999391567, 0.4562999965855852], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1517.5234375, "cpu_memory_peak_mb": 1517.5234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.6310947}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.765599998587277, 3.076399996643886], "ttft_ms": [0.4512000014074147, 0.4304999965825118], "tokens_processed": [8, 8], "throughput_tok_s": [2124.4954331318577, 2600.442077989657], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8156999974744394, 0.5615999980363995, 0.47669999912614003], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1517.53125, "cpu_memory_peak_mb": 1517.53125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.7567346}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.998799998953473, 3.604300000006333], "ttft_ms": [0.5014999987906776, 0.45980000140843913], "tokens_processed": [8, 8], "throughput_tok_s": [2000.6001805775938, 2219.57106788723], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9551999974064529, 0.502200004120823, 0.4585999995470047], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1517.5390625, "cpu_memory_peak_mb": 1517.5390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595238.8809724}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.416999996872619, 6.74050000088755], "ttft_ms": [0.7790000017848797, 0.8448000007774681], "tokens_processed": [8, 8], "throughput_tok_s": [1246.6884843227162, 1186.8555743559982], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9984000062104315, 0.6495000052382238, 0.5620999945676886], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.466, "gpu_power_peak_watts": 21.466, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1517.55859375, "cpu_memory_peak_mb": 1517.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.0073507}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.503600001044106, 4.662899998947978], "ttft_ms": [0.5511999988812022, 0.6121999977040105], "tokens_processed": [8, 8], "throughput_tok_s": [1776.3566920120118, 1715.6705058665043], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.906000001123175, 0.5626999991363846, 0.5365000033634715], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1517.55859375, "cpu_memory_peak_mb": 1517.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.1335647}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.48670000070706, 6.842599999799859], "ttft_ms": [0.8899999957066029, 0.891099996806588], "tokens_processed": [32, 32], "throughput_tok_s": [3373.1434532150256, 4676.584923996139], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.607899994472973, 1.0564000040176325, 1.0177000003750436], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1541.34765625, "cpu_memory_peak_mb": 1541.34765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.2606804}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.943800002976786, 6.171000000904314], "ttft_ms": [1.0566999990260229, 0.9366000012960285], "tokens_processed": [32, 32], "throughput_tok_s": [4028.298797553893, 5185.545291737261], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.473600001190789, 0.9891000008792616, 0.9521000029053539], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1541.375, "cpu_memory_peak_mb": 1541.375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.385635}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.046800001466181, 7.202699998742901], "ttft_ms": [0.782499999331776, 1.0341999950469472], "tokens_processed": [32, 32], "throughput_tok_s": [3976.736093126384, 4442.778403318896], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.256100003956817, 0.9949000013875775, 0.8773000008659437], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1541.4921875, "cpu_memory_peak_mb": 1541.4921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.5115497}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.832099999883212, 6.809300000895746], "ttft_ms": [0.6894000034662895, 1.0241999989375472], "tokens_processed": [32, 32], "throughput_tok_s": [4683.772193110026, 4699.455156299544], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2315000058151782, 0.9257000056095421, 0.7478999978047796], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1541.6484375, "cpu_memory_peak_mb": 1541.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.6346939}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.689699999697041, 9.33150000491878], "ttft_ms": [0.6950999959371984, 1.1616999981924891], "tokens_processed": [32, 32], "throughput_tok_s": [4783.473100654618, 3429.245028466195], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2507999999797903, 0.9033000023919158, 0.7107999990694225], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1541.73828125, "cpu_memory_peak_mb": 1541.73828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.760565}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.868299996829592, 10.904199996730313], "ttft_ms": [0.7889999978942797, 1.5614000003552064], "tokens_processed": [32, 32], "throughput_tok_s": [2307.420520706609, 2934.6490351970237], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.6549999965936877, 0.89490000391379, 0.8719999968889169], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1570.6875, "cpu_memory_peak_mb": 1570.6875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595239.8871336}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.19660000060685, 12.069299998984206], "ttft_ms": [0.9869999994407408, 1.7099000033340417], "tokens_processed": [32, 32], "throughput_tok_s": [2623.6820096098763, 2651.355091239196], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6340000001946464, 1.3730999999097548, 0.98489999800222], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.684, "gpu_power_peak_watts": 13.684, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1570.6953125, "cpu_memory_peak_mb": 1570.6953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.0126858}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.902599995664787, 11.499599997478072], "ttft_ms": [1.1247999937040731, 1.6653000056976452], "tokens_processed": [32, 32], "throughput_tok_s": [2688.4882304416824, 2782.705486018451], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7353000002913177, 1.3250999982119538, 1.3010999973630533], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 8.362, "gpu_power_peak_watts": 8.362, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1570.78125, "cpu_memory_peak_mb": 1570.78125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.140487}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.597199998504948, 10.789200001454446], "ttft_ms": [1.1430000013206154, 1.6381000023102388], "tokens_processed": [32, 32], "throughput_tok_s": [2759.2867247374606, 2965.9288914549934], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0758999962708913, 1.1769999982789159, 0.9480000007897615], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 8.362, "gpu_power_peak_watts": 8.362, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1570.8125, "cpu_memory_peak_mb": 1570.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.26548}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.087300001643598, 11.73949999792967], "ttft_ms": [1.0003999996115454, 1.7513999991933815], "tokens_processed": [32, 32], "throughput_tok_s": [2445.118549737624, 2725.8401129216227], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8389999968349002, 1.302399999985937, 1.1555000019143336], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 670.01953125, "gpu_memory_peak_mb": 670.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 8.362, "gpu_power_peak_watts": 8.362, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1570.8125, "cpu_memory_peak_mb": 1570.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 156.25880000152392, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.3899758}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.142500001820736, 39.81509999721311], "ttft_ms": [7.0474000021931715, 4.503299998759758], "tokens_processed": [8, 8], "throughput_tok_s": [199.29002926168388, 200.92879336131182], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [97.56930000003194, 5.0132999967900105, 4.4940000007045455], "resource_metrics": {"samples": 2, "duration_s": 0.11634159088134766, "gpu_memory_mean_mb": 682.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 11.5, "gpu_power_mean_watts": 5.3805, "gpu_power_peak_watts": 8.362, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1649.525390625, "cpu_memory_peak_mb": 1705.4296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.6143095}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [39.88200000458164, 39.79149999941001], "ttft_ms": [4.729000000224914, 5.084399999759626], "tokens_processed": [8, 8], "throughput_tok_s": [200.59174562662255, 201.04796250753594], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.951100003381725, 5.601500000921078, 4.497299996728543], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.399, "gpu_power_peak_watts": 2.399, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1708.72265625, "cpu_memory_peak_mb": 1708.72265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.7336996}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.62730000016745, 35.6335999967996], "ttft_ms": [4.281300003640354, 4.401300000608899], "tokens_processed": [8, 8], "throughput_tok_s": [212.61158786212133, 224.50720670149838], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.6499999953084625, 4.815799999050796, 4.058500002429355], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.399, "gpu_power_peak_watts": 2.399, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1708.734375, "cpu_memory_peak_mb": 1708.734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.8592072}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.761600004159845, 37.87899999588262], "ttft_ms": [6.795799999963492, 4.238599998643622], "tokens_processed": [8, 8], "throughput_tok_s": [211.85542983132902, 211.1988173095801], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.627399997843895, 6.450699998822529, 4.379000005428679], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.399, "gpu_power_peak_watts": 2.399, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1708.734375, "cpu_memory_peak_mb": 1708.734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595240.983511}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.31749999860767, 39.354399996227585], "ttft_ms": [7.175300001108553, 6.967200002691243], "tokens_processed": [8, 8], "throughput_tok_s": [214.37663295500724, 203.28095462684885], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.792599997017533, 5.804699998407159, 3.575700000510551], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.613, "gpu_power_peak_watts": 2.613, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1708.8125, "cpu_memory_peak_mb": 1708.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.108321}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.682699997152667, 28.82580000004964], "ttft_ms": [2.904099994339049, 4.448100000445265], "tokens_processed": [8, 8], "throughput_tok_s": [337.79932190847444, 277.5291579066747], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.4434999979566783, 3.1904999996186234, 3.1670000025769696], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.613, "gpu_power_peak_watts": 2.613, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1709.41015625, "cpu_memory_peak_mb": 1709.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.2468174}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.677300000097603, 22.59760000015376], "ttft_ms": [2.756900001259055, 3.1201000019791536], "tokens_processed": [8, 8], "throughput_tok_s": [311.5592371460236, 354.01989591574176], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.581800003303215, 3.0845000001136214, 2.536399995733518], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.613, "gpu_power_peak_watts": 2.613, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1709.97265625, "cpu_memory_peak_mb": 1709.97265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.374643}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.489100000006147, 22.817299999587703], "ttft_ms": [3.2508999938727356, 2.7658000035444275], "tokens_processed": [8, 8], "throughput_tok_s": [340.58350468932, 350.61115908300087], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.108499994094018, 2.634199998283293, 3.6766999983228743], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 2.613, "gpu_power_peak_watts": 2.613, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1709.97265625, "cpu_memory_peak_mb": 1709.97265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.4983473}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.805200000235345, 24.055400004726835], "ttft_ms": [5.669999998644926, 2.928900001279544], "tokens_processed": [8, 8], "throughput_tok_s": [251.5311961547421, 332.5656608673319], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.467100003035739, 3.8779000024078414, 4.0537999957450666], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 12.425, "gpu_power_peak_watts": 12.425, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1709.9765625, "cpu_memory_peak_mb": 1709.9765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.6218693}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.51130000251578, 23.186000005807728], "ttft_ms": [5.298399999446701, 2.81649999669753], "tokens_processed": [8, 8], "throughput_tok_s": [290.7896027911599, 345.03579737756087], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.565599996363744, 3.372599996509962, 2.967499996884726], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 684.01953125, "gpu_memory_peak_mb": 684.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 12.425, "gpu_power_peak_watts": 12.425, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1710.015625, "cpu_memory_peak_mb": 1710.015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.7436318}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.490200002328493, 29.030599995166995], "ttft_ms": [3.473899996606633, 3.857299998344388], "tokens_processed": [8, 8], "throughput_tok_s": [271.27655964925077, 275.5712937842083], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.692799998039845, 5.065099998319056, 3.3647000018390827], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 12.425, "gpu_power_peak_watts": 12.425, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1715.8046875, "cpu_memory_peak_mb": 1715.8046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595241.8713014}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.95340000174474, 36.61110000393819], "ttft_ms": [5.889400003070477, 4.316499995184131], "tokens_processed": [8, 8], "throughput_tok_s": [222.51024936756406, 218.51296462382868], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.225199998472817, 4.789500002516434, 4.604099995049182], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 12.425, "gpu_power_peak_watts": 12.425, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1721.58203125, "cpu_memory_peak_mb": 1721.58203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.012207}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.734100004541688, 29.927399998996407], "ttft_ms": [3.728299998329021, 3.6569000003510155], "tokens_processed": [8, 8], "throughput_tok_s": [252.0947497756377, 267.31356550412914], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.429699998581782, 3.833999995549675, 6.291799996688496], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 13.53, "gpu_power_peak_watts": 13.53, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1721.58203125, "cpu_memory_peak_mb": 1721.58203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.1352267}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.007399996975437, 27.587999997194856], "ttft_ms": [3.536800002621021, 3.8867999974172562], "tokens_processed": [8, 8], "throughput_tok_s": [266.6009051369446, 289.98115125465563], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7958000029902905, 3.5681000008480623, 3.3859000031952746], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 13.53, "gpu_power_peak_watts": 13.53, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1721.58203125, "cpu_memory_peak_mb": 1721.58203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.2600818}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.284300000057556, 37.14320000290172], "ttft_ms": [4.664699998102151, 5.24079999740934], "tokens_processed": [8, 8], "throughput_tok_s": [226.7297353210054, 215.3826272204608], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7408999996841885, 5.786200003058184, 3.555800001777243], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 13.53, "gpu_power_peak_watts": 13.53, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1721.58984375, "cpu_memory_peak_mb": 1721.58984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.3838828}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.60299999965355, 31.632999998691957], "ttft_ms": [4.845700001169462, 3.8548000011360273], "tokens_processed": [8, 8], "throughput_tok_s": [245.37619237754228, 252.90045207001563], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.393399999069516, 3.599499999836553, 4.295999999158084], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 13.53, "gpu_power_peak_watts": 13.53, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1723.1171875, "cpu_memory_peak_mb": 1723.1171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.508516}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.07929999876069, 36.04489999997895], "ttft_ms": [3.695399995194748, 4.7411999985342845], "tokens_processed": [8, 8], "throughput_tok_s": [265.9636361328093, 221.94540697864807], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9476000019931234, 3.550300003553275, 3.8834000006318092], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.527, "gpu_power_peak_watts": 13.527, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1724.70703125, "cpu_memory_peak_mb": 1724.70703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.6390238}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.784100003074855, 32.58780000032857], "ttft_ms": [3.8453000015579164, 3.6686000021290965], "tokens_processed": [8, 8], "throughput_tok_s": [259.874415662661, 245.49064373536535], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.271800004062243, 4.046299996844027, 7.073200002196245], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.527, "gpu_power_peak_watts": 13.527, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1724.71875, "cpu_memory_peak_mb": 1724.71875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.7695324}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.490699999267235, 31.950500000675675], "ttft_ms": [3.583400000934489, 3.5721000022022054], "tokens_processed": [8, 8], "throughput_tok_s": [254.0432572215338, 250.3873178770542], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.120899997767992, 5.164700000023004, 3.77180000214139], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.527, "gpu_power_peak_watts": 13.527, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1724.734375, "cpu_memory_peak_mb": 1724.734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595242.8974745}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.78859999711858, 32.2978999975021], "ttft_ms": [3.8592999990214594, 3.800899998168461], "tokens_processed": [8, 8], "throughput_tok_s": [268.5591132437856, 247.69412254724656], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.848999997193459, 4.435499999090098, 4.397799995786045], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.527, "gpu_power_peak_watts": 13.527, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1724.734375, "cpu_memory_peak_mb": 1724.734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595243.0206022}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.434799999697134, 52.17899999843212], "ttft_ms": [6.019399996148422, 6.042599998181686], "tokens_processed": [32, 32], "throughput_tok_s": [622.1468733267832, 613.2735391816926], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.484000002383254, 7.284300001629163, 6.352800002787262], "resource_metrics": {"samples": 2, "duration_s": 0.11655163764953613, "gpu_memory_mean_mb": 708.01953125, "gpu_memory_peak_mb": 708.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.333, "gpu_power_peak_watts": 13.333, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1741.263671875, "cpu_memory_peak_mb": 1749.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595243.2446997}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.376700001128484, 50.6872000041767], "ttft_ms": [6.275100000493694, 6.109599999035709], "tokens_processed": [32, 32], "throughput_tok_s": [622.8504360789448, 631.3230953251147], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.964600004721433, 7.101600000169128, 6.628199997066986], "resource_metrics": {"samples": 2, "duration_s": 0.10974669456481934, "gpu_memory_mean_mb": 708.01953125, "gpu_memory_peak_mb": 708.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.333, "gpu_power_peak_watts": 13.333, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1749.0625, "cpu_memory_peak_mb": 1749.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595243.4646046}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.30439999670489, 51.67810000421014], "ttft_ms": [6.338599996524863, 6.0021999961463735], "tokens_processed": [32, 32], "throughput_tok_s": [600.3256767167089, 619.2178117499097], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.60760000027949, 8.409200003370643, 6.429699998989236], "resource_metrics": {"samples": 2, "duration_s": 0.10688138008117676, "gpu_memory_mean_mb": 708.01953125, "gpu_memory_peak_mb": 708.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 62.0, "gpu_power_mean_watts": 14.028, "gpu_power_peak_watts": 14.028, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1749.0625, "cpu_memory_peak_mb": 1749.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595243.6799338}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.465000000665896, 50.551300002553035], "ttft_ms": [6.012999998347368, 6.479700001364108], "tokens_processed": [32, 32], "throughput_tok_s": [609.9304298026084, 633.0203179420486], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.360200000926852, 6.311899996944703, 6.24940000125207], "resource_metrics": {"samples": 2, "duration_s": 0.11122322082519531, "gpu_memory_mean_mb": 708.01953125, "gpu_memory_peak_mb": 708.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 62.0, "gpu_power_mean_watts": 14.028, "gpu_power_peak_watts": 14.028, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1749.0625, "cpu_memory_peak_mb": 1749.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595243.9034495}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.629600002546795, 52.64440000610193], "ttft_ms": [6.21089999913238, 5.800400002044626], "tokens_processed": [32, 32], "throughput_tok_s": [608.0228616301756, 607.8519272000617], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.026100003509782, 6.408699999155942, 8.52630000008503], "resource_metrics": {"samples": 2, "duration_s": 0.12218666076660156, "gpu_memory_mean_mb": 708.01953125, "gpu_memory_peak_mb": 708.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 62.0, "gpu_power_mean_watts": 13.565, "gpu_power_peak_watts": 13.565, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1749.0625, "cpu_memory_peak_mb": 1749.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.1499448}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [39.33429999597138, 37.05880000052275], "ttft_ms": [4.332399999839254, 4.4167999949422665], "tokens_processed": [32, 32], "throughput_tok_s": [813.5393283540686, 863.4926117291603], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.874700001091696, 5.508399997779634, 4.392499999084976], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 740.01953125, "gpu_memory_peak_mb": 740.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 62.0, "gpu_power_mean_watts": 13.565, "gpu_power_peak_watts": 13.565, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1756.19140625, "cpu_memory_peak_mb": 1756.19140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.2677774}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.56720000092173, 39.82009999890579], "ttft_ms": [4.825999996683095, 4.821700000320561], "tokens_processed": [32, 32], "throughput_tok_s": [851.8068953559185, 803.6142551344503], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.740900003525894, 4.446200000529643, 4.660999999032356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 740.01953125, "gpu_memory_peak_mb": 740.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 62.0, "gpu_power_mean_watts": 13.565, "gpu_power_peak_watts": 13.565, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1778.91796875, "cpu_memory_peak_mb": 1778.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.3941164}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.152000000409316, 40.16539999429369], "ttft_ms": [4.76370000251336, 5.826799999340437], "tokens_processed": [32, 32], "throughput_tok_s": [602.0469596582175, 796.7056223651762], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.951800001435913, 6.281500005570706, 6.510100000014063], "resource_metrics": {"samples": 2, "duration_s": 0.11247730255126953, "gpu_memory_mean_mb": 740.01953125, "gpu_memory_peak_mb": 740.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 46.0, "gpu_power_mean_watts": 14.689, "gpu_power_peak_watts": 15.813, "gpu_temperature_mean_c": 49.5, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1778.91796875, "cpu_memory_peak_mb": 1778.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.6176918}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.98400000191759, 36.79059999558376], "ttft_ms": [4.902300002868287, 4.378700003144331], "tokens_processed": [32, 32], "throughput_tok_s": [889.2841262309558, 869.7873914489351], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.55560000409605, 4.499900001974311, 4.616099999111611], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 740.01953125, "gpu_memory_peak_mb": 740.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 30.0, "gpu_power_mean_watts": 15.813, "gpu_power_peak_watts": 15.813, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1778.91796875, "cpu_memory_peak_mb": 1778.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.7344656}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [45.45500000676839, 60.46970000170404], "ttft_ms": [4.491699997743126, 8.229399994888809], "tokens_processed": [32, 32], "throughput_tok_s": [703.9929599655726, 529.1906524936991], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.4252000045380555, 4.4096999990870245, 5.18490000104066], "resource_metrics": {"samples": 2, "duration_s": 0.11391949653625488, "gpu_memory_mean_mb": 740.01953125, "gpu_memory_peak_mb": 740.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 30.0, "gpu_power_mean_watts": 15.813, "gpu_power_peak_watts": 15.813, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1778.91796875, "cpu_memory_peak_mb": 1778.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1916.5584000002127, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765595244.959041}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.205099998624064, 6.471900000178721], "ttft_ms": [0.8029999953578226, 0.6659999999101274], "tokens_processed": [8, 8], "throughput_tok_s": [1289.2620589150768, 1236.113042503605], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [35.538500000257045, 0.8903000052669086, 0.7899999982328154], "resource_metrics": {"samples": 2, "duration_s": 0.11413311958312988, "gpu_memory_mean_mb": 832.529296875, "gpu_memory_peak_mb": 925.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 30.0, "gpu_power_mean_watts": 19.5, "gpu_power_peak_watts": 19.5, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 1853.388671875, "cpu_memory_peak_mb": 1913.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32"}, "started_at": 1765595245.1800938}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.503399996494409, 6.431699999666307], "ttft_ms": [0.9372999993502162, 0.7499999992433004], "tokens_processed": [8, 8], "throughput_tok_s": [1066.1833307217532, 1243.8391094757312], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6505999956279993, 0.5897999944863841, 0.9035999974003062], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 30.0, "gpu_power_mean_watts": 19.5, "gpu_power_peak_watts": 19.5, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2057.484375, "cpu_memory_peak_mb": 2057.484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.2974584}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.534000003535766, 6.536799999594223], "ttft_ms": [0.7547999994130805, 0.9082000033231452], "tokens_processed": [8, 8], "throughput_tok_s": [1445.6089618519452, 1223.840411286349], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8344999989494681, 0.9401999996043742, 0.9814000004553236], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 30.0, "gpu_power_mean_watts": 19.5, "gpu_power_peak_watts": 19.5, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2057.5625, "cpu_memory_peak_mb": 2057.5625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.4344397}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.923700002313126, 11.483300004329067], "ttft_ms": [0.8977000034064986, 0.7592000038130209], "tokens_processed": [8, 8], "throughput_tok_s": [1350.5072837713096, 696.6638507209683], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1891000031027943, 0.7681999995838851, 0.8616000050096773], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 24.777, "gpu_power_peak_watts": 24.777, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2057.5625, "cpu_memory_peak_mb": 2057.5625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.5618002}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.1118999947211705, 6.663600004685577], "ttft_ms": [0.7876000017859042, 0.6247000055736862], "tokens_processed": [8, 8], "throughput_tok_s": [1308.9219402983647, 1200.5522531926765], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6900999980862252, 0.9541999970679171, 0.7846000007702969], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 24.777, "gpu_power_peak_watts": 24.777, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2057.578125, "cpu_memory_peak_mb": 2057.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.684566}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.3196999946958385, 7.903899997472763], "ttft_ms": [0.8111000061035156, 0.8544000011170283], "tokens_processed": [8, 8], "throughput_tok_s": [1092.9409683179824, 1012.1585549612172], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.517299999250099, 1.0885000010603108, 0.8430000016232952], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 24.777, "gpu_power_peak_watts": 24.777, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2058.21484375, "cpu_memory_peak_mb": 2058.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.8096368}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.616399998369161, 7.12059999932535], "ttft_ms": [1.059200003510341, 0.8502999990014359], "tokens_processed": [8, 8], "throughput_tok_s": [1050.365002063045, 1123.5008286883085], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0718999949167483, 0.7598000011057593, 0.6675999975414015], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 24.777, "gpu_power_peak_watts": 24.777, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2058.23046875, "cpu_memory_peak_mb": 2058.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595245.933983}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.305699997232296, 7.877899995946791], "ttft_ms": [0.8204999976442195, 0.8940000043367036], "tokens_processed": [8, 8], "throughput_tok_s": [1095.035383745669, 1015.4990548389837], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.1866999959456734, 1.7427000057068653, 1.037399997585453], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 28.901, "gpu_power_peak_watts": 28.901, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2058.23828125, "cpu_memory_peak_mb": 2058.23828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.0585263}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.347599996137433, 7.7518999969470315], "ttft_ms": [0.8728000029805116, 0.5993999948259443], "tokens_processed": [8, 8], "throughput_tok_s": [1088.7908982804627, 1032.005057231218], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0036999994772486, 0.7861999984015711, 0.7719999994151294], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 28.901, "gpu_power_peak_watts": 28.901, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2058.24609375, "cpu_memory_peak_mb": 2058.24609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.1831622}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.394699998258147, 7.95990000187885], "ttft_ms": [0.9271999952034093, 0.8739000040804967], "tokens_processed": [8, 8], "throughput_tok_s": [1081.8559240921786, 1005.0377514933208], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6857000009622425, 1.1638999931165017, 0.8025999995879829], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 28.901, "gpu_power_peak_watts": 28.901, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2058.24609375, "cpu_memory_peak_mb": 2058.24609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.3045893}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.815200002572965, 9.992500003136229], "ttft_ms": [1.0307000047760084, 1.1735000007320195], "tokens_processed": [8, 8], "throughput_tok_s": [907.5233684618584, 800.6004500864782], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0856000046478584, 1.2322999973548576, 1.469799994083587], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 28.901, "gpu_power_peak_watts": 28.901, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2059.79296875, "cpu_memory_peak_mb": 2059.79296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.4281511}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.462299996812362, 9.254999997210689], "ttft_ms": [1.0496000031707808, 1.2452999944798648], "tokens_processed": [8, 8], "throughput_tok_s": [845.4604063171768, 864.3976231670529], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7850000003818423, 1.3212000048952177, 1.0076000035041943], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.04, "gpu_power_peak_watts": 31.04, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2059.79296875, "cpu_memory_peak_mb": 2059.79296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.5512826}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.62909999745898, 9.399499998835381], "ttft_ms": [2.1990000022924505, 0.8218000002671033], "tokens_processed": [8, 8], "throughput_tok_s": [752.6507420113181, 851.109101653409], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.6669999972218648, 1.8266999977640808, 1.2967999937245622], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.04, "gpu_power_peak_watts": 31.04, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2059.80078125, "cpu_memory_peak_mb": 2059.80078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.6770532}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.918499999912456, 9.65810000343481], "ttft_ms": [1.2562999982037582, 1.2270000006537884], "tokens_processed": [8, 8], "throughput_tok_s": [806.5735746403802, 828.3202697378239], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0765000008395873, 1.1449000012362376, 1.4421000014408492], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.04, "gpu_power_peak_watts": 31.04, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2059.8125, "cpu_memory_peak_mb": 2059.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.8018072}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.306600001058541, 9.817799997108523], "ttft_ms": [1.4400000000023283, 1.077399996574968], "tokens_processed": [8, 8], "throughput_tok_s": [776.2016571108181, 814.8465035299263], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.420299999357667, 1.0365000052843243, 1.283300000068266], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.04, "gpu_power_peak_watts": 31.04, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2059.8125, "cpu_memory_peak_mb": 2059.8125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595246.9250252}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.4635999998427, 11.479100001452025], "ttft_ms": [1.385799994750414, 1.1447000069892965], "tokens_processed": [8, 8], "throughput_tok_s": [697.8610558733534, 696.9187478973139], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7156999969738536, 1.3296999968588352, 1.0436000011395663], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 975.0390625, "gpu_memory_peak_mb": 975.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.052, "gpu_power_peak_watts": 31.052, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2061.4140625, "cpu_memory_peak_mb": 2061.4140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.0496047}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.204900001757778, 11.164499999722466], "ttft_ms": [1.6737999976612628, 1.58200000441866], "tokens_processed": [8, 8], "throughput_tok_s": [713.9733508326708, 716.556943902447], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.0283999987877905, 1.8427000031806529, 1.0005000003729947], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 975.0390625, "gpu_memory_peak_mb": 975.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.052, "gpu_power_peak_watts": 31.052, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2061.4375, "cpu_memory_peak_mb": 2061.4375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.1739614}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.958000002778135, 11.10550000157673], "ttft_ms": [1.2911000012536533, 1.188099995488301], "tokens_processed": [8, 8], "throughput_tok_s": [669.0081951949661, 720.3637836084986], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.5822000025073066, 1.3780000008409843, 1.5797999949427322], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 975.0390625, "gpu_memory_peak_mb": 975.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.052, "gpu_power_peak_watts": 31.052, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2061.4375, "cpu_memory_peak_mb": 2061.4375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.2995267}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.867400004528463, 11.456300002464559], "ttft_ms": [1.150100004451815, 1.2711000017588958], "tokens_processed": [8, 8], "throughput_tok_s": [576.8925679931035, 698.3057355585123], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.0210000040824525, 1.5211999998427927, 1.4143000007607043], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 975.0390625, "gpu_memory_peak_mb": 975.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.052, "gpu_power_peak_watts": 31.052, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2061.44921875, "cpu_memory_peak_mb": 2061.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.4250953}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.576600001717452, 11.559500002476852], "ttft_ms": [1.5501000016229227, 1.3132999956724234], "tokens_processed": [8, 8], "throughput_tok_s": [691.0491853232518, 692.0714562295809], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.4927999950014055, 1.3058999975328334, 1.3792000027024187], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 975.0390625, "gpu_memory_peak_mb": 975.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.128, "gpu_power_peak_watts": 31.128, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2061.46484375, "cpu_memory_peak_mb": 2061.46484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.5466077}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.986899996292777, 22.049899998819456], "ttft_ms": [2.442299999529496, 2.246300005936064], "tokens_processed": [32, 32], "throughput_tok_s": [1280.671071831549, 1451.2537472602266], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.602100002055522, 2.3784000004525296, 3.3075999963330105], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.128, "gpu_power_peak_watts": 31.128, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2056.9140625, "cpu_memory_peak_mb": 2056.9140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.6719306}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.00199999648612, 29.350099997827783], "ttft_ms": [4.257399996276945, 3.7738000028184615], "tokens_processed": [32, 32], "throughput_tok_s": [969.6382038484695, 1090.2858934848036], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.230200000980403, 4.634099997929297, 4.621699998097029], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.128, "gpu_power_peak_watts": 31.128, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.0625, "cpu_memory_peak_mb": 2071.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.7952855}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.684700002486352, 24.04700000624871], "ttft_ms": [3.0450999984168448, 2.8110999992350116], "tokens_processed": [32, 32], "throughput_tok_s": [1115.5772937219592, 1330.727325308133], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.3817000007256866, 2.8535000019473955, 3.5066999989794567], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.128, "gpu_power_peak_watts": 31.128, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2073.27734375, "cpu_memory_peak_mb": 2073.27734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595247.9219327}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.450500000966713, 26.981599999999162], "ttft_ms": [2.6622000004863366, 2.933900002972223], "tokens_processed": [32, 32], "throughput_tok_s": [1308.766691835946, 1185.9934177365685], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.277200001932215, 3.5403000001679175, 2.3455000045942143], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.128, "gpu_power_peak_watts": 31.128, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2073.27734375, "cpu_memory_peak_mb": 2073.27734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.043574}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.97149999788962, 25.92010000080336], "ttft_ms": [5.059999995864928, 4.927200003294274], "tokens_processed": [32, 32], "throughput_tok_s": [865.5315581414494, 1234.5631382212339], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.170100001327228, 2.044400003796909, 2.849600001354702], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.45, "gpu_power_peak_watts": 31.45, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2073.28515625, "cpu_memory_peak_mb": 2073.28515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.1661599}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.39670000365004, 45.67809999571182], "ttft_ms": [5.5765999932191335, 6.384099993738346], "tokens_processed": [32, 32], "throughput_tok_s": [661.2021067053453, 700.554532763055], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.675699999614153, 6.29349999508122, 6.443699996452779], "resource_metrics": {"samples": 2, "duration_s": 0.11868810653686523, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.45, "gpu_power_peak_watts": 31.45, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2079.419921875, "cpu_memory_peak_mb": 2079.421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.3968844}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.26399999827845, 41.98349999933271], "ttft_ms": [4.72490000538528, 7.341299999097828], "tokens_processed": [32, 32], "throughput_tok_s": [907.4410163782386, 762.204199280875], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.422800002037548, 7.057000002532732, 6.405599997378886], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.45, "gpu_power_peak_watts": 31.45, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2079.4453125, "cpu_memory_peak_mb": 2079.4453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.5277061}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.169499999436084, 44.919400002982], "ttft_ms": [3.9209000024129637, 5.78970000060508], "tokens_processed": [32, 32], "throughput_tok_s": [860.9209163557617, 712.3870754701902], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.216699996613897, 4.467499995371327, 3.9058000038494356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.805, "gpu_power_peak_watts": 31.805, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2079.4453125, "cpu_memory_peak_mb": 2079.4453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.6553357}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.965099999681115, 38.214200001675636], "ttft_ms": [4.446099999768194, 4.632700001820922], "tokens_processed": [32, 32], "throughput_tok_s": [915.1982977395129, 837.3850557802295], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.117499997955747, 4.624399996828288, 5.256600001303013], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.805, "gpu_power_peak_watts": 31.805, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2075.4453125, "cpu_memory_peak_mb": 2075.4453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.7765725}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.138699999952223, 31.282900003134273], "ttft_ms": [3.1692000047769397, 3.4337999968556687], "tokens_processed": [32, 32], "throughput_tok_s": [1061.757806410055, 1022.9230664930003], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.110499999020249, 4.069700000400189, 4.201699994155206], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.805, "gpu_power_peak_watts": 31.805, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2079.44921875, "cpu_memory_peak_mb": 2079.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.68509999482194, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595248.899676}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.219900002179202, 6.570500001544133], "ttft_ms": [0.7495000027120113, 0.672499998472631], "tokens_processed": [8, 8], "throughput_tok_s": [1286.1943113550255, 1217.563351056985], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.5705000007292256, 0.9630000058677979, 0.7000999976298772], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.805, "gpu_power_peak_watts": 31.805, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2062.81640625, "cpu_memory_peak_mb": 2062.81640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16"}, "started_at": 1765595249.02048}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.888899995828979, 6.236299996089656], "ttft_ms": [0.949699999182485, 0.8691999973962083], "tokens_processed": [8, 8], "throughput_tok_s": [1161.288450237884, 1282.811924541192], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.743700006045401, 0.6788000027881935, 0.596500001847744], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 32.247, "gpu_power_peak_watts": 32.247, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.04296875, "cpu_memory_peak_mb": 2068.04296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.1474762}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.656300000031479, 9.360399999422953], "ttft_ms": [0.807399999757763, 0.5524000007426366], "tokens_processed": [8, 8], "throughput_tok_s": [1201.868906143378, 854.6643306368512], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6304000018863007, 0.734199995349627, 0.6862000009277835], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 32.247, "gpu_power_peak_watts": 32.247, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.0546875, "cpu_memory_peak_mb": 2068.0546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.2752995}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.137699994724244, 7.785499998135492], "ttft_ms": [0.8395000040763989, 0.6859999994048849], "tokens_processed": [8, 8], "throughput_tok_s": [1303.4198489461078, 1027.5512172520555], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6947000040090643, 0.738000002456829, 0.7208999959402718], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 32.247, "gpu_power_peak_watts": 32.247, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.0625, "cpu_memory_peak_mb": 2068.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.400952}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.118499997886829, 6.870099998195656], "ttft_ms": [1.0230999978375621, 0.9216000034939498], "tokens_processed": [8, 8], "throughput_tok_s": [1123.8322683676124, 1164.4663108398856], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.475899996876251, 0.774300002376549, 0.651299997116439], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 32.247, "gpu_power_peak_watts": 32.247, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.0625, "cpu_memory_peak_mb": 2068.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.5255198}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.3299999965238385, 8.267499993962701], "ttft_ms": [1.0345999980927445, 0.9952000036719255], "tokens_processed": [8, 8], "throughput_tok_s": [1263.823065464969, 967.64439139304], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.480999999330379, 0.9017999982461333, 1.1244999986956827], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.753, "gpu_power_peak_watts": 31.753, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.65234375, "cpu_memory_peak_mb": 2068.65234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.6504855}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.609700001718011, 8.460599994577933], "ttft_ms": [0.8094999939203262, 0.7609000022057444], "tokens_processed": [8, 8], "throughput_tok_s": [1210.3423752849023, 945.5594171958119], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.832900001318194, 0.8280999973067082, 0.8066000009421259], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.753, "gpu_power_peak_watts": 31.753, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.66796875, "cpu_memory_peak_mb": 2068.66796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.7738001}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.596899998839945, 10.731199996371288], "ttft_ms": [0.6751000037183985, 0.8933999997680075], "tokens_processed": [8, 8], "throughput_tok_s": [1212.6908095327788, 745.4897870420053], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0370999991428107, 0.8974999946076423, 0.822000001790002], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.753, "gpu_power_peak_watts": 31.753, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.66796875, "cpu_memory_peak_mb": 2068.66796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595249.902422}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.565499999851454, 7.174499995016959], "ttft_ms": [0.8681999970576726, 0.7269999987329356], "tokens_processed": [8, 8], "throughput_tok_s": [1218.4905948032902, 1115.0602837210108], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8487999986973591, 1.3084000020171516, 0.8525000012014061], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.753, "gpu_power_peak_watts": 31.753, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.66796875, "cpu_memory_peak_mb": 2068.66796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.0261455}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.905599999299739, 8.52669999585487], "ttft_ms": [1.029200000630226, 1.037399997585453], "tokens_processed": [8, 8], "throughput_tok_s": [1158.4800742602004, 938.2293271592858], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5602999992552213, 0.8450999957858585, 1.1253999982727692], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 977.0390625, "gpu_memory_peak_mb": 977.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.193, "gpu_power_peak_watts": 31.193, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2068.67578125, "cpu_memory_peak_mb": 2068.67578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.1485324}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.13419999415055, 9.094200002436992], "ttft_ms": [1.0766999985207804, 1.1948000028496608], "tokens_processed": [8, 8], "throughput_tok_s": [875.829301430133, 879.6815550412596], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4116999993566424, 1.2069999938830733, 1.053799998771865], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.193, "gpu_power_peak_watts": 31.193, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2070.2109375, "cpu_memory_peak_mb": 2070.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.2758067}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.665200002724305, 9.292499998991843], "ttft_ms": [1.0397000005468726, 1.0638999956427142], "tokens_processed": [8, 8], "throughput_tok_s": [923.2331622449374, 860.9093355790079], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.1785999997518957, 1.1216999992029741, 1.4115000012679957], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.193, "gpu_power_peak_watts": 31.193, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2070.21484375, "cpu_memory_peak_mb": 2070.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.4014478}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.598299996170681, 10.099899998749606], "ttft_ms": [1.6733000011299737, 1.3774999970337376], "tokens_processed": [8, 8], "throughput_tok_s": [635.0063105682228, 792.0870504648977], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.4611000010045245, 1.84350000199629, 1.572200002556201], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.193, "gpu_power_peak_watts": 31.193, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2070.21484375, "cpu_memory_peak_mb": 2070.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.5248358}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.353399997053202, 10.844399999768939], "ttft_ms": [1.2171999987913296, 1.3355999981286004], "tokens_processed": [8, 8], "throughput_tok_s": [855.3039539119901, 737.7079414417077], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4371999970753677, 1.1982999931205995, 1.4590999999199994], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2070.21875, "cpu_memory_peak_mb": 2070.21875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.6511877}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.74899999750778, 9.782699999050237], "ttft_ms": [0.883699998666998, 0.8736000017961487], "tokens_processed": [8, 8], "throughput_tok_s": [680.9090136775023, 817.7701453358161], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.402799993637018, 1.1101999989477918, 1.3978999995742925], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2070.22265625, "cpu_memory_peak_mb": 2070.22265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.7724884}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.969300004944671, 11.084099998697639], "ttft_ms": [1.2564999997266568, 1.6824999984237365], "tokens_processed": [8, 8], "throughput_tok_s": [729.3081597179229, 721.7545854819052], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.8191000019432977, 0.9118999951169826, 1.5723000033176504], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 981.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.76953125, "cpu_memory_peak_mb": 2071.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595250.8969364}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.118800001393538, 13.217499996244442], "ttft_ms": [1.5510000012000091, 1.3765999974566512], "tokens_processed": [8, 8], "throughput_tok_s": [609.811872972391, 605.2581806145703], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.3107999988715164, 1.2105999994673766, 2.2739999985788018], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 981.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.76953125, "cpu_memory_peak_mb": 2071.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.0218005}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.5594999990426, 15.607499997713603], "ttft_ms": [1.5815000006114133, 1.2786000006599352], "tokens_processed": [8, 8], "throughput_tok_s": [636.9680322154411, 512.5740830480185], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.80909999855794, 0.9715999985928647, 1.3421000039670616], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 981.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.76953125, "cpu_memory_peak_mb": 2071.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.146448}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.955099999497179, 17.11349999823142], "ttft_ms": [2.005899994401261, 2.1487999983946793], "tokens_processed": [8, 8], "throughput_tok_s": [534.9345708332927, 467.4672043022616], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.1489000029978342, 2.2865999999339692, 3.0060999997658655], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 981.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.76953125, "cpu_memory_peak_mb": 2071.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.2703185}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.828299999877345, 21.375899996201042], "ttft_ms": [2.696700001251884, 2.6050999949802645], "tokens_processed": [8, 8], "throughput_tok_s": [335.73523919210265, 374.25324788297905], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.8872000038973056, 2.6020000004791655, 2.536700005293824], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 981.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 51.0, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2071.76953125, "cpu_memory_peak_mb": 2071.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.3935828}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [44.011999998474494, 41.69430000183638], "ttft_ms": [5.375399996410124, 5.1338999983272515], "tokens_processed": [32, 32], "throughput_tok_s": [727.0744342704071, 767.4909999350174], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.674400000018068, 5.696400003216695, 5.317700000887271], "resource_metrics": {"samples": 2, "duration_s": 0.11235237121582031, "gpu_memory_mean_mb": 981.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.5, "gpu_power_mean_watts": 29.6785, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 50.5, "gpu_temperature_peak_c": 51, "cpu_memory_mean_mb": 2075.39453125, "cpu_memory_peak_mb": 2083.70703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.619378}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [41.839199999230914, 30.931100001907907], "ttft_ms": [5.196199999772944, 5.096399996546097], "tokens_processed": [32, 32], "throughput_tok_s": [764.8329796121394, 1034.557451821182], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.541599999764003, 5.373100000724662, 5.139499997312669], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 28.318, "gpu_power_peak_watts": 28.318, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2083.7109375, "cpu_memory_peak_mb": 2083.7109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.748627}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.773299993597902, 22.62190000328701], "ttft_ms": [5.133399994520005, 2.9943999979877844], "tokens_processed": [32, 32], "throughput_tok_s": [1039.8624784035935, 1414.5584586330208], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.462400001008064, 4.933100004564039, 3.614100001868792], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 28.318, "gpu_power_peak_watts": 28.318, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2083.7734375, "cpu_memory_peak_mb": 2083.7734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.8706326}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.685500000079628, 23.275899999134708], "ttft_ms": [2.5180000011459924, 3.4145999961765483], "tokens_processed": [32, 32], "throughput_tok_s": [1410.5926693212703, 1374.8125744306176], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.238100002519786, 2.5433000046177767, 2.353299998503644], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 28.318, "gpu_power_peak_watts": 28.318, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2083.78125, "cpu_memory_peak_mb": 2083.78125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595251.9998176}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.615499998209998, 23.24899999803165], "ttft_ms": [2.296599996043369, 2.5133000017376617], "tokens_processed": [32, 32], "throughput_tok_s": [1414.9587673291671, 1376.4032862793774], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.590099994151387, 2.883100001781713, 3.6400000026333146], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 989.0390625, "gpu_memory_peak_mb": 989.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 22.283, "gpu_power_peak_watts": 22.283, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2083.78125, "cpu_memory_peak_mb": 2083.78125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.1232195}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.28170000127284, 30.203399997844826], "ttft_ms": [3.390999998373445, 3.3074000020860694], "tokens_processed": [32, 32], "throughput_tok_s": [1022.9623069941191, 1059.4833694975855], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.023500002513174, 4.32399999408517, 3.3379999949829653], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 22.283, "gpu_power_peak_watts": 22.283, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2089.921875, "cpu_memory_peak_mb": 2089.921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.2454793}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.747300003189594, 31.520500000624452], "ttft_ms": [4.22660000185715, 4.298799998650793], "tokens_processed": [32, 32], "throughput_tok_s": [1007.9597319074384, 1015.2123221194477], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.027699998114258, 3.381399998033885, 4.240000002027955], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 22.283, "gpu_power_peak_watts": 22.283, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2084.234375, "cpu_memory_peak_mb": 2084.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.3650732}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.220300000626594, 48.71149999962654], "ttft_ms": [3.9777999991201796, 4.602799999702256], "tokens_processed": [32, 32], "throughput_tok_s": [859.7458913405128, 656.9290619308651], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.031299999856856, 4.263100003299769, 3.7793000010424294], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 22.283, "gpu_power_peak_watts": 22.283, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2089.94921875, "cpu_memory_peak_mb": 2089.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.4927053}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.37900000467198, 39.685999996436294], "ttft_ms": [4.386900000099558, 5.107299999508541], "tokens_processed": [32, 32], "throughput_tok_s": [879.628356906193, 806.3296881236083], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.319099997810554, 4.542500006209593, 4.160899996350054], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 19.785, "gpu_power_peak_watts": 19.785, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2089.94921875, "cpu_memory_peak_mb": 2089.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.633716}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.178299997525755, 41.37839999748394], "ttft_ms": [4.655600001569837, 5.654900000081398], "tokens_processed": [32, 32], "throughput_tok_s": [838.1724697556948, 773.3503470879926], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.353899997018743, 4.798000001756009, 5.034100002376363], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 19.785, "gpu_power_peak_watts": 19.785, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2089.94921875, "cpu_memory_peak_mb": 2089.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.173600002541207, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595252.7609856}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.86470000114059, 8.566000004066154], "ttft_ms": [0.7342000026255846, 0.705599995853845], "tokens_processed": [8, 8], "throughput_tok_s": [902.4558077510426, 933.9248186087459], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2855999995954335, 1.335000000835862, 1.3296999968588352], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 19.785, "gpu_power_peak_watts": 19.785, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2073.95703125, "cpu_memory_peak_mb": 2073.95703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8"}, "started_at": 1765595252.8820646}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.298099997569807, 8.969300004537217], "ttft_ms": [1.0566999990260229, 1.6353000028175302], "tokens_processed": [8, 8], "throughput_tok_s": [1096.1757173324452, 891.9313654302023], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.323299995623529, 0.8865999989211559, 1.0993999967467971], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 19.785, "gpu_power_peak_watts": 19.785, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.015625, "cpu_memory_peak_mb": 2079.015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.0069458}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.869900003948715, 7.20730000466574], "ttft_ms": [0.6877999985590577, 0.9524999986751936], "tokens_processed": [8, 8], "throughput_tok_s": [1164.500210396326, 1109.9857082154338], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4979999978095293, 0.9744999988470227, 0.8069000032264739], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 22.763, "gpu_power_peak_watts": 22.763, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.01953125, "cpu_memory_peak_mb": 2079.01953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.1312964}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.660700004431419, 6.728100001055282], "ttft_ms": [0.603899999987334, 1.0579000008874573], "tokens_processed": [8, 8], "throughput_tok_s": [1201.0749612919863, 1189.042968853796], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6814999980852008, 0.8241999967140146, 0.6206000034580939], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 22.763, "gpu_power_peak_watts": 22.763, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.02734375, "cpu_memory_peak_mb": 2079.02734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.2549253}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.046700001112185, 8.29130000056466], "ttft_ms": [1.0203000056208111, 0.6797000023652799], "tokens_processed": [8, 8], "throughput_tok_s": [796.2813659325338, 964.8667880133606], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4728000062168576, 0.6789999970351346, 0.9084999983315356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 22.763, "gpu_power_peak_watts": 22.763, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.02734375, "cpu_memory_peak_mb": 2079.02734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.3826847}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.515100002114195, 8.95930000115186], "ttft_ms": [0.669100001687184, 0.7917999973869883], "tokens_processed": [8, 8], "throughput_tok_s": [939.5074629791427, 892.926902656622], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.103300005022902, 1.468199996452313, 0.8153000017045997], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 28.0, "gpu_power_mean_watts": 22.763, "gpu_power_peak_watts": 22.763, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.6015625, "cpu_memory_peak_mb": 2079.6015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.5062757}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [16.064299998106435, 15.012899995781481], "ttft_ms": [2.0681999958469532, 1.7003000029944815], "tokens_processed": [8, 8], "throughput_tok_s": [497.99866791226464, 532.8750609307957], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.517599998100195, 2.001399996515829, 1.8442000000504777], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 22.479, "gpu_power_peak_watts": 22.479, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.609375, "cpu_memory_peak_mb": 2079.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.6287835}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [16.04399999632733, 17.05020000372315], "ttft_ms": [1.6828000007080846, 1.7425000041839667], "tokens_processed": [8, 8], "throughput_tok_s": [498.62877099422207, 469.20270719716416], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.582499997515697, 2.0751999982167035, 1.6908999969018623], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 22.479, "gpu_power_peak_watts": 22.479, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.609375, "cpu_memory_peak_mb": 2079.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.7557988}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.654300000984222, 15.34209999954328], "ttft_ms": [2.0960000038030557, 1.8149999959859997], "tokens_processed": [8, 8], "throughput_tok_s": [545.914850894461, 521.4410022251291], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.0914000017219223, 2.0327999955043197, 1.7499000023235567], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 22.479, "gpu_power_peak_watts": 22.479, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.609375, "cpu_memory_peak_mb": 2079.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595253.8790057}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.4298000013805, 13.337600001250394], "ttft_ms": [1.5647000036551617, 1.520000005257316], "tokens_processed": [8, 8], "throughput_tok_s": [554.408238453384, 599.8080613641138], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.1737999961478636, 1.8202999999630265, 1.8446999965817668], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 983.0390625, "gpu_memory_peak_mb": 983.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 22.479, "gpu_power_peak_watts": 22.479, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2079.609375, "cpu_memory_peak_mb": 2079.609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.0035453}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.128299998759758, 19.341899998835288], "ttft_ms": [2.4550000016461127, 2.2089999984018505], "tokens_processed": [8, 8], "throughput_tok_s": [397.4503559909647, 413.60983153060124], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.26399999903515, 2.6357000024290755, 2.2868999949423596], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 19.103, "gpu_power_peak_watts": 19.103, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.1484375, "cpu_memory_peak_mb": 2081.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.1295502}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [19.857700004649814, 19.18100000330014], "ttft_ms": [2.3824000018066727, 2.606900001410395], "tokens_processed": [8, 8], "throughput_tok_s": [402.86639430179457, 417.07940141929924], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.5490000009303913, 2.4981000024126843, 2.875600002880674], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 19.103, "gpu_power_peak_watts": 19.103, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.1484375, "cpu_memory_peak_mb": 2081.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.2544866}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.796699999365956, 19.045499997446314], "ttft_ms": [2.952700000605546, 2.2801999948569573], "tokens_processed": [8, 8], "throughput_tok_s": [336.1810671317096, 420.04673025505593], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.63179999840213, 6.225600001926068, 2.5381000014021993], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 19.103, "gpu_power_peak_watts": 19.103, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.1484375, "cpu_memory_peak_mb": 2081.1484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.3779364}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.47799999936251, 25.113399999099784], "ttft_ms": [2.8713000065181404, 2.5247000012313947], "tokens_processed": [8, 8], "throughput_tok_s": [326.8240869437187, 318.5550343755433], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.0291000002762303, 2.582799999800045, 3.4802000009221956], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 19.103, "gpu_power_peak_watts": 19.103, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.15234375, "cpu_memory_peak_mb": 2081.15234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.5031655}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.091899995459244, 26.3623999999254], "ttft_ms": [2.741199998126831, 2.976799994939938], "tokens_processed": [8, 8], "throughput_tok_s": [318.82798837265096, 303.46250720809326], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.765699995914474, 3.4567000038805418, 2.7698000048985705], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 15.437, "gpu_power_peak_watts": 15.437, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.15234375, "cpu_memory_peak_mb": 2081.15234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.6426728}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.076299997337628, 27.00409999670228], "ttft_ms": [3.575399998226203, 3.3121000014944], "tokens_processed": [8, 8], "throughput_tok_s": [284.9378301542087, 296.2513100224393], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.65960000292398, 5.342399999790359, 3.536899996106513], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 987.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 15.437, "gpu_power_peak_watts": 15.437, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2082.94140625, "cpu_memory_peak_mb": 2082.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.7656965}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.265099998156074, 26.261999999405816], "ttft_ms": [3.151500000967644, 3.06060000002617], "tokens_processed": [8, 8], "throughput_tok_s": [273.36315271446404, 304.62264870082254], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.4461999932536855, 3.0918000047677197, 3.526400003465824], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 987.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 15.437, "gpu_power_peak_watts": 15.437, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2082.94921875, "cpu_memory_peak_mb": 2082.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595254.891054}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.718499998329207, 28.924199999892153], "ttft_ms": [6.600000000617001, 3.583700003218837], "tokens_processed": [8, 8], "throughput_tok_s": [260.42938295929565, 276.5850049449882], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9652000050409697, 6.434800001443364, 3.14319999597501], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 987.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 15.437, "gpu_power_peak_watts": 15.437, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2082.94921875, "cpu_memory_peak_mb": 2082.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.0136142}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.01320000051055, 26.4757000040845], "ttft_ms": [3.8541999965673313, 3.3363000038661994], "tokens_processed": [8, 8], "throughput_tok_s": [296.15151110748815, 302.16387097473574], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.635300003632437, 3.05579999985639, 3.1050000034156255], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 987.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 13.168, "gpu_power_peak_watts": 13.168, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2082.94921875, "cpu_memory_peak_mb": 2082.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.1377735}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.384300003177486, 30.079100004513748], "ttft_ms": [3.485999994154554, 3.1180000005406328], "tokens_processed": [8, 8], "throughput_tok_s": [292.1381959397076, 265.9654045100917], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.658300000301097, 5.601800003205426, 3.357100002176594], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 987.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 13.168, "gpu_power_peak_watts": 13.168, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2082.94921875, "cpu_memory_peak_mb": 2082.94921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.2634346}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.01949999900535, 50.48299999907613], "ttft_ms": [6.063200002245139, 5.945600001723506], "tokens_processed": [32, 32], "throughput_tok_s": [615.1539326716302, 633.8767506009076], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.277199995878618, 6.0874000046169385, 5.749899995862506], "resource_metrics": {"samples": 2, "duration_s": 0.1118001937866211, "gpu_memory_mean_mb": 987.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 25.0, "gpu_power_mean_watts": 13.168, "gpu_power_peak_watts": 13.168, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.501953125, "cpu_memory_peak_mb": 2094.7578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.488288}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.07499999878928, 50.74210000020685], "ttft_ms": [5.895399997825734, 5.869100001291372], "tokens_processed": [32, 32], "throughput_tok_s": [665.6266250817657, 630.6400405160518], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.389800001168624, 5.143999995198101, 5.5697999996482395], "resource_metrics": {"samples": 2, "duration_s": 0.11569714546203613, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 12.827, "gpu_power_peak_watts": 12.827, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2094.763671875, "cpu_memory_peak_mb": 2094.765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.7238493}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.765400001546368, 29.177999997045845], "ttft_ms": [2.9810999985784292, 4.5144000032451], "tokens_processed": [32, 32], "throughput_tok_s": [1241.9756727269692, 1096.7167044773416], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.917699996847659, 2.9638999985763803, 3.004400001373142], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 12.827, "gpu_power_peak_watts": 12.827, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2094.76953125, "cpu_memory_peak_mb": 2094.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.8561497}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.831200002576225, 24.76559999922756], "ttft_ms": [3.995099999883678, 3.09249999554595], "tokens_processed": [32, 32], "throughput_tok_s": [893.0764249508594, 1292.114869052156], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.691800001543015, 4.176599999482278, 4.762900003697723], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 12.827, "gpu_power_peak_watts": 12.827, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2094.7734375, "cpu_memory_peak_mb": 2094.7734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595255.9812765}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.14849999413127, 25.645800000347663], "ttft_ms": [4.119199998967815, 3.2551999975112267], "tokens_processed": [32, 32], "throughput_tok_s": [1136.8278951514906, 1247.7676656437388], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.46749999577878, 4.736099996080156, 2.532699996663723], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 995.0390625, "gpu_memory_peak_mb": 995.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 14.594, "gpu_power_peak_watts": 14.594, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2094.859375, "cpu_memory_peak_mb": 2094.859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.105629}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.794300001929514, 34.84509999543661], "ttft_ms": [4.274200000509154, 4.292400000849739], "tokens_processed": [32, 32], "throughput_tok_s": [869.69992630168, 918.3500694270009], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.318699998606462, 4.525399999693036, 4.230800004734192], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 1001.0390625, "gpu_memory_peak_mb": 1001.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 14.594, "gpu_power_peak_watts": 14.594, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2100.9921875, "cpu_memory_peak_mb": 2100.9921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.2417753}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.58020000491524, 36.20080000109738], "ttft_ms": [3.6296000034781173, 4.63040000613546], "tokens_processed": [32, 32], "throughput_tok_s": [899.3766194563085, 883.9583655341861], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.262900002999231, 4.4980000020586886, 4.658199999539647], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 1001.0390625, "gpu_memory_peak_mb": 1001.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 14.594, "gpu_power_peak_watts": 14.594, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2100.99609375, "cpu_memory_peak_mb": 2100.99609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.368411}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.670799995481502, 52.15660000249045], "ttft_ms": [4.4824999931734055, 4.037600003357511], "tokens_processed": [32, 32], "throughput_tok_s": [1010.3944328708294, 613.5369253070947], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.184700004581828, 3.116399995633401, 3.456000005826354], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 1001.0390625, "gpu_memory_peak_mb": 1001.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 66.0, "gpu_power_mean_watts": 14.594, "gpu_power_peak_watts": 14.594, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.48046875, "cpu_memory_peak_mb": 2097.48046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.4923277}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.953200000629295, 42.365900000731926], "ttft_ms": [3.9517999975942075, 3.4621000013430603], "tokens_processed": [32, 32], "throughput_tok_s": [843.1436611265826, 755.3244472428806], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.194500002602581, 3.592200002458412, 5.585999999311753], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 1001.0390625, "gpu_memory_peak_mb": 1001.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 18.581, "gpu_power_peak_watts": 18.581, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2101.0, "cpu_memory_peak_mb": 2101.0, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.6334326}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [43.32520000025397, 49.365500002750196], "ttft_ms": [5.260000005364418, 4.469899999094196], "tokens_processed": [32, 32], "throughput_tok_s": [738.6001680272086, 648.2259877488783], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [13.105899997754022, 4.48070000129519, 3.555400006007403], "resource_metrics": {"samples": 2, "duration_s": 0.1103510856628418, "gpu_memory_mean_mb": 1001.0390625, "gpu_memory_peak_mb": 1001.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 18.581, "gpu_power_peak_watts": 18.581, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2095.890625, "cpu_memory_peak_mb": 2101.0, "cpu_utilization_mean_pct": 0.0}, "export_metadata": null, "trt_build_metadata": [], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 33.793800001149066, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765595256.8545878}
