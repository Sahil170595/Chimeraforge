{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.317500000586733, 10.380500003520865], "ttft_ms": [1.3263000000733882, 1.1295999938738532], "tokens_processed": [8, 8], "throughput_tok_s": [706.8698917239016, 770.6757860687399], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1909.2776000034064, 267.7267000035499, 1.2984999993932433], "resource_metrics": {"samples": 15, "duration_s": 2.194608688354492, "gpu_memory_mean_mb": 499.48619791666664, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.26666666666666666, "gpu_power_mean_watts": 30.571933333333334, "gpu_power_peak_watts": 31.578, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1011.28046875, "cpu_memory_peak_mb": 1370.765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597065.3990893}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.90190000104485, 11.203499998373445], "ttft_ms": [1.4674000049126334, 1.1300999976811], "tokens_processed": [8, 8], "throughput_tok_s": [733.8170409959063, 714.0625698363424], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6807000065455213, 1.4785000021220185, 1.2377000020933338], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.559, "gpu_power_peak_watts": 30.559, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1372.83984375, "cpu_memory_peak_mb": 1372.83984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597065.5297854}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.536299996601883, 10.748499997134786], "ttft_ms": [1.273499998205807, 1.3625999999931082], "tokens_processed": [8, 8], "throughput_tok_s": [759.2798233326805, 744.2899011148113], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.521900005172938, 1.13749999582069, 1.3106000042171218], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.997, "gpu_power_peak_watts": 30.997, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1372.91796875, "cpu_memory_peak_mb": 1372.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597065.6544158}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.461300004157238, 11.708900005032774], "ttft_ms": [1.7117000024882145, 1.3487000032910146], "tokens_processed": [8, 8], "throughput_tok_s": [698.0010990985528, 683.2409531690768], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8503999963286333, 2.089399997203145, 1.7078000018955208], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.997, "gpu_power_peak_watts": 30.997, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1372.93359375, "cpu_memory_peak_mb": 1372.93359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597065.778825}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.12509999802569, 12.075999999069609], "ttft_ms": [1.1298999961582012, 1.1176999978488311], "tokens_processed": [8, 8], "throughput_tok_s": [719.0946599508962, 662.4710169440508], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3617000004160218, 1.3840000028721988, 1.338699999905657], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.997, "gpu_power_peak_watts": 30.997, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1373.015625, "cpu_memory_peak_mb": 1373.015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597065.9046285}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.01429999922402, 48.93420000007609], "ttft_ms": [6.788799997593742, 6.043600005796179], "tokens_processed": [8, 8], "throughput_tok_s": [156.8187743460498, 163.48484291124737], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [690.8611999970162, 6.202199998369906, 5.500699997355696], "resource_metrics": {"samples": 8, "duration_s": 0.7916684150695801, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.5, "gpu_power_mean_watts": 30.520125, "gpu_power_peak_watts": 30.997, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1380.26025390625, "cpu_memory_peak_mb": 1415.40625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597066.8043747}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.90180000459077, 52.80969999876106], "ttft_ms": [5.962700000964105, 5.843700004334096], "tokens_processed": [8, 8], "throughput_tok_s": [160.31485836711366, 151.48732146154367], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.153000002086628, 6.733700000040699, 6.112700000812765], "resource_metrics": {"samples": 2, "duration_s": 0.1125631332397461, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.182, "gpu_power_peak_watts": 30.182, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1415.41015625, "cpu_memory_peak_mb": 1415.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597067.0236714}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.022800001490396, 48.56070000096224], "ttft_ms": [5.248900000879075, 5.575299997872207], "tokens_processed": [8, 8], "throughput_tok_s": [166.5875375811431, 164.74227101012707], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.767299997387454, 5.685500000254251, 6.101300001319032], "resource_metrics": {"samples": 2, "duration_s": 0.11106729507446289, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.85, "gpu_power_peak_watts": 30.85, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1415.49609375, "cpu_memory_peak_mb": 1415.49609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597067.2408466}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.332299997331575, 49.7590000013588], "ttft_ms": [5.640399998810608, 5.7087000022875145], "tokens_processed": [8, 8], "throughput_tok_s": [162.16555888196427, 160.77493518321387], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.836699994688388, 6.354199998895638, 5.387199998949654], "resource_metrics": {"samples": 2, "duration_s": 0.1072227954864502, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 30.85, "gpu_power_peak_watts": 30.85, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1415.50390625, "cpu_memory_peak_mb": 1415.50390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597067.4584632}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [47.44159999972908, 49.25820000062231], "ttft_ms": [5.959499998425599, 6.88540000555804], "tokens_processed": [8, 8], "throughput_tok_s": [168.62837678420806, 162.40950745051444], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.571899993810803, 6.184700003359467, 7.232200005091727], "resource_metrics": {"samples": 2, "duration_s": 0.11370182037353516, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 31.298, "gpu_power_peak_watts": 31.298, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1415.51171875, "cpu_memory_peak_mb": 1415.51171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597067.6793394}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.28470000252128, 51.22300000221003], "ttft_ms": [6.021500004862901, 5.9624999994412065], "tokens_processed": [8, 8], "throughput_tok_s": [159.09411808360952, 156.1798410802733], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [685.0044000020716, 5.557399999815971, 6.346599999233149], "resource_metrics": {"samples": 8, "duration_s": 0.7984163761138916, "gpu_memory_mean_mb": 565.76953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 3.75, "gpu_power_mean_watts": 30.6895, "gpu_power_peak_watts": 31.298, "gpu_temperature_mean_c": 48.375, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1418.17822265625, "cpu_memory_peak_mb": 1423.58984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597068.5852284}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.540199994225986, 49.54179999913322], "ttft_ms": [5.842400001711212, 6.219000002602115], "tokens_processed": [8, 8], "throughput_tok_s": [161.48501622787992, 161.47980089823074], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.042400003934745, 7.676999994146172, 6.1300000015762635], "resource_metrics": {"samples": 2, "duration_s": 0.1126103401184082, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.418, "gpu_power_peak_watts": 30.418, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1423.62109375, "cpu_memory_peak_mb": 1423.62109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597068.8041866}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.690899999812245, 50.251599997864105], "ttft_ms": [6.6907000000355765, 6.537399996886961], "tokens_processed": [8, 8], "throughput_tok_s": [154.76611937553918, 159.19891108621482], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.422600006568246, 6.3746000014361925, 5.890899999940302], "resource_metrics": {"samples": 2, "duration_s": 0.11085224151611328, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.418, "gpu_power_peak_watts": 30.418, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1423.640625, "cpu_memory_peak_mb": 1423.640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597069.021823}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.15429999952903, 51.806700001179706], "ttft_ms": [6.265600000915583, 5.8759000021382235], "tokens_processed": [8, 8], "throughput_tok_s": [153.390995566468, 154.4201811699612], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.772699998691678, 6.1777000009897165, 6.504500001028646], "resource_metrics": {"samples": 2, "duration_s": 0.11236929893493652, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 11.0, "gpu_power_mean_watts": 32.677, "gpu_power_peak_watts": 32.677, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1423.65234375, "cpu_memory_peak_mb": 1423.65234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597069.241803}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.729300001170486, 49.63259999931324], "ttft_ms": [5.667099998390768, 6.30789999559056], "tokens_processed": [8, 8], "throughput_tok_s": [154.6512324701665, 161.1843828473764], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.727700001443736, 7.204899993666913, 6.556600004842039], "resource_metrics": {"samples": 2, "duration_s": 0.11071920394897461, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 11.0, "gpu_power_mean_watts": 32.677, "gpu_power_peak_watts": 32.677, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1423.67578125, "cpu_memory_peak_mb": 1423.67578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597069.4597704}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.23619999701623, 53.14440000074683], "ttft_ms": [6.054700003005564, 6.122500002675224], "tokens_processed": [8, 8], "throughput_tok_s": [159.24771380946726, 150.53326408591644], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [677.2130999961519, 7.4354999960632995, 6.481500000518281], "resource_metrics": {"samples": 8, "duration_s": 0.7923805713653564, "gpu_memory_mean_mb": 572.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 6.875, "gpu_power_mean_watts": 31.085375, "gpu_power_peak_watts": 31.64, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1425.86328125, "cpu_memory_peak_mb": 1430.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597070.3593597}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.8512999988161, 53.10820000158856], "ttft_ms": [6.212499996763654, 6.109599999035709], "tokens_processed": [8, 8], "throughput_tok_s": [154.28735634752957, 150.63587166879515], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.968100005702581, 7.2580000050948, 6.112400005804375], "resource_metrics": {"samples": 2, "duration_s": 0.12085270881652832, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 30.950499999999998, "gpu_power_peak_watts": 31.74, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1431.04296875, "cpu_memory_peak_mb": 1431.04296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597070.5867734}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.40060000505764, 50.091400000383146], "ttft_ms": [7.307899999432266, 6.302500005403999], "tokens_processed": [8, 8], "throughput_tok_s": [152.67000758059737, 159.70805367665525], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.4497999992454425, 6.501099996967241, 6.879000000481028], "resource_metrics": {"samples": 2, "duration_s": 0.1070256233215332, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.74, "gpu_power_peak_watts": 31.74, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1431.0546875, "cpu_memory_peak_mb": 1431.0546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597070.8026907}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.02709999826038, 51.640199999383185], "ttft_ms": [5.735600003390573, 6.088400004955474], "tokens_processed": [8, 8], "throughput_tok_s": [159.91332698233933, 154.9180677087919], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.2829000016790815, 6.235600005311426, 5.905899997742381], "resource_metrics": {"samples": 2, "duration_s": 0.10950279235839844, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 31.74, "gpu_power_peak_watts": 31.74, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1431.0546875, "cpu_memory_peak_mb": 1431.0546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597071.0197287}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.48399999982212, 54.9375999980839], "ttft_ms": [6.020000000717118, 8.004799994523637], "tokens_processed": [8, 8], "throughput_tok_s": [152.4274064481959, 145.61975769380211], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.633100005274173, 7.911000000603963, 6.4430000056745484], "resource_metrics": {"samples": 2, "duration_s": 0.11125493049621582, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 32.871, "gpu_power_peak_watts": 32.871, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1431.0703125, "cpu_memory_peak_mb": 1431.0703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597071.2380464}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [89.06019999994896, 87.71349999733502], "ttft_ms": [10.441500002343673, 10.786500002723187], "tokens_processed": [32, 32], "throughput_tok_s": [359.30752457347205, 364.82411488507756], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [682.7392999985022, 11.384299999917857, 10.067700000945479], "resource_metrics": {"samples": 9, "duration_s": 0.894484281539917, "gpu_memory_mean_mb": 595.3528645833334, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.11111111111111, "gpu_power_mean_watts": 30.58966666666667, "gpu_power_peak_watts": 32.871, "gpu_temperature_mean_c": 48.77777777777778, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 1436.3680555555557, "cpu_memory_peak_mb": 1457.38671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597072.2477267}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [87.83309999853373, 87.38799999991897], "ttft_ms": [10.010000005422626, 10.175600000366103], "tokens_processed": [32, 32], "throughput_tok_s": [364.32734357018256, 366.18299995456664], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.981200004403945, 10.2201999980025, 10.220699994533788], "resource_metrics": {"samples": 2, "duration_s": 0.1147918701171875, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 27.575, "gpu_power_peak_watts": 27.575, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1457.41015625, "cpu_memory_peak_mb": 1457.41015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597072.4696515}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [86.46730000327807, 87.25670000421815], "ttft_ms": [9.964299999410287, 10.498600000573788], "tokens_processed": [32, 32], "throughput_tok_s": [370.0821003869306, 366.7340158228888], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.80960000056075, 10.604299997794442, 10.084399997140281], "resource_metrics": {"samples": 2, "duration_s": 0.11169004440307617, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.392, "gpu_power_peak_watts": 20.392, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1457.51953125, "cpu_memory_peak_mb": 1457.51953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597072.6877615}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [89.67300000222167, 87.59809999901336], "ttft_ms": [10.557800000242423, 10.070900003483985], "tokens_processed": [32, 32], "throughput_tok_s": [356.85211824302957, 365.3047269331233], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.790399999474175, 10.187799998675473, 10.555499997281004], "resource_metrics": {"samples": 2, "duration_s": 0.1127011775970459, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.392, "gpu_power_peak_watts": 20.392, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1457.53125, "cpu_memory_peak_mb": 1457.53125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597072.906877}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [89.6770999970613, 93.9689000006183], "ttft_ms": [10.601899994071573, 10.797200004162733], "tokens_processed": [32, 32], "throughput_tok_s": [356.8358031319995, 340.53819933817937], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.633800004143268, 10.322799993446097, 11.687300000630785], "resource_metrics": {"samples": 3, "duration_s": 0.21129512786865234, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 29.333333333333332, "gpu_power_mean_watts": 16.646, "gpu_power_peak_watts": 20.392, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1454.7942708333333, "cpu_memory_peak_mb": 1457.5859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597073.2264764}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [111.50809999526246, 110.07519999839133], "ttft_ms": [11.620799996308051, 11.8746000007377], "tokens_processed": [32, 32], "throughput_tok_s": [286.97466822015224, 290.7103507462867], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [714.7707999974955, 15.378700001747347, 11.787099996581674], "resource_metrics": {"samples": 9, "duration_s": 0.9262139797210693, "gpu_memory_mean_mb": 619.7973090277778, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.22222222222222, "gpu_power_mean_watts": 14.442666666666668, "gpu_power_peak_watts": 15.058, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1469.873263888889, "cpu_memory_peak_mb": 1508.078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597074.2600062}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [73.29340000433149, 73.16030000220053], "ttft_ms": [8.102300002065022, 7.9996999993454665], "tokens_processed": [32, 32], "throughput_tok_s": [436.6013856378455, 437.39569136591155], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.574399999110028, 10.058499996375758, 8.805299999949057], "resource_metrics": {"samples": 2, "duration_s": 0.12038898468017578, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 12.574, "gpu_power_peak_watts": 12.574, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1508.07421875, "cpu_memory_peak_mb": 1508.07421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597074.4878726}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [74.63000000279862, 74.83240000146907], "ttft_ms": [8.571999998821411, 8.021900001040194], "tokens_processed": [32, 32], "throughput_tok_s": [428.7819911402921, 427.62225986834306], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.24710000253981, 8.123499996145256, 9.158600005321205], "resource_metrics": {"samples": 2, "duration_s": 0.11207199096679688, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 14.955, "gpu_power_peak_watts": 14.955, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1508.07421875, "cpu_memory_peak_mb": 1508.07421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597074.7062912}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [75.55720000527799, 73.74620000337018], "ttft_ms": [9.371000000101048, 8.636699996714015], "tokens_processed": [32, 32], "throughput_tok_s": [423.52019394266415, 433.9206630109431], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.503099998051766, 9.324200000264682, 8.293300001241732], "resource_metrics": {"samples": 2, "duration_s": 0.1123511791229248, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 14.955, "gpu_power_peak_watts": 14.955, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1508.07421875, "cpu_memory_peak_mb": 1508.07421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597074.9247303}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [73.67909999447875, 78.09950000228127], "ttft_ms": [8.966000001237262, 9.001499995065387], "tokens_processed": [32, 32], "throughput_tok_s": [434.3158372238256, 409.7337370798185], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.381800002302043, 10.124300002644304, 8.526200006599538], "resource_metrics": {"samples": 2, "duration_s": 0.11324834823608398, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 15.9605, "gpu_power_peak_watts": 16.966, "gpu_temperature_mean_c": 47.5, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1508.0859375, "cpu_memory_peak_mb": 1508.0859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 1574.1600999972434, "compile_ms": 432.1072999955504, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765597075.1448188}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.016600003524218, 3.8626999958069064], "ttft_ms": [0.4115999981877394, 0.49810000200523064], "tokens_processed": [8, 8], "throughput_tok_s": [1991.7343008964533, 2071.0901723365196], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.3297000045422465, 0.5445999995572492, 0.3915999986929819], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 16.966, "gpu_power_peak_watts": 16.966, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1514.9765625, "cpu_memory_peak_mb": 1514.9765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.2620168}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [1.938599998538848, 1.8295999980182387], "ttft_ms": [0.24909999774536118, 0.23040000087348744], "tokens_processed": [8, 8], "throughput_tok_s": [4126.689366568514, 4372.540450735314], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5658000009134412, 0.31670000316808, 0.24219999613706023], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 16.966, "gpu_power_peak_watts": 16.966, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1514.9765625, "cpu_memory_peak_mb": 1514.9765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.393774}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.1286999981384724, 1.8968000003951602], "ttft_ms": [0.2565000031609088, 0.25829999503912404], "tokens_processed": [8, 8], "throughput_tok_s": [3758.162261941994, 4217.629691234374], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.530999997863546, 0.27260000206297264, 0.24059999850578606], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 16.966, "gpu_power_peak_watts": 16.966, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1515.0546875, "cpu_memory_peak_mb": 1515.0546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.5149307}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.047800000582356, 1.9262999994680285], "ttft_ms": [0.2736999958870001, 0.23610000062035397], "tokens_processed": [8, 8], "throughput_tok_s": [3906.631505872131, 4153.039506935212], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.465299999632407, 0.26839999918593094, 0.24410000332864001], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.322, "gpu_power_peak_watts": 21.322, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1515.0625, "cpu_memory_peak_mb": 1515.0625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.6408536}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.2525000022142194, 2.5335999962408096], "ttft_ms": [0.26730000536190346, 0.2367999986745417], "tokens_processed": [8, 8], "throughput_tok_s": [3551.609319483222, 3157.562366541632], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5748999974457547, 0.3093999985139817, 0.2949999980046414], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.322, "gpu_power_peak_watts": 21.322, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1515.1953125, "cpu_memory_peak_mb": 1515.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.7644958}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.631199997267686, 2.317499995115213], "ttft_ms": [0.3375000014784746, 0.3034999972442165], "tokens_processed": [8, 8], "throughput_tok_s": [3040.437826203797, 3451.9956922814513], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7203999994089827, 0.34610000147949904, 0.3119999964837916], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.322, "gpu_power_peak_watts": 21.322, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1516.46875, "cpu_memory_peak_mb": 1516.46875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597075.8904037}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.750699997704942, 3.177800004777964], "ttft_ms": [0.28620000375667587, 0.534200000402052], "tokens_processed": [8, 8], "throughput_tok_s": [2908.3506040916254, 2517.4649090476564], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8701999977347441, 0.42240000038873404, 0.28899999597342685], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 14.0, "gpu_power_mean_watts": 21.322, "gpu_power_peak_watts": 21.322, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1516.46875, "cpu_memory_peak_mb": 1516.46875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.0143166}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.2809000001871027, 2.255300001706928], "ttft_ms": [0.2916000012191944, 0.28059999749530107], "tokens_processed": [8, 8], "throughput_tok_s": [3507.387434496803, 3547.1999263712964], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5074000000604428, 0.30160000460455194, 0.27749999571824446], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.668, "gpu_power_peak_watts": 21.668, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1516.46875, "cpu_memory_peak_mb": 1516.46875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.1408713}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.7649999974528328, 2.746600002865307], "ttft_ms": [0.3491000024951063, 0.37779999547638], "tokens_processed": [8, 8], "throughput_tok_s": [2893.3092250885143, 2912.692052593841], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8689999958733097, 0.39739999920129776, 0.33370000164723024], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.668, "gpu_power_peak_watts": 21.668, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1516.4765625, "cpu_memory_peak_mb": 1516.4765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.2650163}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.2443000018247403, 3.262199999880977], "ttft_ms": [0.4456999959074892, 0.45849999878555536], "tokens_processed": [8, 8], "throughput_tok_s": [2465.8632048517215, 2452.3327816479323], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7211999982246198, 0.4222999996272847, 0.4098999997950159], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.668, "gpu_power_peak_watts": 21.668, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1516.55859375, "cpu_memory_peak_mb": 1516.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.3916514}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.9774999968358316, 2.5703999999677762], "ttft_ms": [0.3364000003784895, 0.3685000046971254], "tokens_processed": [8, 8], "throughput_tok_s": [2011.3136408206499, 3112.356053571542], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.600999996298924, 0.4543000031844713, 0.34330000198679045], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.668, "gpu_power_peak_watts": 21.668, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1527.67578125, "cpu_memory_peak_mb": 1527.67578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.5167387}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.457999999227468, 3.8777000008849427], "ttft_ms": [0.42050000047311187, 0.5340999996406026], "tokens_processed": [8, 8], "throughput_tok_s": [2313.4759982033643, 2063.078628613429], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.6744000056642108, 0.36890000046696514, 0.37779999547638], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1527.67578125, "cpu_memory_peak_mb": 1527.67578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.6419008}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.224199994292576, 3.2458999994560145], "ttft_ms": [0.32749999809311703, 0.41870000131893903], "tokens_processed": [8, 8], "throughput_tok_s": [2481.2356597486087, 2464.6477098310893], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.6798000031267293, 0.36830000317422673, 0.3543999991961755], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1527.76171875, "cpu_memory_peak_mb": 1527.76171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.7666292}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.552900001523085, 3.396300002350472], "ttft_ms": [0.39599999581696466, 0.5084000003989786], "tokens_processed": [8, 8], "throughput_tok_s": [2251.681723822933, 2355.5045179941267], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.782000002800487, 0.5301999990479089, 0.4732000015792437], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1527.76171875, "cpu_memory_peak_mb": 1527.76171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597076.8916757}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.87430000025779, 3.220899998268578], "ttft_ms": [0.3532999980961904, 0.35770000249613076], "tokens_processed": [8, 8], "throughput_tok_s": [2783.2863651262905, 2483.777827408633], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.6619000050704926, 0.37940000038361177, 0.3404000017326325], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1527.76171875, "cpu_memory_peak_mb": 1527.76171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.017347}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.464500001631677, 3.6082999940845184], "ttft_ms": [0.4459000047063455, 0.44489999709185213], "tokens_processed": [8, 8], "throughput_tok_s": [1791.9139874736645, 2217.1105543095855], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2274999971850775, 0.6667999987257645, 0.4014999940409325], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1530.91796875, "cpu_memory_peak_mb": 1530.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.143284}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.867199997534044, 4.469399995286949], "ttft_ms": [0.575300000491552, 0.5592000015894882], "tokens_processed": [8, 8], "throughput_tok_s": [1643.6554906420904, 1789.9494358160207], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0447000022395514, 0.6226000041351654, 0.5748999974457547], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1530.91796875, "cpu_memory_peak_mb": 1530.91796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.2678049}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.8767999940318987, 3.103499999269843], "ttft_ms": [0.5068000027677044, 0.4343999971752055], "tokens_processed": [8, 8], "throughput_tok_s": [2063.557576433017, 2577.734816137313], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9101000032387674, 0.6081999963498674, 0.512699996761512], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1531.00390625, "cpu_memory_peak_mb": 1531.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.3926113}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.570100001525134, 3.4996999966097064], "ttft_ms": [0.43400000140536577, 0.4596999933710322], "tokens_processed": [8, 8], "throughput_tok_s": [2240.8335891382394, 2285.910223090524], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.647599998046644, 0.5433999976958148, 0.4231999992043711], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 21.662, "gpu_power_peak_watts": 21.662, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1531.00390625, "cpu_memory_peak_mb": 1531.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.5192482}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.900600000633858, 3.714300000865478], "ttft_ms": [0.47590000031050295, 0.5780999999842606], "tokens_processed": [8, 8], "throughput_tok_s": [2050.9665176383064, 2153.8378693524755], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9627999970689416, 0.4493000014917925, 0.37209999572951347], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1531.00390625, "cpu_memory_peak_mb": 1531.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.644784}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.232000003976282, 5.784899993159343], "ttft_ms": [0.575300000491552, 0.8249000020441599], "tokens_processed": [32, 32], "throughput_tok_s": [3466.2045045729415, 5531.642731566678], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2379000001819804, 0.901699997484684, 0.6003000016789883], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1554.77734375, "cpu_memory_peak_mb": 1554.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.7702034}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.270699996093754, 5.803999993077014], "ttft_ms": [0.6802999996580184, 0.7628999956068583], "tokens_processed": [32, 32], "throughput_tok_s": [5103.0985408222305, 5513.439014157385], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1943999998038635, 0.9067999999388121, 0.6802999996580184], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1554.89453125, "cpu_memory_peak_mb": 1554.89453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597077.8969162}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.251899998460431, 6.114399999205489], "ttft_ms": [0.62189999880502, 0.9921000018948689], "tokens_processed": [32, 32], "throughput_tok_s": [5118.443994286569, 5233.547037184043], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2010999998892657, 0.7610000029671937, 0.5757000035373494], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1554.9140625, "cpu_memory_peak_mb": 1554.9140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.023744}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.950099999608938, 5.662800002028234], "ttft_ms": [0.8321999994223006, 0.5744000009144656], "tokens_processed": [32, 32], "throughput_tok_s": [5378.060873279971, 5650.914739799855], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7556000020704232, 0.6601999994018115, 0.5476000005728565], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1554.9140625, "cpu_memory_peak_mb": 1554.9140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.1500697}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.446200000937097, 4.804499993042555], "ttft_ms": [0.6724000049871393, 0.7829000023775734], "tokens_processed": [32, 32], "throughput_tok_s": [4964.164933658294, 6660.4225301986735], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3410999963525683, 0.8954000004450791, 0.8860000016284175], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1554.9140625, "cpu_memory_peak_mb": 1554.9140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.2770689}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [15.538300001935568, 10.87099999858765], "ttft_ms": [1.1597000047913752, 1.574499998241663], "tokens_processed": [32, 32], "throughput_tok_s": [2059.427350225819, 2943.611443671917], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.5615000049583614, 1.2349000026006252, 1.1464999988675117], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1583.796875, "cpu_memory_peak_mb": 1583.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.4030385}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.95120000152383, 11.584700005187187], "ttft_ms": [0.8161000005202368, 1.7507000011391938], "tokens_processed": [32, 32], "throughput_tok_s": [3215.6925792969523, 2762.264019411087], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8249999993713573, 0.8455999995931052, 0.7811999967088923], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.696, "gpu_power_peak_watts": 11.696, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1583.796875, "cpu_memory_peak_mb": 1583.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.5300224}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.63059999392135, 8.466699997370597], "ttft_ms": [0.7304000027943403, 1.2038999993819743], "tokens_processed": [32, 32], "throughput_tok_s": [3707.737587483841, 3779.5126802576992], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5069000000949018, 0.9929999941959977, 0.789099998655729], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.841, "gpu_power_peak_watts": 6.841, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1583.796875, "cpu_memory_peak_mb": 1583.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.6552877}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.56819999939762, 9.985900003812276], "ttft_ms": [0.6788000027881935, 1.5785000068717636], "tokens_processed": [32, 32], "throughput_tok_s": [3027.9517800404965, 3204.5183696795975], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3738000052399002, 0.9061000018846244, 0.6905999980517663], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.841, "gpu_power_peak_watts": 6.841, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1583.80859375, "cpu_memory_peak_mb": 1583.80859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.7812884}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.417299999971874, 10.301399997842964], "ttft_ms": [0.8729999972274527, 1.2443000014172867], "tokens_processed": [32, 32], "throughput_tok_s": [3071.813233763681, 3106.373891577898], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.904899996588938, 0.9711999955470674, 1.0524000026634894], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.841, "gpu_power_peak_watts": 6.841, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1583.82421875, "cpu_memory_peak_mb": 1583.82421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 110.17020000144839, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597078.906}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.27469999709865, 39.91370000585448], "ttft_ms": [6.341000000247732, 6.139400000392925], "tokens_processed": [8, 8], "throughput_tok_s": [198.63586818961562, 200.4324329447426], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [81.87979999638628, 4.8064000002341345, 4.866499999479856], "resource_metrics": {"samples": 2, "duration_s": 0.11679267883300781, "gpu_memory_mean_mb": 666.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 4.4025, "gpu_power_peak_watts": 6.841, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1670.37109375, "cpu_memory_peak_mb": 1722.328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.1308587}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [42.27320000063628, 42.82750000129454], "ttft_ms": [5.585800005064812, 4.833400002098642], "tokens_processed": [8, 8], "throughput_tok_s": [189.24519553474985, 186.79586713579326], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.041799995524343, 6.364499997289386, 5.022099998313934], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 1.964, "gpu_power_peak_watts": 1.964, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1722.34375, "cpu_memory_peak_mb": 1722.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.251139}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.945600000966806, 40.82650000054855], "ttft_ms": [4.5118000052752905, 4.449499996553641], "tokens_processed": [8, 8], "throughput_tok_s": [210.82813290068336, 195.95115917094316], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.49819999630563, 4.366599998320453, 4.602399996656459], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 1.964, "gpu_power_peak_watts": 1.964, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1722.34375, "cpu_memory_peak_mb": 1722.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.3754578}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.7917999982601, 34.97799999604467], "ttft_ms": [4.535400003078394, 3.964700001233723], "tokens_processed": [8, 8], "throughput_tok_s": [196.11784722275615, 228.71519243251882], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.604300003848039, 4.284499998902902, 4.001299996161833], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 1.964, "gpu_power_peak_watts": 1.964, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1722.34375, "cpu_memory_peak_mb": 1722.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.4999716}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.26919999887468, 34.68289999727858], "ttft_ms": [6.8532999939634465, 6.717100004607346], "tokens_processed": [8, 8], "throughput_tok_s": [198.6629980288548, 230.66121923563847], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.261500005668495, 7.0360000026994385, 4.440999997314066], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 2.783, "gpu_power_peak_watts": 2.783, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1722.34375, "cpu_memory_peak_mb": 1722.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.6240962}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.384599995973986, 35.178199999791104], "ttft_ms": [3.967299999203533, 4.708300002675969], "tokens_processed": [8, 8], "throughput_tok_s": [232.66229652043944, 227.41356863192277], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.995700001018122, 4.334799996286165, 4.167099999904167], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 2.783, "gpu_power_peak_watts": 2.783, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1723.02734375, "cpu_memory_peak_mb": 1723.02734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.7507646}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [39.61880000133533, 33.699899999191985], "ttft_ms": [4.680500001995824, 4.655400000046939], "tokens_processed": [8, 8], "throughput_tok_s": [201.9243389433896, 237.38942846096916], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.893199999060016, 6.572399994183797, 4.55109999893466], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 2.783, "gpu_power_peak_watts": 2.783, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1723.65234375, "cpu_memory_peak_mb": 1723.65234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.875853}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.976899999775924, 23.542299997643568], "ttft_ms": [2.450400002999231, 3.0057999974815175], "tokens_processed": [8, 8], "throughput_tok_s": [348.1757765441821, 339.8138669883889], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.968900000269059, 2.8752999933203682, 2.6273000039509498], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 2.783, "gpu_power_peak_watts": 2.783, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1723.65234375, "cpu_memory_peak_mb": 1723.65234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597079.9983485}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.965899999893736, 22.623199998633936], "ttft_ms": [3.247599997848738, 2.7319000000716187], "tokens_processed": [8, 8], "throughput_tok_s": [333.8076183258493, 353.61929349000434], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.2998000024235807, 3.707599993504118, 3.843799997412134], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 5.137, "gpu_power_peak_watts": 5.137, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1723.65234375, "cpu_memory_peak_mb": 1723.65234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.1240287}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.8657999986317, 27.651299998979084], "ttft_ms": [3.195199999026954, 2.9037000058451667], "tokens_processed": [8, 8], "throughput_tok_s": [297.7763550836918, 289.3173196303743], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.5236000039731152, 2.9525999998440966, 3.044400000362657], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 5.137, "gpu_power_peak_watts": 5.137, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1723.765625, "cpu_memory_peak_mb": 1723.765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.248138}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.273799998511095, 29.719199999817647], "ttft_ms": [3.524799998558592, 3.54920000245329], "tokens_processed": [8, 8], "throughput_tok_s": [293.32179602536974, 269.1862499680034], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.130600002303254, 2.7908000047318637, 3.505000000586733], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 5.137, "gpu_power_peak_watts": 5.137, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1729.4921875, "cpu_memory_peak_mb": 1729.4921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.3716962}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.10679999756394, 25.958499994885642], "ttft_ms": [5.882500001462176, 3.3645999938016757], "tokens_processed": [8, 8], "throughput_tok_s": [249.16840048235855, 308.1842171764996], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.347699996083975, 3.4624000036274083, 3.3174999989569187], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 5.137, "gpu_power_peak_watts": 5.137, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1735.23046875, "cpu_memory_peak_mb": 1735.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.4975102}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.081800007086713, 25.904800000716932], "ttft_ms": [5.431300000054762, 3.4472999977879226], "tokens_processed": [8, 8], "throughput_tok_s": [257.3853508540684, 308.82307525163657], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7205000044195913, 3.3537999988766387, 3.1649999946239404], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 13.36, "gpu_power_peak_watts": 13.36, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1735.23046875, "cpu_memory_peak_mb": 1735.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.6232865}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.919399999722373, 29.39660000265576], "ttft_ms": [3.0160999958752654, 3.285999999206979], "tokens_processed": [8, 8], "throughput_tok_s": [276.63091212393067, 272.1403155221101], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.3055000021704473, 3.2302999970852397, 3.5271000015200116], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 13.36, "gpu_power_peak_watts": 13.36, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1735.23046875, "cpu_memory_peak_mb": 1735.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.7459674}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.08440000039991, 29.240399999252986], "ttft_ms": [3.3599999951547943, 3.3546000049682334], "tokens_processed": [8, 8], "throughput_tok_s": [306.69672294081323, 273.59406848758493], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.677900000591762, 3.518699995765928, 3.3096999977715313], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 13.36, "gpu_power_peak_watts": 13.36, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1735.23046875, "cpu_memory_peak_mb": 1735.23046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597080.8908434}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.51849999994738, 33.70820000418462], "ttft_ms": [5.030600004829466, 4.02149999717949], "tokens_processed": [8, 8], "throughput_tok_s": [253.81918555811208, 237.33097581617707], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.436600000190083, 3.4688000014284626, 3.486299996438902], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 47.0, "gpu_power_mean_watts": 13.36, "gpu_power_peak_watts": 13.36, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1736.80078125, "cpu_memory_peak_mb": 1736.80078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.0106556}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.78379999776371, 34.18789999705041], "ttft_ms": [3.919299997505732, 4.999099997803569], "tokens_processed": [8, 8], "throughput_tok_s": [236.79988635172933, 234.00091847379358], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.3054000052507035, 3.7652999963029288, 4.478099996049423], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.247, "gpu_power_peak_watts": 13.247, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1738.30078125, "cpu_memory_peak_mb": 1738.30078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.1352465}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.4765000010957, 29.64300000166986], "ttft_ms": [4.26290000177687, 3.314200002932921], "tokens_processed": [8, 8], "throughput_tok_s": [225.5013882359567, 269.87821743917084], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7688999946112745, 3.810500005783979, 4.011300006823149], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.247, "gpu_power_peak_watts": 13.247, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1738.30078125, "cpu_memory_peak_mb": 1738.30078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.2604482}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.654999998863786, 36.36230000120122], "ttft_ms": [3.3705999958328903, 3.8526999996975064], "tokens_processed": [8, 8], "throughput_tok_s": [252.7246880520344, 220.00808528986676], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.948600002331659, 3.7550000051851384, 3.574900001694914], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.247, "gpu_power_peak_watts": 13.247, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1738.30078125, "cpu_memory_peak_mb": 1738.30078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.385655}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.712000003200956, 32.712699998228345], "ttft_ms": [3.824999999778811, 3.5093000042252243], "tokens_processed": [8, 8], "throughput_tok_s": [269.25148085413764, 244.5533386248541], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9935999957378954, 3.9344999968307093, 6.83759999810718], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.247, "gpu_power_peak_watts": 13.247, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1738.30078125, "cpu_memory_peak_mb": 1738.30078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.5094926}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.18629999883706, 49.530600001162384], "ttft_ms": [6.15529999777209, 5.944299999100622], "tokens_processed": [32, 32], "throughput_tok_s": [650.5876636534279, 646.0652606519812], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.640700001909863, 5.916699999943376, 6.233300002350006], "resource_metrics": {"samples": 2, "duration_s": 0.11907958984375, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.739, "gpu_power_peak_watts": 13.739, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1754.8515625, "cpu_memory_peak_mb": 1762.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.7358122}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.36289999342989, 30.802600005699787], "ttft_ms": [3.8904999964870512, 3.083600000536535], "tokens_processed": [32, 32], "throughput_tok_s": [1089.8106115935475, 1038.8733416685163], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9517999975942075, 3.2773999992059544, 3.4694999994826503], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.739, "gpu_power_peak_watts": 13.739, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1762.6484375, "cpu_memory_peak_mb": 1762.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.854155}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.69940000528004, 29.20319999975618], "ttft_ms": [3.558700002031401, 4.102299993974157], "tokens_processed": [32, 32], "throughput_tok_s": [1115.0058884197135, 1095.770326548706], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.033600002003368, 4.029099996841978, 3.146700000797864], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 23.0, "gpu_power_mean_watts": 13.739, "gpu_power_peak_watts": 13.739, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1762.6484375, "cpu_memory_peak_mb": 1762.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597081.9810665}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.52129999944009, 29.014600004302338], "ttft_ms": [3.9150000011431985, 3.9065000019036233], "tokens_processed": [32, 32], "throughput_tok_s": [1015.1865564100597, 1102.8930261059943], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.779999999096617, 3.368500001670327, 4.088100002263673], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 16.6, "gpu_power_peak_watts": 16.6, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1762.6484375, "cpu_memory_peak_mb": 1762.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.1060941}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.605599994189106, 28.510299998742994], "ttft_ms": [3.3944999959203415, 3.353299995069392], "tokens_processed": [32, 32], "throughput_tok_s": [952.2222488374932, 1122.4013777971772], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.9600000018253922, 3.199800004949793, 3.8780999966547824], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 16.6, "gpu_power_peak_watts": 16.6, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1762.6484375, "cpu_memory_peak_mb": 1762.6484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.2317693}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.09299999597715, 37.69000000465894], "ttft_ms": [5.193500001041684, 5.179400002816692], "tokens_processed": [32, 32], "throughput_tok_s": [798.1443145489441, 849.0315732566838], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [22.42609999666456, 4.862100002355874, 5.056800000602379], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 16.6, "gpu_power_peak_watts": 16.6, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1763.66015625, "cpu_memory_peak_mb": 1763.66015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.3575046}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.90920000028564, 37.81610000442015], "ttft_ms": [4.230200000165496, 4.291699995519593], "tokens_processed": [32, 32], "throughput_tok_s": [844.1222711046101, 846.2004277611828], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.887700000836048, 4.757799993967637, 4.600400003255345], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 16.6, "gpu_power_peak_watts": 16.6, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1792.796875, "cpu_memory_peak_mb": 1792.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.48193}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.2742999936454, 37.77499999705469], "ttft_ms": [4.913299999316223, 4.888799994660076], "tokens_processed": [32, 32], "throughput_tok_s": [836.0701568758382, 847.1211119125091], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.706700005044695, 4.959899997629691, 4.66450000385521], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 18.786, "gpu_power_peak_watts": 18.786, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1792.796875, "cpu_memory_peak_mb": 1792.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.6056335}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.92339999927208, 37.68770000169752], "ttft_ms": [4.712800000561401, 4.434100002981722], "tokens_processed": [32, 32], "throughput_tok_s": [843.8061988274844, 849.0833879106091], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.073399999469984, 4.681999998865649, 4.986100000678562], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 18.786, "gpu_power_peak_watts": 18.786, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1792.796875, "cpu_memory_peak_mb": 1792.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.7405946}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.61329999926966, 37.721100001363084], "ttft_ms": [5.513999996765051, 5.171500000869855], "tokens_processed": [32, 32], "throughput_tok_s": [828.7299971928132, 848.3315703636334], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.460199997993186, 5.090099999506492, 4.883399997197557], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 31.0, "gpu_power_mean_watts": 18.786, "gpu_power_peak_watts": 18.786, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1792.796875, "cpu_memory_peak_mb": 1792.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 2172.1276999960537, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765597082.8613849}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.8810000011581, 6.366000001435168], "ttft_ms": [0.7513000018661842, 0.7181999972090125], "tokens_processed": [8, 8], "throughput_tok_s": [1162.621711764797, 1256.6760914540462], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [27.45169999980135, 1.090599995222874, 0.7411000042338856], "resource_metrics": {"samples": 2, "duration_s": 0.11449384689331055, "gpu_memory_mean_mb": 816.529296875, "gpu_memory_peak_mb": 909.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 18.0, "gpu_power_mean_watts": 19.608, "gpu_power_peak_watts": 20.43, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 1874.4609375, "cpu_memory_peak_mb": 1936.09375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32"}, "started_at": 1765597083.0826697}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.642799999099225, 6.7856000023311935], "ttft_ms": [0.7474000012734905, 0.6666000044788234], "tokens_processed": [8, 8], "throughput_tok_s": [1204.3114351003815, 1178.9672243061186], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.501099999586586, 0.988599997072015, 0.690799999574665], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 20.43, "gpu_power_peak_watts": 20.43, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2074.15625, "cpu_memory_peak_mb": 2074.15625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.1996098}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.269699995755218, 6.640300001890864], "ttft_ms": [0.726799997210037, 0.7843999992473982], "tokens_processed": [8, 8], "throughput_tok_s": [1275.9781178391709, 1204.7648446187602], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2312000035308301, 1.1003999970853329, 1.01600000198232], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 20.43, "gpu_power_peak_watts": 20.43, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2074.16796875, "cpu_memory_peak_mb": 2074.16796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.3244214}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.184099995356519, 6.554899999173358], "ttft_ms": [0.7504999957745895, 0.8288999961223453], "tokens_processed": [8, 8], "throughput_tok_s": [1113.570246122805, 1220.4610293076757], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5034000025480054, 0.7253000003402121, 0.6850999998277985], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 20.43, "gpu_power_peak_watts": 20.43, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2074.1796875, "cpu_memory_peak_mb": 2074.1796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.45034}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.815000000642613, 6.579900000360794], "ttft_ms": [0.7136999993235804, 0.7255999953486025], "tokens_processed": [8, 8], "throughput_tok_s": [1173.8811444234261, 1215.8239486255623], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3122999953338876, 0.9531000032438897, 0.760499999159947], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.18359375, "cpu_memory_peak_mb": 2074.18359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.5748892}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.073000000673346, 7.388900005025789], "ttft_ms": [0.7589000015286729, 0.727099999494385], "tokens_processed": [8, 8], "throughput_tok_s": [1131.0617841422882, 1082.7051380528296], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6096999970613979, 1.0772999958135188, 1.0959999999613501], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.7578125, "cpu_memory_peak_mb": 2074.7578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.7014077}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.535899996582884, 7.772199998726137], "ttft_ms": [1.0053000005427748, 0.9799000035854988], "tokens_processed": [8, 8], "throughput_tok_s": [1061.5852125993656, 1029.3095907608142], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.306099999055732, 0.8742999998503365, 0.6252000021049753], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.765625, "cpu_memory_peak_mb": 2074.765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.8311687}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.708700002694968, 7.646300000487827], "ttft_ms": [0.7430000041495077, 0.782499999331776], "tokens_processed": [8, 8], "throughput_tok_s": [1037.788472401727, 1046.2576670402166], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3311000002431683, 1.0920999993686564, 0.7614999994984828], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 31.035, "gpu_power_peak_watts": 31.035, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.77734375, "cpu_memory_peak_mb": 2074.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597083.9657977}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.140099995012861, 7.5303999983589165], "ttft_ms": [0.6047000060789287, 1.0580000016489066], "tokens_processed": [8, 8], "throughput_tok_s": [1120.4324877225463, 1062.3605654073385], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2718999932985753, 0.8390000002691522, 0.7603999983984977], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.97, "gpu_power_peak_watts": 30.97, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.83203125, "cpu_memory_peak_mb": 2074.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.091142}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.3118000000249594, 7.541899998614099], "ttft_ms": [1.0204999998677522, 0.6495999987237155], "tokens_processed": [8, 8], "throughput_tok_s": [1094.1218304620875, 1060.7406623622803], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4616999978898093, 1.000799995381385, 1.2090000018361025], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.97, "gpu_power_peak_watts": 30.97, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2074.83984375, "cpu_memory_peak_mb": 2074.83984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.217257}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.90619999699993, 10.280300004524179], "ttft_ms": [1.039199996739626, 1.3581000021076761], "tokens_processed": [8, 8], "throughput_tok_s": [898.2506571483701, 778.18740663982], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.860599993960932, 1.4700000028824434, 1.006599995889701], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.97, "gpu_power_peak_watts": 30.97, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2076.37890625, "cpu_memory_peak_mb": 2076.37890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.3399775}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.328899999673013, 10.115800003404729], "ttft_ms": [1.0936999969999306, 0.9337000010418706], "tokens_processed": [8, 8], "throughput_tok_s": [857.5501935148204, 790.8420488055712], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5932000023894943, 0.840000000607688, 1.3911999994888902], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 30.97, "gpu_power_peak_watts": 30.97, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2076.3984375, "cpu_memory_peak_mb": 2076.3984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.465796}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.167600004933774, 9.503900000709109], "ttft_ms": [1.458500002627261, 1.01379999978235], "tokens_processed": [8, 8], "throughput_tok_s": [786.8130135054528, 841.7596985872221], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5440999995917082, 1.066600001649931, 0.7436000014422461], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2076.41796875, "cpu_memory_peak_mb": 2076.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.5909605}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.246000001439825, 10.022099995694589], "ttft_ms": [1.1240000021643937, 1.448700000764802], "tokens_processed": [8, 8], "throughput_tok_s": [865.2390221451662, 798.2358990068682], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.502999999502208, 1.1757999964174815, 1.5702000018791296], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2076.41796875, "cpu_memory_peak_mb": 2076.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.716821}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.574600000632927, 9.224800000083633], "ttft_ms": [1.0347999996156432, 1.0461999991093762], "tokens_processed": [8, 8], "throughput_tok_s": [835.5440435601656, 867.2274737585066], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.1993999980622903, 1.1281999977654777, 0.9542000043438748], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2076.41796875, "cpu_memory_peak_mb": 2076.41796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.84297}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.358499999914784, 11.789799995312933], "ttft_ms": [1.292100001592189, 1.2723000036203302], "tokens_processed": [8, 8], "throughput_tok_s": [704.3183519003406, 678.5526474732753], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.1728000030852854, 1.396000006934628, 1.312200001848396], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.039, "gpu_power_peak_watts": 31.039, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2078.06640625, "cpu_memory_peak_mb": 2078.06640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597084.968319}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.465099996712524, 12.176299998827744], "ttft_ms": [1.2650000062421896, 1.266100000066217], "tokens_processed": [8, 8], "throughput_tok_s": [697.7697536256902, 657.0140355255857], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.042200001596939, 1.7672999965725467, 1.413599995430559], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.115, "gpu_power_peak_watts": 31.115, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2078.06640625, "cpu_memory_peak_mb": 2078.06640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.0931745}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.993000000074971, 11.783399997511879], "ttft_ms": [1.2741999962599948, 1.1140999995404854], "tokens_processed": [8, 8], "throughput_tok_s": [667.055782535645, 678.9211943657381], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.079300000332296, 1.181300001917407, 1.7250000018975697], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.115, "gpu_power_peak_watts": 31.115, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2078.06640625, "cpu_memory_peak_mb": 2078.06640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.2195685}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.172600007033907, 11.27460000134306], "ttft_ms": [1.5806000010343269, 1.6970000069704838], "tokens_processed": [8, 8], "throughput_tok_s": [657.213741959583, 709.5595408304524], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.656700002262369, 1.1763000002247281, 1.2324999988777563], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.115, "gpu_power_peak_watts": 31.115, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2078.06640625, "cpu_memory_peak_mb": 2078.06640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.3436913}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.10210000135703, 11.930100001336541], "ttft_ms": [1.4045999996596947, 1.207799999974668], "tokens_processed": [8, 8], "throughput_tok_s": [661.0422983699478, 670.5727528774908], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.98610000370536, 1.6749999995226972, 1.223200000822544], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.115, "gpu_power_peak_watts": 31.115, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2078.0703125, "cpu_memory_peak_mb": 2078.0703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.4699929}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.034399997617584, 22.316399998089764], "ttft_ms": [2.5555999964126386, 3.1686000002082437], "tokens_processed": [32, 32], "throughput_tok_s": [1331.4249576928073, 1433.923034303881], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.691899998055305, 2.708200001507066, 3.42100000125356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 951.0390625, "gpu_memory_peak_mb": 951.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.142, "gpu_power_peak_watts": 31.142, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2073.8984375, "cpu_memory_peak_mb": 2073.8984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.5946174}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.276799998711795, 22.19449999392964], "ttft_ms": [3.342500000144355, 2.463600001647137], "tokens_processed": [32, 32], "throughput_tok_s": [1374.7594171780904, 1441.7986442024933], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.617600003257394, 2.4481000000378117, 2.5781000003917143], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.142, "gpu_power_peak_watts": 31.142, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2091.28515625, "cpu_memory_peak_mb": 2091.28515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.7197864}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.875700004282407, 22.64399999694433], "ttft_ms": [2.6482000030227937, 2.3098000019672327], "tokens_processed": [32, 32], "throughput_tok_s": [1340.2748398690048, 1413.1778839568187], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.608100003679283, 2.5378999998793006, 3.124099996057339], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.142, "gpu_power_peak_watts": 31.142, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2091.3046875, "cpu_memory_peak_mb": 2091.3046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.8457792}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.014100002706982, 22.853700000268873], "ttft_ms": [2.3218999995151535, 2.9110000032233074], "tokens_processed": [32, 32], "throughput_tok_s": [1453.6138200546511, 1400.2109067513584], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.9010000002454035, 2.421700002742, 3.100800000538584], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.142, "gpu_power_peak_watts": 31.142, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2091.3046875, "cpu_memory_peak_mb": 2091.3046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597085.9691021}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.796599998197053, 23.065599998517428], "ttft_ms": [3.3819000018411316, 2.993300004163757], "tokens_processed": [32, 32], "throughput_tok_s": [1468.1188810478213, 1387.3473918760772], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.795399999944493, 3.279399999883026, 2.376399999775458], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.385, "gpu_power_peak_watts": 32.385, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2091.3046875, "cpu_memory_peak_mb": 2091.3046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.0945134}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.87270000105491, 31.759299999976065], "ttft_ms": [4.187799997453112, 3.8783999989391305], "tokens_processed": [32, 32], "throughput_tok_s": [1036.5144609608674, 1007.5788824068577], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.277699998463504, 3.2089000014821067, 3.8735999987693503], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.385, "gpu_power_peak_watts": 32.385, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.4375, "cpu_memory_peak_mb": 2097.4375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.218819}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.462799993983936, 30.677800001285505], "ttft_ms": [4.349800001364201, 3.4978999974555336], "tokens_processed": [32, 32], "throughput_tok_s": [1050.4615467494664, 1043.099570329655], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.543100003909785, 3.133400001388509, 3.7097000022185966], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.385, "gpu_power_peak_watts": 32.385, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.4375, "cpu_memory_peak_mb": 2097.4375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.3433082}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.644899994309526, 31.59600000071805], "ttft_ms": [3.500099999655504, 4.49300000036601], "tokens_processed": [32, 32], "throughput_tok_s": [1079.4436819197415, 1012.7864286388394], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.946300007053651, 3.613599998061545, 3.985200004535727], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.385, "gpu_power_peak_watts": 32.385, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.44921875, "cpu_memory_peak_mb": 2097.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.4698293}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.253000001015607, 30.281400002422743], "ttft_ms": [3.563800004485529, 3.34619999921415], "tokens_processed": [32, 32], "throughput_tok_s": [1023.901705403005, 1056.7543111428056], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.928600003244355, 3.348700003698468, 4.196900001261383], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.966, "gpu_power_peak_watts": 32.966, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.44921875, "cpu_memory_peak_mb": 2097.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.5940404}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.49149999767542, 31.459099998755846], "ttft_ms": [3.9887000020826235, 4.290099997888319], "tokens_processed": [32, 32], "throughput_tok_s": [1085.0584067450725, 1017.1937531990917], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.3045999995665625, 3.0547999995178543, 3.8953999974182807], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.966, "gpu_power_peak_watts": 32.966, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.44921875, "cpu_memory_peak_mb": 2097.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 154.9760999987484, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.716134}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.984500003454741, 6.191899999976158], "ttft_ms": [0.7269000052474439, 0.7239000042318366], "tokens_processed": [8, 8], "throughput_tok_s": [1145.3933704693195, 1292.0105298907934], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8506000051274896, 1.047199999447912, 0.7362000033026561], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.966, "gpu_power_peak_watts": 32.966, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2081.33203125, "cpu_memory_peak_mb": 2081.33203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16"}, "started_at": 1765597086.8397627}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.429699998989236, 6.554400002642069], "ttft_ms": [0.7651999985682778, 0.6738000010955147], "tokens_processed": [8, 8], "throughput_tok_s": [1244.2260138509757, 1220.5541310837318], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8152999982703477, 0.8858999935910106, 0.7590000022901222], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 32.966, "gpu_power_peak_watts": 32.966, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.1953125, "cpu_memory_peak_mb": 2086.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597086.9656658}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.623999994189944, 6.83950000529876], "ttft_ms": [0.9535999997751787, 0.8044999995036051], "tokens_processed": [8, 8], "throughput_tok_s": [1207.72946965836, 1169.676145010917], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1528000031830743, 0.6859999994048849, 0.7191999975475483], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.05, "gpu_power_peak_watts": 31.05, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.1953125, "cpu_memory_peak_mb": 2086.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.0892298}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.167699997604359, 6.530799997563008], "ttft_ms": [0.7278000048245303, 0.673100003041327], "tokens_processed": [8, 8], "throughput_tok_s": [1297.0799492691503, 1224.96478271961], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.43680000473978, 0.9426999968127348, 0.6686999986413866], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.05, "gpu_power_peak_watts": 31.05, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.19921875, "cpu_memory_peak_mb": 2086.19921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.2147338}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.332100005238317, 7.511800002248492], "ttft_ms": [0.7060000061756, 0.7029999978840351], "tokens_processed": [8, 8], "throughput_tok_s": [1263.4039249825319, 1064.9910803809175], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4024000047356822, 0.9600000048521906, 0.7106999983079731], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.05, "gpu_power_peak_watts": 31.05, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.19921875, "cpu_memory_peak_mb": 2086.19921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.3403282}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.624199999554548, 8.407199995417614], "ttft_ms": [0.8495000001857989, 1.0231000051135197], "tokens_processed": [8, 8], "throughput_tok_s": [1049.2904174165694, 951.5653254782139], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6721000065444969, 0.796799999079667, 0.7897999967099167], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.05, "gpu_power_peak_watts": 31.05, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.77734375, "cpu_memory_peak_mb": 2086.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.4647214}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.439400003931951, 7.449299999279901], "ttft_ms": [1.0783000034280121, 0.7134999978006817], "tokens_processed": [8, 8], "throughput_tok_s": [1075.355538856864, 1073.926409296623], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2645000024349429, 0.7085999968694523, 0.9168999968096614], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.061, "gpu_power_peak_watts": 31.061, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.77734375, "cpu_memory_peak_mb": 2086.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.5902956}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.215799996629357, 7.056100002955645], "ttft_ms": [0.6939000013517216, 0.7609000022057444], "tokens_processed": [8, 8], "throughput_tok_s": [1108.6781789596382, 1133.7707794176636], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1380000069038942, 1.1012999966624193, 1.0136999990209006], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.061, "gpu_power_peak_watts": 31.061, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.77734375, "cpu_memory_peak_mb": 2086.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.7136507}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.732100002409425, 7.205400004750118], "ttft_ms": [0.8089999973890372, 0.8041999972192571], "tokens_processed": [8, 8], "throughput_tok_s": [1188.33647704829, 1110.2784015774346], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9918000034522265, 0.8233999978983775, 0.800499998149462], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.061, "gpu_power_peak_watts": 31.061, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.77734375, "cpu_memory_peak_mb": 2086.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.8394923}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.812400002672803, 8.179300006304402], "ttft_ms": [0.8156999974744394, 0.8292999991681427], "tokens_processed": [8, 8], "throughput_tok_s": [1174.3291640040588, 978.0788079461321], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4001000017742626, 1.1109999977634288, 1.0077999977511354], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.061, "gpu_power_peak_watts": 31.061, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2086.77734375, "cpu_memory_peak_mb": 2086.77734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597087.9650855}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.447099997487385, 9.807399997953326], "ttft_ms": [0.9044000034919009, 1.4129999981378205], "tokens_processed": [8, 8], "throughput_tok_s": [846.8207176940791, 815.7105860543563], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6021000046748668, 1.2351000041235238, 1.2732000031974167], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.048, "gpu_power_peak_watts": 31.048, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2088.3125, "cpu_memory_peak_mb": 2088.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.090131}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.458499996981118, 9.561900005792268], "ttft_ms": [1.1407000056351535, 1.0316999978385866], "tokens_processed": [8, 8], "throughput_tok_s": [845.8000742774616, 836.6538026076279], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4909000019542873, 1.095200001145713, 1.3717000038013794], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.048, "gpu_power_peak_watts": 31.048, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2088.3125, "cpu_memory_peak_mb": 2088.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.2126117}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.878400000161491, 10.025000003224704], "ttft_ms": [1.064700001734309, 1.4653999969596043], "tokens_processed": [8, 8], "throughput_tok_s": [809.8477486100195, 798.0049872744808], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6692999997758307, 1.1346999963279814, 1.390400000673253], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.048, "gpu_power_peak_watts": 31.048, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2088.3125, "cpu_memory_peak_mb": 2088.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.338212}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.649900006479584, 9.212999997544102], "ttft_ms": [1.050400001986418, 1.0265999953844585], "tokens_processed": [8, 8], "throughput_tok_s": [829.0241344084673, 868.3382179672799], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.119600001606159, 1.1321999991196208, 0.9429999990970828], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.048, "gpu_power_peak_watts": 31.048, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2088.3125, "cpu_memory_peak_mb": 2088.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.4640994}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.802699998544995, 9.850199996435549], "ttft_ms": [1.4899999951012433, 1.0184999991906807], "tokens_processed": [8, 8], "throughput_tok_s": [816.1016863912424, 812.1662507253585], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.564199999847915, 0.9867000044323504, 1.3996999987284653], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.106, "gpu_power_peak_watts": 31.106, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2088.328125, "cpu_memory_peak_mb": 2088.328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.5896096}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.635700000624638, 11.50210000196239], "ttft_ms": [1.5794999999343418, 1.4308000027085654], "tokens_processed": [8, 8], "throughput_tok_s": [687.5392111837308, 695.5251648512105], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.568500000052154, 1.1804999958258122, 1.1578999983612448], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.106, "gpu_power_peak_watts": 31.106, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2089.87109375, "cpu_memory_peak_mb": 2089.87109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.7136912}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.67880000139121, 11.640400000032969], "ttft_ms": [1.0361999957240187, 1.240700003108941], "tokens_processed": [8, 8], "throughput_tok_s": [685.0018836735811, 687.261606128427], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.6542999985395, 1.2223000012454577, 1.2698999998974614], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.106, "gpu_power_peak_watts": 31.106, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2089.87109375, "cpu_memory_peak_mb": 2089.87109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.8392453}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.156699998537078, 10.96510000206763], "ttft_ms": [1.163499997346662, 1.4594999956898391], "tokens_processed": [8, 8], "throughput_tok_s": [717.0579114835928, 729.5875093242637], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8569999956525862, 0.9537999940221198, 1.752299998770468], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.106, "gpu_power_peak_watts": 31.106, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2089.87109375, "cpu_memory_peak_mb": 2089.87109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597088.9622772}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.7612999965786, 11.504199996124953], "ttft_ms": [1.6355000043404289, 1.6239999968092889], "tokens_processed": [8, 8], "throughput_tok_s": [680.1969172053451, 695.398202629883], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9834999984595925, 1.208100002259016, 0.9151999984169379], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.151, "gpu_power_peak_watts": 31.151, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2089.875, "cpu_memory_peak_mb": 2089.875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.0870214}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.470799996459391, 10.93859999673441], "ttft_ms": [1.3087999977869913, 1.5003000007709488], "tokens_processed": [8, 8], "throughput_tok_s": [697.4230221492226, 731.3550182279547], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.919800000905525, 0.9912000023177825, 1.7069999958039261], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.151, "gpu_power_peak_watts": 31.151, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2089.875, "cpu_memory_peak_mb": 2089.875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.213652}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.972300004563294, 21.083199993881863], "ttft_ms": [2.3475999987567775, 2.9794000001857057], "tokens_processed": [32, 32], "throughput_tok_s": [1456.3791680140046, 1517.7961604161649], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.855900002643466, 2.9121000043232925, 2.4448999974993058], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.151, "gpu_power_peak_watts": 31.151, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2085.32421875, "cpu_memory_peak_mb": 2085.32421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.3380277}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.997999999963213, 23.086199995304924], "ttft_ms": [2.38280000485247, 2.3189999992609955], "tokens_processed": [32, 32], "throughput_tok_s": [1523.954662351465, 1386.1094509493935], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.055400004494004, 2.4176000006264076, 3.0948999992688186], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.151, "gpu_power_peak_watts": 31.151, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2101.19921875, "cpu_memory_peak_mb": 2101.19921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.4624152}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.846300002129283, 22.71689999906812], "ttft_ms": [3.207100002327934, 2.7742000020225532], "tokens_processed": [32, 32], "throughput_tok_s": [1400.6644400632745, 1408.642904679454], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.8438999981735833, 2.9016999978921376, 2.587599999969825], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.424, "gpu_power_peak_watts": 31.424, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2101.203125, "cpu_memory_peak_mb": 2101.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.5865016}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.024399997666478, 21.56900000409223], "ttft_ms": [3.2735000058892183, 2.434799993352499], "tokens_processed": [32, 32], "throughput_tok_s": [1389.8299197044525, 1483.610737351232], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.6935000025550835, 3.153100005874876, 2.355300006456673], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.424, "gpu_power_peak_watts": 31.424, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2101.203125, "cpu_memory_peak_mb": 2101.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.7125926}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.784900000260677, 21.85540000209585], "ttft_ms": [3.078400004596915, 2.4062000011326745], "tokens_processed": [32, 32], "throughput_tok_s": [1404.4389046971412, 1464.1690381750655], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.835700001218356, 3.042500000447035, 2.3602999935974367], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.424, "gpu_power_peak_watts": 31.424, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2101.2109375, "cpu_memory_peak_mb": 2101.2109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.8492994}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.106400001386646, 30.477299995254725], "ttft_ms": [3.330400002596434, 4.619500003173016], "tokens_processed": [32, 32], "throughput_tok_s": [1062.8969255216878, 1049.9617749926128], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.417899996449705, 4.028900002595037, 3.874900001392234], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 13.0, "gpu_power_mean_watts": 31.424, "gpu_power_peak_watts": 31.424, "gpu_temperature_mean_c": 50.0, "gpu_temperature_peak_c": 50, "cpu_memory_mean_mb": 2107.34375, "cpu_memory_peak_mb": 2107.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597089.9787374}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.82340000057593, 29.206100000010338], "ttft_ms": [4.385899999761023, 2.94469999789726], "tokens_processed": [32, 32], "throughput_tok_s": [1005.5493755984864, 1095.6615227636923], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.429499997466337, 2.9495000053429976, 3.68989999697078], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 24.431, "gpu_power_peak_watts": 24.431, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.34375, "cpu_memory_peak_mb": 2107.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.1061928}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.01279999827966, 30.42260000074748], "ttft_ms": [3.891499996825587, 3.920399998605717], "tokens_processed": [32, 32], "throughput_tok_s": [1031.8320178047484, 1051.849611775909], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.162500001664739, 4.165199999988545, 3.3055000021704473], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 24.431, "gpu_power_peak_watts": 24.431, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.34375, "cpu_memory_peak_mb": 2107.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.228582}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.193300000973977, 30.808499999693595], "ttft_ms": [3.27839999954449, 4.058599995914847], "tokens_processed": [32, 32], "throughput_tok_s": [1059.8377785458279, 1038.6743918177858], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.828100001963321, 4.4793999986723065, 4.6612000005552545], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 24.431, "gpu_power_peak_watts": 24.431, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.34375, "cpu_memory_peak_mb": 2107.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.3565953}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.55939999851398, 29.06710000388557], "ttft_ms": [3.4530999982962385, 4.041000000142958], "tokens_processed": [32, 32], "throughput_tok_s": [1013.9609752247117, 1100.90101853031], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.377000005159061, 4.291799996281043, 3.6082000005990267], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 24.431, "gpu_power_peak_watts": 24.431, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.35546875, "cpu_memory_peak_mb": 2107.35546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.45070000248961, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.4805846}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.695900003251154, 6.924799999978859], "ttft_ms": [0.9717000066302717, 0.8815999972284772], "tokens_processed": [8, 8], "throughput_tok_s": [1194.7609725527036, 1155.268022184673], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0125000010011718, 0.9037000054377131, 1.0660999978426844], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 23.764, "gpu_power_peak_watts": 23.764, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2092.203125, "cpu_memory_peak_mb": 2092.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8"}, "started_at": 1765597090.6035073}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.334099998639431, 6.768199993530288], "ttft_ms": [0.9499999941908754, 0.9028999993461184], "tokens_processed": [8, 8], "throughput_tok_s": [1263.0050049286253, 1181.99816903271], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1015999989467673, 0.8240000024670735, 0.9471000012126751], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 23.764, "gpu_power_peak_watts": 23.764, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.25, "cpu_memory_peak_mb": 2097.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.7277238}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.026600003882777, 6.760700001905207], "ttft_ms": [0.8968999973149039, 0.7200999971246347], "tokens_processed": [8, 8], "throughput_tok_s": [1138.5307254688382, 1183.3094202886612], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5894999960437417, 0.8211000022129156, 0.7983999967109412], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 23.764, "gpu_power_peak_watts": 23.764, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.25, "cpu_memory_peak_mb": 2097.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.8531744}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.668800000625197, 6.950199996936135], "ttft_ms": [0.5971999999019317, 0.9041000012075529], "tokens_processed": [8, 8], "throughput_tok_s": [1199.6161227282275, 1151.0460135717892], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5032000010251068, 0.7725999967078678, 0.6816000022809021], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 23.764, "gpu_power_peak_watts": 23.764, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.328125, "cpu_memory_peak_mb": 2097.328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597090.9771821}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.405899999663234, 6.779700001061428], "ttft_ms": [0.7399000023724511, 0.7726000039838254], "tokens_processed": [8, 8], "throughput_tok_s": [1248.8487176541266, 1179.9932148542741], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.494000003731344, 0.6961999970371835, 0.7219999970402569], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 21.265, "gpu_power_peak_watts": 21.265, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.33203125, "cpu_memory_peak_mb": 2097.33203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.1020803}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.20920000114711, 7.696099994063843], "ttft_ms": [1.0529999999562278, 0.8548999976483174], "tokens_processed": [8, 8], "throughput_tok_s": [974.516396102193, 1039.487533448183], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6007999947760254, 0.7587000000057742, 0.8233999978983775], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 21.265, "gpu_power_peak_watts": 21.265, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.94140625, "cpu_memory_peak_mb": 2097.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.2279413}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.193700004194397, 13.133600004948676], "ttft_ms": [1.5548999945167452, 1.58940000255825], "tokens_processed": [8, 8], "throughput_tok_s": [606.3500001862045, 609.1246875940823], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.09280000126455, 1.9000999964191578, 2.0311999978730455], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 21.265, "gpu_power_peak_watts": 21.265, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.953125, "cpu_memory_peak_mb": 2097.953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.3693674}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.175200001569465, 12.88120000390336], "ttft_ms": [1.4953999998397194, 1.5093999973032624], "tokens_processed": [8, 8], "throughput_tok_s": [607.2014086349368, 621.0601494872977], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.806899999792222, 1.5879999991739169, 1.5504000039072707], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 21.265, "gpu_power_peak_watts": 21.265, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2097.953125, "cpu_memory_peak_mb": 2097.953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.492426}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.781000004906673, 13.519799998903181], "ttft_ms": [1.6238000025623478, 1.5664000020478852], "tokens_processed": [8, 8], "throughput_tok_s": [625.9291132875965, 591.7247297037688], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.1686999971279874, 1.5581000043312088, 1.5184000003500842], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 18.417, "gpu_power_peak_watts": 18.417, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.96484375, "cpu_memory_peak_mb": 2097.96484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.618685}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.465899995935615, 12.823500001104549], "ttft_ms": [1.5817999956198037, 1.5724999975645915], "tokens_processed": [8, 8], "throughput_tok_s": [594.0932282591303, 623.8546418147091], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7521000045235269, 1.6472999996040016, 1.58200000441866], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 18.417, "gpu_power_peak_watts": 18.417, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.96484375, "cpu_memory_peak_mb": 2097.96484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.7434819}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [17.27000000391854, 17.08610000059707], "ttft_ms": [2.1706000043195672, 1.9801000016741455], "tokens_processed": [8, 8], "throughput_tok_s": [463.23103637433763, 468.216854619863], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.7332000026945025, 2.094399998895824, 2.0589999985531904], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 18.417, "gpu_power_peak_watts": 18.417, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2099.5, "cpu_memory_peak_mb": 2099.5, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.865416}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [17.05729999957839, 17.261500004678965], "ttft_ms": [2.0758000027853996, 2.0813000010093674], "tokens_processed": [8, 8], "throughput_tok_s": [469.0074044659904, 463.45914305428215], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.487899997504428, 2.064200001768768, 2.1311000018613413], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 18.417, "gpu_power_peak_watts": 18.417, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2099.5, "cpu_memory_peak_mb": 2099.5, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597091.989579}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [17.092999994929414, 17.19790000061039], "ttft_ms": [2.0409999997355044, 2.1309000003384426], "tokens_processed": [8, 8], "throughput_tok_s": [468.0278477957746, 465.173073440133], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.5433000046177767, 2.0414000027813017, 2.17759999941336], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.697, "gpu_power_peak_watts": 14.697, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2099.5546875, "cpu_memory_peak_mb": 2099.5546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.1151204}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [17.443599994294345, 17.41570000012871], "ttft_ms": [2.122199999575969, 2.1310999945853837], "tokens_processed": [8, 8], "throughput_tok_s": [458.62092702290425, 459.3556388741697], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.531099999032449, 2.1228999976301566, 2.100999998219777], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.697, "gpu_power_peak_watts": 14.697, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2099.5546875, "cpu_memory_peak_mb": 2099.5546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.2395768}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.225599998317193, 21.967200002109166], "ttft_ms": [3.082900002482347, 2.0674999977927655], "tokens_processed": [8, 8], "throughput_tok_s": [330.22917907319993, 364.1793218631362], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.139499996905215, 2.846199997293297, 2.4845999942044728], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.697, "gpu_power_peak_watts": 14.697, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2099.55859375, "cpu_memory_peak_mb": 2099.55859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.3659935}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.338999996776693, 24.25809999840567], "ttft_ms": [3.091600003244821, 2.9066999995848164], "tokens_processed": [8, 8], "throughput_tok_s": [282.29648191220326, 329.78675166339445], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.783200001635123, 2.9108000017004088, 3.1889000019873492], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.697, "gpu_power_peak_watts": 14.697, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.09375, "cpu_memory_peak_mb": 2101.09375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.491094}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.46679999754997, 25.309099997684825], "ttft_ms": [2.597400001832284, 2.8326999963610433], "tokens_processed": [8, 8], "throughput_tok_s": [340.9071539722175, 316.0918405131675], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.73770000442164, 3.1543000004603527, 3.0726000040885992], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.53, "gpu_power_peak_watts": 14.53, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.1015625, "cpu_memory_peak_mb": 2101.1015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.6184072}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.152999998885207, 28.920700002345257], "ttft_ms": [2.6182000001426786, 3.0046999963815324], "tokens_processed": [8, 8], "throughput_tok_s": [305.89224946816836, 276.6184774003139], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.6477000030572526, 2.925900000263937, 2.9448999994201586], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.53, "gpu_power_peak_watts": 14.53, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.1015625, "cpu_memory_peak_mb": 2101.1015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.7432768}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.43779999925755, 24.975200001790654], "ttft_ms": [3.218199999537319, 4.2831000027945265], "tokens_processed": [8, 8], "throughput_tok_s": [302.5970390964703, 320.31775519020556], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.951200000301469, 2.7245000019320287, 3.261099998780992], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.53, "gpu_power_peak_watts": 14.53, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.10546875, "cpu_memory_peak_mb": 2101.10546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.8691442}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.758299998822622, 24.735999999393243], "ttft_ms": [4.862000001594424, 3.247900000133086], "tokens_processed": [8, 8], "throughput_tok_s": [288.2020873158415, 323.4152652084506], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.54920000245329, 3.3114000034402125, 3.046999998332467], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 14.53, "gpu_power_peak_watts": 14.53, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.109375, "cpu_memory_peak_mb": 2101.109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597092.9934304}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.71340000338387, 47.38690000522183], "ttft_ms": [5.579200005740859, 5.706500000087544], "tokens_processed": [32, 32], "throughput_tok_s": [643.6896289093452, 675.2921165232108], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.436799998686183, 6.453699999838136, 5.358499998692423], "resource_metrics": {"samples": 2, "duration_s": 0.11480498313903809, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 14.143, "gpu_power_peak_watts": 14.143, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2104.173828125, "cpu_memory_peak_mb": 2111.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.2192616}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.818300000333693, 24.346099999092985], "ttft_ms": [2.9188999978941865, 3.608300001360476], "tokens_processed": [32, 32], "throughput_tok_s": [1289.3711494973365, 1314.378894409871], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.25660000211792, 5.684600000677165, 5.77270000212593], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 14.143, "gpu_power_peak_watts": 14.143, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.94140625, "cpu_memory_peak_mb": 2111.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.352048}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.949499998707324, 22.1570000067004], "ttft_ms": [2.6520999963395298, 2.490600003511645], "tokens_processed": [32, 32], "throughput_tok_s": [1336.144804765327, 1444.2388405615843], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.835500003537163, 2.2166000053402968, 2.791799997794442], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 14.143, "gpu_power_peak_watts": 14.143, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.94140625, "cpu_memory_peak_mb": 2111.94140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.4763749}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.582599998509977, 23.09409999725176], "ttft_ms": [2.938400000857655, 3.008400002727285], "tokens_processed": [32, 32], "throughput_tok_s": [1482.6758593593552, 1385.6352922957838], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.769899998791516, 3.376399996341206, 2.444699995976407], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 16.06, "gpu_power_peak_watts": 16.06, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.953125, "cpu_memory_peak_mb": 2111.953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.5998564}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.110100006510038, 22.325100006128196], "ttft_ms": [2.8074000001652166, 2.3265999989234842], "tokens_processed": [32, 32], "throughput_tok_s": [1327.244598378256, 1433.3642398563081], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.4853999934275635, 2.780800001346506, 2.6699000009102747], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 16.06, "gpu_power_peak_watts": 16.06, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2111.953125, "cpu_memory_peak_mb": 2111.953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.724071}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.671500000811648, 29.426400004012976], "ttft_ms": [4.0865000046323985, 3.8561999972444028], "tokens_processed": [32, 32], "throughput_tok_s": [1010.3721010744655, 1087.4588803127822], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.50570000289008, 4.358400001365226, 3.7688000011257827], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 16.06, "gpu_power_peak_watts": 16.06, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.08984375, "cpu_memory_peak_mb": 2118.08984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.8489535}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.830399999103975, 31.17499999643769], "ttft_ms": [3.344299999298528, 4.152900000917725], "tokens_processed": [32, 32], "throughput_tok_s": [1037.9365821050008, 1026.4635125471236], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.471500004408881, 4.115199997613672, 3.7052000043331645], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 34.0, "gpu_power_mean_watts": 16.06, "gpu_power_peak_watts": 16.06, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.09375, "cpu_memory_peak_mb": 2118.09375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597093.9744558}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.56170000531711, 30.72299999621464], "ttft_ms": [4.414100003486965, 3.269100001489278], "tokens_processed": [32, 32], "throughput_tok_s": [982.7496720003751, 1041.564951467718], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.6547000024002045, 2.9521999967982993, 3.9127000054577366], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.333, "gpu_power_peak_watts": 18.333, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.09375, "cpu_memory_peak_mb": 2118.09375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597094.0975888}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.36480000487063, 31.28689999721246], "ttft_ms": [3.9562999954796396, 4.2567999989842065], "tokens_processed": [32, 32], "throughput_tok_s": [988.7284950064351, 1022.7922869587935], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.215800000063609, 4.087199995410629, 3.4624999971129], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.333, "gpu_power_peak_watts": 18.333, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.09375, "cpu_memory_peak_mb": 2118.09375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597094.221157}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.27040000294801, 31.587699995725416], "ttft_ms": [3.7482999978237785, 3.529100002197083], "tokens_processed": [32, 32], "throughput_tok_s": [961.8159083499014, 1013.0525490722774], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.01610000396613, 4.260000001522712, 4.221499999403022], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 35.0, "gpu_power_mean_watts": 18.333, "gpu_power_peak_watts": 18.333, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2118.09765625, "cpu_memory_peak_mb": 2118.09765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765596917.9415715}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "build_time_s": 20.716067200002726, "file_size_mb": 4.992092132568359, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596938.7016673, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "build_time_s": 42.63142470000457, "file_size_mb": 4.228824615478516, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765596984.085151, "built": true, "reused": false}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "build_time_s": 25.155046700005187, "file_size_mb": 5.178691864013672, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765597012.6948059, "built": true, "reused": false}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 25.381500003277324, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765597094.3527894}
