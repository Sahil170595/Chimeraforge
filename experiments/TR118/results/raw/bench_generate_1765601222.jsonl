{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [12.17200000246521, 11.351100001775194], "ttft_ms": [1.7619999998714775, 1.4319000038085505], "tokens_processed": [8, 8], "throughput_tok_s": [657.2461385458223, 704.7775104394185], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2594.925299999886, 264.88619999872753, 1.6145999979926273], "resource_metrics": {"samples": 21, "duration_s": 2.8886871337890625, "gpu_memory_mean_mb": 497.3528645833333, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.800142857142855, "gpu_power_peak_watts": 28.823, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 998.0872395833334, "cpu_memory_peak_mb": 1370.50390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601290.4782126}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.430500002461486, 10.816199996042997], "ttft_ms": [1.340999995591119, 1.094400002330076], "tokens_processed": [8, 8], "throughput_tok_s": [699.8818947795154, 739.6312940706277], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6969999996945262, 1.0987999994540587, 1.1024999985238537], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.991, "gpu_power_peak_watts": 28.991, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1372.6328125, "cpu_memory_peak_mb": 1372.6328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601290.5949664}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.074099995312281, 10.356399994634558], "ttft_ms": [1.1408999998820946, 1.223200000822544], "tokens_processed": [8, 8], "throughput_tok_s": [722.4063358093604, 772.4691981909389], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5518000000156462, 1.3443999996525235, 1.5201000060187653], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.991, "gpu_power_peak_watts": 28.991, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1372.7578125, "cpu_memory_peak_mb": 1372.7578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601290.7205024}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.767699997813907, 10.76910000119824], "ttft_ms": [1.1079999967478216, 1.5364000064437278], "tokens_processed": [8, 8], "throughput_tok_s": [742.9627498559754, 742.8661632921849], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4899000016157515, 1.4203999962774105, 1.435599995602388], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.991, "gpu_power_peak_watts": 28.991, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1372.76953125, "cpu_memory_peak_mb": 1372.76953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601290.845414}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.672099997464102, 11.017800003173761], "ttft_ms": [1.2581999981193803, 1.3202999980421737], "tokens_processed": [8, 8], "throughput_tok_s": [749.6181634262193, 726.0977688554466], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5061000012792647, 1.3808000003336929, 1.3973999957670458], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 562.01953125, "gpu_memory_peak_mb": 562.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 28.991, "gpu_power_peak_watts": 28.991, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1372.78125, "cpu_memory_peak_mb": 1372.78125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601290.97109}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.63150000164751, 47.77549999562325], "ttft_ms": [7.133500002964865, 5.18010000087088], "tokens_processed": [8, 8], "throughput_tok_s": [164.50243154599346, 167.44984355439266], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [681.2688000063645, 5.683200004568789, 5.860199999005999], "resource_metrics": {"samples": 8, "duration_s": 0.7822062969207764, "gpu_memory_mean_mb": 563.76953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.464875, "gpu_power_peak_watts": 29.713, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1380.0234375, "cpu_memory_peak_mb": 1415.6171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601291.8608}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.571800005447585, 50.65170000307262], "ttft_ms": [5.991099998936988, 5.706800002371892], "tokens_processed": [8, 8], "throughput_tok_s": [164.70462282852927, 157.94139188842044], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.474399997387081, 6.444500002544373, 5.405900003097486], "resource_metrics": {"samples": 2, "duration_s": 0.10875391960144043, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 29.713, "gpu_power_peak_watts": 29.713, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1415.671875, "cpu_memory_peak_mb": 1415.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601292.084246}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [47.91539999860106, 49.29279999487335], "ttft_ms": [5.490000003192108, 5.5445999969379045], "tokens_processed": [8, 8], "throughput_tok_s": [166.96093532003422, 162.29550767722736], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.463699999381788, 5.722499998228159, 5.430400000477675], "resource_metrics": {"samples": 2, "duration_s": 0.11261630058288574, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.903, "gpu_power_peak_watts": 31.903, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1415.671875, "cpu_memory_peak_mb": 1415.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601292.3041155}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.505000003497116, 50.070099998265505], "ttft_ms": [7.150400000682566, 5.92500000493601], "tokens_processed": [8, 8], "throughput_tok_s": [149.51873655690338, 159.77599406186786], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.08980000106385, 7.5379000045359135, 6.124200001067948], "resource_metrics": {"samples": 2, "duration_s": 0.10893654823303223, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.903, "gpu_power_peak_watts": 31.903, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1415.73046875, "cpu_memory_peak_mb": 1415.73046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601292.5232937}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [52.34329999802867, 51.251099997898564], "ttft_ms": [5.585300001257565, 6.090900002163835], "tokens_processed": [8, 8], "throughput_tok_s": [152.83713484440784, 156.09421066724465], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.939300004683901, 5.576900002779439, 5.669199999829289], "resource_metrics": {"samples": 2, "duration_s": 0.11302709579467773, "gpu_memory_mean_mb": 564.01953125, "gpu_memory_peak_mb": 564.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 8.0, "gpu_power_mean_watts": 31.129, "gpu_power_peak_watts": 31.129, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1415.796875, "cpu_memory_peak_mb": 1415.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601292.742534}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.25270000624005, 48.46680000628112], "ttft_ms": [5.805900000268593, 5.610300002445001], "tokens_processed": [8, 8], "throughput_tok_s": [159.19542629563412, 165.06144410118327], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [670.8053999973345, 6.089500006055459, 6.18199999735225], "resource_metrics": {"samples": 8, "duration_s": 0.7800800800323486, "gpu_memory_mean_mb": 565.76953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 3.0, "gpu_power_mean_watts": 29.945875, "gpu_power_peak_watts": 31.129, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1418.24658203125, "cpu_memory_peak_mb": 1423.49609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601293.6285105}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.48659999697702, 51.12340000050608], "ttft_ms": [5.801100000098813, 5.947500001639128], "tokens_processed": [8, 8], "throughput_tok_s": [164.99403960060664, 156.48411490473651], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.626300000585616, 6.5740999998524785, 5.483100001583807], "resource_metrics": {"samples": 2, "duration_s": 0.11204385757446289, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 29.124, "gpu_power_peak_watts": 29.124, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1423.51171875, "cpu_memory_peak_mb": 1423.51171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601293.8469038}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [47.782499997993, 50.86730000039097], "ttft_ms": [5.226699999184348, 5.910200001380872], "tokens_processed": [8, 8], "throughput_tok_s": [167.42531262148324, 157.2719605707107], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.269599994993769, 5.9486999962246045, 5.830000001878943], "resource_metrics": {"samples": 2, "duration_s": 0.11177372932434082, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 29.124, "gpu_power_peak_watts": 29.124, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1423.51171875, "cpu_memory_peak_mb": 1423.51171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601294.0665283}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.798199994664174, 50.326299999142066], "ttft_ms": [5.405399999290239, 5.835399999341462], "tokens_processed": [8, 8], "throughput_tok_s": [154.4455212888497, 158.9626100098036], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.585099999734666, 7.880800003476907, 6.106900000304449], "resource_metrics": {"samples": 2, "duration_s": 0.1084749698638916, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 32.78, "gpu_power_peak_watts": 32.78, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1423.51171875, "cpu_memory_peak_mb": 1423.51171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601294.282016}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [50.454999996873084, 51.129300001775846], "ttft_ms": [6.083899999794085, 6.11870000284398], "tokens_processed": [8, 8], "throughput_tok_s": [158.55713012577138, 156.46605761710293], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.8439000024227425, 6.644499997491948, 6.088800000725314], "resource_metrics": {"samples": 2, "duration_s": 0.11072826385498047, "gpu_memory_mean_mb": 566.01953125, "gpu_memory_peak_mb": 566.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 12.0, "gpu_power_mean_watts": 32.78, "gpu_power_peak_watts": 32.78, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1423.53515625, "cpu_memory_peak_mb": 1423.53515625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601294.4988406}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [62.409099999058526, 61.63370000285795], "ttft_ms": [7.926700003736187, 7.588000000396278], "tokens_processed": [8, 8], "throughput_tok_s": [128.18643435205257, 129.79911963145227], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [692.7728000009665, 8.135900003253482, 7.260200000018813], "resource_metrics": {"samples": 8, "duration_s": 0.80613112449646, "gpu_memory_mean_mb": 572.51953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.5, "gpu_power_mean_watts": 24.36875, "gpu_power_peak_watts": 30.386, "gpu_temperature_mean_c": 46.625, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1425.95556640625, "cpu_memory_peak_mb": 1431.359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601295.4146585}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [61.52910000673728, 63.26210000406718], "ttft_ms": [7.485499998438172, 6.968100002268329], "tokens_processed": [8, 8], "throughput_tok_s": [130.01977924468292, 126.4580214612805], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.126000001037028, 7.421700000122655, 7.3013999935938045], "resource_metrics": {"samples": 2, "duration_s": 0.10811305046081543, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.983, "gpu_power_peak_watts": 14.34, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1431.35546875, "cpu_memory_peak_mb": 1431.35546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601295.631132}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [67.73239999893121, 65.23109999398002], "ttft_ms": [10.849599995708559, 7.73289999779081], "tokens_processed": [8, 8], "throughput_tok_s": [118.11186374801774, 122.64088756342136], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.4431999964872375, 7.742900001176167, 12.973600001714658], "resource_metrics": {"samples": 2, "duration_s": 0.1112051010131836, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.626, "gpu_power_peak_watts": 13.626, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1431.37109375, "cpu_memory_peak_mb": 1431.37109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601295.8490462}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [65.23770000057993, 67.58289999561384], "ttft_ms": [7.938700000522658, 7.353499997407198], "tokens_processed": [8, 8], "throughput_tok_s": [122.62848015685537, 118.37313877503338], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [11.012199996912386, 8.028699994611088, 7.8008000054978766], "resource_metrics": {"samples": 2, "duration_s": 0.10866308212280273, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.626, "gpu_power_peak_watts": 13.626, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1431.3828125, "cpu_memory_peak_mb": 1431.3828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601296.0662673}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [65.46470000466798, 64.30620000173803], "ttft_ms": [7.933600005344488, 7.095600005413871], "tokens_processed": [8, 8], "throughput_tok_s": [122.2032637349527, 124.40480077789981], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.052299999690149, 7.951899999170564, 8.047800001804717], "resource_metrics": {"samples": 2, "duration_s": 0.11857795715332031, "gpu_memory_mean_mb": 590.01953125, "gpu_memory_peak_mb": 590.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 32.0, "gpu_power_mean_watts": 13.66, "gpu_power_peak_watts": 13.66, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1431.3828125, "cpu_memory_peak_mb": 1431.3828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601296.2992346}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [130.61690000176895, 112.24199999560369], "ttft_ms": [14.42930000484921, 14.466900000115857], "tokens_processed": [32, 32], "throughput_tok_s": [244.99126835475823, 285.09826982104187], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [701.0042000038084, 17.227900003490504, 14.900599999236874], "resource_metrics": {"samples": 9, "duration_s": 0.9122786521911621, "gpu_memory_mean_mb": 595.3528645833334, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 21.333333333333332, "gpu_power_mean_watts": 10.834111111111111, "gpu_power_peak_watts": 13.66, "gpu_temperature_mean_c": 45.666666666666664, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1438.7630208333333, "cpu_memory_peak_mb": 1458.5234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601297.3234196}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [99.19399999489542, 92.64849999453872], "ttft_ms": [10.230899999442045, 10.245400000712834], "tokens_processed": [32, 32], "throughput_tok_s": [322.60015728417784, 345.39145266125496], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.426599998027086, 10.459199998877011, 9.945199999492615], "resource_metrics": {"samples": 3, "duration_s": 0.22469234466552734, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 9.472, "gpu_power_peak_watts": 13.454, "gpu_temperature_mean_c": 45.333333333333336, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1458.51953125, "cpu_memory_peak_mb": 1458.51953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601297.6562805}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [90.29600000212668, 92.96899999753805], "ttft_ms": [9.889200002362486, 9.98260000051232], "tokens_processed": [32, 32], "throughput_tok_s": [354.3900061934784, 344.2007550995214], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.29019999987213, 11.534400000527967, 10.514699999475852], "resource_metrics": {"samples": 3, "duration_s": 0.21528387069702148, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 13.454, "gpu_power_peak_watts": 13.454, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1458.53125, "cpu_memory_peak_mb": 1458.53125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601297.9782894}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [95.98279999772785, 108.5490999976173], "ttft_ms": [10.076399994431995, 18.425200003548525], "tokens_processed": [32, 32], "throughput_tok_s": [333.39306626559676, 294.7974695386918], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.635800000978634, 9.841899998718873, 10.129599999345373], "resource_metrics": {"samples": 3, "duration_s": 0.21784090995788574, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.757, "gpu_power_peak_watts": 12.757, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1453.67578125, "cpu_memory_peak_mb": 1458.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601298.3034105}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [97.34140000364278, 92.13260000251466], "ttft_ms": [10.143199993763119, 10.21749999927124], "tokens_processed": [32, 32], "throughput_tok_s": [328.73987839503513, 347.32548521507687], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.18849999672966, 10.730000001785811, 10.045800001535099], "resource_metrics": {"samples": 3, "duration_s": 0.21986770629882812, "gpu_memory_mean_mb": 608.01953125, "gpu_memory_peak_mb": 608.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 17.0, "gpu_power_mean_watts": 12.865, "gpu_power_peak_watts": 13.081, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1456.8841145833333, "cpu_memory_peak_mb": 1458.55078125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601298.6308506}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [109.86609999963548, 104.77019999962067], "ttft_ms": [12.666899994655978, 12.986899993848056], "tokens_processed": [32, 32], "throughput_tok_s": [291.263638193275, 305.43036092434545], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [687.3439000046346, 14.79739999922458, 12.477800002670847], "resource_metrics": {"samples": 9, "duration_s": 0.8976948261260986, "gpu_memory_mean_mb": 619.5750868055555, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.666666666666667, "gpu_power_mean_watts": 12.160888888888888, "gpu_power_peak_watts": 13.081, "gpu_temperature_mean_c": 45.44444444444444, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1470.7118055555557, "cpu_memory_peak_mb": 1508.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601299.6382344}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [104.17030000098748, 94.47910000017146], "ttft_ms": [11.791600001743063, 12.036000000080094], "tokens_processed": [32, 32], "throughput_tok_s": [307.1892852348189, 338.699246711092], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [12.509500003943685, 11.88220000040019, 11.57680000324035], "resource_metrics": {"samples": 3, "duration_s": 0.20751667022705078, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 12.875, "gpu_power_peak_watts": 12.875, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1508.69921875, "cpu_memory_peak_mb": 1508.69921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601299.9539936}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [76.67109999601962, 78.67070000065723], "ttft_ms": [8.75860000087414, 9.860799997113645], "tokens_processed": [32, 32], "throughput_tok_s": [417.36716965924944, 406.75880600697167], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.684399999969173, 8.551999999326654, 10.087300004670396], "resource_metrics": {"samples": 2, "duration_s": 0.1190946102142334, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 10.0, "gpu_power_mean_watts": 14.0375, "gpu_power_peak_watts": 15.2, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1508.703125, "cpu_memory_peak_mb": 1508.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601300.1801977}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [79.45930000278167, 80.60130000376375], "ttft_ms": [9.66849999531405, 9.478599997237325], "tokens_processed": [32, 32], "throughput_tok_s": [402.7218966046738, 397.01592900493824], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.88929999928223, 9.979999995266553, 8.51189999957569], "resource_metrics": {"samples": 2, "duration_s": 0.11101508140563965, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 15.2, "gpu_power_peak_watts": 15.2, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1508.73828125, "cpu_memory_peak_mb": 1508.73828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601300.398763}
{"spec": {"backend": "transformers-gpu-compile", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [78.52459999412531, 72.60659999883501], "ttft_ms": [9.67300000047544, 8.778600000368897], "tokens_processed": [32, 32], "throughput_tok_s": [407.51560660473314, 440.73128338902313], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.154999999736901, 10.34819999767933, 10.305299998435657], "resource_metrics": {"samples": 2, "duration_s": 0.11082816123962402, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 15.529499999999999, "gpu_power_peak_watts": 15.859, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1508.73828125, "cpu_memory_peak_mb": 1508.73828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "transformers-gpu-compile", "init_ms": 2565.1152999998885, "compile_ms": 728.9250000030734, "compile_error": null, "compile_backend": "cudagraphs", "device": "cuda", "compile": true}, "started_at": 1765601300.6165426}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.075400000147056, 2.859099993656855], "ttft_ms": [0.42409999878145754, 0.3752000047825277], "tokens_processed": [8, 8], "throughput_tok_s": [1962.9974971073586, 2798.083319138417], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0334000000730157, 0.43870000081369653, 0.39739999920129776], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 15.859, "gpu_power_peak_watts": 15.859, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1515.796875, "cpu_memory_peak_mb": 1515.796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601300.734229}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.630300004966557, 2.46020000486169], "ttft_ms": [0.323799999023322, 0.29990000621182844], "tokens_processed": [8, 8], "throughput_tok_s": [3041.4781526420275, 3251.7681425050446], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7943000018713064, 0.4087000052095391, 0.33540000003995374], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 15.859, "gpu_power_peak_watts": 15.859, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1515.80859375, "cpu_memory_peak_mb": 1515.80859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601300.858022}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.725900005316362, 2.5642999971751124], "ttft_ms": [0.2986999970744364, 0.35989999742014334], "tokens_processed": [8, 8], "throughput_tok_s": [2934.810515571916, 3119.75978193385], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.6273999970289879, 0.3699000008055009, 0.28419999580364674], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 20.0, "gpu_power_mean_watts": 15.859, "gpu_power_peak_watts": 15.859, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1515.80859375, "cpu_memory_peak_mb": 1515.80859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601300.9835515}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.3894999976619147, 2.100100005918648], "ttft_ms": [0.3097000007983297, 0.2530999990995042], "tokens_processed": [8, 8], "throughput_tok_s": [3347.9807523866352, 3809.3424015303285], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.644200001261197, 0.3540999969118275, 0.39570000080857426], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 17.573, "gpu_power_peak_watts": 17.573, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1515.8671875, "cpu_memory_peak_mb": 1515.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.1083376}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.6658999995561317, 2.5330999997095205], "ttft_ms": [0.26709999656304717, 0.35349999961908907], "tokens_processed": [8, 8], "throughput_tok_s": [3000.8627485397005, 3158.1856227221147], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7031999994069338, 0.46209999709390104, 0.28059999749530107], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 17.573, "gpu_power_peak_watts": 17.573, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1515.96484375, "cpu_memory_peak_mb": 1515.96484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.23524}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.711600005568471, 2.326100002392195], "ttft_ms": [0.2950999987660907, 0.355799995304551], "tokens_processed": [8, 8], "throughput_tok_s": [2950.2876469875387, 3439.233047492663], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0057999970740639, 0.3906999991158955, 0.29859999631298706], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 17.573, "gpu_power_peak_watts": 17.573, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1517.12890625, "cpu_memory_peak_mb": 1517.12890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.3594522}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.6276999962865375, 3.465200003120117], "ttft_ms": [0.3991000048699789, 0.42190000385744497], "tokens_processed": [8, 8], "throughput_tok_s": [2205.254019954552, 2308.6690502125944], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7774000041536056, 0.43790000199805945, 0.4250999991199933], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 17.573, "gpu_power_peak_watts": 17.573, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1517.1796875, "cpu_memory_peak_mb": 1517.1796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.4844084}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.601599997433368, 2.2959999987506308], "ttft_ms": [0.27799999952549115, 0.3145000009681098], "tokens_processed": [8, 8], "throughput_tok_s": [3075.0307533412024, 3484.3205593872835], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.800499998149462, 0.35179999395040795, 0.37509999674512073], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.362, "gpu_power_peak_watts": 19.362, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1517.1953125, "cpu_memory_peak_mb": 1517.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.6107793}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.489599999738857, 2.6810000053956173], "ttft_ms": [0.43030000233557075, 0.2921999985119328], "tokens_processed": [8, 8], "throughput_tok_s": [2292.526364224747, 2983.961202498951], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8174999966286123, 0.4630999974324368, 0.43849999929079786], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.362, "gpu_power_peak_watts": 19.362, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1517.1953125, "cpu_memory_peak_mb": 1517.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.7373998}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.9196999967098236, 2.903299995523412], "ttft_ms": [0.28480000037234277, 0.37339999835239723], "tokens_processed": [8, 8], "throughput_tok_s": [2740.0075381084043, 2755.485141850712], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.5596000046352856, 0.3167999966535717, 0.28359999851090834], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.362, "gpu_power_peak_watts": 19.362, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1517.1953125, "cpu_memory_peak_mb": 1517.1953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.860497}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.161699995165691, 3.955699998186901], "ttft_ms": [0.33740000071702525, 0.4195000001345761], "tokens_processed": [8, 8], "throughput_tok_s": [1922.291373547577, 2022.398059424832], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9992999950773083, 0.48859999515116215, 0.35480000224197283], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.362, "gpu_power_peak_watts": 19.362, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1528.3203125, "cpu_memory_peak_mb": 1528.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601301.9872453}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [2.8989000056753866, 3.2987000013235956], "ttft_ms": [0.36610000097425655, 0.3457000057096593], "tokens_processed": [8, 8], "throughput_tok_s": [2759.6674546682602, 2425.1978042228816], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.6472000022768043, 0.4340999948908575, 0.3615000023273751], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1528.3203125, "cpu_memory_peak_mb": 1528.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.1134546}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.4106000059400685, 4.001699999207631], "ttft_ms": [0.5327999970177189, 0.44660000276053324], "tokens_processed": [8, 8], "throughput_tok_s": [1813.8121773059972, 1999.150361492382], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7938999988255091, 0.5636999994749203, 0.5251999973552302], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1528.3203125, "cpu_memory_peak_mb": 1528.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.2385223}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.6469999977271073, 3.3451999988756143], "ttft_ms": [0.5063999997219071, 0.47619999531889334], "tokens_processed": [8, 8], "throughput_tok_s": [2193.5837688472116, 2391.4863095447067], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7283000013558194, 0.683400001435075, 0.5390000005718321], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1528.34375, "cpu_memory_peak_mb": 1528.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.364241}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.1277000016416423, 3.319199997349642], "ttft_ms": [0.49250000301981345, 0.4334999975981191], "tokens_processed": [8, 8], "throughput_tok_s": [2557.7900680375433, 2410.2193318835693], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.9204000016325153, 0.5403000031947158, 0.49449999642092735], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1528.34375, "cpu_memory_peak_mb": 1528.34375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.4901881}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.412499998579733, 3.612000000430271], "ttft_ms": [0.5040999967604876, 0.5275000003166497], "tokens_processed": [8, 8], "throughput_tok_s": [1813.0311620566545, 2214.8394238779124], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.374999999825377, 0.622200001089368, 0.585900001169648], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1531.42578125, "cpu_memory_peak_mb": 1531.42578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.6150632}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.6051999995834194, 4.006800001661759], "ttft_ms": [0.3851000001304783, 0.42839999514399096], "tokens_processed": [8, 8], "throughput_tok_s": [2219.0169757362696, 1996.6057693626142], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.8968999973149039, 0.48750000132713467, 0.38779999886173755], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1531.42578125, "cpu_memory_peak_mb": 1531.42578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.740242}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [5.305599996063393, 3.6957999982405454], "ttft_ms": [1.053799998771865, 0.6207000042195432], "tokens_processed": [8, 8], "throughput_tok_s": [1507.840773133251, 2164.619298611545], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.196400000480935, 0.5028000014135614, 0.46740000107092783], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1531.44921875, "cpu_memory_peak_mb": 1531.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.8639488}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [4.012299999885727, 3.616999994846992], "ttft_ms": [0.37790000351378694, 0.4278999986127019], "tokens_processed": [8, 8], "throughput_tok_s": [1993.8688533329625, 2211.777719490547], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0192999980063178, 0.4830999969271943, 0.4043000008095987], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 19.517, "gpu_power_peak_watts": 19.517, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1531.4609375, "cpu_memory_peak_mb": 1531.4609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601302.9888918}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [3.990099998190999, 4.071000003023073], "ttft_ms": [0.4941000006510876, 0.4895000020042062], "tokens_processed": [8, 8], "throughput_tok_s": [2004.962282556071, 1965.119133888308], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [0.7433999999193475, 0.6386000022757798, 0.40559999615652487], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1531.4609375, "cpu_memory_peak_mb": 1531.4609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.1139789}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.286100001190789, 8.416500000748783], "ttft_ms": [0.7416999942506664, 0.7155999992392026], "tokens_processed": [32, 32], "throughput_tok_s": [3446.010703728856, 3802.0554859089984], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.7560000016819686, 0.8949999973992817, 0.9979999958886765], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1555.25, "cpu_memory_peak_mb": 1555.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.2392356}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.71419999707723, 6.1770999964210205], "ttft_ms": [0.690499997290317, 0.741700001526624], "tokens_processed": [32, 32], "throughput_tok_s": [4766.01829166989, 5180.424474031606], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2771000037901103, 0.8859000008669682, 0.6820000053266995], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1555.3046875, "cpu_memory_peak_mb": 1555.3046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.3647366}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.155999995826278, 7.180699998571072], "ttft_ms": [0.6570000041392632, 1.1788000047090463], "tokens_processed": [32, 32], "throughput_tok_s": [5198.180640301456, 4456.3900464255385], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2795000002370216, 0.8295999941765331, 0.8118000041577034], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1555.328125, "cpu_memory_peak_mb": 1555.328125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.4918537}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.750500000838656, 8.006900003238115], "ttft_ms": [0.9277000062866136, 1.109999997424893], "tokens_processed": [32, 32], "throughput_tok_s": [4128.765885625106, 3996.5529714444665], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5265000038198195, 1.7100999975809827, 0.9767999945324846], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1555.35546875, "cpu_memory_peak_mb": 1555.35546875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.6165957}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.409599999140482, 6.6829999996116385], "ttft_ms": [0.7116999986465089, 0.7293000016943552], "tokens_processed": [32, 32], "throughput_tok_s": [3805.175038440665, 4788.268741861377], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2710999944829382, 0.8782000004430301, 0.7419999965350144], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1555.4140625, "cpu_memory_peak_mb": 1555.4140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.7406006}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [16.026099998271093, 11.635800001386087], "ttft_ms": [1.0933999947155826, 1.607400001375936], "tokens_processed": [32, 32], "throughput_tok_s": [1996.7428135012376, 2750.133209249736], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4040999996941537, 1.3055000017629936, 1.3484000010066666], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1584.3203125, "cpu_memory_peak_mb": 1584.3203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.8671918}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.303599996201228, 12.198000003991183], "ttft_ms": [0.72340000042459, 1.092300000891555], "tokens_processed": [32, 32], "throughput_tok_s": [3439.5287859609166, 2623.380881253452], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6661000045132823, 0.9494000041740946, 0.7786999995005317], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 11.469, "gpu_power_peak_watts": 11.469, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1584.32421875, "cpu_memory_peak_mb": 1584.32421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601303.991829}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.641299999610055, 10.90690000273753], "ttft_ms": [1.0301000002073124, 1.6505999956279993], "tokens_processed": [32, 32], "throughput_tok_s": [2748.833893213979, 2933.9225620449715], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.717400002235081, 1.538800002890639, 1.214700001582969], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.691, "gpu_power_peak_watts": 6.691, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1584.32421875, "cpu_memory_peak_mb": 1584.32421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.1184757}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.600800000247546, 9.694100001070183], "ttft_ms": [0.9587999957147986, 1.4926000003470108], "tokens_processed": [32, 32], "throughput_tok_s": [3018.6401025632736, 3300.9768824818552], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.483899999584537, 1.2896999978693202, 0.9860999998636544], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.691, "gpu_power_peak_watts": 6.691, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1584.32421875, "cpu_memory_peak_mb": 1584.32421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.2440882}
{"spec": {"backend": "onnxruntime-cpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.357600000337698, 12.211799999931827], "ttft_ms": [0.7549000001745299, 1.7580999992787838], "tokens_processed": [32, 32], "throughput_tok_s": [2817.49665413895, 2620.4163186572528], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7348000037600286, 0.9135000000242144, 0.83969999832334], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 654.01953125, "gpu_memory_peak_mb": 654.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 6.691, "gpu_power_peak_watts": 6.691, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1584.32421875, "cpu_memory_peak_mb": 1584.32421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-cpu", "init_ms": 170.36730000108946, "providers": ["CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.3694103}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [43.045600003097206, 42.66939999797614], "ttft_ms": [4.894199999398552, 6.580799999937881], "tokens_processed": [8, 8], "throughput_tok_s": [185.84942478265808, 187.4879890595942], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [89.24770000157878, 4.2601999957696535, 4.920100000163075], "resource_metrics": {"samples": 2, "duration_s": 0.11702752113342285, "gpu_memory_mean_mb": 666.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 4.2375, "gpu_power_peak_watts": 6.691, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1669.111328125, "cpu_memory_peak_mb": 1723.171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.5941532}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.26190000108909, 37.940400005027186], "ttft_ms": [4.544800001895055, 3.9204999993671663], "tokens_processed": [8, 8], "throughput_tok_s": [209.0852780382649, 210.85702836395984], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.9442000017734244, 6.605599999602418, 4.815000000235159], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 1.784, "gpu_power_peak_watts": 1.784, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1723.26953125, "cpu_memory_peak_mb": 1723.26953125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.7129545}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.612800002330914, 36.32590000052005], "ttft_ms": [4.151200002525002, 4.303200003050733], "tokens_processed": [8, 8], "throughput_tok_s": [218.5028186724503, 220.2285421664837], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.169299998669885, 6.204400000569876, 4.4463000012910925], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 1.784, "gpu_power_peak_watts": 1.784, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1723.29296875, "cpu_memory_peak_mb": 1723.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.8392432}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [35.95130000030622, 35.0484000009601], "ttft_ms": [3.7980999986757524, 4.169399995589629], "tokens_processed": [8, 8], "throughput_tok_s": [222.52324672353598, 228.25578342465994], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.13779999507824, 4.142500001762528, 3.7441000022226945], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 1.784, "gpu_power_peak_watts": 1.784, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1723.29296875, "cpu_memory_peak_mb": 1723.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601304.962894}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.49829999549547, 38.59499999816762], "ttft_ms": [4.063000000314787, 4.063000000314787], "tokens_processed": [8, 8], "throughput_tok_s": [246.16672260114728, 207.28073585645333], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.232400002365466, 4.4360000028973445, 3.6332999952719547], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 1.784, "gpu_power_peak_watts": 1.784, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1723.29296875, "cpu_memory_peak_mb": 1723.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.0872955}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.352400003816, 23.065899993525818], "ttft_ms": [3.070400001888629, 3.1339999986812472], "tokens_processed": [8, 8], "throughput_tok_s": [303.5776627116144, 346.83233701028166], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.510000002279412, 2.733499997702893, 3.122000001894776], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 2.403, "gpu_power_peak_watts": 2.403, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1724.42578125, "cpu_memory_peak_mb": 1724.42578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.2129724}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.627699996519368, 26.290800000424497], "ttft_ms": [2.788899997540284, 3.1248000013874844], "tokens_processed": [8, 8], "throughput_tok_s": [353.54896879623533, 304.28895278465586], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.1667000002926216, 2.8832999960286543, 2.916599994932767], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 2.403, "gpu_power_peak_watts": 2.403, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1724.42578125, "cpu_memory_peak_mb": 1724.42578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.3371692}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.476500002492685, 21.919699996942654], "ttft_ms": [2.6829999987967312, 2.645900000061374], "tokens_processed": [8, 8], "throughput_tok_s": [326.8441157512422, 364.9684987073653], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.3623000053921714, 2.7215000009164214, 2.9808999970555305], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 2.403, "gpu_power_peak_watts": 2.403, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1724.4375, "cpu_memory_peak_mb": 1724.4375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.4622617}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.273300001979806, 23.26139999786392], "ttft_ms": [2.7966999987256713, 2.7773999972851016], "tokens_processed": [8, 8], "throughput_tok_s": [293.3271734413976, 343.9173910742533], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.1014999985927716, 2.976499999931548, 2.573199999460485], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 2.403, "gpu_power_peak_watts": 2.403, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1724.44921875, "cpu_memory_peak_mb": 1724.44921875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.5857065}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.468200004368555, 23.175199996330775], "ttft_ms": [2.987200001371093, 3.117600004770793], "tokens_processed": [8, 8], "throughput_tok_s": [356.0587852362244, 345.1965895123496], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.235199998016469, 2.778400004899595, 2.987399995618034], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 668.01953125, "gpu_memory_peak_mb": 668.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 6.637, "gpu_power_peak_watts": 6.637, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1724.4609375, "cpu_memory_peak_mb": 1724.4609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.7098782}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.014800001576077, 26.941399999486748], "ttft_ms": [3.5904000033042394, 3.31469999946421], "tokens_processed": [8, 8], "throughput_tok_s": [307.51725938755357, 296.94076774601194], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.637800004682504, 2.960899997560773, 3.5963999980594963], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 6.637, "gpu_power_peak_watts": 6.637, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1730.21484375, "cpu_memory_peak_mb": 1730.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.8361604}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.637099999992643, 27.406299996073358], "ttft_ms": [2.9489000007743016, 3.2398000039393082], "tokens_processed": [8, 8], "throughput_tok_s": [289.46597146596895, 291.9036864205019], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.90549999428913, 3.0737000051885843, 3.3830999964266084], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 6.637, "gpu_power_peak_watts": 6.637, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1735.96875, "cpu_memory_peak_mb": 1735.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601305.9619608}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.629700002260506, 25.784999997995328], "ttft_ms": [3.010699998412747, 3.446599999733735], "tokens_processed": [8, 8], "throughput_tok_s": [269.9993587309242, 310.2579019050597], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7333000000217, 3.2388000036007725, 3.3643999995547347], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 58.0, "gpu_power_mean_watts": 6.637, "gpu_power_peak_watts": 6.637, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1735.96875, "cpu_memory_peak_mb": 1735.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.0875752}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.394900000013877, 27.403899999626447], "ttft_ms": [3.2304999986081384, 3.2209999990300275], "tokens_processed": [8, 8], "throughput_tok_s": [303.08885428608534, 291.92925095001266], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.4908000016002916, 3.0986000056145713, 4.834200000914279], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 10.842, "gpu_power_peak_watts": 10.842, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1735.96875, "cpu_memory_peak_mb": 1735.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.210521}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.093599997693673, 25.48150000075111], "ttft_ms": [3.4114999943994917, 3.3104000031016767], "tokens_processed": [8, 8], "throughput_tok_s": [306.5885887998242, 313.95326019913216], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.511599999910686, 3.3438000027672388, 2.869000003556721], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 10.842, "gpu_power_peak_watts": 10.842, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1735.96875, "cpu_memory_peak_mb": 1735.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.3364305}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [32.52970000175992, 32.202399997913744], "ttft_ms": [3.574400005163625, 3.685900002892595], "tokens_processed": [8, 8], "throughput_tok_s": [245.9291047740122, 248.4286885610478], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.581699995673262, 3.5212000002502464, 3.5363999995752238], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 10.842, "gpu_power_peak_watts": 10.842, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1737.46875, "cpu_memory_peak_mb": 1737.46875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.460796}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.195999996270984, 33.89969999989262], "ttft_ms": [3.9864999998826534, 4.216900000756141], "tokens_processed": [8, 8], "throughput_tok_s": [256.4431337657482, 235.9902890003552], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.238099994836375, 6.521199997223448, 3.950899998017121], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 10.842, "gpu_power_peak_watts": 10.842, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1738.96875, "cpu_memory_peak_mb": 1738.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.5851576}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.877899993560277, 31.91060000244761], "ttft_ms": [4.207600002700929, 6.784600001992658], "tokens_processed": [8, 8], "throughput_tok_s": [250.9575599903412, 250.70039420714068], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.099599995242897, 4.913400000077672, 3.8545999996131286], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 11.914, "gpu_power_peak_watts": 11.914, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1738.96875, "cpu_memory_peak_mb": 1738.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.71089}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [33.646600000793114, 32.31960000266554], "ttft_ms": [3.6084000021219254, 4.02619999658782], "tokens_processed": [8, 8], "throughput_tok_s": [237.7654800131789, 247.52781591790136], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.041200001665857, 3.7126000024727546, 3.664700001536403], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 11.914, "gpu_power_peak_watts": 11.914, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1738.96875, "cpu_memory_peak_mb": 1738.96875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.841848}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.449100002238993, 32.86640000442276], "ttft_ms": [3.686800002469681, 3.886300000885967], "tokens_processed": [8, 8], "throughput_tok_s": [271.6551609180507, 243.40968280442817], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.019799998786766, 3.965999996580649, 3.838100004941225], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 676.01953125, "gpu_memory_peak_mb": 676.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 43.0, "gpu_power_mean_watts": 11.914, "gpu_power_peak_watts": 11.914, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1738.98046875, "cpu_memory_peak_mb": 1738.98046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601306.9616995}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.45069999748375, 56.120400004147086], "ttft_ms": [6.42350000271108, 6.913000004715286], "tokens_processed": [32, 32], "throughput_tok_s": [598.6825242982119, 570.202635719548], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.006399995996617, 6.350799994834233, 6.229300000995863], "resource_metrics": {"samples": 2, "duration_s": 0.1165921688079834, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 48.0, "gpu_power_mean_watts": 12.5535, "gpu_power_peak_watts": 13.193, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1755.4140625, "cpu_memory_peak_mb": 1763.22265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601307.187251}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [49.88179999782005, 51.042300001427066], "ttft_ms": [6.314199999906123, 6.244399999559391], "tokens_processed": [32, 32], "throughput_tok_s": [641.516545140682, 626.9309964305161], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.045800000720192, 5.9834999992745, 6.076999998185784], "resource_metrics": {"samples": 2, "duration_s": 0.10899949073791504, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 53.0, "gpu_power_mean_watts": 13.193, "gpu_power_peak_watts": 13.193, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1763.234375, "cpu_memory_peak_mb": 1763.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601307.4064436}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [53.614699994795956, 50.40670000016689], "ttft_ms": [6.377400000928901, 6.363000000419561], "tokens_processed": [32, 32], "throughput_tok_s": [596.8512367523466, 634.836242005409], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.020399996486958, 6.090100003348198, 6.335700003546663], "resource_metrics": {"samples": 2, "duration_s": 0.1192634105682373, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 53.0, "gpu_power_mean_watts": 13.075, "gpu_power_peak_watts": 13.193, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1763.234375, "cpu_memory_peak_mb": 1763.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601307.6363244}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.62370000471128, 52.03289999917615], "ttft_ms": [6.2716999964322895, 5.921099997067358], "tokens_processed": [32, 32], "throughput_tok_s": [619.8703308185894, 614.9955124643574], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.355099998472724, 6.011199999193195, 6.130899993877392], "resource_metrics": {"samples": 2, "duration_s": 0.11321544647216797, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 53.0, "gpu_power_mean_watts": 12.957, "gpu_power_peak_watts": 12.957, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1763.234375, "cpu_memory_peak_mb": 1763.234375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601307.857671}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [51.20799999713199, 52.41789999854518], "ttft_ms": [5.943000003753696, 6.335799997032154], "tokens_processed": [32, 32], "throughput_tok_s": [624.9023590414042, 610.47848160434], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.195400004799012, 6.19379999989178, 6.034600002749357], "resource_metrics": {"samples": 2, "duration_s": 0.11047911643981934, "gpu_memory_mean_mb": 692.01953125, "gpu_memory_peak_mb": 692.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 53.0, "gpu_power_mean_watts": 12.957, "gpu_power_peak_watts": 12.957, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1763.236328125, "cpu_memory_peak_mb": 1763.23828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.0762181}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [39.095000000088476, 35.14310000173282], "ttft_ms": [4.75890000234358, 4.71780000225408], "tokens_processed": [32, 32], "throughput_tok_s": [818.5189921966385, 910.5628131389137], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.916799997270573, 4.73970000166446, 4.231999999319669], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 14.091, "gpu_power_peak_watts": 14.091, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1770.3671875, "cpu_memory_peak_mb": 1770.3671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.1938734}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.74650000175461, 34.517100000812206], "ttft_ms": [4.971499998646323, 5.139200002304278], "tokens_processed": [32, 32], "throughput_tok_s": [870.8312355863015, 927.0767242684647], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.948300000047311, 4.4286000047577545, 4.724500002339482], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 14.091, "gpu_power_peak_watts": 14.091, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1793.13671875, "cpu_memory_peak_mb": 1793.13671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.319426}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.014400000160094, 36.537899999530055], "ttft_ms": [3.5120999964419752, 4.316999998991378], "tokens_processed": [32, 32], "throughput_tok_s": [940.7780234209448, 875.8029334037145], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.378199999337085, 5.004799997550435, 5.320599993865471], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 14.091, "gpu_power_peak_watts": 14.091, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1793.140625, "cpu_memory_peak_mb": 1793.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.4442873}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [40.90310000174213, 36.21659999771509], "ttft_ms": [4.959500001859851, 3.933500003768131], "tokens_processed": [32, 32], "throughput_tok_s": [782.3367910656422, 883.572726374615], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.047799997555558, 5.456300001242198, 4.496699999435805], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 14.091, "gpu_power_peak_watts": 14.091, "gpu_temperature_mean_c": 46.0, "gpu_temperature_peak_c": 46, "cpu_memory_mean_mb": 1793.140625, "cpu_memory_peak_mb": 1793.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.5879304}
{"spec": {"backend": "onnxruntime-gpu", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [37.05999999510823, 36.56770000088727], "ttft_ms": [4.733299996587448, 4.567799995129462], "tokens_processed": [32, 32], "throughput_tok_s": [863.4646520297861, 875.0892180592041], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.989799999748357, 5.149800002982374, 3.7193999960436486], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 724.01953125, "gpu_memory_peak_mb": 724.01953125, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 18.459, "gpu_power_peak_watts": 18.459, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1793.140625, "cpu_memory_peak_mb": 1793.140625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "onnxruntime-gpu", "init_ms": 1803.19469999813, "providers": ["CUDAExecutionProvider", "CPUExecutionProvider"], "input_type": "tensor(int32)", "onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx"}, "started_at": 1765601308.7088926}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.283000002440531, 6.648200003837701], "ttft_ms": [0.6001000001560897, 0.9072000029846095], "tokens_processed": [8, 8], "throughput_tok_s": [1273.2770964336348, 1203.3332323609347], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [29.71330000582384, 0.7985999982338399, 0.7162000038078986], "resource_metrics": {"samples": 2, "duration_s": 0.1176614761352539, "gpu_memory_mean_mb": 816.529296875, "gpu_memory_peak_mb": 909.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 18.459, "gpu_power_peak_watts": 18.459, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 1878.744140625, "cpu_memory_peak_mb": 1941.3984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32"}, "started_at": 1765601308.9349456}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.335499998589512, 6.577599997399375], "ttft_ms": [0.8636999991722405, 0.72340000042459], "tokens_processed": [8, 8], "throughput_tok_s": [1090.5868722702282, 1216.24908829406], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1899000019184314, 0.7333999965339899, 0.8342999935848638], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 64.0, "gpu_power_mean_watts": 18.459, "gpu_power_peak_watts": 18.459, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2074.265625, "cpu_memory_peak_mb": 2074.265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.052473}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.838900000730064, 5.973800005449448], "ttft_ms": [0.7504000022890978, 0.7564000043203123], "tokens_processed": [8, 8], "throughput_tok_s": [1169.7787654660817, 1339.1810895413644], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.155799996922724, 1.0747999986051582, 0.9888000058708712], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 30.766, "gpu_power_peak_watts": 30.766, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2074.390625, "cpu_memory_peak_mb": 2074.390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.1759298}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.901200002175756, 6.762600001820829], "ttft_ms": [0.8643000037409365, 0.7193000055849552], "tokens_processed": [8, 8], "throughput_tok_s": [1159.2186862397589, 1182.976961205158], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.2859000053140335, 0.9234999961336143, 0.8913000056054443], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 30.766, "gpu_power_peak_watts": 30.766, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2074.3984375, "cpu_memory_peak_mb": 2074.3984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.3000546}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.799100003263447, 6.946000001335051], "ttft_ms": [0.9850999995251186, 0.6787999955122359], "tokens_processed": [8, 8], "throughput_tok_s": [1176.6263176244124, 1151.7420095684374], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1726000011549331, 0.8604999966337346, 0.9701000017230399], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 30.766, "gpu_power_peak_watts": 30.766, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2074.40625, "cpu_memory_peak_mb": 2074.40625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.4269853}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.612400004290976, 8.41029999719467], "ttft_ms": [0.7798000006005168, 0.9687999991001561], "tokens_processed": [8, 8], "throughput_tok_s": [1050.9169244246941, 951.2145824368299], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4258000010158867, 1.207199995405972, 1.097499996831175], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 30.766, "gpu_power_peak_watts": 30.766, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2075.00390625, "cpu_memory_peak_mb": 2075.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.551429}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.284600003913511, 7.229999995615799], "ttft_ms": [0.891099996806588, 0.6514999986393377], "tokens_processed": [8, 8], "throughput_tok_s": [1098.2071761939096, 1106.5006922339035], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5291000017896295, 0.9014999959617853, 0.826300005428493], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 29.617, "gpu_power_peak_watts": 29.617, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2075.00390625, "cpu_memory_peak_mb": 2075.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.6777103}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.372199994279072, 7.58539999515051], "ttft_ms": [0.7729999997536652, 1.021500000206288], "tokens_processed": [8, 8], "throughput_tok_s": [1085.1577556507027, 1054.6576324405505], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.610799998161383, 0.8385999972233549, 0.7320000004256144], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 29.617, "gpu_power_peak_watts": 29.617, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2075.00390625, "cpu_memory_peak_mb": 2075.00390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.8023274}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.860099998652004, 7.280999998329207], "ttft_ms": [0.8720000041648746, 1.0044000009656884], "tokens_processed": [8, 8], "throughput_tok_s": [1017.7987559155722, 1098.7501719318477], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.5229999989969656, 0.7904999947641045, 0.8148999986588024], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 29.617, "gpu_power_peak_watts": 29.617, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2075.015625, "cpu_memory_peak_mb": 2075.015625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601309.926768}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.364600001892541, 8.214200002839789], "ttft_ms": [0.8512999993399717, 0.8669000017107464], "tokens_processed": [8, 8], "throughput_tok_s": [1086.2775979610808, 973.9232058184929], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6373000034946017, 1.0967999987769872, 0.828799995360896], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 955.0390625, "gpu_memory_peak_mb": 955.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 9.0, "gpu_power_mean_watts": 29.617, "gpu_power_peak_watts": 29.617, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2075.02734375, "cpu_memory_peak_mb": 2075.02734375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.0529873}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.485599999607075, 9.509499999694526], "ttft_ms": [1.406999996106606, 1.0878999964916147], "tokens_processed": [8, 8], "throughput_tok_s": [843.3836552596974, 841.26399918576], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9409000015002675, 0.9544999993522651, 1.152899996668566], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 29.644, "gpu_power_peak_watts": 29.644, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2076.57421875, "cpu_memory_peak_mb": 2076.57421875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.17777}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.81119999778457, 9.366400001454167], "ttft_ms": [1.3659000032930635, 1.0599999950500205], "tokens_processed": [8, 8], "throughput_tok_s": [815.3946511952106, 854.1168430515429], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9488000034471042, 1.0309999997843988, 0.9777000013855286], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 29.644, "gpu_power_peak_watts": 29.644, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2076.83203125, "cpu_memory_peak_mb": 2076.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.3011434}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.233100001642015, 9.700199996586889], "ttft_ms": [1.552000001538545, 1.032100000884384], "tokens_processed": [8, 8], "throughput_tok_s": [781.7767830585366, 824.7252636868187], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0003000026918016, 1.029999999445863, 1.0270999991917051], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 29.644, "gpu_power_peak_watts": 29.644, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2076.83203125, "cpu_memory_peak_mb": 2076.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.42545}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [14.133399999991525, 9.297099997638725], "ttft_ms": [1.5117000002646819, 1.1089000036008656], "tokens_processed": [8, 8], "throughput_tok_s": [566.0350658726702, 860.4833767553149], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.6770999936852604, 0.7666999954381026, 1.46159999712836], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 29.644, "gpu_power_peak_watts": 29.644, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2076.8359375, "cpu_memory_peak_mb": 2076.8359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.5498354}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.027900003478862, 9.491000004345551], "ttft_ms": [1.4525000005960464, 0.9729000012157485], "tokens_processed": [8, 8], "throughput_tok_s": [797.7742096774653, 842.9038032174817], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9281999993836507, 1.1106999954790808, 1.1416999986977316], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 32.953, "gpu_power_peak_watts": 32.953, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2076.83984375, "cpu_memory_peak_mb": 2076.83984375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.6742651}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.93290000082925, 10.276499997416977], "ttft_ms": [1.7845000038505532, 1.1183999959030189], "tokens_processed": [8, 8], "throughput_tok_s": [670.4154060994442, 778.4751619725412], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.7501000004122034, 1.2014000021736138, 1.32310000481084], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 32.953, "gpu_power_peak_watts": 32.953, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2078.375, "cpu_memory_peak_mb": 2078.375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.7982755}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.859500005608425, 10.499100004381035], "ttft_ms": [1.5289000002667308, 1.2967999937245622], "tokens_processed": [8, 8], "throughput_tok_s": [736.6821673068165, 761.9700733074056], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.526299998862669, 1.3561000014306046, 1.237800002854783], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 32.953, "gpu_power_peak_watts": 32.953, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2078.375, "cpu_memory_peak_mb": 2078.375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601310.9233465}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.730500002158806, 10.777099996630568], "ttft_ms": [1.6959999993559904, 1.1576999968383461], "tokens_processed": [8, 8], "throughput_tok_s": [681.9828650550046, 742.3147231167178], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2992000012891367, 1.2388000031933188, 1.1987999969278462], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 5.0, "gpu_power_mean_watts": 32.953, "gpu_power_peak_watts": 32.953, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2078.375, "cpu_memory_peak_mb": 2078.375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.0478494}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.482100005901884, 12.42669999919599], "ttft_ms": [1.2901000009151176, 1.876900001661852], "tokens_processed": [8, 8], "throughput_tok_s": [763.205845727063, 643.7750972114561], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9151000014971942, 1.8385000003036112, 1.4759000041522086], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 29.917, "gpu_power_peak_watts": 29.917, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2078.38671875, "cpu_memory_peak_mb": 2078.38671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.1740105}
{"spec": {"backend": "tensorrt-fp32", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.820699997770134, 11.259399994742125], "ttft_ms": [1.5354999995906837, 1.6153999968082644], "tokens_processed": [8, 8], "throughput_tok_s": [676.7788710913165, 710.5174346533396], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8930000005639158, 0.9719999943627045, 1.722900000459049], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 959.0390625, "gpu_memory_peak_mb": 959.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 29.917, "gpu_power_peak_watts": 29.917, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2078.38671875, "cpu_memory_peak_mb": 2078.38671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.3084574}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.530900001176633, 21.609199997328687], "ttft_ms": [2.2915999943506904, 2.6647999984561466], "tokens_processed": [32, 32], "throughput_tok_s": [1558.6262656856773, 1480.8507489382214], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.058399998233654, 2.3699000012129545, 3.1802000012248755], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 951.0390625, "gpu_memory_peak_mb": 951.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 29.917, "gpu_power_peak_watts": 29.917, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2074.37109375, "cpu_memory_peak_mb": 2074.37109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.42453}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.32310000160942, 22.465800000645686], "ttft_ms": [2.56690000242088, 3.383100003702566], "tokens_processed": [32, 32], "throughput_tok_s": [1500.7198764525192, 1424.387290863459], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.753799999889452, 2.5652000040281564, 3.0541000014636666], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 29.917, "gpu_power_peak_watts": 29.917, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2090.8671875, "cpu_memory_peak_mb": 2090.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.550514}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.029899995890446, 21.962899998470675], "ttft_ms": [2.4979000008897856, 3.3449999973527156], "tokens_processed": [32, 32], "throughput_tok_s": [1389.4980006734818, 1457.0024906650865], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.770799998368602, 2.3771000051056035, 3.0618000018876046], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.266, "gpu_power_peak_watts": 31.266, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2090.8671875, "cpu_memory_peak_mb": 2090.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.674572}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [23.340100000496022, 18.798799996147864], "ttft_ms": [2.9397000034805387, 1.8896000037784688], "tokens_processed": [32, 32], "throughput_tok_s": [1371.030972417425, 1702.236313304958], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.187900002056267, 2.4402999988524243, 3.0756999985896982], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.266, "gpu_power_peak_watts": 31.266, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2090.8671875, "cpu_memory_peak_mb": 2090.8671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.7988684}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.372599996742792, 20.358499998110346], "ttft_ms": [3.437100000155624, 3.122299996903166], "tokens_processed": [32, 32], "throughput_tok_s": [1430.3210178816432, 1571.8250363715501], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.735999998752959, 3.0790000018896535, 2.344199994695373], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.266, "gpu_power_peak_watts": 31.266, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2090.87109375, "cpu_memory_peak_mb": 2090.87109375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601311.9236207}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.48200000153156, 27.924100002564956], "ttft_ms": [2.9257999995024875, 3.7741999985883012], "tokens_processed": [32, 32], "throughput_tok_s": [1123.5166069194322, 1145.963522443361], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.328900002699811, 3.448199997365009, 3.805399996053893], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 7.0, "gpu_power_mean_watts": 31.266, "gpu_power_peak_watts": 31.266, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.0, "cpu_memory_peak_mb": 2097.0, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.048309}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.351299995847512, 28.060899996489752], "ttft_ms": [3.407000003790017, 3.804599997238256], "tokens_processed": [32, 32], "throughput_tok_s": [1128.696038794937, 1140.3768234091924], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.9049999981652945, 4.29739999526646, 4.164600002695806], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 30.601, "gpu_power_peak_watts": 30.601, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.0, "cpu_memory_peak_mb": 2097.0, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.175342}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.300800000375602, 27.69080000143731], "ttft_ms": [2.9500000018742867, 3.744199995708186], "tokens_processed": [32, 32], "throughput_tok_s": [1172.126824106244, 1155.6184725013006], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.977000000711996, 3.4081000048900023, 3.4850000010919757], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 30.601, "gpu_power_peak_watts": 30.601, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.24609375, "cpu_memory_peak_mb": 2097.24609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.300538}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.793299999961164, 27.54080000158865], "ttft_ms": [3.8471000007120892, 3.191300005710218], "tokens_processed": [32, 32], "throughput_tok_s": [1111.3696589152046, 1161.9125079211253], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.449099997349549, 3.2924999977694824, 4.232700004649814], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 30.601, "gpu_power_peak_watts": 30.601, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.25, "cpu_memory_peak_mb": 2097.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.4274118}
{"spec": {"backend": "tensorrt-fp32", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.946400005428586, 29.38160000485368], "ttft_ms": [2.9866000040783547, 3.5524999984772876], "tokens_processed": [32, 32], "throughput_tok_s": [1068.5758553348364, 1089.1169982136362], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.415900003048591, 3.1344000017270446, 3.804899999522604], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 30.601, "gpu_power_peak_watts": 30.601, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2097.25, "cpu_memory_peak_mb": 2097.25, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp32", "init_ms": 158.58480000315467, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.5519767}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.439300003170501, 6.3368999981321394], "ttft_ms": [0.9669999999459833, 0.6142999991425313], "tokens_processed": [8, 8], "throughput_tok_s": [1075.3699940304246, 1262.4469381492645], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9922999999835156, 0.5757000035373494, 0.9254000033251941], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 31.15, "gpu_power_peak_watts": 31.15, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2080.7265625, "cpu_memory_peak_mb": 2080.7265625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16"}, "started_at": 1765601312.6785722}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.686599997919984, 6.815200002165511], "ttft_ms": [0.9127000012085773, 0.8930999974836595], "tokens_processed": [8, 8], "throughput_tok_s": [1196.4226965107196, 1173.8466952485653], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.086099997337442, 0.5647999932989478, 0.9659999996074475], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 31.15, "gpu_power_peak_watts": 31.15, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2085.578125, "cpu_memory_peak_mb": 2085.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.8020124}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.689300003927201, 6.768799998098984], "ttft_ms": [0.5549999987124465, 0.9667999984230846], "tokens_processed": [8, 8], "throughput_tok_s": [1195.939783729733, 1181.8933935478663], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0924999951384962, 0.7505999965360388, 0.6987000015215017], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 31.15, "gpu_power_peak_watts": 31.15, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2085.578125, "cpu_memory_peak_mb": 2085.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601312.9267318}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.724500002746936, 6.556600004842039], "ttft_ms": [1.0112999952980317, 0.8755999951972626], "tokens_processed": [8, 8], "throughput_tok_s": [1189.6795295906054, 1220.1445862325004], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.1324999941280112, 0.7687000033911318, 0.607299996772781], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 15.0, "gpu_power_mean_watts": 31.15, "gpu_power_peak_watts": 31.15, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2085.578125, "cpu_memory_peak_mb": 2085.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.0523474}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.531900005938951, 6.4099999945028685], "ttft_ms": [0.828799995360896, 0.8626999988337047], "tokens_processed": [8, 8], "throughput_tok_s": [1224.7584918210964, 1248.049923067191], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.0752000016509555, 0.7789999945089221, 0.850799995532725], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.694, "gpu_power_peak_watts": 29.694, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2085.578125, "cpu_memory_peak_mb": 2085.578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.1766925}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.552999995823484, 7.406100005027838], "ttft_ms": [0.811400001111906, 0.9037999989232048], "tokens_processed": [8, 8], "throughput_tok_s": [1059.1817826590348, 1080.1906529170517], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8341000031796284, 1.0839000024134293, 0.7179000022006221], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.694, "gpu_power_peak_watts": 29.694, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2086.1640625, "cpu_memory_peak_mb": 2086.1640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.3031948}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.306100000278093, 8.221199997933581], "ttft_ms": [1.0883999930229038, 0.9577000018907711], "tokens_processed": [8, 8], "throughput_tok_s": [1094.9754314470779, 973.0939524656765], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.3333000024431385, 0.8125999956973828, 0.8028000011108816], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.694, "gpu_power_peak_watts": 29.694, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2086.1640625, "cpu_memory_peak_mb": 2086.1640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.427257}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.203499997558538, 7.6360999955795705], "ttft_ms": [0.8305000010295771, 1.1030999958165921], "tokens_processed": [8, 8], "throughput_tok_s": [1110.5712504631663, 1047.6552172746672], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8506999986129813, 0.9768000018084422, 0.8258000016212463], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.694, "gpu_power_peak_watts": 29.694, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2086.1640625, "cpu_memory_peak_mb": 2086.1640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.5539165}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.9564999978174455, 7.3308999999426305], "ttft_ms": [1.1709000027622096, 0.8175000039045699], "tokens_processed": [8, 8], "throughput_tok_s": [1005.4672283283462, 1091.2711945412714], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4133000004221685, 0.8817000052658841, 1.0436000011395663], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.049, "gpu_power_peak_watts": 30.049, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2086.16796875, "cpu_memory_peak_mb": 2086.16796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.6770513}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.791299998643808, 8.290899997518864], "ttft_ms": [1.1687999940477312, 0.977999996393919], "tokens_processed": [8, 8], "throughput_tok_s": [1026.786287447861, 964.9133390095263], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7430999942007475, 1.4442999963648617, 1.1254999990342185], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 961.0390625, "gpu_memory_peak_mb": 961.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.049, "gpu_power_peak_watts": 30.049, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2086.16796875, "cpu_memory_peak_mb": 2086.16796875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.801957}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.39710000238847, 8.57230000110576], "ttft_ms": [1.0562999959802255, 0.7685999953537248], "tokens_processed": [8, 8], "throughput_tok_s": [851.3264728444555, 933.2384539701204], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8718999999691732, 1.4848999999230728, 1.0046999959740788], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.049, "gpu_power_peak_watts": 30.049, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2087.703125, "cpu_memory_peak_mb": 2087.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601313.9272876}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.47930000256747, 10.23900000291178], "ttft_ms": [1.3045000014244579, 1.1320999983581714], "tokens_processed": [8, 8], "throughput_tok_s": [843.9441728643675, 781.3263011744261], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.750499999616295, 1.097799999115523, 1.2860999995609745], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 30.049, "gpu_power_peak_watts": 30.049, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2087.703125, "cpu_memory_peak_mb": 2087.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.0545676}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.875700001430232, 9.564699998009019], "ttft_ms": [1.3266000023577362, 1.0847000012290664], "tokens_processed": [8, 8], "throughput_tok_s": [810.0691595371885, 836.4088786543514], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7678000003797933, 1.084600000467617, 1.0165999992750585], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.748, "gpu_power_peak_watts": 29.748, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2087.703125, "cpu_memory_peak_mb": 2087.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.177405}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.2884999976377, 9.25439999991795], "ttft_ms": [1.1116999958176166, 1.20269999752054], "tokens_processed": [8, 8], "throughput_tok_s": [861.2800777342522, 864.4536652912051], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8361999973421916, 0.8044999995036051, 1.2348000018391758], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.748, "gpu_power_peak_watts": 29.748, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2087.703125, "cpu_memory_peak_mb": 2087.703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.303928}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [9.224900000845082, 10.41790000454057], "ttft_ms": [0.9830999988480471, 1.4429000002564862], "tokens_processed": [8, 8], "throughput_tok_s": [867.2180727451929, 767.9090792302916], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.531900001282338, 1.4778999975533225, 1.3761999944108538], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.748, "gpu_power_peak_watts": 29.748, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2087.95703125, "cpu_memory_peak_mb": 2087.95703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.4302478}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.980899998685345, 11.089400002674665], "ttft_ms": [1.2563999989652075, 1.2112999975215644], "tokens_processed": [8, 8], "throughput_tok_s": [728.5377337884669, 721.4096342516701], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.0287999941501766, 1.4844999968772754, 1.6397999934270047], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.748, "gpu_power_peak_watts": 29.748, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2089.48828125, "cpu_memory_peak_mb": 2089.48828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.555625}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.663699998985976, 10.102799999003764], "ttft_ms": [1.2781999976141378, 1.0388000009697862], "tokens_processed": [8, 8], "throughput_tok_s": [750.2086518526149, 791.8596825423525], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.004299996769987, 1.9294000012450852, 1.6490000052726828], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.449, "gpu_power_peak_watts": 29.449, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2089.48828125, "cpu_memory_peak_mb": 2089.48828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.6810117}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.967300000833347, 10.754799994174391], "ttft_ms": [1.2435000026016496, 1.3412000043899752], "tokens_processed": [8, 8], "throughput_tok_s": [668.4882972301954, 743.8539074955743], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.2013999987393618, 1.7762999996193685, 1.452200005587656], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.449, "gpu_power_peak_watts": 29.449, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2089.48828125, "cpu_memory_peak_mb": 2089.48828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.8078158}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.623999998846557, 11.493899997731205], "ttft_ms": [1.6058999972301535, 1.6937000036705285], "tokens_processed": [8, 8], "throughput_tok_s": [688.2312457668476, 696.0213679933817], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.220800000941381, 1.237199998286087, 1.0761999947135337], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.449, "gpu_power_peak_watts": 29.449, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2089.48828125, "cpu_memory_peak_mb": 2089.48828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601314.933706}
{"spec": {"backend": "tensorrt-fp16", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [11.916100003872998, 10.325499999453314], "ttft_ms": [1.4759999976377003, 1.3617000004160218], "tokens_processed": [8, 8], "throughput_tok_s": [671.3605959500022, 774.7808823227507], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.320699997653719, 1.3802999965264462, 1.2867999976151623], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 965.0390625, "gpu_memory_peak_mb": 965.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 4.0, "gpu_power_mean_watts": 29.449, "gpu_power_peak_watts": 29.449, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2089.48828125, "cpu_memory_peak_mb": 2089.48828125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.0603704}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.45689999795286, 21.264099996187724], "ttft_ms": [2.548099997511599, 2.8861000027973205], "tokens_processed": [32, 32], "throughput_tok_s": [1564.2643803901015, 1504.883818536267], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.881199994997587, 2.6980000038747676, 2.634800002851989], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 957.0390625, "gpu_memory_peak_mb": 957.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.229, "gpu_power_peak_watts": 31.229, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2085.30859375, "cpu_memory_peak_mb": 2085.30859375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.1845455}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.265499999572057, 20.41149999422487], "ttft_ms": [2.743099998042453, 3.053899999940768], "tokens_processed": [32, 32], "throughput_tok_s": [1504.784745274927, 1567.7436743528856], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.925000004528556, 2.787899997201748, 2.6163000002270564], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.229, "gpu_power_peak_watts": 31.229, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.46484375, "cpu_memory_peak_mb": 2101.46484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.3097758}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.31809999991674, 19.255400002293754], "ttft_ms": [2.550500001234468, 2.392199996393174], "tokens_processed": [32, 32], "throughput_tok_s": [1501.0718591302686, 1661.8714748168345], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.446500002813991, 2.940799997304566, 2.370300004258752], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.229, "gpu_power_peak_watts": 31.229, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.4765625, "cpu_memory_peak_mb": 2101.4765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.4353175}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [20.56589999847347, 21.427300001960248], "ttft_ms": [2.1411999987321906, 2.8292999995755963], "tokens_processed": [32, 32], "throughput_tok_s": [1555.9737236092387, 1493.421942898663], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.494499997235835, 2.8800000000046566, 2.7446000021882355], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 31.229, "gpu_power_peak_watts": 31.229, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2101.48046875, "cpu_memory_peak_mb": 2101.48046875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.563907}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [21.881299995584413, 19.61029999802122], "ttft_ms": [3.359099995577708, 2.1715999973821454], "tokens_processed": [32, 32], "throughput_tok_s": [1462.435961595404, 1631.7955361839934], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.692599999543745, 2.6647999984561466, 2.247299998998642], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 973.0390625, "gpu_memory_peak_mb": 973.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 30.389, "gpu_power_peak_watts": 30.389, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2101.484375, "cpu_memory_peak_mb": 2101.484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.6875598}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.841600003943313, 30.93319999607047], "ttft_ms": [3.25880000309553, 3.784699998504948], "tokens_processed": [32, 32], "throughput_tok_s": [1109.5084875882358, 1034.4872177487312], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.848499997227918, 3.8666999971610494, 4.2828000005101785], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 30.389, "gpu_power_peak_watts": 30.389, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.62890625, "cpu_memory_peak_mb": 2107.62890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.8108544}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [29.968700000608806, 28.2636000047205], "ttft_ms": [4.1409999976167455, 3.042900003492832], "tokens_processed": [32, 32], "throughput_tok_s": [1067.7807178606322, 1132.1983043439427], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.126699998276308, 4.1522000028635375, 3.5425000023678876], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 30.389, "gpu_power_peak_watts": 30.389, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.62890625, "cpu_memory_peak_mb": 2107.62890625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601315.9369617}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [30.793400001130067, 29.344300004595425], "ttft_ms": [4.202700001769699, 2.927800000179559], "tokens_processed": [32, 32], "throughput_tok_s": [1039.1837211488712, 1090.5013919224068], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.016300001647323, 4.111200003535487, 3.5910000005969778], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 1.0, "gpu_power_mean_watts": 30.389, "gpu_power_peak_watts": 30.389, "gpu_temperature_mean_c": 49.0, "gpu_temperature_peak_c": 49, "cpu_memory_mean_mb": 2107.625, "cpu_memory_peak_mb": 2107.625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.061859}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [31.525599995802622, 29.506200000469107], "ttft_ms": [3.682300004584249, 3.307999999378808], "tokens_processed": [32, 32], "throughput_tok_s": [1015.0480880383099, 1084.517830133709], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.659400005242787, 3.881800003000535, 4.3624999962048605], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.41, "gpu_power_peak_watts": 22.41, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.625, "cpu_memory_peak_mb": 2107.625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.1855936}
{"spec": {"backend": "tensorrt-fp16", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [28.033399998093955, 27.82180000212975], "ttft_ms": [3.2547999944654293, 3.8299000007100403], "tokens_processed": [32, 32], "throughput_tok_s": [1141.4955018719006, 1150.1771990867023], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [6.50350000069011, 3.023899997060653, 4.1115999993053265], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.41, "gpu_power_peak_watts": 22.41, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2107.88671875, "cpu_memory_peak_mb": 2107.88671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-fp16", "init_ms": 24.463199995807372, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 5, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.3100219}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.572400001459755, 6.596500003070105], "ttft_ms": [0.7364000048255548, 0.6594000005861744], "tokens_processed": [8, 8], "throughput_tok_s": [1217.2113684838368, 1212.7643441638272], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.463699995132629, 0.8076999947661534, 0.6759000025340356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 963.0390625, "gpu_memory_peak_mb": 963.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.41, "gpu_power_peak_watts": 22.41, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2091.81640625, "cpu_memory_peak_mb": 2091.81640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8"}, "started_at": 1765601316.4353695}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.638400001975242, 8.86079999327194], "ttft_ms": [0.7070999999996275, 0.7325000042328611], "tokens_processed": [8, 8], "throughput_tok_s": [1205.1096646209353, 902.8530162146138], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4961999986553565, 0.9150000041699968, 0.763399999414105], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.41, "gpu_power_peak_watts": 22.41, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.21484375, "cpu_memory_peak_mb": 2096.21484375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.5606456}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.237300000269897, 8.050600001297425], "ttft_ms": [1.1818999992101453, 1.2849999984609894], "tokens_processed": [8, 8], "throughput_tok_s": [1105.3846047146947, 993.7147540196668], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.4512000052491203, 1.0274000014760531, 0.9451000005356036], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.215, "gpu_power_peak_watts": 22.215, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.24609375, "cpu_memory_peak_mb": 2096.24609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.68538}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [7.140400004573166, 6.995799994911067], "ttft_ms": [0.7650000043213367, 0.8054999998421408], "tokens_processed": [8, 8], "throughput_tok_s": [1120.3854118643621, 1143.543269650278], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.186299996334128, 1.0654999932739884, 1.0315000035916455], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.215, "gpu_power_peak_watts": 22.215, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.24609375, "cpu_memory_peak_mb": 2096.24609375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.8105724}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_micro", "prompt_set": "micro", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 16, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [6.963600004382897, 7.2978000025614165], "ttft_ms": [0.8092999996733852, 1.0770000008051284], "tokens_processed": [8, 8], "throughput_tok_s": [1148.8310636689057, 1096.2207784801067], "predicted_tokens": ["", ""], "outputs": ["Hello stairs stairs stairs stairs stairs stairs stairs stairs", "Test stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8860999989556149, 1.2387999959173612, 1.058100002410356], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.215, "gpu_power_peak_watts": 22.215, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.25390625, "cpu_memory_peak_mb": 2096.25390625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601316.9374633}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.390399998461362, 9.347700004582293], "ttft_ms": [0.7213999997475185, 0.8406999986618757], "tokens_processed": [8, 8], "throughput_tok_s": [953.4706332793486, 855.8254967615944], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.56309999874793, 0.8384999964619055, 0.7134999978006817], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 26.0, "gpu_power_mean_watts": 22.215, "gpu_power_peak_watts": 22.215, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.83203125, "cpu_memory_peak_mb": 2096.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.0624115}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [10.130300004675519, 12.245500001881737], "ttft_ms": [1.3499999986379407, 1.0016000014729798], "tokens_processed": [8, 8], "throughput_tok_s": [789.7100773232477, 653.3012125899847], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.8415000013192184, 1.6751999937696382, 1.3633999988087453], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.076, "gpu_power_peak_watts": 20.076, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.83203125, "cpu_memory_peak_mb": 2096.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.1864226}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [8.160099998349324, 9.17680000566179], "ttft_ms": [1.0818000009749085, 1.087999997253064], "tokens_processed": [8, 8], "throughput_tok_s": [980.3801425985331, 871.763577179873], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.9069999980274588, 1.0143999970750883, 1.35529999533901], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.076, "gpu_power_peak_watts": 20.076, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.83203125, "cpu_memory_peak_mb": 2096.83203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.3087397}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.417100002698135, 13.467500000842847], "ttft_ms": [1.58200000441866, 1.561600001878105], "tokens_processed": [8, 8], "throughput_tok_s": [596.2540339112943, 594.022647076245], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.825699997425545, 1.5827000024728477, 1.575599999341648], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.076, "gpu_power_peak_watts": 20.076, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.8359375, "cpu_memory_peak_mb": 2096.8359375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.4343894}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [13.50030000321567, 13.00729999638861], "ttft_ms": [1.5510000012000091, 1.5567999944323674], "tokens_processed": [8, 8], "throughput_tok_s": [592.5794240197965, 615.0392473627229], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [1.7406999977538362, 1.6869999963091686, 1.659299996390473], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 967.0390625, "gpu_memory_peak_mb": 967.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 20.076, "gpu_power_peak_watts": 20.076, "gpu_temperature_mean_c": 48.0, "gpu_temperature_peak_c": 48, "cpu_memory_mean_mb": 2096.84375, "cpu_memory_peak_mb": 2096.84375, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.5610588}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [17.360500001814216, 17.63539999956265], "ttft_ms": [1.989199998206459, 2.1570999961113557], "tokens_processed": [8, 8], "throughput_tok_s": [460.81622068281325, 453.63303356875355], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.4803999986033887, 2.225100004579872, 2.1643999934894964], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 18.041, "gpu_power_peak_watts": 18.041, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.38671875, "cpu_memory_peak_mb": 2098.38671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.6858566}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [18.21370000106981, 17.73220000177389], "ttft_ms": [2.187099998991471, 2.213200001278892], "tokens_processed": [8, 8], "throughput_tok_s": [439.22981050144165, 451.15665282365967], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.8249000024516135, 2.709399996092543, 2.2266000014496967], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 18.041, "gpu_power_peak_watts": 18.041, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.64453125, "cpu_memory_peak_mb": 2098.64453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.8097444}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [18.231800000648946, 17.414800000551622], "ttft_ms": [2.048599999397993, 2.0491999966907315], "tokens_processed": [8, 8], "throughput_tok_s": [438.79375594923414, 459.3793784451499], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.8261999977985397, 2.2873000052641146, 2.117500000167638], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 18.041, "gpu_power_peak_watts": 18.041, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.64453125, "cpu_memory_peak_mb": 2098.64453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601317.9346764}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [19.65409999684198, 18.48690000042552], "ttft_ms": [2.300000000104774, 2.181100004236214], "tokens_processed": [8, 8], "throughput_tok_s": [407.03975258523366, 432.7388583167465], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.146599996194709, 2.2019000025466084, 2.070100003038533], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 0.0, "gpu_power_mean_watts": 18.041, "gpu_power_peak_watts": 18.041, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.64453125, "cpu_memory_peak_mb": 2098.64453125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.059901}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [18.18009999988135, 18.467199995939154], "ttft_ms": [2.1766999998362735, 2.2517000033985823], "tokens_processed": [8, 8], "throughput_tok_s": [440.04158393255324, 433.200485279802], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [2.704500002437271, 2.7058999985456467, 2.252899997984059], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 969.0390625, "gpu_memory_peak_mb": 969.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.01, "gpu_power_peak_watts": 14.01, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2098.671875, "cpu_memory_peak_mb": 2098.671875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.1850376}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [22.533499999553896, 23.112599999876693], "ttft_ms": [2.639199999975972, 2.6919000010821037], "tokens_processed": [8, 8], "throughput_tok_s": [355.02695986679294, 346.1315472963959], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.7068999954499304, 2.8290999980526976, 2.7365999994799495], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.01, "gpu_power_peak_watts": 14.01, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2100.203125, "cpu_memory_peak_mb": 2100.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.3129525}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.940000003378373, 29.039600005489774], "ttft_ms": [3.245699997933116, 2.9977999947732314], "tokens_processed": [8, 8], "throughput_tok_s": [296.9561989234139, 275.4858881832961], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.691199999593664, 4.196199995931238, 2.7674000011757016], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.01, "gpu_power_peak_watts": 14.01, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2100.203125, "cpu_memory_peak_mb": 2100.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.4417582}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [26.835600001504645, 29.141500002879184], "ttft_ms": [3.074799999012612, 3.049400002055336], "tokens_processed": [8, 8], "throughput_tok_s": [298.11146385962854, 274.5225880345761], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [4.000000000814907, 3.4321999992243946, 3.198600003088359], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.01, "gpu_power_peak_watts": 14.01, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2100.203125, "cpu_memory_peak_mb": 2100.203125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.5613744}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [27.29350000299746, 26.514099998166785], "ttft_ms": [3.028399994946085, 2.6212999946437776], "tokens_processed": [8, 8], "throughput_tok_s": [293.1100811226634, 301.7262513361996], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.729600000951905, 3.2189000048674643, 3.2642000005580485], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.288, "gpu_power_peak_watts": 14.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2100.20703125, "cpu_memory_peak_mb": 2100.20703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.6865482}
{"spec": {"backend": "tensorrt-int8", "scenario": "single_long", "prompt_set": "long", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 1, "seq_len": 35, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.98100000229897, 26.55609999783337], "ttft_ms": [2.685499996005092, 3.2591999988653697], "tokens_processed": [8, 8], "throughput_tok_s": [320.24338494310757, 301.2490539142681], "predicted_tokens": ["", ""], "outputs": ["Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words. stairs stairs stairs stairs stairs stairs stairs stairs", "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [3.8061000013840385, 2.7934000027016737, 2.855500002624467], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 971.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.288, "gpu_power_peak_watts": 14.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2100.20703125, "cpu_memory_peak_mb": 2100.20703125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601318.8087068}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [48.24559999542544, 49.93079999985639], "ttft_ms": [5.439600005047396, 5.8172999997623265], "tokens_processed": [32, 32], "throughput_tok_s": [663.2729202877399, 640.886987592669], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.087900001963135, 5.443900001409929, 5.836500000441447], "resource_metrics": {"samples": 2, "duration_s": 0.11292719841003418, "gpu_memory_mean_mb": 971.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 16.0, "gpu_power_mean_watts": 14.288, "gpu_power_peak_watts": 14.288, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2103.8125, "cpu_memory_peak_mb": 2112.171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.0339599}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [25.55189999839058, 23.00400000240188], "ttft_ms": [3.275399998528883, 2.6477999999769963], "tokens_processed": [32, 32], "throughput_tok_s": [1252.3530540592112, 1391.062423781031], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.148599998094141, 5.263899998681154, 4.982600003131665], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 13.507, "gpu_power_peak_watts": 13.507, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2106.29296875, "cpu_memory_peak_mb": 2106.29296875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.1524036}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.15560000372352, 24.634200002765283], "ttft_ms": [3.484799999569077, 2.658600002177991], "tokens_processed": [32, 32], "throughput_tok_s": [1324.7445724828729, 1299.0070713239268], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.45379999675788, 3.328699996927753, 3.3342999959131703], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 13.507, "gpu_power_peak_watts": 13.507, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.171875, "cpu_memory_peak_mb": 2112.171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.2747808}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [24.356200003239792, 28.495200000179466], "ttft_ms": [3.507800000079442, 3.6269999982323498], "tokens_processed": [32, 32], "throughput_tok_s": [1313.8338491120721, 1122.9961537311008], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [5.445599999802653, 3.7031999963801354, 2.72170000243932], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 13.507, "gpu_power_peak_watts": 13.507, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.171875, "cpu_memory_peak_mb": 2112.171875, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.3999534}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_short", "prompt_set": "short", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 19, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [34.498200002417434, 28.648100000282284], "ttft_ms": [4.341600004408974, 4.2725000021164306], "tokens_processed": [32, 32], "throughput_tok_s": [927.5846275387593, 1117.0025237165707], "predicted_tokens": ["", ""], "outputs": ["Summarize RLHF in one sentence. factors factors factors factors factors factors factors factors", "List two ways to improve throughput on local LLMs. stairs stairs stairs stairs stairs stairs stairs stairs"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.594800001243129, 4.324900000938214, 3.6965000035706908], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 979.0390625, "gpu_memory_peak_mb": 979.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 13.507, "gpu_power_peak_watts": 13.507, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2112.17578125, "cpu_memory_peak_mb": 2112.17578125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.5282035}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 0, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [38.667399996484164, 39.74880000168923], "ttft_ms": [4.965200001606718, 4.976600001100451], "tokens_processed": [32, 32], "throughput_tok_s": [827.5705116689926, 805.0557500764821], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [9.843399995588697, 4.05260000115959, 4.368499998236075], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 20.515, "gpu_power_peak_watts": 20.515, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.3125, "cpu_memory_peak_mb": 2118.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.6523294}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 1, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [41.529299996909685, 38.71479999361327], "ttft_ms": [5.928699996729847, 4.314599995268509], "tokens_processed": [32, 32], "throughput_tok_s": [770.5403173754726, 826.5572857222302], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [7.995799998752773, 5.7740999982343055, 5.2680000007967465], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 20.515, "gpu_power_peak_watts": 20.515, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.3125, "cpu_memory_peak_mb": 2118.3125, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.776092}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 2, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [41.44349999842234, 38.94620000210125], "ttft_ms": [5.390700003772508, 5.188999995880295], "tokens_processed": [32, 32], "throughput_tok_s": [772.1355580783035, 821.646270965422], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [8.558699999412056, 4.934899996442255, 4.709499997261446], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 75.0, "gpu_power_mean_watts": 20.515, "gpu_power_peak_watts": 20.515, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2118.31640625, "cpu_memory_peak_mb": 2118.31640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601319.9009476}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 3, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [43.79730000073323, 40.23589999997057], "ttft_ms": [4.537100001471117, 5.803399995784275], "tokens_processed": [32, 32], "throughput_tok_s": [730.6386466623347, 795.3096612732263], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [18.0809000012232, 4.850500001339242, 4.923500004224479], "resource_metrics": {"samples": 2, "duration_s": 0.1154332160949707, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 51.0, "gpu_power_mean_watts": 20.6645, "gpu_power_peak_watts": 20.814, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2115.142578125, "cpu_memory_peak_mb": 2118.31640625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601320.127445}
{"spec": {"backend": "tensorrt-int8", "scenario": "batch_medium", "prompt_set": "medium", "model": "models/tiny-gpt2", "repetition": 4, "batch_size": 4, "seq_len": 27, "mode": "generate"}, "status": "ok", "error": null, "latencies_ms": [36.313199998403434, 42.453000001842156], "ttft_ms": [4.584600006637629, 3.593200002796948], "tokens_processed": [32, 32], "throughput_tok_s": [881.2222553068011, 753.7747626460186], "predicted_tokens": ["", ""], "outputs": ["Explain how backpressure works in an inference service and when to enable queueing. factors factors factors factors factors factors factors factors", "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences. factors factors factors factors factors factors factors factors"], "degraded_count": 0, "degraded_reasons": [], "warmup_latencies_ms": [10.764799997559749, 5.076800000097137, 4.649299997254275], "resource_metrics": {"samples": 1, "duration_s": 0.0, "gpu_memory_mean_mb": 985.0390625, "gpu_memory_peak_mb": 985.0390625, "gpu_memory_total_mb": 12282.0, "gpu_utilization_mean_pct": 27.0, "gpu_power_mean_watts": 20.814, "gpu_power_peak_watts": 20.814, "gpu_temperature_mean_c": 47.0, "gpu_temperature_peak_c": 47, "cpu_memory_mean_mb": 2110.4765625, "cpu_memory_peak_mb": 2110.4765625, "cpu_utilization_mean_pct": 0.0}, "export_metadata": {"model": "models/tiny-gpt2", "output_path": "artifacts\\onnx\\tiny-gpt2.onnx", "export_time_s": null, "file_size_mb": 1.8593082427978516, "opset_version": 17, "dynamic_axes": true, "trt_friendly_inputs": true, "valid": null, "reused": true, "timestamp": 1765601234.1680925}, "trt_build_metadata": [{"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp32.plan", "precision": "fp32", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1689758, "build_time_s": null, "file_size_mb": 4.992092132568359, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_fp16.plan", "precision": "fp16", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1690826, "build_time_s": null, "file_size_mb": 4.228824615478516, "built": false, "reused": true, "error": null}, {"onnx_path": "artifacts\\onnx\\tiny-gpt2.onnx", "output_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "precision": "int8", "workspace_gb": 6, "dynamic_shapes": true, "profiles": 5, "timestamp": 1765601234.1691473, "build_time_s": null, "file_size_mb": 5.178691864013672, "built": false, "reused": true, "error": null}], "backend_metadata": {"backend": "tensorrt-int8", "init_ms": 24.509800001396798, "engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "engine": {"engine_path": "artifacts\\tensorrt\\tiny-gpt2_int8.plan", "api": "tensors", "num_io_tensors": 3, "num_profiles": 6, "input_names": ["input_ids", "attention_mask"], "output_names": ["logits"], "output_dtype": "float32"}}, "started_at": 1765601320.2464528}
