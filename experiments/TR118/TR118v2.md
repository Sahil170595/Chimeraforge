# TR118v2: Tiny vs GPT-2 (124M) Data Runs

Goal: gather **artifact-driven** runs for two model sizes (tiny/toy vs 124M GPT-2) using the existing TR118 pipeline, without overwriting results.

Recommended defaults:

- Tiny: repo-local `models/tiny-gpt2` (toy/sanity fixture; not a 124M model)
- 124M: `gpt2` (HuggingFace Hub)

TR118 writes some processed artifacts to fixed filenames (e.g. `latency_summary_<mode>.csv`, `perplexity_results.csv`), so **each run must use a unique `output.results_dir`**.

## Recommended Runner

Use the TR118v2 orchestrator which generates isolated configs and run folders automatically:

```bash
python scripts/tr118/run_tr118v2.py --device cuda --label tr118v2
```

This creates one run bundle per model under:

- `scripts/tr118/results/tr118v2/<run_slug>/<model_tag>/`
- `artifacts/tr118v2/<model_tag>/` (ONNX/TRT artifacts + INT8 calibration cache)

It also writes a study manifest you can use later for TR118v2 report generation:

- `scripts/tr118/results/tr118v2/<run_slug>/tr118v2_run_manifest.json`

## Options

Run only one side:

```bash
python scripts/tr118/run_tr118v2.py --models tiny --device cuda --label tiny_only
python scripts/tr118/run_tr118v2.py --models gpt2 --device cuda --label gpt2_only
```

Skip perplexity (faster, not publishable):

```bash
python scripts/tr118/run_tr118v2.py --skip-accuracy
```

Generate per-run plots/reports into the run bundle:

```bash
python scripts/tr118/run_tr118v2.py --with-plots --with-report
```

Force re-export / TRT rebuild (first mode only):

```bash
python scripts/tr118/run_tr118v2.py --force-export --force-trt-rebuild
```

## Notes

- For GPT-2 (124M), always use `--device cuda` for perplexity unless you explicitly want a very slow CPU baseline.
- If you rerun, you get a new `<run_slug>` folder, so nothing is overwritten.
- If one model run fails, `run_tr118v2.py` still writes `tr118v2_run_manifest.json` with `success/error` per model.
