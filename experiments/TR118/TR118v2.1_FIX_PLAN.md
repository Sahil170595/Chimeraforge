# TR118v2.1 Fix Plan

**Status:** Planning  
**Target:** TR118v2.2 (Corrected Report)  
**Date:** 2025-12-13

---

## Issues to Fix

### 1. Run-Count/Degradation Math Inconsistencies

**Problem:**
- Inconsistent reporting of total runs (720 vs 360)
- Degradation counts may not match actual data
- Per-model vs aggregate counts unclear

**Action Items:**
- [ ] Audit all run-count statements in report
- [ ] Verify: 720 total = 360 prefill + 360 generate
- [ ] Verify: 360 per model = 6 backends × 6 scenarios × 5 reps × 2 modes
- [ ] Create verification script: `scripts/tr118/verify_run_counts.py`
- [ ] Check degradation counts match actual JSONL data
- [ ] Update all tables/sections with correct counts

**Files to Update:**
- `reports/generated/Technical_Report_118_v2.1.md` (multiple sections)
- Section 2.1 (Benchmark Configuration)
- Section 4 (Comprehensive Results)
- Section 7 (Generate Mode Degradation Analysis)
- Section 15.3 (Degradation Rate Statistics)

**Verification:**
```python
# scripts/tr118/verify_run_counts.py
# Count actual runs from JSONL files
# Verify against report claims
```

**Script Location:** `scripts/tr118/verify_run_counts.py`

---

### 2. Crossover Power-Law Calculations Need Verification

**Problem:**
- Power-law fit (k ≈ -0.28) may be incorrect
- Crossover point (30-50M) needs validation
- 3-point model may need 4th point for confidence

**Action Items:**
- [ ] Re-calculate power-law fit with proper least-squares
- [ ] Verify formula: `A(P) = A₀ × (P/P₀)^k`
- [ ] Check all intermediate calculations in Section 13.1
- [ ] Add confidence intervals for crossover estimate
- [ ] Consider adding 4th data point (45M model already tested - see Addendum A)
- [ ] Update Section 13.4 with 45M validation data

**Files to Update:**
- Section 5 (The Crossover Phenomenon)
- Section 13.1 (The Inflection Point)
- Section 13.4 (Empirical Validation: 11M Parameter Model)
- Addendum A (45M Model) - integrate into main report

**Verification:**
```python
# scripts/tr118/verify_crossover_math.py
# Re-fit power-law with scipy.optimize.curve_fit
# Calculate 95% CI for crossover point
```

**Script Location:** `scripts/tr118/verify_crossover_math.py`

---

### 3. Tiny Model Vocab/Perplexity Reconciliation Needed

**Problem:**
- Tiny-gpt2 vocab size inconsistency (50,257 vs 256 mentioned)
- Perplexity ~50,286 explanation needs clarity
- Model architecture details may conflict

**Action Items:**
- [ ] Verify actual vocab size from model config
- [ ] Check if tiny-gpt2 uses full GPT-2 vocab or subset
- [ ] Reconcile perplexity calculation (exp(ln(50257)) ≈ 50286)
- [ ] Update Section 3.1 (Tiny-GPT2 Specifications)
- [ ] Update Section 8.1 (Tiny-GPT2 Perplexity) with correct interpretation
- [ ] Add note about untrained model behavior

**Files to Update:**
- Section 3.1 (Model Specifications - Tiny-GPT2)
- Section 8.1 (Tiny-GPT2 Perplexity)
- Section 11.5 (Known Limitations)

**Verification:**
```python
# scripts/tr118/verify_tiny_model.py
# Load model config, check vocab size
# Verify perplexity calculation matches report
```

**Script Location:** `scripts/tr118/verify_tiny_model.py`

---

### 4. TensorRT Degradation Root Cause Needs Clearer Split (Hard vs Soft Failures)

**Problem:**
- All TRT generate failures lumped together
- Need to distinguish:
  - Hard failures (profile mismatch, engine build issues)
  - Soft failures (timeout due to uncached generate, but engine works)
- Current explanation mixes both

**Action Items:**
- [ ] Analyze actual error messages from degraded runs
- [ ] Categorize failures:
  - Profile mismatch (hard failure)
  - Timeout on uncached generate (soft failure - engine works but slow)
  - Invalid output (hard failure)
- [ ] Update Section 7 (Generate Mode Degradation Analysis)
- [ ] Add failure classification table
- [ ] Clarify: prefill works → engine is valid, generate timeout → uncached issue
- [ ] Update recommendations to note KV-cache would fix soft failures

**Files to Update:**
- Section 7.1 (TensorRT 100% Degradation)
- Section 7.2 (Uncached Generate Limitation)
- Section 10.3 (Future Work - TR120)

**Verification:**
```python
# scripts/tr118/analyze_trt_failures.py
# Parse error logs from degraded runs
# Classify failure types
# Generate failure breakdown table
```

**Script Location:** `scripts/tr118/analyze_trt_failures.py`  
**Note:** Similar to existing `audit_degraded.py`, but with failure classification

---

### 5. Break-Even Amortization Math Needs Correction

**Problem:**
- Section 9.2 calculation may be incorrect
- Formula: `Time to recover 240s: 240 × 2121 / 4163 ≈ 122s` looks wrong
- Need to verify break-even token calculation

**Action Items:**
- [ ] Re-calculate break-even point correctly
- [ ] Formula should be: `build_time / (throughput_delta)`
- [ ] Verify: 240s build time, 4,163 tok/s delta
- [ ] Calculate: tokens needed = 240s × 2,121 tok/s = 509,040 tokens
- [ ] Or: time needed = 240s / (4163/2121) = 240s / 1.96 = 122s (this seems right)
- [ ] But then: tokens = 122s × 2,121 = 258,762 tokens (not 0.77M)
- [ ] Fix the calculation chain
- [ ] Add unit verification

**Files to Update:**
- Section 9.2 (Cost Analysis - GPT-2 Example)

**Correct Calculation:**
```
Build overhead: 240s (TRT INT8 build time)
Throughput delta: 6,284 - 2,121 = 4,163 tok/s
Time to recover: 240s / (4,163 / 2,121) = 240s / 1.96 = 122.4s
Tokens processed in 122.4s at baseline: 122.4s × 2,121 tok/s = 259,610 tokens
Total time (build + recover): 240s + 122.4s = 362.4s
Total tokens: 362.4s × 2,121 tok/s = 768,650 tokens ≈ 0.77M tokens ✓
```

**Verification:**
```python
# scripts/tr118/verify_amortization.py
# Calculate break-even with unit tests
# Verify all intermediate steps
```

**Script Location:** `scripts/tr118/verify_amortization.py`

---

## Script Locations

All verification scripts will be created in: **`scripts/tr118/`**

This matches the existing pattern:
- `analyze_results.py` - analysis script
- `audit_degraded.py` - verification/audit script
- `validate_accuracy.py` - validation script

**Naming Convention:**
- `verify_*.py` - for verification/audit scripts
- `analyze_*.py` - for analysis scripts (already exists: `analyze_trt_failures.py`)

---

## Implementation Order

### Phase 1: Data Verification (Foundation)
1. Run-count verification script
2. Degradation count audit
3. Tiny model vocab check

### Phase 2: Math Corrections
4. Crossover power-law re-calculation
5. Break-even amortization fix
6. Add 45M model data to crossover analysis

### Phase 3: Clarity Improvements
7. TRT failure classification
8. Update all affected sections
9. Generate TR118v2.2 report

---

## Verification Scripts to Create

1. **`scripts/tr118/verify_run_counts.py`**
   - Count runs from JSONL files
   - Verify against report claims
   - Generate discrepancy report

2. **`scripts/tr118/verify_crossover_math.py`**
   - Re-fit power-law with proper method
   - Calculate confidence intervals
   - Validate crossover point

3. **`scripts/tr118/verify_tiny_model.py`**
   - Load model config
   - Check vocab size
   - Verify perplexity calculation

4. **`scripts/tr118/analyze_trt_failures.py`**
   - Parse error logs
   - Classify failure types
   - Generate breakdown table

5. **`scripts/tr118/verify_amortization.py`**
   - Unit test break-even calculation
   - Verify all steps
   - Generate corrected section

---

## Success Criteria

- [ ] All run counts match actual data (0 discrepancies)
- [ ] Power-law fit verified with confidence intervals
- [ ] Tiny model specs reconciled (vocab, perplexity)
- [ ] TRT failures clearly categorized (hard vs soft)
- [ ] Break-even math verified and corrected
- [ ] TR118v2.2 report generated with all fixes
- [ ] All verification scripts pass

---

## Timeline Estimate

- **Phase 1:** 2-3 hours (data verification)
- **Phase 2:** 3-4 hours (math corrections)
- **Phase 3:** 2-3 hours (clarity + report generation)
- **Total:** ~8-10 hours

---

## Notes

- Keep all original data intact
- Document all changes in changelog
- Maintain reproducibility (all scripts should work)
- Update git commit message with fixes applied
