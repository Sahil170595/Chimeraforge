{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/e2e_kv/runs/single_long/transformers-gpu-compile/fp32/run_1766334855432.json",
  "spec": {
    "mode": "e2e_kv",
    "scenario": "single_long",
    "prompt_set": "long",
    "prompts": [
      "Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words.",
      "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    150.94239999962156,
    106.02719999997134,
    104.43550000036339,
    122.08830000008675,
    249.3984000002456,
    248.64100000013423,
    258.9778000001388,
    250.55380000003424,
    109.42339999974138,
    101.5717999998742,
    99.23319999961677,
    100.6681000003482,
    96.62969999999405,
    97.12720000015906,
    99.06820000014704,
    177.32400000022608,
    105.05360000024666,
    104.42030000012892,
    127.82749999996668,
    104.58169999992606,
    103.52880000027653,
    101.75659999958953,
    103.12740000017584,
    106.58289999992121,
    103.7494000006518,
    104.54489999938232,
    101.63960000045336,
    102.82469999992827,
    106.93350000065038,
    108.23139999956766,
    134.29269999960525,
    108.03720000012618,
    201.3134000003447,
    224.62110000014945,
    218.61010000020542,
    261.46090000020195,
    241.99339999995573,
    214.0767999999298,
    225.57650000044305,
    204.17759999963891,
    226.71309999941514,
    221.1539999998422,
    219.4088000001102,
    231.54490000024452,
    230.6757999999718,
    234.58949999985634,
    230.47420000011698,
    233.76989999997022,
    231.82590000033088,
    242.39499999976033,
    233.3917000000838,
    244.47280000003957,
    240.9480999999687,
    242.14610000035464,
    200.66959999985556,
    212.76669999997466,
    225.45399999989968,
    222.93409999974756,
    220.66339999992124,
    226.07519999974102
  ],
  "ttft_ms": [
    150.94239999962156,
    106.02719999997134,
    104.43550000036339,
    122.08830000008675,
    249.3984000002456,
    248.64100000013423,
    258.9778000001388,
    250.55380000003424,
    109.42339999974138,
    101.5717999998742,
    99.23319999961677,
    100.6681000003482,
    96.62969999999405,
    97.12720000015906,
    99.06820000014704,
    177.32400000022608,
    105.05360000024666,
    104.42030000012892,
    127.82749999996668,
    104.58169999992606,
    103.52880000027653,
    101.75659999958953,
    103.12740000017584,
    106.58289999992121,
    103.7494000006518,
    104.54489999938232,
    101.63960000045336,
    102.82469999992827,
    106.93350000065038,
    108.23139999956766,
    134.29269999960525,
    108.03720000012618,
    201.3134000003447,
    224.62110000014945,
    218.61010000020542,
    261.46090000020195,
    241.99339999995573,
    214.0767999999298,
    225.57650000044305,
    204.17759999963891,
    226.71309999941514,
    221.1539999998422,
    219.4088000001102,
    231.54490000024452,
    230.6757999999718,
    234.58949999985634,
    230.47420000011698,
    233.76989999997022,
    231.82590000033088,
    242.39499999976033,
    233.3917000000838,
    244.47280000003957,
    240.9480999999687,
    242.14610000035464,
    200.66959999985556,
    212.76669999997466,
    225.45399999989968,
    222.93409999974756,
    220.66339999992124,
    226.07519999974102
  ],
  "tokens": [
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89,
    91,
    89
  ],
  "tokens_per_s": [
    602.8789790027729,
    839.4072464426492,
    871.3512167767029,
    728.9805820863814,
    364.87804252116445,
    357.94579333236254,
    351.38146976285697,
    355.2131318702324,
    831.6319909655072,
    876.2274568345764,
    917.0317998447237,
    884.0933721774043,
    941.7394444979712,
    916.3241604808359,
    918.5591340093484,
    501.9061153588151,
    866.2244796921414,
    852.3246916537313,
    711.8968922964442,
    851.009306600131,
    878.9824667122282,
    874.6361415412761,
    882.4037064819324,
    835.0307600944035,
    877.1135061930796,
    851.3088634694359,
    895.3203278996975,
    865.5507869224232,
    850.9961798636211,
    822.3121940615711,
    677.6243235877117,
    823.7903240725977,
    452.0315090790985,
    396.2227947416373,
    416.2662200873358,
    340.39506480675027,
    376.0433135780424,
    415.7386508020915,
    403.41081628547863,
    435.89502472434486,
    401.388362649687,
    402.4345026545462,
    414.7509124517991,
    384.3746936335286,
    394.49305041972815,
    379.3861191573131,
    394.8381207091892,
    380.7162513223958,
    392.53595046916723,
    367.1692897959446,
    389.9024686823367,
    364.04867944403463,
    377.67469425993323,
    367.5467001115015,
    453.4817431243472,
    418.29854013814474,
    403.6300087824589,
    399.2211151192248,
    412.39281185748285,
    393.6743172187925
  ],
  "started_at": 1766334844.966313,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}