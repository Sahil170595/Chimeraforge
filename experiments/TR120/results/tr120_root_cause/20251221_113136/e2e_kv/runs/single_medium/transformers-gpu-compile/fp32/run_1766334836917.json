{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/e2e_kv/runs/single_medium/transformers-gpu-compile/fp32/run_1766334836917.json",
  "spec": {
    "mode": "e2e_kv",
    "scenario": "single_medium",
    "prompt_set": "medium",
    "prompts": [
      "Explain how backpressure works in an inference service and when to enable queueing.",
      "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    110.04330000014306,
    110.10300000043571,
    177.59739999974045,
    118.72889999995095,
    115.2035000000069,
    115.31130000003031,
    112.72090000056778,
    113.481899999897,
    129.3804000001728,
    119.03780000011466,
    113.39650000036272,
    112.09240000016507,
    111.48019999973258,
    143.71020000044155,
    118.05259999982809,
    118.31849999998667,
    130.81510000029084,
    110.68650000061098,
    110.96090000000913,
    112.39220000015848,
    109.68730000013238,
    111.97599999968588,
    107.7596000000085,
    113.03620000035153,
    132.9009999999471,
    113.60449999983757,
    112.09929999995438,
    111.0900000003312,
    112.84599999999045,
    112.40420000012818,
    112.01459999983854,
    125.40069999977277,
    126.57089999993332,
    110.70870000048671,
    110.80950000041412,
    107.41809999944962,
    106.88449999997829,
    107.95159999952375,
    108.33850000017264,
    135.19359999963854,
    117.72870000004332,
    117.90019999989454,
    119.64230000012321,
    114.95500000000902,
    109.19060000014724,
    108.80150000002686,
    111.07690000017101,
    132.68280000011146,
    121.72819999977946,
    110.7878000002529,
    112.29890000004161,
    106.072299999596,
    108.68749999963256,
    114.61340000005293,
    110.88289999952394,
    121.07830000059039,
    112.19930000015665,
    111.58500000010463,
    109.10139999987223,
    107.85059999989244
  ],
  "ttft_ms": [
    110.04330000014306,
    110.10300000043571,
    177.59739999974045,
    118.72889999995095,
    115.2035000000069,
    115.31130000003031,
    112.72090000056778,
    113.481899999897,
    129.3804000001728,
    119.03780000011466,
    113.39650000036272,
    112.09240000016507,
    111.48019999973258,
    143.71020000044155,
    118.05259999982809,
    118.31849999998667,
    130.81510000029084,
    110.68650000061098,
    110.96090000000913,
    112.39220000015848,
    109.68730000013238,
    111.97599999968588,
    107.7596000000085,
    113.03620000035153,
    132.9009999999471,
    113.60449999983757,
    112.09929999995438,
    111.0900000003312,
    112.84599999999045,
    112.40420000012818,
    112.01459999983854,
    125.40069999977277,
    126.57089999993332,
    110.70870000048671,
    110.80950000041412,
    107.41809999944962,
    106.88449999997829,
    107.95159999952375,
    108.33850000017264,
    135.19359999963854,
    117.72870000004332,
    117.90019999989454,
    119.64230000012321,
    114.95500000000902,
    109.19060000014724,
    108.80150000002686,
    111.07690000017101,
    132.68280000011146,
    121.72819999977946,
    110.7878000002529,
    112.29890000004161,
    106.072299999596,
    108.68749999963256,
    114.61340000005293,
    110.88289999952394,
    121.07830000059039,
    112.19930000015665,
    111.58500000010463,
    109.10139999987223,
    107.85059999989244
  ],
  "tokens": [
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83,
    81,
    83
  ],
  "tokens_per_s": [
    736.0738909128925,
    753.8395865659568,
    456.0877580421694,
    699.0715824035622,
    703.103638344279,
    719.7906883365132,
    718.5890105525417,
    731.3941694673365,
    626.0608252864562,
    697.2575097987367,
    714.3077608192573,
    740.4605486177276,
    726.5864252144713,
    577.551210698649,
    686.1348246469621,
    701.4963847581685,
    619.1945731021871,
    749.8656114299562,
    729.986869248477,
    738.4854109082567,
    738.4628849456795,
    741.2302636299996,
    751.6731687941826,
    734.2780454380268,
    609.4762266652037,
    730.6048616042382,
    722.5736467581239,
    747.1419569695971,
    717.7923896284037,
    738.406571995578,
    723.1200218553363,
    661.8782829772912,
    639.9575257823298,
    749.7152436948054,
    730.9842567622567,
    772.6816988982794,
    757.8273744089785,
    768.8630830887747,
    747.6566502201057,
    613.9343874282652,
    688.0225467534272,
    703.985234970545,
    677.0180780536364,
    722.0216606497628,
    741.8220982382254,
    762.8571297268834,
    729.2245282311201,
    625.5520685418929,
    665.4168877889163,
    749.1799638571263,
    721.2893447751491,
    782.4851539970014,
    745.2558941945839,
    724.1736132071962,
    730.5003747227729,
    685.5068166599241,
    721.9296377061792,
    743.8275753902601,
    742.4286031168698,
    769.583108486024
  ],
  "started_at": 1766334829.9443996,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}