{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/e2e_kv/runs/single_micro/transformers-gpu-compile/fp32/run_1766334806215.json",
  "spec": {
    "mode": "e2e_kv",
    "scenario": "single_micro",
    "prompt_set": "micro",
    "prompts": [
      "Hello",
      "Test"
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    109.80620000009367,
    142.10589999947842,
    198.14610000003086,
    107.09099999985483,
    109.71519999975499,
    113.12150000003385,
    106.26360000014756,
    104.97820000045976,
    126.78069999992658,
    198.85710000016843,
    105.60179999993125,
    106.0616999998274,
    107.39560000001802,
    111.83010000013383,
    126.34170000001177,
    115.33579999968424,
    108.06869999987612,
    106.25980000031632,
    106.82500000029904,
    106.50470000018686,
    118.06010000009337,
    110.67270000012286,
    185.93989999999394,
    106.22030000013183,
    107.42960000015955,
    104.9857999996675,
    209.07330000000002,
    108.72900000003938,
    107.079800000065,
    108.63779999999679,
    186.95819999993546,
    111.88469999979134,
    107.94099999975515,
    106.27829999975802,
    106.94549999971059,
    105.89649999974426,
    104.10679999949934,
    107.90600000018458,
    107.29319999973086,
    108.98439999982656,
    106.16399999980786,
    105.64690000001065,
    173.02199999949153,
    105.92630000019199,
    104.74399999975503,
    105.79349999989063,
    104.16230000009818,
    104.24760000023525,
    106.88860000027489,
    107.07820000061474,
    104.4597000000067,
    103.95989999960875,
    103.51290000016888,
    104.80429999961416,
    106.9843000000219,
    128.57160000021395,
    104.91570000021966,
    108.52539999996225,
    202.36590000013166,
    104.47470000008252
  ],
  "ttft_ms": [
    109.80620000009367,
    142.10589999947842,
    198.14610000003086,
    107.09099999985483,
    109.71519999975499,
    113.12150000003385,
    106.26360000014756,
    104.97820000045976,
    126.78069999992658,
    198.85710000016843,
    105.60179999993125,
    106.0616999998274,
    107.39560000001802,
    111.83010000013383,
    126.34170000001177,
    115.33579999968424,
    108.06869999987612,
    106.25980000031632,
    106.82500000029904,
    106.50470000018686,
    118.06010000009337,
    110.67270000012286,
    185.93989999999394,
    106.22030000013183,
    107.42960000015955,
    104.9857999996675,
    209.07330000000002,
    108.72900000003938,
    107.079800000065,
    108.63779999999679,
    186.95819999993546,
    111.88469999979134,
    107.94099999975515,
    106.27829999975802,
    106.94549999971059,
    105.89649999974426,
    104.10679999949934,
    107.90600000018458,
    107.29319999973086,
    108.98439999982656,
    106.16399999980786,
    105.64690000001065,
    173.02199999949153,
    105.92630000019199,
    104.74399999975503,
    105.79349999989063,
    104.16230000009818,
    104.24760000023525,
    106.88860000027489,
    107.07820000061474,
    104.4597000000067,
    103.95989999960875,
    103.51290000016888,
    104.80429999961416,
    106.9843000000219,
    128.57160000021395,
    104.91570000021966,
    108.52539999996225,
    202.36590000013166,
    104.47470000008252
  ],
  "tokens": [
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65,
    65
  ],
  "tokens_per_s": [
    591.9520027097245,
    457.4053575554468,
    328.04077395411707,
    606.96043551828,
    592.4429796431593,
    574.6034131441021,
    611.6864100210207,
    619.1761718120079,
    512.6963331172461,
    326.8678865373424,
    615.5198112157399,
    612.8508217396645,
    605.2389483367018,
    581.2388614507383,
    514.47780107434,
    563.5717617615516,
    601.4692505792566,
    611.7082847869702,
    608.471799670658,
    610.3017049941079,
    550.5670417012063,
    587.3173781784292,
    349.5753197673125,
    611.9357599246032,
    605.0473984814563,
    619.1313491939468,
    310.89574804625937,
    597.8165898700113,
    607.0239204776301,
    598.3184490113194,
    347.67129764847135,
    580.9552155041862,
    602.180821005433,
    611.6018039444364,
    607.7862088650378,
    613.8068774714648,
    624.3588315106466,
    602.3761421968085,
    605.8165848363461,
    596.4156337980797,
    612.2602765543653,
    615.2570496625405,
    375.67476968357215,
    613.6341966053963,
    620.5606049048348,
    614.4044766461758,
    624.0261591760045,
    623.5155533542577,
    608.1097516464135,
    607.0329908387218,
    622.2495373813617,
    625.2410785335944,
    627.9410585530301,
    620.2035603523834,
    607.565783016636,
    505.554881481539,
    619.5450251951224,
    598.9381287700631,
    321.20036033717986,
    622.1601976358742
  ],
  "started_at": 1766334799.0786679,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}