{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/prefill/runs/single_long/transformers-gpu-compile/fp32/run_1766334720259.json",
  "spec": {
    "mode": "prefill",
    "scenario": "single_long",
    "prompt_set": "long",
    "prompts": [
      "Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words.",
      "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    3.6958999999114894,
    3.7173999999140506,
    3.3851999996841187,
    3.633900000295398,
    3.6599999998543353,
    3.800599999976839,
    3.4946999999192485,
    3.387600000223756,
    3.5846000000674394,
    3.7704999999732536,
    3.7519000002248504,
    3.444199999648845,
    3.4002000002146815,
    3.7111000001459615,
    3.77859999980501,
    3.69700000010198,
    3.4077000000252156,
    3.69810000029247,
    3.7560999999186606,
    3.8928999997551728,
    3.4530999996604805,
    3.5339000000931264,
    3.6212999998497253,
    3.6383999999998196,
    3.5503000003700436,
    3.345300000091811,
    3.605100000186212,
    3.628099999787082,
    3.644899999926565,
    3.7549999997281702,
    3.905199999735487,
    3.731599999809987,
    3.488600000309816,
    3.466400000434078,
    3.869399999985035,
    3.798300000198651,
    3.4421000000293134,
    3.777399999762565,
    3.981399999702262,
    3.619099999923492,
    3.7093999999342486,
    3.8024999998924613,
    3.7692999999308086,
    3.5587000002124114,
    3.7191000001257635,
    3.8279999998849235,
    3.770400000121299,
    3.874600000017381,
    4.094200000054116,
    3.78469999986919,
    3.749900000002526,
    3.9935999998306215,
    3.72960000004241,
    3.6188999997648352,
    3.8272000001597917,
    3.6482999998952437,
    3.4622999996827275,
    3.633100000115519,
    3.765599999951519,
    3.6617000000660482
  ],
  "ttft_ms": [
    3.6958999999114894,
    3.7173999999140506,
    3.3851999996841187,
    3.633900000295398,
    3.6599999998543353,
    3.800599999976839,
    3.4946999999192485,
    3.387600000223756,
    3.5846000000674394,
    3.7704999999732536,
    3.7519000002248504,
    3.444199999648845,
    3.4002000002146815,
    3.7111000001459615,
    3.77859999980501,
    3.69700000010198,
    3.4077000000252156,
    3.69810000029247,
    3.7560999999186606,
    3.8928999997551728,
    3.4530999996604805,
    3.5339000000931264,
    3.6212999998497253,
    3.6383999999998196,
    3.5503000003700436,
    3.345300000091811,
    3.605100000186212,
    3.628099999787082,
    3.644899999926565,
    3.7549999997281702,
    3.905199999735487,
    3.731599999809987,
    3.488600000309816,
    3.466400000434078,
    3.869399999985035,
    3.798300000198651,
    3.4421000000293134,
    3.777399999762565,
    3.981399999702262,
    3.619099999923492,
    3.7093999999342486,
    3.8024999998924613,
    3.7692999999308086,
    3.5587000002124114,
    3.7191000001257635,
    3.8279999998849235,
    3.770400000121299,
    3.874600000017381,
    4.094200000054116,
    3.78469999986919,
    3.749900000002526,
    3.9935999998306215,
    3.72960000004241,
    3.6188999997648352,
    3.8272000001597917,
    3.6482999998952437,
    3.4622999996827275,
    3.633100000115519,
    3.765599999951519,
    3.6617000000660482
  ],
  "tokens": [
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25,
    27,
    25
  ],
  "tokens_per_s": [
    7305.392462092212,
    6725.130467686561,
    7975.895073413517,
    6879.660969748139,
    7377.049180621469,
    6577.908751289889,
    7725.985063274068,
    7379.855944724499,
    7532.221168189486,
    6630.4203686984065,
    7196.353846952717,
    7258.579641875875,
    7940.709369535697,
    6736.5471151455695,
    7145.503626050204,
    6762.2396535867965,
    7923.232678874376,
    6760.228224770242,
    7188.307020735521,
    6421.947648686652,
    7819.061134243064,
    7074.337134424062,
    7455.886008096659,
    6871.152154793656,
    7604.991126717692,
    7473.171314774125,
    7489.390030402869,
    6890.659023033308,
    7407.610634185842,
    6657.78961433017,
    6913.858445618356,
    6699.539072053007,
    7739.494352348272,
    7212.09323703825,
    6977.8260195649,
    6581.891898663218,
    7844.048691139149,
    6618.3088901285055,
    6781.534134228945,
    6907.794755748253,
    7278.805197735103,
    6574.621959423281,
    7163.133738491399,
    7025.037232278023,
    7259.820924171703,
    6530.825496539066,
    7161.043920839002,
    6452.278944894403,
    6594.6949342101325,
    6605.543372226087,
    7200.192005115286,
    6260.016025906529,
    7239.382239299919,
    6908.176518175291,
    7054.765885993078,
    6852.506647128209,
    7798.28437815157,
    6881.1758551113635,
    7170.172084222333,
    6827.4298821719585
  ],
  "started_at": 1766334720.037827,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}