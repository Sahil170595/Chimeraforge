{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/prefill/runs/single_medium/transformers-gpu-compile/fp32/run_1766334718269.json",
  "spec": {
    "mode": "prefill",
    "scenario": "single_medium",
    "prompt_set": "medium",
    "prompts": [
      "Explain how backpressure works in an inference service and when to enable queueing.",
      "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    3.377999999884196,
    3.874399999858724,
    4.791199999999662,
    3.9750000000822183,
    4.117299999961688,
    4.306499999984226,
    4.205499999898166,
    3.8376000002244837,
    4.268799999863404,
    3.991000000041822,
    3.6833999997725186,
    4.0598999999019725,
    4.0454999998473795,
    3.7078000000292377,
    3.964699999869481,
    4.1570000003048335,
    3.6076999999750115,
    3.737300000011601,
    3.947199999856821,
    3.5889999999199063,
    3.3754000000953965,
    3.8815000002614397,
    3.7067000002934947,
    3.646300000127667,
    4.028099999686674,
    4.197900000235677,
    3.6838999999417865,
    4.186499999832449,
    4.438899999968271,
    3.8770999999542255,
    4.297500000120635,
    4.512599999998201,
    3.947499999867432,
    4.159499999786931,
    4.373499999928754,
    3.9048999997248757,
    4.211099999793078,
    4.40779999962615,
    3.917700000329205,
    3.9768999999978405,
    4.207199999655131,
    4.379499999686232,
    3.898799999660696,
    4.035599999951955,
    4.218499999751657,
    3.8008999999874504,
    3.975900000114052,
    4.2424000002938556,
    3.706200000124227,
    3.7894999995842227,
    3.91719999970519,
    3.592199999729928,
    3.5278999998809013,
    4.053999999996449,
    3.8521000001310313,
    3.819000000021333,
    4.078999999819644,
    4.016799999590148,
    3.821799999968789,
    4.042899999603833
  ],
  "ttft_ms": [
    3.377999999884196,
    3.874399999858724,
    4.791199999999662,
    3.9750000000822183,
    4.117299999961688,
    4.306499999984226,
    4.205499999898166,
    3.8376000002244837,
    4.268799999863404,
    3.991000000041822,
    3.6833999997725186,
    4.0598999999019725,
    4.0454999998473795,
    3.7078000000292377,
    3.964699999869481,
    4.1570000003048335,
    3.6076999999750115,
    3.737300000011601,
    3.947199999856821,
    3.5889999999199063,
    3.3754000000953965,
    3.8815000002614397,
    3.7067000002934947,
    3.646300000127667,
    4.028099999686674,
    4.197900000235677,
    3.6838999999417865,
    4.186499999832449,
    4.438899999968271,
    3.8770999999542255,
    4.297500000120635,
    4.512599999998201,
    3.947499999867432,
    4.159499999786931,
    4.373499999928754,
    3.9048999997248757,
    4.211099999793078,
    4.40779999962615,
    3.917700000329205,
    3.9768999999978405,
    4.207199999655131,
    4.379499999686232,
    3.898799999660696,
    4.035599999951955,
    4.218499999751657,
    3.8008999999874504,
    3.975900000114052,
    4.2424000002938556,
    3.706200000124227,
    3.7894999995842227,
    3.91719999970519,
    3.592199999729928,
    3.5278999998809013,
    4.053999999996449,
    3.8521000001310313,
    3.819000000021333,
    4.078999999819644,
    4.016799999590148,
    3.821799999968789,
    4.042899999603833
  ],
  "tokens": [
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19,
    17,
    19
  ],
  "tokens_per_s": [
    5032.5636473010045,
    4903.98513336073,
    3548.1716480216223,
    4779.874213737612,
    4128.919437533867,
    4411.935446434365,
    4042.325526194661,
    4951.01104828241,
    3982.383808223383,
    4760.711601052592,
    4615.301080808464,
    4679.918224699811,
    4202.199975439708,
    5124.332488227568,
    4287.840189815029,
    4570.603800482734,
    4712.143470942082,
    5083.884087427025,
    4306.850425774384,
    5293.953747680137,
    5036.440125472401,
    4895.0148135309155,
    4586.2896912763235,
    5210.761593762104,
    4220.352027338533,
    4526.0725598354675,
    4614.674665509008,
    4538.397229370695,
    3829.7776476427753,
    4900.570013727869,
    3955.788248870923,
    4210.433009796475,
    4306.523116040762,
    4567.856713781289,
    3887.0469876019065,
    4865.681579896711,
    4036.9499657655565,
    4310.54040601014,
    4339.2807000463245,
    4777.590585634619,
    4040.692147127189,
    4338.3947942370705,
    4360.315995044494,
    4708.097928492963,
    4029.868436885336,
    4998.816069894691,
    4275.761462690797,
    4478.597020244187,
    4586.908423568664,
    5013.854071007955,
    4339.834576043967,
    5289.237793393596,
    4818.730689808074,
    4686.729156392857,
    4413.177227855387,
    4975.124378081661,
    4167.688159046744,
    4730.13344003651,
    4448.16578579173,
    4699.596824522453
  ],
  "started_at": 1766334718.0292823,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}