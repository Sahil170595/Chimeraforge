{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/prefill/runs/single_micro/transformers-gpu-compile/fp32/run_1766334714441.json",
  "spec": {
    "mode": "prefill",
    "scenario": "single_micro",
    "prompt_set": "micro",
    "prompts": [
      "Hello",
      "Test"
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    4.116600000088511,
    3.9071000001058565,
    4.442999999810127,
    3.535000000283617,
    3.863999999794032,
    3.9982999996936996,
    3.747599999769591,
    4.103499999928317,
    3.7448999996740895,
    3.7817000002178247,
    4.058700000314275,
    3.9148999999270018,
    4.013400000076217,
    3.6847999999736203,
    3.458199999840872,
    4.060299999764538,
    4.0715000000091095,
    4.125399999793444,
    4.091599999810569,
    3.498099999887927,
    3.8738999996894563,
    4.001899999821035,
    3.8199999999051215,
    3.94559999995181,
    3.5574000003180117,
    3.4365000001344015,
    3.842499999791471,
    3.806199999871751,
    4.366399999980786,
    4.0726000001996,
    3.703999999743246,
    4.094400000212772,
    3.9993000000322354,
    3.638499999851774,
    4.094300000360818,
    4.113299999971787,
    5.720400000427617,
    3.8518999999723746,
    4.212200000438315,
    6.59540000015113,
    5.988299999899027,
    7.970900000145775,
    8.167299999968236,
    4.676699999890843,
    3.9562000001751585,
    3.9090000000214786,
    3.4950999997818144,
    3.79800000018804,
    3.9215000001604494,
    4.083899999841378,
    3.5342000001037377,
    3.7471000000550703,
    5.951500000264787,
    4.947699999775068,
    8.172299999841925,
    8.421800000178337,
    4.547900000034133,
    4.077399999914633,
    3.837000000203261,
    3.7317999999686435
  ],
  "ttft_ms": [
    4.116600000088511,
    3.9071000001058565,
    4.442999999810127,
    3.535000000283617,
    3.863999999794032,
    3.9982999996936996,
    3.747599999769591,
    4.103499999928317,
    3.7448999996740895,
    3.7817000002178247,
    4.058700000314275,
    3.9148999999270018,
    4.013400000076217,
    3.6847999999736203,
    3.458199999840872,
    4.060299999764538,
    4.0715000000091095,
    4.125399999793444,
    4.091599999810569,
    3.498099999887927,
    3.8738999996894563,
    4.001899999821035,
    3.8199999999051215,
    3.94559999995181,
    3.5574000003180117,
    3.4365000001344015,
    3.842499999791471,
    3.806199999871751,
    4.366399999980786,
    4.0726000001996,
    3.703999999743246,
    4.094400000212772,
    3.9993000000322354,
    3.638499999851774,
    4.094300000360818,
    4.113299999971787,
    5.720400000427617,
    3.8518999999723746,
    4.212200000438315,
    6.59540000015113,
    5.988299999899027,
    7.970900000145775,
    8.167299999968236,
    4.676699999890843,
    3.9562000001751585,
    3.9090000000214786,
    3.4950999997818144,
    3.79800000018804,
    3.9215000001604494,
    4.083899999841378,
    3.5342000001037377,
    3.7471000000550703,
    5.951500000264787,
    4.947699999775068,
    8.172299999841925,
    8.421800000178337,
    4.547900000034133,
    4.077399999914633,
    3.837000000203261,
    3.7317999999686435
  ],
  "tokens": [
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1,
    1
  ],
  "tokens_per_s": [
    242.91891366139512,
    255.94430651196708,
    225.07314878296992,
    282.8854313775867,
    258.7991718564452,
    250.10629519460963,
    266.8374426463555,
    243.69440721761148,
    267.0298272549408,
    264.4313403872333,
    246.38431022804534,
    255.43436614438332,
    249.16529625280543,
    271.38514980654554,
    289.167775156444,
    246.28722017042858,
    245.60972614460582,
    242.400736910377,
    244.4031674763656,
    285.869472008244,
    258.137793975106,
    249.88130639064445,
    261.7801047185438,
    253.4468775375643,
    281.10417718294417,
    290.9937436231311,
    260.24723488725283,
    262.72923126312196,
    229.0216196419019,
    245.54338750454,
    269.97840174657614,
    244.23602968640907,
    250.04375765557467,
    274.8385323734336,
    244.24199494709063,
    243.1138015721827,
    174.81295013027884,
    259.6121394655032,
    237.40563123686945,
    151.62082663327251,
    166.99230165770948,
    125.45634746160553,
    122.43948428536838,
    213.82598841562225,
    252.76780748084667,
    255.8199027870313,
    286.1148465172459,
    263.29647181424167,
    255.00446256766153,
    244.86397806969828,
    282.94946521720544,
    266.87304848691076,
    168.02486767294116,
    202.11411363774317,
    122.36457301118935,
    118.73946187024441,
    219.88170364178956,
    245.25432874403705,
    260.62027624368676,
    267.96720081687187
  ],
  "started_at": 1766334714.1776037,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}