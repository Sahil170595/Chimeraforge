{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/prefill/runs/single_short/transformers-gpu-compile/fp32/run_1766334716346.json",
  "spec": {
    "mode": "prefill",
    "scenario": "single_short",
    "prompt_set": "short",
    "prompts": [
      "Summarize RLHF in one sentence.",
      "List two ways to improve throughput on local LLMs."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    3.3645999997133913,
    3.615099999933591,
    3.8765000003877503,
    3.7550000001829176,
    3.472200000032899,
    3.7297999997463194,
    3.7056000001030043,
    3.8488000000143074,
    3.8121000002320216,
    3.6026000002493674,
    3.6926999996467202,
    3.974800000378309,
    4.13059999982579,
    3.6279999999351276,
    3.764500000215776,
    4.123499999877822,
    3.8619000001745007,
    3.9805000001251756,
    3.7486000001081266,
    3.967099999954371,
    3.6743000000569737,
    3.371899999820016,
    3.5202999997636653,
    3.7510000001930166,
    3.9133999998739455,
    3.6955000000489235,
    3.5911999998461397,
    3.604400000313035,
    3.76170000026832,
    3.6432000001695997,
    3.2969000003504334,
    3.5266999998384563,
    3.811099999893486,
    3.854900000078487,
    3.7043000002086046,
    3.818300000148156,
    3.70660000044154,
    3.7088000003677735,
    3.5965000001851877,
    3.7800999998580664,
    3.9397999998982414,
    4.059699999743316,
    3.7418000001707696,
    3.8429000001087843,
    3.8944000002629764,
    3.728200000296056,
    3.840200000013283,
    3.69570000020758,
    3.640200000063487,
    3.6325000000942964,
    3.9366000000882195,
    3.7049999996270344,
    3.6835999999311753,
    3.967799999827548,
    3.978700000061508,
    3.6368999999467633,
    3.6196000000927597,
    4.186200000276585,
    3.6466999999902328,
    3.8359000000127708
  ],
  "ttft_ms": [
    3.3645999997133913,
    3.615099999933591,
    3.8765000003877503,
    3.7550000001829176,
    3.472200000032899,
    3.7297999997463194,
    3.7056000001030043,
    3.8488000000143074,
    3.8121000002320216,
    3.6026000002493674,
    3.6926999996467202,
    3.974800000378309,
    4.13059999982579,
    3.6279999999351276,
    3.764500000215776,
    4.123499999877822,
    3.8619000001745007,
    3.9805000001251756,
    3.7486000001081266,
    3.967099999954371,
    3.6743000000569737,
    3.371899999820016,
    3.5202999997636653,
    3.7510000001930166,
    3.9133999998739455,
    3.6955000000489235,
    3.5911999998461397,
    3.604400000313035,
    3.76170000026832,
    3.6432000001695997,
    3.2969000003504334,
    3.5266999998384563,
    3.811099999893486,
    3.854900000078487,
    3.7043000002086046,
    3.818300000148156,
    3.70660000044154,
    3.7088000003677735,
    3.5965000001851877,
    3.7800999998580664,
    3.9397999998982414,
    4.059699999743316,
    3.7418000001707696,
    3.8429000001087843,
    3.8944000002629764,
    3.728200000296056,
    3.840200000013283,
    3.69570000020758,
    3.640200000063487,
    3.6325000000942964,
    3.9366000000882195,
    3.7049999996270344,
    3.6835999999311753,
    3.967799999827548,
    3.978700000061508,
    3.6368999999467633,
    3.6196000000927597,
    4.186200000276585,
    3.6466999999902328,
    3.8359000000127708
  ],
  "tokens": [
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11,
    9,
    11
  ],
  "tokens_per_s": [
    2674.9093505220985,
    3042.792730547445,
    2321.68192934342,
    2929.4274299505078,
    2592.0165888816096,
    2949.2197975087565,
    2428.756476616426,
    2858.0336728224665,
    2360.903438905648,
    3053.350357863375,
    2437.2410433723367,
    2767.4348392253837,
    2178.8602141043866,
    3031.9735391942368,
    2390.75574431774,
    2667.636716460756,
    2330.459100337485,
    2763.4719255505793,
    2400.8963345623433,
    2772.8063321132618,
    2449.446152970755,
    3262.2557017073914,
    2556.6002899196696,
    2932.5513194971923,
    2299.790463609623,
    2976.59315379634,
    2506.1260860953425,
    3051.8255462891666,
    2392.5352897248677,
    3019.3236713570277,
    2729.837119428364,
    3119.0631469940354,
    2361.5229199578957,
    2853.5111156647476,
    2429.608832841069,
    2880.863211265009,
    2428.1012245529314,
    2965.9188953055473,
    2502.432920766462,
    2909.975926672052,
    2284.379917821325,
    2709.5598198624284,
    2405.259500665256,
    2862.421608599915,
    2311.010681848875,
    2950.485488741616,
    2343.6279360368912,
    2976.432069535447,
    2472.3916267905706,
    3028.217480995031,
    2286.236854086854,
    2968.9608639965777,
    2443.2620263242907,
    2772.3171532028055,
    2262.0453916758906,
    3024.5538783472234,
    2486.4625924879424,
    2627.681429285085,
    2467.9847533452453,
    2867.6451419388873
  ],
  "started_at": 1766334716.1202571,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}