{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/kv_decode/runs/single_micro/transformers-gpu-compile/fp32/run_1766334736516.json",
  "spec": {
    "mode": "kv_decode",
    "scenario": "single_micro",
    "prompt_set": "micro",
    "prompts": [
      "Hello",
      "Test"
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    100.85650000019086,
    97.75289999970482,
    104.26370000004681,
    98.18559999985155,
    96.16370000003371,
    100.67220000019006,
    103.60169999967184,
    107.09369999995033,
    198.14499999984037,
    149.25990000028833,
    105.30969999990702,
    112.81129999997574,
    125.7024999999885,
    127.05989999994927,
    100.91719999991255,
    98.00169999971331,
    102.78419999986,
    97.24179999966509,
    187.95669999963138,
    228.65289999981542,
    116.89639999985957,
    110.68280000017694,
    108.0112999998164,
    100.73639999973238,
    102.64689999985421,
    157.91789999957473,
    111.17079999985435,
    97.81470000007175,
    120.79640000001746,
    181.23190000005707,
    216.3775999997597,
    102.4876999999833,
    103.30309999972087,
    109.0116999998827,
    118.4504000002562,
    105.24569999961386,
    104.49450000032812,
    114.73079999996116,
    150.92070000036983,
    106.64070000029824,
    106.5496000001076,
    107.89759999988746,
    104.1574999999284,
    102.54809999969439,
    149.45600000010018,
    109.37670000021171,
    224.95410000010452,
    107.37620000008974,
    101.11540000025343,
    99.2738999998437,
    102.58659999999509,
    101.96840000025986,
    104.86260000016046,
    104.58079999989423,
    102.99959999974817,
    101.5346999997746,
    103.08050000003277,
    101.92780000033963,
    124.67579999974987,
    98.22720000011032
  ],
  "ttft_ms": [
    100.85650000019086,
    97.75289999970482,
    104.26370000004681,
    98.18559999985155,
    96.16370000003371,
    100.67220000019006,
    103.60169999967184,
    107.09369999995033,
    198.14499999984037,
    149.25990000028833,
    105.30969999990702,
    112.81129999997574,
    125.7024999999885,
    127.05989999994927,
    100.91719999991255,
    98.00169999971331,
    102.78419999986,
    97.24179999966509,
    187.95669999963138,
    228.65289999981542,
    116.89639999985957,
    110.68280000017694,
    108.0112999998164,
    100.73639999973238,
    102.64689999985421,
    157.91789999957473,
    111.17079999985435,
    97.81470000007175,
    120.79640000001746,
    181.23190000005707,
    216.3775999997597,
    102.4876999999833,
    103.30309999972087,
    109.0116999998827,
    118.4504000002562,
    105.24569999961386,
    104.49450000032812,
    114.73079999996116,
    150.92070000036983,
    106.64070000029824,
    106.5496000001076,
    107.89759999988746,
    104.1574999999284,
    102.54809999969439,
    149.45600000010018,
    109.37670000021171,
    224.95410000010452,
    107.37620000008974,
    101.11540000025343,
    99.2738999998437,
    102.58659999999509,
    101.96840000025986,
    104.86260000016046,
    104.58079999989423,
    102.99959999974817,
    101.5346999997746,
    103.08050000003277,
    101.92780000033963,
    124.67579999974987,
    98.22720000011032
  ],
  "tokens": [
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64
  ],
  "tokens_per_s": [
    634.5649511918308,
    654.7120341206579,
    613.8282067485737,
    651.8267444523103,
    665.5317963012817,
    635.7266454878226,
    617.7504809303585,
    597.607515661796,
    322.99578591461585,
    428.7822784276043,
    607.7312916099514,
    567.3190540310568,
    509.1386408385343,
    503.69943625034773,
    634.1832710385886,
    653.0498960751418,
    622.6637946307621,
    658.1531810416963,
    340.503956497031,
    279.9002330609044,
    547.4933359802088,
    578.2289569824552,
    592.5305963367609,
    635.3214925307041,
    623.4966667292524,
    405.2738796562793,
    575.6907389357983,
    654.2983825534716,
    529.8171137549691,
    353.13871343830664,
    295.779230382771,
    624.4651797241077,
    619.5361029840627,
    587.0929450698308,
    540.3105434836993,
    608.1008535287884,
    612.4724267765197,
    557.8275406431547,
    424.06376328656813,
    600.1460980640695,
    600.6592234971822,
    593.1549913998713,
    614.4540719587546,
    624.097374794762,
    428.2196766938571,
    585.1337624912446,
    284.50248295083423,
    596.0352480339825,
    632.9401851729766,
    644.6810289522298,
    623.8631556168453,
    627.6454274053226,
    610.322460056322,
    611.9670149785117,
    621.3616363573885,
    630.3263810317268,
    620.8739771341782,
    627.8954318624237,
    513.3313762584912,
    651.5506906430004
  ],
  "started_at": 1766334729.1095388,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}