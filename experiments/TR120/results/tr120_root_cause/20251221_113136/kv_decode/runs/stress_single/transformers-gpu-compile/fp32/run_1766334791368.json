{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/kv_decode/runs/stress_single/transformers-gpu-compile/fp32/run_1766334791368.json",
  "spec": {
    "mode": "kv_decode",
    "scenario": "stress_single",
    "prompt_set": "stress",
    "prompts": [
      "latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency"
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    102.14339999993172,
    99.47079999983544,
    98.2573000001139,
    132.83289999981207,
    101.4498999998068,
    100.28109999984736,
    101.52850000031322,
    108.05230000005395,
    106.76250000005894,
    103.65109999975175,
    103.50509999989299,
    104.51879999982339,
    102.09549999990486,
    102.26379999994606,
    101.81179999972301,
    101.70119999975213,
    114.62829999982205,
    106.97360000040135,
    107.11140000012165,
    105.39079999989553,
    102.77949999999691,
    103.2214999995631,
    131.71680000004926,
    142.19209999964733,
    167.80229999994845,
    101.88950000019759,
    101.59079999993992,
    105.05289999991874,
    106.67359999979453,
    106.53559999991558
  ],
  "ttft_ms": [
    102.14339999993172,
    99.47079999983544,
    98.2573000001139,
    132.83289999981207,
    101.4498999998068,
    100.28109999984736,
    101.52850000031322,
    108.05230000005395,
    106.76250000005894,
    103.65109999975175,
    103.50509999989299,
    104.51879999982339,
    102.09549999990486,
    102.26379999994606,
    101.81179999972301,
    101.70119999975213,
    114.62829999982205,
    106.97360000040135,
    107.11140000012165,
    105.39079999989553,
    102.77949999999691,
    103.2214999995631,
    131.71680000004926,
    142.19209999964733,
    167.80229999994845,
    101.88950000019759,
    101.59079999993992,
    105.05289999991874,
    106.67359999979453,
    106.53559999991558
  ],
  "tokens": [
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64
  ],
  "tokens_per_s": [
    626.5700965509546,
    643.4048987251122,
    651.3510955412555,
    481.8083471797314,
    630.8532586047091,
    638.2060029267471,
    630.364872915512,
    592.3057630422309,
    599.4614213788986,
    617.4560617316486,
    618.3270196354206,
    612.3300305792656,
    626.8640635489286,
    625.8324059934577,
    628.610829001885,
    629.2944429382935,
    558.3263469849885,
    598.2784537470916,
    597.5087619051503,
    607.2636321202936,
    622.6922683998455,
    620.025866706751,
    485.8909417779362,
    450.0953287852049,
    381.4012084460086,
    628.1314561350864,
    629.9783051224899,
    609.2168802579415,
    599.9610025359908,
    600.7381570109026
  ],
  "started_at": 1766334787.9731727,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}