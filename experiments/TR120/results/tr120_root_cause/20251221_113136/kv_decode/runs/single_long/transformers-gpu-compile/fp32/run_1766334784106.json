{
  "path": "scripts/tr120/results/tr120_root_cause/20251221_113136/kv_decode/runs/single_long/transformers-gpu-compile/fp32/run_1766334784106.json",
  "spec": {
    "mode": "kv_decode",
    "scenario": "single_long",
    "prompt_set": "long",
    "prompts": [
      "Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words.",
      "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles."
    ],
    "backend": "transformers-gpu-compile",
    "model": "models/tiny-gpt2",
    "quantization": "fp32"
  },
  "status": "ok",
  "error": null,
  "latencies_ms": [
    140.93080000020564,
    143.45529999991413,
    256.5364000001864,
    286.90770000002885,
    114.91089999981341,
    111.55830000006972,
    107.39970000031462,
    101.644700000179,
    100.90709999985847,
    109.2014000000745,
    144.1663000000517,
    130.9673999999177,
    130.32699999985198,
    99.26229999973657,
    213.56890000015483,
    247.0368999997845,
    228.24020000007295,
    184.72919999976511,
    200.60929999999644,
    125.87889999986146,
    101.83020000022225,
    150.6200000003446,
    232.43060000004334,
    254.1031999999177,
    242.87869999989198,
    231.95369999984905,
    223.607500000071,
    130.36290000036388,
    100.5433999998786,
    101.46119999990333,
    107.88800000000265,
    136.18859999996857,
    242.87189999995462,
    136.4136000001963,
    168.94020000017917,
    105.5873000000247,
    142.82060000005004,
    102.54680000025473,
    103.4018999998807,
    98.63420000010592,
    169.73540000026333,
    104.93230000020048,
    105.83229999974719,
    107.78660000005402,
    102.20429999981206,
    101.86110000040571,
    102.4649999999383,
    105.42190000023766,
    111.35850000027858,
    103.49499999983891,
    104.94530000005398,
    115.03339999990203,
    188.41080000038346,
    234.77950000005876,
    223.5107000001335,
    177.93620000020383,
    99.21750000012253,
    104.35009999991962,
    96.83380000024044,
    100.56019999956334
  ],
  "ttft_ms": [
    140.93080000020564,
    143.45529999991413,
    256.5364000001864,
    286.90770000002885,
    114.91089999981341,
    111.55830000006972,
    107.39970000031462,
    101.644700000179,
    100.90709999985847,
    109.2014000000745,
    144.1663000000517,
    130.9673999999177,
    130.32699999985198,
    99.26229999973657,
    213.56890000015483,
    247.0368999997845,
    228.24020000007295,
    184.72919999976511,
    200.60929999999644,
    125.87889999986146,
    101.83020000022225,
    150.6200000003446,
    232.43060000004334,
    254.1031999999177,
    242.87869999989198,
    231.95369999984905,
    223.607500000071,
    130.36290000036388,
    100.5433999998786,
    101.46119999990333,
    107.88800000000265,
    136.18859999996857,
    242.87189999995462,
    136.4136000001963,
    168.94020000017917,
    105.5873000000247,
    142.82060000005004,
    102.54680000025473,
    103.4018999998807,
    98.63420000010592,
    169.73540000026333,
    104.93230000020048,
    105.83229999974719,
    107.78660000005402,
    102.20429999981206,
    101.86110000040571,
    102.4649999999383,
    105.42190000023766,
    111.35850000027858,
    103.49499999983891,
    104.94530000005398,
    115.03339999990203,
    188.41080000038346,
    234.77950000005876,
    223.5107000001335,
    177.93620000020383,
    99.21750000012253,
    104.35009999991962,
    96.83380000024044,
    100.56019999956334
  ],
  "tokens": [
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64,
    64
  ],
  "tokens_per_s": [
    454.12358405619364,
    446.13200070013664,
    249.4772671634649,
    223.06825505203787,
    556.9532568285856,
    573.6910655680483,
    595.9048302724543,
    629.6442411644414,
    634.2467477520389,
    586.0730723228488,
    443.9317649130001,
    488.6712265803568,
    491.07245620686956,
    644.7563677264162,
    299.66909976103074,
    259.07060848017375,
    280.4063438429319,
    346.4530783443082,
    319.02808095138727,
    508.42516100848064,
    628.4972434489996,
    424.91037046775716,
    275.3510079997559,
    251.8661709101685,
    263.506021730306,
    275.91713346259036,
    286.21580224267825,
    490.9372221684341,
    636.5410360110885,
    630.7829988218252,
    593.2077710217858,
    469.9365438811675,
    263.51339945054144,
    469.1614325837592,
    378.832273194492,
    606.1335027980167,
    448.1146277216142,
    624.1052865602927,
    618.9441393250398,
    648.8621593720158,
    377.05746709231374,
    609.9170608085186,
    604.730313903722,
    593.7658298895032,
    626.1967451478821,
    628.3065861231136,
    624.6035231546239,
    607.0844862391564,
    574.7203850612202,
    618.3873617092576,
    609.8415079090448,
    556.3601527908808,
    339.6832877938512,
    272.59620196816155,
    286.3397591254547,
    359.67948062241794,
    645.0474966605786,
    613.3199680695016,
    660.9262468253966,
    636.4346928534143
  ],
  "started_at": 1766334774.9390996,
  "degraded_count": 0,
  "degraded_reasons": [],
  "compile": {
    "enabled": true,
    "backend": "aot_eager",
    "mode": "reduce-overhead",
    "dynamic": false,
    "fullgraph": false,
    "compile_wrapper_ms": 0.6936999998288229,
    "fallback_from": "inductor",
    "fallback_error": "compile_smoke_failed: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
  }
}