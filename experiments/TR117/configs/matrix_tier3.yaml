# TR117 Tier 3 Comprehensive Benchmark Matrix
# Frontier lab quality with statistical rigor

repetitions: 7  # For 95% CI with low variance

prompt_sets:
  micro:
    - "Hello"
    - "Test"
  short:
    - "Summarize RLHF in one sentence."
    - "List two ways to improve throughput on local LLMs."
  medium:
    - "Explain how backpressure works in an inference service and when to enable queueing."
    - "Compare torch.compile vs ONNXRuntime on CPU for small models in two sentences."
  long:
    - "Provide a concise overview of the attention mechanism and how KV cache impacts latency in local inference pipelines. Keep it under 120 words."
    - "Explain how dynamic shapes affect TensorRT engine builds and runtime performance, and when you would choose static vs dynamic profiles."
  stress:
    - "latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency latency"  # 50 repetitions

scenarios:
  # Single-shot inference
  - name: single_micro
    prompt_set: micro
    stream: false
    mode: single
  - name: single_short
    prompt_set: short
    stream: false
    mode: single
  - name: single_medium
    prompt_set: medium
    stream: false
    mode: single
  - name: single_long
    prompt_set: long
    stream: false
    mode: single
  
  # Batch inference (when supported)
  - name: batch_short
    prompt_set: short
    stream: false
    mode: batch
    batch_size: 4
  
  - name: batch_medium
    prompt_set: medium
    stream: false
    mode: batch
    batch_size: 4
  
  # Dual-agent coordination
  - name: dual_short
    prompt_set: short
    stream: false
    mode: dual
  
  - name: dual_medium
    prompt_set: medium
    stream: false
    mode: dual
  
  # Stress testing
  - name: stress_single
    prompt_set: stress
    stream: false
    mode: single

models:
  # HuggingFace models (for transformers/ORT/TRT)
  - name: models/tiny-gpt2
    description: "124M params - Baseline for transformers"
    params_millions: 124
    
  # Add more model sizes as they become available:
  # - name: models/gpt2-medium
  #   description: "355M params"
  #   params_millions: 355
  #
  # - name: models/gpt2-large  
  #   description: "774M params"
  #   params_millions: 774
  
  # Ollama models
  - name: gemma3:270m
    description: "270M params - Ollama small"
    params_millions: 270
    
  - name: gemma3:latest
    description: "2B+ params - Ollama default"
    params_millions: 2000

backends:
  - transformers-cpu
  - transformers-cpu-compile
  - transformers-gpu
  - transformers-gpu-compile
  - onnxruntime
  - tensorrt
  - ollama

quantization:
  modes:
    - fp32
    - fp16
    - int8  # When calibrated

baseline_backend: transformers-cpu
accuracy_threshold: 0.70

# Resource monitoring
enable_resource_monitoring: true
resource_sampling_interval_s: 0.1

# Reproducibility
seed: 42
temperature: 0.7
top_p: 0.9
max_tokens: 128
