{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547110.0134625, "prompt_tokens": 46, "prefill_ms": 24.138, "prefill_cuda_event_ms": null, "kv_decode_ms": 1465.2902, "kv_decode_ms_equiv": 1465.2902, "kv_decode_ms_per_token": 11.4475796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 8666.22690000031, "ollama_total_duration_ms": 8661.6337, "ollama_load_ms": 7041.157, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 24.138, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 1905.708840831883}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547110.0134625, "prompt_tokens": 46, "prefill_ms": 24.138, "prefill_cuda_event_ms": null, "kv_decode_ms": 1465.2902, "kv_decode_ms_equiv": 1465.2902, "kv_decode_ms_per_token": 11.4475796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 8666.22690000031, "ollama_total_duration_ms": 8661.6337, "ollama_load_ms": 7041.157, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1465.2902, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.35470966775047}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547110.0134625, "prompt_tokens": 46, "prefill_ms": 24.138, "prefill_cuda_event_ms": null, "kv_decode_ms": 1465.2902, "kv_decode_ms_equiv": 1465.2902, "kv_decode_ms_per_token": 11.4475796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 8666.22690000031, "ollama_total_duration_ms": 8661.6337, "ollama_load_ms": 7041.157, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1489.4281999999998, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 116.82335543264188}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547118.6798382, "prompt_tokens": 46, "prefill_ms": 11.6939, "prefill_cuda_event_ms": null, "kv_decode_ms": 1441.99, "kv_decode_ms_equiv": 1441.99, "kv_decode_ms_per_token": 11.265546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1733.399400000053, "ollama_total_duration_ms": 1712.8496, "ollama_load_ms": 132.8751, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6939, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3933.6748219157}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547118.6798382, "prompt_tokens": 46, "prefill_ms": 11.6939, "prefill_cuda_event_ms": null, "kv_decode_ms": 1441.99, "kv_decode_ms_equiv": 1441.99, "kv_decode_ms_per_token": 11.265546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1733.399400000053, "ollama_total_duration_ms": 1712.8496, "ollama_load_ms": 132.8751, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1441.99, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.76621890581765}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547118.6798382, "prompt_tokens": 46, "prefill_ms": 11.6939, "prefill_cuda_event_ms": null, "kv_decode_ms": 1441.99, "kv_decode_ms_equiv": 1441.99, "kv_decode_ms_per_token": 11.265546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1733.399400000053, "ollama_total_duration_ms": 1712.8496, "ollama_load_ms": 132.8751, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1453.6839, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 119.69589812475739}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547120.4133751, "prompt_tokens": 46, "prefill_ms": 11.6463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1450.6971, "kv_decode_ms_equiv": 1450.6971, "kv_decode_ms_per_token": 11.33357109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1749.003299999913, "ollama_total_duration_ms": 1726.8833, "ollama_load_ms": 140.5127, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6463, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3949.7522818405846}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547120.4133751, "prompt_tokens": 46, "prefill_ms": 11.6463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1450.6971, "kv_decode_ms_equiv": 1450.6971, "kv_decode_ms_per_token": 11.33357109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1749.003299999913, "ollama_total_duration_ms": 1726.8833, "ollama_load_ms": 140.5127, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1450.6971, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.23344308057139}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547120.4133751, "prompt_tokens": 46, "prefill_ms": 11.6463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1450.6971, "kv_decode_ms_equiv": 1450.6971, "kv_decode_ms_per_token": 11.33357109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1749.003299999913, "ollama_total_duration_ms": 1726.8833, "ollama_load_ms": 140.5127, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1462.3434000000002, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.9870997468857}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547122.1628199, "prompt_tokens": 46, "prefill_ms": 12.3395, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.4306, "kv_decode_ms_equiv": 1459.4306, "kv_decode_ms_per_token": 11.4018015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1746.1826000003384, "ollama_total_duration_ms": 1722.356, "ollama_load_ms": 134.7166, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.3395, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3727.865796831314}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547122.1628199, "prompt_tokens": 46, "prefill_ms": 12.3395, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.4306, "kv_decode_ms_equiv": 1459.4306, "kv_decode_ms_per_token": 11.4018015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1746.1826000003384, "ollama_total_duration_ms": 1722.356, "ollama_load_ms": 134.7166, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1459.4306, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.70543799753138}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547122.1628199, "prompt_tokens": 46, "prefill_ms": 12.3395, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.4306, "kv_decode_ms_equiv": 1459.4306, "kv_decode_ms_per_token": 11.4018015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1746.1826000003384, "ollama_total_duration_ms": 1722.356, "ollama_load_ms": 134.7166, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1471.7701, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.22498636166071}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547123.9091685, "prompt_tokens": 30, "prefill_ms": 66.9239, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.3287, "kv_decode_ms_equiv": 1413.8646068965518, "kv_decode_ms_per_token": 11.04581724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 569.0826000000015, "ollama_total_duration_ms": 541.7228, "ollama_load_ms": 122.5168, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 66.9239, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 448.2703488589278}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547123.9091685, "prompt_tokens": 30, "prefill_ms": 66.9239, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.3287, "kv_decode_ms_equiv": 1413.8646068965518, "kv_decode_ms_per_token": 11.04581724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 569.0826000000015, "ollama_total_duration_ms": 541.7228, "ollama_load_ms": 122.5168, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1413.8646068965518, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 90.53200665441467}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547123.9091685, "prompt_tokens": 30, "prefill_ms": 66.9239, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.3287, "kv_decode_ms_equiv": 1413.8646068965518, "kv_decode_ms_per_token": 11.04581724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 569.0826000000015, "ollama_total_duration_ms": 541.7228, "ollama_load_ms": 122.5168, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1480.7885068965518, "cuda_event_ms": null, "tokens_total": 158, "tokens_per_s": 106.69990971981383}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547124.478389, "prompt_tokens": 30, "prefill_ms": 11.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.411, "kv_decode_ms_equiv": 1427.4692413793105, "kv_decode_ms_per_token": 11.152103448275863, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 510.60629999983576, "ollama_total_duration_ms": 494.3613, "ollama_load_ms": 131.7589, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.5811, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2590.4275068862203}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547124.478389, "prompt_tokens": 30, "prefill_ms": 11.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.411, "kv_decode_ms_equiv": 1427.4692413793105, "kv_decode_ms_per_token": 11.152103448275863, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 510.60629999983576, "ollama_total_duration_ms": 494.3613, "ollama_load_ms": 131.7589, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1427.4692413793105, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.66918255717955}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547124.478389, "prompt_tokens": 30, "prefill_ms": 11.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.411, "kv_decode_ms_equiv": 1427.4692413793105, "kv_decode_ms_per_token": 11.152103448275863, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 510.60629999983576, "ollama_total_duration_ms": 494.3613, "ollama_load_ms": 131.7589, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1439.0503413793106, "cuda_event_ms": null, "tokens_total": 158, "tokens_per_s": 109.79463015071391}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547124.9892213, "prompt_tokens": 30, "prefill_ms": 12.0088, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.1329, "kv_decode_ms_equiv": 1408.5865931034482, "kv_decode_ms_per_token": 11.004582758620689, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 505.5537000002914, "ollama_total_duration_ms": 489.681, "ollama_load_ms": 128.8568, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.0088, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2498.1680101259076}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547124.9892213, "prompt_tokens": 30, "prefill_ms": 12.0088, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.1329, "kv_decode_ms_equiv": 1408.5865931034482, "kv_decode_ms_per_token": 11.004582758620689, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 505.5537000002914, "ollama_total_duration_ms": 489.681, "ollama_load_ms": 128.8568, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1408.5865931034482, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 90.87123264320289}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547124.9892213, "prompt_tokens": 30, "prefill_ms": 12.0088, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.1329, "kv_decode_ms_equiv": 1408.5865931034482, "kv_decode_ms_per_token": 11.004582758620689, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 505.5537000002914, "ollama_total_duration_ms": 489.681, "ollama_load_ms": 128.8568, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1420.5953931034483, "cuda_event_ms": null, "tokens_total": 158, "tokens_per_s": 111.22097169049061}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547125.4949028, "prompt_tokens": 30, "prefill_ms": 11.8725, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3482, "kv_decode_ms_equiv": 1396.295503448276, "kv_decode_ms_per_token": 10.908558620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 504.17430000015884, "ollama_total_duration_ms": 482.3399, "ollama_load_ms": 123.9561, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.8725, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2526.847757422615}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547125.4949028, "prompt_tokens": 30, "prefill_ms": 11.8725, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3482, "kv_decode_ms_equiv": 1396.295503448276, "kv_decode_ms_per_token": 10.908558620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 504.17430000015884, "ollama_total_duration_ms": 482.3399, "ollama_load_ms": 123.9561, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1396.295503448276, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 91.67113958606372}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547125.4949028, "prompt_tokens": 30, "prefill_ms": 11.8725, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3482, "kv_decode_ms_equiv": 1396.295503448276, "kv_decode_ms_per_token": 10.908558620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 504.17430000015884, "ollama_total_duration_ms": 482.3399, "ollama_load_ms": 123.9561, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1408.1680034482758, "cuda_event_ms": null, "tokens_total": 158, "tokens_per_s": 112.20252101531548}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547125.999234, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 45.29520000005505, "prefill_cuda_event_ms": null, "kv_decode_ms": 1774.2134999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9884.024699999827, "params_millions_measured": 25.016064, "latency_ms": 45.29520000005505, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 375.3157067410971}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547125.999234, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 45.29520000005505, "prefill_cuda_event_ms": null, "kv_decode_ms": 1774.2134999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9884.024699999827, "params_millions_measured": 25.016064, "latency_ms": 1774.2134999998598, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 72.14464324615392}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547125.999234, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 45.29520000005505, "prefill_cuda_event_ms": null, "kv_decode_ms": 1774.2134999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9884.024699999827, "params_millions_measured": 25.016064, "latency_ms": 1819.5086999999148, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 79.69184208902479}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547137.706103, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.359799999605457, "prefill_cuda_event_ms": null, "kv_decode_ms": 1426.6302000000906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.359799999605457, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1640.958319721175}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547137.706103, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 10.359799999605457, "prefill_cuda_event_ms": null, "kv_decode_ms": 1426.6302000000906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1426.6302000000906, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.72191952756354}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547137.706103, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 10.359799999605457, "prefill_cuda_event_ms": null, "kv_decode_ms": 1426.6302000000906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1436.989999999696, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 100.9053646859273}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547139.143801, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.238099999900442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1616.0611999998764, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.238099999900442, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1512.711223440849}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547139.143801, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.238099999900442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1616.0611999998764, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1616.0611999998764, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 79.2049211997725}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547139.143801, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.238099999900442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1616.0611999998764, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1627.2992999997768, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 89.10469020666321}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547140.7716944, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.291099999918515, "prefill_cuda_event_ms": null, "kv_decode_ms": 1738.157300000239, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.291099999918515, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 881.235388343423}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547140.7716944, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 19.291099999918515, "prefill_cuda_event_ms": null, "kv_decode_ms": 1738.157300000239, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1738.157300000239, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.64120612097788}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547140.7716944, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 19.291099999918515, "prefill_cuda_event_ms": null, "kv_decode_ms": 1738.157300000239, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1757.4484000001576, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 82.50597855390065}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547142.5300088, "prompt_tokens": 10, "prefill_ms": 9.021, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7682, "kv_decode_ms_equiv": 264.93905454545455, "kv_decode_ms_per_token": 2.0698363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1639.6813999999722, "ollama_total_duration_ms": 1563.9347, "ollama_load_ms": 1495.3749, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 9.021, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1108.524553818867}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547142.5300088, "prompt_tokens": 10, "prefill_ms": 9.021, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7682, "kv_decode_ms_equiv": 264.93905454545455, "kv_decode_ms_per_token": 2.0698363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1639.6813999999722, "ollama_total_duration_ms": 1563.9347, "ollama_load_ms": 1495.3749, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 264.93905454545455, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 483.1299795328572}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547142.5300088, "prompt_tokens": 10, "prefill_ms": 9.021, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7682, "kv_decode_ms_equiv": 264.93905454545455, "kv_decode_ms_per_token": 2.0698363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1639.6813999999722, "ollama_total_duration_ms": 1563.9347, "ollama_load_ms": 1495.3749, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 273.96005454545457, "cuda_event_ms": null, "tokens_total": 138, "tokens_per_s": 503.72307097458065}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547144.1698384, "prompt_tokens": 10, "prefill_ms": 2.9535, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.5728, "kv_decode_ms_equiv": 227.75621818181818, "kv_decode_ms_per_token": 1.7793454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 224.05369999978575, "ollama_total_duration_ms": 192.031, "ollama_load_ms": 153.9458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9535, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3385.8134416793637}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547144.1698384, "prompt_tokens": 10, "prefill_ms": 2.9535, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.5728, "kv_decode_ms_equiv": 227.75621818181818, "kv_decode_ms_per_token": 1.7793454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 224.05369999978575, "ollama_total_duration_ms": 192.031, "ollama_load_ms": 153.9458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 227.75621818181818, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 562.0044142892177}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547144.1698384, "prompt_tokens": 10, "prefill_ms": 2.9535, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.5728, "kv_decode_ms_equiv": 227.75621818181818, "kv_decode_ms_per_token": 1.7793454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 224.05369999978575, "ollama_total_duration_ms": 192.031, "ollama_load_ms": 153.9458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 230.70971818181818, "cuda_event_ms": null, "tokens_total": 138, "tokens_per_s": 598.1542567324567}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547144.3940334, "prompt_tokens": 10, "prefill_ms": 2.1569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1165, "kv_decode_ms_equiv": 245.7192727272727, "kv_decode_ms_per_token": 1.919681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 193.7606000001324, "ollama_total_duration_ms": 184.4511, "ollama_load_ms": 149.8162, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1569, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4636.283555102231}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547144.3940334, "prompt_tokens": 10, "prefill_ms": 2.1569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1165, "kv_decode_ms_equiv": 245.7192727272727, "kv_decode_ms_per_token": 1.919681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 193.7606000001324, "ollama_total_duration_ms": 184.4511, "ollama_load_ms": 149.8162, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 245.7192727272727, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 520.919659981531}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547144.3940334, "prompt_tokens": 10, "prefill_ms": 2.1569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1165, "kv_decode_ms_equiv": 245.7192727272727, "kv_decode_ms_per_token": 1.919681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 193.7606000001324, "ollama_total_duration_ms": 184.4511, "ollama_load_ms": 149.8162, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 247.87617272727272, "cuda_event_ms": null, "tokens_total": 138, "tokens_per_s": 556.7295899466519}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547144.5880194, "prompt_tokens": 10, "prefill_ms": 2.7569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.6513, "kv_decode_ms_equiv": 251.9424, "kv_decode_ms_per_token": 1.9683, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 212.17250000017884, "ollama_total_duration_ms": 185.5392, "ollama_load_ms": 146.4889, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.7569, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3627.262504987486}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547144.5880194, "prompt_tokens": 10, "prefill_ms": 2.7569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.6513, "kv_decode_ms_equiv": 251.9424, "kv_decode_ms_per_token": 1.9683, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 212.17250000017884, "ollama_total_duration_ms": 185.5392, "ollama_load_ms": 146.4889, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 251.9424, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 508.0526342529086}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547144.5880194, "prompt_tokens": 10, "prefill_ms": 2.7569, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.6513, "kv_decode_ms_equiv": 251.9424, "kv_decode_ms_per_token": 1.9683, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 212.17250000017884, "ollama_total_duration_ms": 185.5392, "ollama_load_ms": 146.4889, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 254.6993, "cuda_event_ms": null, "tokens_total": 138, "tokens_per_s": 541.8153877925852}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547144.800293, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 187.44859999969776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3608.372700000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 156.20149999995192, "params_millions_measured": 96.08832, "latency_ms": 187.44859999969776, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 48.01316200822258}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547144.800293, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 187.44859999969776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3608.372700000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 156.20149999995192, "params_millions_measured": 96.08832, "latency_ms": 3608.372700000018, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 35.47305409998234}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547144.800293, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 187.44859999969776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3608.372700000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 156.20149999995192, "params_millions_measured": 96.08832, "latency_ms": 3795.821299999716, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 36.09232078444005}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547148.7535279, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 56.01249999972424, "prefill_cuda_event_ms": null, "kv_decode_ms": 4721.8612000001485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 56.01249999972424, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 160.67841999632776}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547148.7535279, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 56.01249999972424, "prefill_cuda_event_ms": null, "kv_decode_ms": 4721.8612000001485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4721.8612000001485, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 27.107954803922652}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547148.7535279, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 56.01249999972424, "prefill_cuda_event_ms": null, "kv_decode_ms": 4721.8612000001485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4777.873699999873, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 28.673842927242646}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547153.5320644, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.63889999983439, "prefill_cuda_event_ms": null, "kv_decode_ms": 4489.061199999924, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 46.63889999983439, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 192.97196117472663}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547153.5320644, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 46.63889999983439, "prefill_cuda_event_ms": null, "kv_decode_ms": 4489.061199999924, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4489.061199999924, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 28.513756952122232}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547153.5320644, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 46.63889999983439, "prefill_cuda_event_ms": null, "kv_decode_ms": 4489.061199999924, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4535.700099999758, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 30.204818876805216}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547158.0684624, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 61.987000000044645, "prefill_cuda_event_ms": null, "kv_decode_ms": 4630.47669999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 61.987000000044645, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 145.19173375052057}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547158.0684624, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 61.987000000044645, "prefill_cuda_event_ms": null, "kv_decode_ms": 4630.47669999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4630.47669999969, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 27.642942248258926}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547158.0684624, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 61.987000000044645, "prefill_cuda_event_ms": null, "kv_decode_ms": 4630.47669999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4692.463699999735, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 29.195750624561622}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547162.7615848, "prompt_tokens": 92, "prefill_ms": 856.988, "prefill_cuda_event_ms": null, "kv_decode_ms": 6229.0241, "kv_decode_ms_equiv": 6229.0241, "kv_decode_ms_per_token": 48.66425078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 17742.349600000125, "ollama_total_duration_ms": 17627.1399, "ollama_load_ms": 10442.087, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 856.988, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 107.35272839292965}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547162.7615848, "prompt_tokens": 92, "prefill_ms": 856.988, "prefill_cuda_event_ms": null, "kv_decode_ms": 6229.0241, "kv_decode_ms_equiv": 6229.0241, "kv_decode_ms_per_token": 48.66425078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 17742.349600000125, "ollama_total_duration_ms": 17627.1399, "ollama_load_ms": 10442.087, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6229.0241, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 20.548965286552676}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547162.7615848, "prompt_tokens": 92, "prefill_ms": 856.988, "prefill_cuda_event_ms": null, "kv_decode_ms": 6229.0241, "kv_decode_ms_equiv": 6229.0241, "kv_decode_ms_per_token": 48.66425078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 17742.349600000125, "ollama_total_duration_ms": 17627.1399, "ollama_load_ms": 10442.087, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7086.0121, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 31.047082180398764}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547180.504123, "prompt_tokens": 92, "prefill_ms": 64.8048, "prefill_cuda_event_ms": null, "kv_decode_ms": 7367.4756, "kv_decode_ms_equiv": 7367.4756, "kv_decode_ms_per_token": 57.558403125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7712.174599999798, "ollama_total_duration_ms": 7695.6511, "ollama_load_ms": 235.8037, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 64.8048, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1419.6479273140262}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547180.504123, "prompt_tokens": 92, "prefill_ms": 64.8048, "prefill_cuda_event_ms": null, "kv_decode_ms": 7367.4756, "kv_decode_ms_equiv": 7367.4756, "kv_decode_ms_per_token": 57.558403125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7712.174599999798, "ollama_total_duration_ms": 7695.6511, "ollama_load_ms": 235.8037, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7367.4756, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.37365781028172}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547180.504123, "prompt_tokens": 92, "prefill_ms": 64.8048, "prefill_cuda_event_ms": null, "kv_decode_ms": 7367.4756, "kv_decode_ms_equiv": 7367.4756, "kv_decode_ms_per_token": 57.558403125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7712.174599999798, "ollama_total_duration_ms": 7695.6511, "ollama_load_ms": 235.8037, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7432.2804, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 29.60060548845816}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547188.217183, "prompt_tokens": 92, "prefill_ms": 61.8003, "prefill_cuda_event_ms": null, "kv_decode_ms": 7247.6429, "kv_decode_ms_equiv": 7247.6429, "kv_decode_ms_per_token": 56.62221015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7589.821100000336, "ollama_total_duration_ms": 7573.2015, "ollama_load_ms": 252.1271, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.8003, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1488.6659126250195}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547188.217183, "prompt_tokens": 92, "prefill_ms": 61.8003, "prefill_cuda_event_ms": null, "kv_decode_ms": 7247.6429, "kv_decode_ms_equiv": 7247.6429, "kv_decode_ms_per_token": 56.62221015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7589.821100000336, "ollama_total_duration_ms": 7573.2015, "ollama_load_ms": 252.1271, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7247.6429, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.66091428152455}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547188.217183, "prompt_tokens": 92, "prefill_ms": 61.8003, "prefill_cuda_event_ms": null, "kv_decode_ms": 7247.6429, "kv_decode_ms_equiv": 7247.6429, "kv_decode_ms_per_token": 56.62221015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7589.821100000336, "ollama_total_duration_ms": 7573.2015, "ollama_load_ms": 252.1271, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7309.4432, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 30.098051791414154}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547195.8071654, "prompt_tokens": 92, "prefill_ms": 63.7591, "prefill_cuda_event_ms": null, "kv_decode_ms": 7271.2806, "kv_decode_ms_equiv": 7271.2806, "kv_decode_ms_per_token": 56.8068796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7602.577200000269, "ollama_total_duration_ms": 7582.6121, "ollama_load_ms": 230.5038, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 63.7591, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1442.9312835344288}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547195.8071654, "prompt_tokens": 92, "prefill_ms": 63.7591, "prefill_cuda_event_ms": null, "kv_decode_ms": 7271.2806, "kv_decode_ms_equiv": 7271.2806, "kv_decode_ms_per_token": 56.8068796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7602.577200000269, "ollama_total_duration_ms": 7582.6121, "ollama_load_ms": 230.5038, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7271.2806, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.603501644538376}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547195.8071654, "prompt_tokens": 92, "prefill_ms": 63.7591, "prefill_cuda_event_ms": null, "kv_decode_ms": 7271.2806, "kv_decode_ms_equiv": 7271.2806, "kv_decode_ms_per_token": 56.8068796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7602.577200000269, "ollama_total_duration_ms": 7582.6121, "ollama_load_ms": 230.5038, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7335.0397, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 29.993021033001362}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547203.409899, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 774.0489999996498, "prefill_cuda_event_ms": 640.8386840820312, "kv_decode_ms": 659.044399999857, "kv_decode_cuda_event_ms": 658.9378662109375, "gpu_peak_mb": 62.28857421875, "hf_load_ms": 490.26290000028894, "params_millions_measured": 25.016064, "latency_ms": 774.0489999996498, "cuda_event_ms": 640.8386840820312, "tokens_total": 17, "tokens_per_s": 21.9624339027732}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547203.409899, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 774.0489999996498, "prefill_cuda_event_ms": 640.8386840820312, "kv_decode_ms": 659.044399999857, "kv_decode_cuda_event_ms": 658.9378662109375, "gpu_peak_mb": 62.28857421875, "hf_load_ms": 490.26290000028894, "params_millions_measured": 25.016064, "latency_ms": 659.044399999857, "cuda_event_ms": 658.9378662109375, "tokens_total": 128, "tokens_per_s": 194.22060182899327}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547203.409899, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 774.0489999996498, "prefill_cuda_event_ms": 640.8386840820312, "kv_decode_ms": 659.044399999857, "kv_decode_cuda_event_ms": 658.9378662109375, "gpu_peak_mb": 62.28857421875, "hf_load_ms": 490.26290000028894, "params_millions_measured": 25.016064, "latency_ms": 1433.0933999995068, "cuda_event_ms": 1299.7765502929688, "tokens_total": 145, "tokens_per_s": 101.1797277135251}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547205.339027, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.8299999996525003, "prefill_cuda_event_ms": 3.744767904281616, "kv_decode_ms": 452.8430999998818, "kv_decode_cuda_event_ms": 452.6703186035156, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 3.8299999996525003, "cuda_event_ms": 3.744767904281616, "tokens_total": 17, "tokens_per_s": 4438.642298052853}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547205.339027, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.8299999996525003, "prefill_cuda_event_ms": 3.744767904281616, "kv_decode_ms": 452.8430999998818, "kv_decode_cuda_event_ms": 452.6703186035156, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 452.8430999998818, "cuda_event_ms": 452.6703186035156, "tokens_total": 128, "tokens_per_s": 282.658607363198}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547205.339027, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.8299999996525003, "prefill_cuda_event_ms": 3.744767904281616, "kv_decode_ms": 452.8430999998818, "kv_decode_cuda_event_ms": 452.6703186035156, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 456.6730999995343, "cuda_event_ms": 456.41508650779724, "tokens_total": 145, "tokens_per_s": 317.5137751712283}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547205.796462, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.102000000078988, "prefill_cuda_event_ms": 4.997119903564453, "kv_decode_ms": 461.89300000014555, "kv_decode_cuda_event_ms": 461.7869873046875, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 5.102000000078988, "cuda_event_ms": 4.997119903564453, "tokens_total": 17, "tokens_per_s": 3332.026656161664}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547205.796462, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.102000000078988, "prefill_cuda_event_ms": 4.997119903564453, "kv_decode_ms": 461.89300000014555, "kv_decode_cuda_event_ms": 461.7869873046875, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 461.89300000014555, "cuda_event_ms": 461.7869873046875, "tokens_total": 128, "tokens_per_s": 277.1204586342717}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547205.796462, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.102000000078988, "prefill_cuda_event_ms": 4.997119903564453, "kv_decode_ms": 461.89300000014555, "kv_decode_cuda_event_ms": 461.7869873046875, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 466.99500000022454, "cuda_event_ms": 466.78410720825195, "tokens_total": 145, "tokens_per_s": 310.495829719655}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547206.2642872, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.455699999653007, "prefill_cuda_event_ms": 4.366335868835449, "kv_decode_ms": 404.7238000002835, "kv_decode_cuda_event_ms": 404.66534423828125, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 4.455699999653007, "cuda_event_ms": 4.366335868835449, "tokens_total": 17, "tokens_per_s": 3815.3376576798028}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547206.2642872, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.455699999653007, "prefill_cuda_event_ms": 4.366335868835449, "kv_decode_ms": 404.7238000002835, "kv_decode_cuda_event_ms": 404.66534423828125, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 404.7238000002835, "cuda_event_ms": 404.66534423828125, "tokens_total": 128, "tokens_per_s": 316.2650676829738}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547206.2642872, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.455699999653007, "prefill_cuda_event_ms": 4.366335868835449, "kv_decode_ms": 404.7238000002835, "kv_decode_cuda_event_ms": 404.66534423828125, "gpu_peak_mb": 62.28857421875, "params_millions_measured": 25.016064, "latency_ms": 409.1794999999365, "cuda_event_ms": 409.0316801071167, "tokens_total": 145, "tokens_per_s": 354.3677041494564}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547206.6743658, "prompt_tokens": 25, "prefill_ms": 7.5637, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.3691, "kv_decode_ms_equiv": 276.3691, "kv_decode_ms_per_token": 2.15913359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2859.754800000246, "ollama_total_duration_ms": 2770.1723, "ollama_load_ms": 2388.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.5637, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3305.260652855084}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547206.6743658, "prompt_tokens": 25, "prefill_ms": 7.5637, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.3691, "kv_decode_ms_equiv": 276.3691, "kv_decode_ms_per_token": 2.15913359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2859.754800000246, "ollama_total_duration_ms": 2770.1723, "ollama_load_ms": 2388.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 276.3691, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 463.14873840816506}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547206.6743658, "prompt_tokens": 25, "prefill_ms": 7.5637, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.3691, "kv_decode_ms_equiv": 276.3691, "kv_decode_ms_per_token": 2.15913359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2859.754800000246, "ollama_total_duration_ms": 2770.1723, "ollama_load_ms": 2388.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 283.9328, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 538.8598992437647}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547209.5342643, "prompt_tokens": 25, "prefill_ms": 2.3584, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.7722, "kv_decode_ms_equiv": 276.7722, "kv_decode_ms_per_token": 2.1622828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 545.9200999998757, "ollama_total_duration_ms": 491.5698, "ollama_load_ms": 147.4105, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3584, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10600.407055630936}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547209.5342643, "prompt_tokens": 25, "prefill_ms": 2.3584, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.7722, "kv_decode_ms_equiv": 276.7722, "kv_decode_ms_per_token": 2.1622828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 545.9200999998757, "ollama_total_duration_ms": 491.5698, "ollama_load_ms": 147.4105, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 276.7722, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 462.4741935786903}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547209.5342643, "prompt_tokens": 25, "prefill_ms": 2.3584, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.7722, "kv_decode_ms_equiv": 276.7722, "kv_decode_ms_per_token": 2.1622828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 545.9200999998757, "ollama_total_duration_ms": 491.5698, "ollama_load_ms": 147.4105, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 279.1306, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 548.1305166828718}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547210.0803332, "prompt_tokens": 25, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.7877, "kv_decode_ms_equiv": 271.7877, "kv_decode_ms_per_token": 2.12334140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 533.7515999999596, "ollama_total_duration_ms": 492.6625, "ollama_load_ms": 148.3938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4896, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10041.77377892031}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547210.0803332, "prompt_tokens": 25, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.7877, "kv_decode_ms_equiv": 271.7877, "kv_decode_ms_per_token": 2.12334140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 533.7515999999596, "ollama_total_duration_ms": 492.6625, "ollama_load_ms": 148.3938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 271.7877, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 470.95582323997746}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547210.0803332, "prompt_tokens": 25, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.7877, "kv_decode_ms_equiv": 271.7877, "kv_decode_ms_per_token": 2.12334140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 533.7515999999596, "ollama_total_duration_ms": 492.6625, "ollama_load_ms": 148.3938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 274.27729999999997, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 557.8296125855112}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547210.614211, "prompt_tokens": 25, "prefill_ms": 2.5807, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.8538, "kv_decode_ms_equiv": 267.8538, "kv_decode_ms_per_token": 2.0926078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 570.9750999999414, "ollama_total_duration_ms": 510.3479, "ollama_load_ms": 155.2727, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5807, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9687.29414499942}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547210.614211, "prompt_tokens": 25, "prefill_ms": 2.5807, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.8538, "kv_decode_ms_equiv": 267.8538, "kv_decode_ms_per_token": 2.0926078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 570.9750999999414, "ollama_total_duration_ms": 510.3479, "ollama_load_ms": 155.2727, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 267.8538, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 477.87263051709556}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547210.614211, "prompt_tokens": 25, "prefill_ms": 2.5807, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.8538, "kv_decode_ms_equiv": 267.8538, "kv_decode_ms_per_token": 2.0926078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 570.9750999999414, "ollama_total_duration_ms": 510.3479, "ollama_load_ms": 155.2727, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 270.43449999999996, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 565.7562182339902}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547211.1853125, "prompt_tokens": 84, "prefill_ms": 936.82, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.0495, "kv_decode_ms_equiv": 6003.666731707317, "kv_decode_ms_per_token": 46.903646341463414, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 13266.502800000126, "ollama_total_duration_ms": 13122.3555, "ollama_load_ms": 10173.2379, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 936.82, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 89.66503704019982}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547211.1853125, "prompt_tokens": 84, "prefill_ms": 936.82, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.0495, "kv_decode_ms_equiv": 6003.666731707317, "kv_decode_ms_per_token": 46.903646341463414, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 13266.502800000126, "ollama_total_duration_ms": 13122.3555, "ollama_load_ms": 10173.2379, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 6003.666731707317, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 21.320304027535432}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547211.1853125, "prompt_tokens": 84, "prefill_ms": 936.82, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.0495, "kv_decode_ms_equiv": 6003.666731707317, "kv_decode_ms_per_token": 46.903646341463414, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 13266.502800000126, "ollama_total_duration_ms": 13122.3555, "ollama_load_ms": 10173.2379, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 6940.486731707317, "cuda_event_ms": null, "tokens_total": 212, "tokens_per_s": 30.545408152930698}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547224.451947, "prompt_tokens": 84, "prefill_ms": 60.0461, "prefill_cuda_event_ms": null, "kv_decode_ms": 2139.1136, "kv_decode_ms_equiv": 6678.208312195123, "kv_decode_ms_per_token": 52.173502439024396, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2495.358699999997, "ollama_total_duration_ms": 2469.1116, "ollama_load_ms": 255.4065, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 60.0461, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1398.9251591693715}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547224.451947, "prompt_tokens": 84, "prefill_ms": 60.0461, "prefill_cuda_event_ms": null, "kv_decode_ms": 2139.1136, "kv_decode_ms_equiv": 6678.208312195123, "kv_decode_ms_per_token": 52.173502439024396, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2495.358699999997, "ollama_total_duration_ms": 2469.1116, "ollama_load_ms": 255.4065, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 6678.208312195123, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 19.166817507962175}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547224.451947, "prompt_tokens": 84, "prefill_ms": 60.0461, "prefill_cuda_event_ms": null, "kv_decode_ms": 2139.1136, "kv_decode_ms_equiv": 6678.208312195123, "kv_decode_ms_per_token": 52.173502439024396, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2495.358699999997, "ollama_total_duration_ms": 2469.1116, "ollama_load_ms": 255.4065, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 6738.254412195122, "cuda_event_ms": null, "tokens_total": 212, "tokens_per_s": 31.462154295675628}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547226.9479992, "prompt_tokens": 84, "prefill_ms": 61.2343, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3693, "kv_decode_ms_equiv": 7459.4944, "kv_decode_ms_per_token": 58.2773, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2753.072699999848, "ollama_total_duration_ms": 2749.6436, "ollama_load_ms": 284.4227, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 61.2343, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1371.7801950867406}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547226.9479992, "prompt_tokens": 84, "prefill_ms": 61.2343, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3693, "kv_decode_ms_equiv": 7459.4944, "kv_decode_ms_per_token": 58.2773, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2753.072699999848, "ollama_total_duration_ms": 2749.6436, "ollama_load_ms": 284.4227, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 7459.4944, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.15933991451217}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547226.9479992, "prompt_tokens": 84, "prefill_ms": 61.2343, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3693, "kv_decode_ms_equiv": 7459.4944, "kv_decode_ms_per_token": 58.2773, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2753.072699999848, "ollama_total_duration_ms": 2749.6436, "ollama_load_ms": 284.4227, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 7520.7287, "cuda_event_ms": null, "tokens_total": 212, "tokens_per_s": 28.18875782608672}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547229.7012184, "prompt_tokens": 84, "prefill_ms": 69.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 2459.884, "kv_decode_ms_equiv": 7679.637853658536, "kv_decode_ms_per_token": 59.997170731707314, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2827.036600000156, "ollama_total_duration_ms": 2807.2611, "ollama_load_ms": 255.1386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 69.6472, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1206.0786363270886}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547229.7012184, "prompt_tokens": 84, "prefill_ms": 69.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 2459.884, "kv_decode_ms_equiv": 7679.637853658536, "kv_decode_ms_per_token": 59.997170731707314, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2827.036600000156, "ollama_total_duration_ms": 2807.2611, "ollama_load_ms": 255.1386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 7679.637853658536, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 16.66745261158656}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547229.7012184, "prompt_tokens": 84, "prefill_ms": 69.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 2459.884, "kv_decode_ms_equiv": 7679.637853658536, "kv_decode_ms_per_token": 59.997170731707314, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2827.036600000156, "ollama_total_duration_ms": 2807.2611, "ollama_load_ms": 255.1386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 7749.2850536585365, "cuda_event_ms": null, "tokens_total": 212, "tokens_per_s": 27.35736245757434}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547232.5284007, "prompt_tokens": 38, "prefill_ms": 23.5938, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.669, "kv_decode_ms_equiv": 1425.0752, "kv_decode_ms_per_token": 11.1334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 6203.152300000056, "ollama_total_duration_ms": 6199.3633, "ollama_load_ms": 5746.2702, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 23.5938, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1610.592613313667}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547232.5284007, "prompt_tokens": 38, "prefill_ms": 23.5938, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.669, "kv_decode_ms_equiv": 1425.0752, "kv_decode_ms_per_token": 11.1334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6203.152300000056, "ollama_total_duration_ms": 6199.3633, "ollama_load_ms": 5746.2702, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1425.0752, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.81982143819498}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547232.5284007, "prompt_tokens": 38, "prefill_ms": 23.5938, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.669, "kv_decode_ms_equiv": 1425.0752, "kv_decode_ms_per_token": 11.1334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6203.152300000056, "ollama_total_duration_ms": 6199.3633, "ollama_load_ms": 5746.2702, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1448.669, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.58794244924132}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547238.731918, "prompt_tokens": 38, "prefill_ms": 11.7466, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.4153, "kv_decode_ms_equiv": 1442.4330971428572, "kv_decode_ms_per_token": 11.269008571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 574.27110000026, "ollama_total_duration_ms": 571.1282, "ollama_load_ms": 132.901, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.7466, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3234.9786321148244}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547238.731918, "prompt_tokens": 38, "prefill_ms": 11.7466, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.4153, "kv_decode_ms_equiv": 1442.4330971428572, "kv_decode_ms_per_token": 11.269008571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 574.27110000026, "ollama_total_duration_ms": 571.1282, "ollama_load_ms": 132.901, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1442.4330971428572, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.73895104981982}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547238.731918, "prompt_tokens": 38, "prefill_ms": 11.7466, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.4153, "kv_decode_ms_equiv": 1442.4330971428572, "kv_decode_ms_per_token": 11.269008571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 574.27110000026, "ollama_total_duration_ms": 571.1282, "ollama_load_ms": 132.901, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1454.179697142857, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.15370488678494}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547239.3064215, "prompt_tokens": 38, "prefill_ms": 12.1666, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.451, "kv_decode_ms_equiv": 1435.2493714285715, "kv_decode_ms_per_token": 11.212885714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 578.274699999838, "ollama_total_duration_ms": 556.8097, "ollama_load_ms": 118.1777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.1666, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3123.3047852316995}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547239.3064215, "prompt_tokens": 38, "prefill_ms": 12.1666, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.451, "kv_decode_ms_equiv": 1435.2493714285715, "kv_decode_ms_per_token": 11.212885714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 578.274699999838, "ollama_total_duration_ms": 556.8097, "ollama_load_ms": 118.1777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1435.2493714285715, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.18310820968732}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547239.3064215, "prompt_tokens": 38, "prefill_ms": 12.1666, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.451, "kv_decode_ms_equiv": 1435.2493714285715, "kv_decode_ms_per_token": 11.212885714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 578.274699999838, "ollama_total_duration_ms": 556.8097, "ollama_load_ms": 118.1777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1447.4159714285715, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.6871412757462}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547239.88482, "prompt_tokens": 38, "prefill_ms": 11.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3682, "kv_decode_ms_equiv": 1434.94656, "kv_decode_ms_per_token": 11.21052, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 579.8257999999805, "ollama_total_duration_ms": 566.6552, "ollama_load_ms": 131.3929, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.7791, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3226.052924247184}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547239.88482, "prompt_tokens": 38, "prefill_ms": 11.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3682, "kv_decode_ms_equiv": 1434.94656, "kv_decode_ms_per_token": 11.21052, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 579.8257999999805, "ollama_total_duration_ms": 566.6552, "ollama_load_ms": 131.3929, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1434.94656, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.20192818887973}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547239.88482, "prompt_tokens": 38, "prefill_ms": 11.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3682, "kv_decode_ms_equiv": 1434.94656, "kv_decode_ms_per_token": 11.21052, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 579.8257999999805, "ollama_total_duration_ms": 566.6552, "ollama_load_ms": 131.3929, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1446.72566, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.74186474303636}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547240.4649127, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 40.99690000020928, "prefill_cuda_event_ms": 40.88608169555664, "kv_decode_ms": 550.7685000002311, "kv_decode_cuda_event_ms": 550.740966796875, "gpu_peak_mb": 260.833984375, "hf_load_ms": 380.4963000002317, "params_millions_measured": 96.08832, "latency_ms": 40.99690000020928, "cuda_event_ms": 40.88608169555664, "tokens_total": 17, "tokens_per_s": 414.66549909659557}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547240.4649127, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 40.99690000020928, "prefill_cuda_event_ms": 40.88608169555664, "kv_decode_ms": 550.7685000002311, "kv_decode_cuda_event_ms": 550.740966796875, "gpu_peak_mb": 260.833984375, "hf_load_ms": 380.4963000002317, "params_millions_measured": 96.08832, "latency_ms": 550.7685000002311, "cuda_event_ms": 550.740966796875, "tokens_total": 128, "tokens_per_s": 232.40254299210338}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547240.4649127, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 40.99690000020928, "prefill_cuda_event_ms": 40.88608169555664, "kv_decode_ms": 550.7685000002311, "kv_decode_cuda_event_ms": 550.740966796875, "gpu_peak_mb": 260.833984375, "hf_load_ms": 380.4963000002317, "params_millions_measured": 96.08832, "latency_ms": 591.7654000004404, "cuda_event_ms": 591.6270484924316, "tokens_total": 145, "tokens_per_s": 245.02953366298888}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547241.4385154, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.365499999697931, "prefill_cuda_event_ms": 5.3155198097229, "kv_decode_ms": 522.3284999997304, "kv_decode_cuda_event_ms": 522.292236328125, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 5.365499999697931, "cuda_event_ms": 5.3155198097229, "tokens_total": 17, "tokens_per_s": 3168.3906441071795}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547241.4385154, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.365499999697931, "prefill_cuda_event_ms": 5.3155198097229, "kv_decode_ms": 522.3284999997304, "kv_decode_cuda_event_ms": 522.292236328125, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 522.3284999997304, "cuda_event_ms": 522.292236328125, "tokens_total": 128, "tokens_per_s": 245.05651137180158}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547241.4385154, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.365499999697931, "prefill_cuda_event_ms": 5.3155198097229, "kv_decode_ms": 522.3284999997304, "kv_decode_cuda_event_ms": 522.292236328125, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 527.6939999994283, "cuda_event_ms": 527.6077561378479, "tokens_total": 145, "tokens_per_s": 274.78045988803564}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547241.966889, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.973900000095455, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 515.8523000000059, "kv_decode_cuda_event_ms": 515.831787109375, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 4.973900000095455, "cuda_event_ms": 4.927487850189209, "tokens_total": 17, "tokens_per_s": 3417.841130636674}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547241.966889, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.973900000095455, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 515.8523000000059, "kv_decode_cuda_event_ms": 515.831787109375, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 515.8523000000059, "cuda_event_ms": 515.831787109375, "tokens_total": 128, "tokens_per_s": 248.13304118252168}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547241.966889, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.973900000095455, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 515.8523000000059, "kv_decode_cuda_event_ms": 515.831787109375, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 520.8262000001014, "cuda_event_ms": 520.7592749595642, "tokens_total": 145, "tokens_per_s": 278.4038130185689}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547242.4883654, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.813900000044669, "prefill_cuda_event_ms": 4.775936126708984, "kv_decode_ms": 523.6589999999524, "kv_decode_cuda_event_ms": 523.6357421875, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 4.813900000044669, "cuda_event_ms": 4.775936126708984, "tokens_total": 17, "tokens_per_s": 3531.4402043753}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547242.4883654, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.813900000044669, "prefill_cuda_event_ms": 4.775936126708984, "kv_decode_ms": 523.6589999999524, "kv_decode_cuda_event_ms": 523.6357421875, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 523.6589999999524, "cuda_event_ms": 523.6357421875, "tokens_total": 128, "tokens_per_s": 244.4338777716255}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547242.4883654, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.813900000044669, "prefill_cuda_event_ms": 4.775936126708984, "kv_decode_ms": 523.6589999999524, "kv_decode_cuda_event_ms": 523.6357421875, "gpu_peak_mb": 260.833984375, "params_millions_measured": 96.08832, "latency_ms": 528.472899999997, "cuda_event_ms": 528.411678314209, "tokens_total": 145, "tokens_per_s": 274.3754693949317}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547243.0173926, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 98.1205999996746, "prefill_cuda_event_ms": 97.99372863769531, "kv_decode_ms": 547.9566999997587, "kv_decode_cuda_event_ms": 547.9127197265625, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 98.1205999996746, "cuda_event_ms": 97.99372863769531, "tokens_total": 9, "tokens_per_s": 91.72385819114281}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547243.0173926, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 98.1205999996746, "prefill_cuda_event_ms": 97.99372863769531, "kv_decode_ms": 547.9566999997587, "kv_decode_cuda_event_ms": 547.9127197265625, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 547.9566999997587, "cuda_event_ms": 547.9127197265625, "tokens_total": 128, "tokens_per_s": 233.59509975889767}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547243.0173926, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 98.1205999996746, "prefill_cuda_event_ms": 97.99372863769531, "kv_decode_ms": 547.9566999997587, "kv_decode_cuda_event_ms": 547.9127197265625, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 646.0772999994333, "cuda_event_ms": 645.9064483642578, "tokens_total": 137, "tokens_per_s": 212.04892974899468}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547243.6640143, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.956100000072183, "prefill_cuda_event_ms": 4.91315221786499, "kv_decode_ms": 537.9369000002043, "kv_decode_cuda_event_ms": 537.9174194335938, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 4.956100000072183, "cuda_event_ms": 4.91315221786499, "tokens_total": 9, "tokens_per_s": 1815.9439881900928}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547243.6640143, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.956100000072183, "prefill_cuda_event_ms": 4.91315221786499, "kv_decode_ms": 537.9369000002043, "kv_decode_cuda_event_ms": 537.9174194335938, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 537.9369000002043, "cuda_event_ms": 537.9174194335938, "tokens_total": 128, "tokens_per_s": 237.94612342070488}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547243.6640143, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.956100000072183, "prefill_cuda_event_ms": 4.91315221786499, "kv_decode_ms": 537.9369000002043, "kv_decode_cuda_event_ms": 537.9174194335938, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 542.8930000002765, "cuda_event_ms": 542.8305716514587, "tokens_total": 137, "tokens_per_s": 252.35175255516322}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547244.2074542, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.142900000009831, "prefill_cuda_event_ms": 5.091328144073486, "kv_decode_ms": 503.24380000029123, "kv_decode_cuda_event_ms": 503.2222595214844, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 5.142900000009831, "cuda_event_ms": 5.091328144073486, "tokens_total": 9, "tokens_per_s": 1749.9854167848482}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547244.2074542, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.142900000009831, "prefill_cuda_event_ms": 5.091328144073486, "kv_decode_ms": 503.24380000029123, "kv_decode_cuda_event_ms": 503.2222595214844, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 503.24380000029123, "cuda_event_ms": 503.2222595214844, "tokens_total": 128, "tokens_per_s": 254.34987972017922}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547244.2074542, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.142900000009831, "prefill_cuda_event_ms": 5.091328144073486, "kv_decode_ms": 503.24380000029123, "kv_decode_cuda_event_ms": 503.2222595214844, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 508.38670000030106, "cuda_event_ms": 508.31358766555786, "tokens_total": 137, "tokens_per_s": 269.4799057487516}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547244.7164116, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.783299999871815, "prefill_cuda_event_ms": 4.741119861602783, "kv_decode_ms": 506.3089000000218, "kv_decode_cuda_event_ms": 506.2868347167969, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 4.783299999871815, "cuda_event_ms": 4.741119861602783, "tokens_total": 9, "tokens_per_s": 1881.5462129160173}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547244.7164116, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.783299999871815, "prefill_cuda_event_ms": 4.741119861602783, "kv_decode_ms": 506.3089000000218, "kv_decode_cuda_event_ms": 506.2868347167969, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 506.3089000000218, "cuda_event_ms": 506.2868347167969, "tokens_total": 128, "tokens_per_s": 252.81009281091934}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547244.7164116, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.783299999871815, "prefill_cuda_event_ms": 4.741119861602783, "kv_decode_ms": 506.3089000000218, "kv_decode_cuda_event_ms": 506.2868347167969, "gpu_peak_mb": 259.50927734375, "params_millions_measured": 96.08832, "latency_ms": 511.09219999989364, "cuda_event_ms": 511.02795457839966, "tokens_total": 137, "tokens_per_s": 268.0533962365861}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547245.2280302, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.491200000051322, "prefill_cuda_event_ms": 5.444608211517334, "kv_decode_ms": 220.36439999965296, "kv_decode_cuda_event_ms": 220.32077026367188, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 5.491200000051322, "cuda_event_ms": 5.444608211517334, "tokens_total": 1, "tokens_per_s": 182.1095571078551}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547245.2280302, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 5.491200000051322, "prefill_cuda_event_ms": 5.444608211517334, "kv_decode_ms": 220.36439999965296, "kv_decode_cuda_event_ms": 220.32077026367188, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 220.36439999965296, "cuda_event_ms": 220.32077026367188, "tokens_total": 128, "tokens_per_s": 580.8560729419161}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547245.2280302, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 5.491200000051322, "prefill_cuda_event_ms": 5.444608211517334, "kv_decode_ms": 220.36439999965296, "kv_decode_cuda_event_ms": 220.32077026367188, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 225.85559999970428, "cuda_event_ms": 225.7653784751892, "tokens_total": 129, "tokens_per_s": 571.1613969286965}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547245.4544284, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.2419000001718814, "prefill_cuda_event_ms": 2.1882879734039307, "kv_decode_ms": 223.56370000034076, "kv_decode_cuda_event_ms": 223.53407287597656, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 2.2419000001718814, "cuda_event_ms": 2.1882879734039307, "tokens_total": 1, "tokens_per_s": 446.0502252211661}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547245.4544284, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.2419000001718814, "prefill_cuda_event_ms": 2.1882879734039307, "kv_decode_ms": 223.56370000034076, "kv_decode_cuda_event_ms": 223.53407287597656, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 223.56370000034076, "cuda_event_ms": 223.53407287597656, "tokens_total": 128, "tokens_per_s": 572.5437537480589}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547245.4544284, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.2419000001718814, "prefill_cuda_event_ms": 2.1882879734039307, "kv_decode_ms": 223.56370000034076, "kv_decode_cuda_event_ms": 223.53407287597656, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 225.80560000051264, "cuda_event_ms": 225.7223608493805, "tokens_total": 129, "tokens_per_s": 571.2878688558084}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547245.680782, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.316999999948166, "prefill_cuda_event_ms": 2.2671360969543457, "kv_decode_ms": 220.35609999966255, "kv_decode_cuda_event_ms": 220.29209899902344, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 2.316999999948166, "cuda_event_ms": 2.2671360969543457, "tokens_total": 1, "tokens_per_s": 431.59257661733756}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547245.680782, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.316999999948166, "prefill_cuda_event_ms": 2.2671360969543457, "kv_decode_ms": 220.35609999966255, "kv_decode_cuda_event_ms": 220.29209899902344, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 220.35609999966255, "cuda_event_ms": 220.29209899902344, "tokens_total": 128, "tokens_per_s": 580.8779516437077}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547245.680782, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.316999999948166, "prefill_cuda_event_ms": 2.2671360969543457, "kv_decode_ms": 220.35609999966255, "kv_decode_cuda_event_ms": 220.29209899902344, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 222.6730999996107, "cuda_event_ms": 222.55923509597778, "tokens_total": 129, "tokens_per_s": 579.3245793956501}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547245.9040084, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.2877999999764143, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 225.56110000004992, "kv_decode_cuda_event_ms": 225.5083465576172, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 2.2877999999764143, "cuda_event_ms": 2.2415359020233154, "tokens_total": 1, "tokens_per_s": 437.1011452095067}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547245.9040084, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.2877999999764143, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 225.56110000004992, "kv_decode_cuda_event_ms": 225.5083465576172, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 225.56110000004992, "cuda_event_ms": 225.5083465576172, "tokens_total": 128, "tokens_per_s": 567.4737354977062}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547245.9040084, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 2.2877999999764143, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 225.56110000004992, "kv_decode_cuda_event_ms": 225.5083465576172, "gpu_peak_mb": 256.09375, "params_millions_measured": 25.016064, "latency_ms": 227.84890000002633, "cuda_event_ms": 227.7498824596405, "tokens_total": 129, "tokens_per_s": 566.164681944855}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547246.132594, "prompt_tokens": 17, "prefill_ms": 9.9248, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.2259, "kv_decode_ms_equiv": 291.3760864864865, "kv_decode_ms_per_token": 2.2763756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1601.014899999882, "ollama_total_duration_ms": 1539.2064, "ollama_load_ms": 1388.6215, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 9.9248, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1712.8808640980171}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547246.132594, "prompt_tokens": 17, "prefill_ms": 9.9248, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.2259, "kv_decode_ms_equiv": 291.3760864864865, "kv_decode_ms_per_token": 2.2763756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1601.014899999882, "ollama_total_duration_ms": 1539.2064, "ollama_load_ms": 1388.6215, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 291.3760864864865, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 439.2948012428481}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547246.132594, "prompt_tokens": 17, "prefill_ms": 9.9248, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.2259, "kv_decode_ms_equiv": 291.3760864864865, "kv_decode_ms_per_token": 2.2763756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1601.014899999882, "ollama_total_duration_ms": 1539.2064, "ollama_load_ms": 1388.6215, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 301.3008864864865, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 481.246509729414}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547247.7338517, "prompt_tokens": 17, "prefill_ms": 2.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2379, "kv_decode_ms_equiv": 277.96773333333334, "kv_decode_ms_per_token": 2.1716229166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 345.07570000005217, "ollama_total_duration_ms": 304.5767, "ollama_load_ms": 157.7018, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.7503, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6181.143875213613}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547247.7338517, "prompt_tokens": 17, "prefill_ms": 2.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2379, "kv_decode_ms_equiv": 277.96773333333334, "kv_decode_ms_per_token": 2.1716229166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 345.07570000005217, "ollama_total_duration_ms": 304.5767, "ollama_load_ms": 157.7018, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 277.96773333333334, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 460.48510186793857}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547247.7338517, "prompt_tokens": 17, "prefill_ms": 2.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2379, "kv_decode_ms_equiv": 277.96773333333334, "kv_decode_ms_per_token": 2.1716229166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 345.07570000005217, "ollama_total_duration_ms": 304.5767, "ollama_load_ms": 157.7018, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 280.7180333333333, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 516.5325443407567}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547248.0791075, "prompt_tokens": 17, "prefill_ms": 2.4736, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5575, "kv_decode_ms_equiv": 265.4866666666667, "kv_decode_ms_per_token": 2.0741145833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 320.7410999998501, "ollama_total_duration_ms": 292.6812, "ollama_load_ms": 149.2711, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4736, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6872.574385510997}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547248.0791075, "prompt_tokens": 17, "prefill_ms": 2.4736, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5575, "kv_decode_ms_equiv": 265.4866666666667, "kv_decode_ms_per_token": 2.0741145833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 320.7410999998501, "ollama_total_duration_ms": 292.6812, "ollama_load_ms": 149.2711, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 265.4866666666667, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 482.1334404740978}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547248.0791075, "prompt_tokens": 17, "prefill_ms": 2.4736, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5575, "kv_decode_ms_equiv": 265.4866666666667, "kv_decode_ms_per_token": 2.0741145833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 320.7410999998501, "ollama_total_duration_ms": 292.6812, "ollama_load_ms": 149.2711, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 267.96026666666666, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 541.1250026123276}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547248.4000123, "prompt_tokens": 17, "prefill_ms": 2.8137, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3902, "kv_decode_ms_equiv": 286.37386666666663, "kv_decode_ms_per_token": 2.237295833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 307.5491999998121, "ollama_total_duration_ms": 297.8544, "ollama_load_ms": 153.2952, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.8137, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6041.866581369727}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547248.4000123, "prompt_tokens": 17, "prefill_ms": 2.8137, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3902, "kv_decode_ms_equiv": 286.37386666666663, "kv_decode_ms_per_token": 2.237295833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 307.5491999998121, "ollama_total_duration_ms": 297.8544, "ollama_load_ms": 153.2952, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 286.37386666666663, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 446.9681591057657}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547248.4000123, "prompt_tokens": 17, "prefill_ms": 2.8137, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3902, "kv_decode_ms_equiv": 286.37386666666663, "kv_decode_ms_per_token": 2.237295833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 307.5491999998121, "ollama_total_duration_ms": 297.8544, "ollama_load_ms": 153.2952, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 289.1875666666666, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 501.4046823359281}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547248.7078662, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 84.73599999979342, "prefill_cuda_event_ms": null, "kv_decode_ms": 5401.977999999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 84.73599999979342, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 11.801359516645086}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547248.7078662, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 84.73599999979342, "prefill_cuda_event_ms": null, "kv_decode_ms": 5401.977999999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5401.977999999872, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 23.69502430406104}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547248.7078662, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 84.73599999979342, "prefill_cuda_event_ms": null, "kv_decode_ms": 5401.977999999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5486.713999999665, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 23.511340303140983}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547254.1953564, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 45.641600000180915, "prefill_cuda_event_ms": null, "kv_decode_ms": 4530.543000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 45.641600000180915, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 21.909836640171164}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547254.1953564, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 45.641600000180915, "prefill_cuda_event_ms": null, "kv_decode_ms": 4530.543000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4530.543000000307, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 28.252684060164825}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547254.1953564, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 45.641600000180915, "prefill_cuda_event_ms": null, "kv_decode_ms": 4530.543000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4576.184600000488, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 28.189422253635975}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547258.7721727, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 27.244599999903585, "prefill_cuda_event_ms": null, "kv_decode_ms": 4072.751100000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 27.244599999903585, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 36.70452126305906}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547258.7721727, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 27.244599999903585, "prefill_cuda_event_ms": null, "kv_decode_ms": 4072.751100000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4072.751100000005, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 31.42838755847364}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547258.7721727, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 27.244599999903585, "prefill_cuda_event_ms": null, "kv_decode_ms": 4072.751100000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4099.9956999999085, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 31.463447632397}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547262.8727381, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 31.652399999984482, "prefill_cuda_event_ms": null, "kv_decode_ms": 4032.6630999998088, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 31.652399999984482, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 31.593180927844028}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547262.8727381, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 31.652399999984482, "prefill_cuda_event_ms": null, "kv_decode_ms": 4032.6630999998088, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4032.6630999998088, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 31.74081167355787}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547262.8727381, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 31.652399999984482, "prefill_cuda_event_ms": null, "kv_decode_ms": 4032.6630999998088, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4064.3154999997932, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 31.739661943076655}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547266.9376352, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 63.672499999938736, "prefill_cuda_event_ms": null, "kv_decode_ms": 4272.174299999733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 63.672499999938736, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 266.991244257982}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547266.9376352, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 63.672499999938736, "prefill_cuda_event_ms": null, "kv_decode_ms": 4272.174299999733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4272.174299999733, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 29.96132437761446}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547266.9376352, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 63.672499999938736, "prefill_cuda_event_ms": null, "kv_decode_ms": 4272.174299999733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4335.846799999672, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 33.44214099077739}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547271.2744188, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 68.29870000001392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3871.860899999774, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 68.29870000001392, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 248.90664097554617}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547271.2744188, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 68.29870000001392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3871.860899999774, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3871.860899999774, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 33.05903887198207}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547271.2744188, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 68.29870000001392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3871.860899999774, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3940.159599999788, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 36.80053975478755}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547275.2151573, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 37.76999999990949, "prefill_cuda_event_ms": null, "kv_decode_ms": 3751.284399999804, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 37.76999999990949, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 450.0926661382245}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547275.2151573, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 37.76999999990949, "prefill_cuda_event_ms": null, "kv_decode_ms": 3751.284399999804, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3751.284399999804, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 34.12164644195111}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547275.2151573, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 37.76999999990949, "prefill_cuda_event_ms": null, "kv_decode_ms": 3751.284399999804, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3789.0543999997135, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 38.26812304410593}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547279.0050197, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 43.62309999987701, "prefill_cuda_event_ms": null, "kv_decode_ms": 4500.98829999979, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 43.62309999987701, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 389.701786439935}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547279.0050197, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 43.62309999987701, "prefill_cuda_event_ms": null, "kv_decode_ms": 4500.98829999979, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4500.98829999979, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 28.438198784032824}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547279.0050197, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 43.62309999987701, "prefill_cuda_event_ms": null, "kv_decode_ms": 4500.98829999979, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4544.611399999667, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 31.90591829259827}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547283.5502589, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 15.824000000066007, "prefill_cuda_event_ms": null, "kv_decode_ms": 1722.1604999999727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.824000000066007, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 568.7563195122888}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547283.5502589, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 15.824000000066007, "prefill_cuda_event_ms": null, "kv_decode_ms": 1722.1604999999727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1722.1604999999727, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 74.325244366017}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547283.5502589, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 15.824000000066007, "prefill_cuda_event_ms": null, "kv_decode_ms": 1722.1604999999727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1737.9845000000387, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 78.82694005613799}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547285.2896638, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 14.842099999896163, "prefill_cuda_event_ms": null, "kv_decode_ms": 1260.3605999997853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.842099999896163, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 606.3831937571479}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547285.2896638, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.842099999896163, "prefill_cuda_event_ms": null, "kv_decode_ms": 1260.3605999997853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1260.3605999997853, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 101.55823658722892}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547285.2896638, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.842099999896163, "prefill_cuda_event_ms": null, "kv_decode_ms": 1260.3605999997853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1275.2026999996815, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 107.43390050854991}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547286.5654457, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.663399999837566, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.9197000003587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.663399999837566, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 710.7096040648992}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547286.5654457, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.663399999837566, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.9197000003587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1415.9197000003587, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 90.40060675754958}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547286.5654457, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.663399999837566, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.9197000003587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1428.5831000001963, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 95.89921650338799}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547287.9945104, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.225999999576743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1808.6272000000463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.225999999576743, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 680.4778466874351}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547287.9945104, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 13.225999999576743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1808.6272000000463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1808.6272000000463, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 70.77190921379305}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547287.9945104, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 13.225999999576743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1808.6272000000463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1821.853199999623, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 75.19815537279752}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547289.8168259, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.674199999906705, "prefill_cuda_event_ms": 6.553599834442139, "kv_decode_ms": 321.99809999974605, "kv_decode_cuda_event_ms": 321.8913269042969, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 6.674199999906705, "cuda_event_ms": 6.553599834442139, "tokens_total": 9, "tokens_per_s": 1348.4762218881374}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547289.8168259, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.674199999906705, "prefill_cuda_event_ms": 6.553599834442139, "kv_decode_ms": 321.99809999974605, "kv_decode_cuda_event_ms": 321.8913269042969, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 321.99809999974605, "cuda_event_ms": 321.8913269042969, "tokens_total": 128, "tokens_per_s": 397.51787355298353}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547289.8168259, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.674199999906705, "prefill_cuda_event_ms": 6.553599834442139, "kv_decode_ms": 321.99809999974605, "kv_decode_cuda_event_ms": 321.8913269042969, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 328.67229999965275, "cuda_event_ms": 328.444926738739, "tokens_total": 137, "tokens_per_s": 416.8285553730714}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547290.1475084, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.3273000001609034, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 240.77430000033928, "kv_decode_cuda_event_ms": 240.70962524414062, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 2.3273000001609034, "cuda_event_ms": 2.269184112548828, "tokens_total": 9, "tokens_per_s": 3867.1421816601915}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547290.1475084, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.3273000001609034, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 240.77430000033928, "kv_decode_cuda_event_ms": 240.70962524414062, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 240.77430000033928, "cuda_event_ms": 240.70962524414062, "tokens_total": 128, "tokens_per_s": 531.6182001144625}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547290.1475084, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.3273000001609034, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 240.77430000033928, "kv_decode_cuda_event_ms": 240.70962524414062, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 243.10160000050018, "cuda_event_ms": 242.97880935668945, "tokens_total": 137, "tokens_per_s": 563.5503838712625}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547290.391346, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.4433000000717584, "prefill_cuda_event_ms": 3.3769280910491943, "kv_decode_ms": 231.04190000003655, "kv_decode_cuda_event_ms": 231.00006103515625, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 3.4433000000717584, "cuda_event_ms": 3.3769280910491943, "tokens_total": 9, "tokens_per_s": 2613.7716724689803}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547290.391346, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.4433000000717584, "prefill_cuda_event_ms": 3.3769280910491943, "kv_decode_ms": 231.04190000003655, "kv_decode_cuda_event_ms": 231.00006103515625, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 231.04190000003655, "cuda_event_ms": 231.00006103515625, "tokens_total": 128, "tokens_per_s": 554.0120644782602}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547290.391346, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.4433000000717584, "prefill_cuda_event_ms": 3.3769280910491943, "kv_decode_ms": 231.04190000003655, "kv_decode_cuda_event_ms": 231.00006103515625, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 234.4852000001083, "cuda_event_ms": 234.37698912620544, "tokens_total": 137, "tokens_per_s": 584.258622718776}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547290.626485, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.613600000131555, "prefill_cuda_event_ms": 2.5517759323120117, "kv_decode_ms": 247.74280000019644, "kv_decode_cuda_event_ms": 247.689208984375, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 2.613600000131555, "cuda_event_ms": 2.5517759323120117, "tokens_total": 9, "tokens_per_s": 3443.5261706255687}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547290.626485, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.613600000131555, "prefill_cuda_event_ms": 2.5517759323120117, "kv_decode_ms": 247.74280000019644, "kv_decode_cuda_event_ms": 247.689208984375, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 247.74280000019644, "cuda_event_ms": 247.689208984375, "tokens_total": 128, "tokens_per_s": 516.6648637211596}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547290.626485, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.613600000131555, "prefill_cuda_event_ms": 2.5517759323120117, "kv_decode_ms": 247.74280000019644, "kv_decode_cuda_event_ms": 247.689208984375, "gpu_peak_mb": 256.8955078125, "params_millions_measured": 25.016064, "latency_ms": 250.356400000328, "cuda_event_ms": 250.240984916687, "tokens_total": 137, "tokens_per_s": 547.2198833336017}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547290.8774076, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.0341999998127, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.4019000002409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.0341999998127, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 90.62732232667294}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547290.8774076, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 11.0341999998127, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.4019000002409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1232.4019000002409, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 103.86222221823496}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547290.8774076, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 11.0341999998127, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.4019000002409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1243.4361000000536, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 103.74477626956016}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547292.1232398, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.053700000298704, "prefill_cuda_event_ms": null, "kv_decode_ms": 1622.514499999852, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.053700000298704, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 99.465868284342}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547292.1232398, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 10.053700000298704, "prefill_cuda_event_ms": null, "kv_decode_ms": 1622.514499999852, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1622.514499999852, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 78.88989589924262}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547292.1232398, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 10.053700000298704, "prefill_cuda_event_ms": null, "kv_decode_ms": 1622.514499999852, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1632.5682000001507, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 79.0166070856875}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547293.7563093, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.394200000017008, "prefill_cuda_event_ms": null, "kv_decode_ms": 1708.755099999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.394200000017008, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 80.68290006604926}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547293.7563093, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 12.394200000017008, "prefill_cuda_event_ms": null, "kv_decode_ms": 1708.755099999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1708.755099999962, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 74.90833531382165}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547293.7563093, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 12.394200000017008, "prefill_cuda_event_ms": null, "kv_decode_ms": 1708.755099999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1721.1492999999791, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 74.94991863866869}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547295.4780061, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.65579999997135, "prefill_cuda_event_ms": null, "kv_decode_ms": 1789.3617999998241, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.65579999997135, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 85.79419688073388}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547295.4780061, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 11.65579999997135, "prefill_cuda_event_ms": null, "kv_decode_ms": 1789.3617999998241, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1789.3617999998241, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 71.53388431563286}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547295.4780061, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 11.65579999997135, "prefill_cuda_event_ms": null, "kv_decode_ms": 1789.3617999998241, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1801.0175999997955, "cuda_event_ms": null, "tokens_total": 129, "tokens_per_s": 71.6261740029718}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547297.2795186, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.39979999977004, "prefill_cuda_event_ms": 18.25177574157715, "kv_decode_ms": 680.7218000003559, "kv_decode_cuda_event_ms": 680.5821533203125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 18.39979999977004, "cuda_event_ms": 18.25177574157715, "tokens_total": 1, "tokens_per_s": 54.34841683129697}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547297.2795186, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 18.39979999977004, "prefill_cuda_event_ms": 18.25177574157715, "kv_decode_ms": 680.7218000003559, "kv_decode_cuda_event_ms": 680.5821533203125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 680.7218000003559, "cuda_event_ms": 680.5821533203125, "tokens_total": 128, "tokens_per_s": 188.0356997527229}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547297.2795186, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 18.39979999977004, "prefill_cuda_event_ms": 18.25177574157715, "kv_decode_ms": 680.7218000003559, "kv_decode_cuda_event_ms": 680.5821533203125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 699.1216000001259, "cuda_event_ms": 698.8339290618896, "tokens_total": 129, "tokens_per_s": 184.517257083713}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547297.9805057, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.817199999706645, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 522.604599999795, "kv_decode_cuda_event_ms": 522.550048828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 4.817199999706645, "cuda_event_ms": 4.772863864898682, "tokens_total": 1, "tokens_per_s": 207.5894710746694}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547297.9805057, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.817199999706645, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 522.604599999795, "kv_decode_cuda_event_ms": 522.550048828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 522.604599999795, "cuda_event_ms": 522.550048828125, "tokens_total": 128, "tokens_per_s": 244.9270442702766}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547297.9805057, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.817199999706645, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 522.604599999795, "kv_decode_cuda_event_ms": 522.550048828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 527.4217999995017, "cuda_event_ms": 527.3229126930237, "tokens_total": 129, "tokens_per_s": 244.58602204179252}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547298.508564, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.945900000166148, "prefill_cuda_event_ms": 4.884479999542236, "kv_decode_ms": 519.4078999998055, "kv_decode_cuda_event_ms": 519.362548828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 4.945900000166148, "cuda_event_ms": 4.884479999542236, "tokens_total": 1, "tokens_per_s": 202.18767058905496}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547298.508564, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.945900000166148, "prefill_cuda_event_ms": 4.884479999542236, "kv_decode_ms": 519.4078999998055, "kv_decode_cuda_event_ms": 519.362548828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 519.4078999998055, "cuda_event_ms": 519.362548828125, "tokens_total": 128, "tokens_per_s": 246.43444968790024}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547298.508564, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.945900000166148, "prefill_cuda_event_ms": 4.884479999542236, "kv_decode_ms": 519.4078999998055, "kv_decode_cuda_event_ms": 519.362548828125, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 524.3537999999717, "cuda_event_ms": 524.2470288276672, "tokens_total": 129, "tokens_per_s": 246.01709761616482}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547299.0336568, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.906000000119093, "prefill_cuda_event_ms": 4.852735996246338, "kv_decode_ms": 513.740400000188, "kv_decode_cuda_event_ms": 513.7141723632812, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 4.906000000119093, "cuda_event_ms": 4.852735996246338, "tokens_total": 1, "tokens_per_s": 203.83204239211682}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547299.0336568, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.906000000119093, "prefill_cuda_event_ms": 4.852735996246338, "kv_decode_ms": 513.740400000188, "kv_decode_cuda_event_ms": 513.7141723632812, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 513.740400000188, "cuda_event_ms": 513.7141723632812, "tokens_total": 128, "tokens_per_s": 249.15307419847295}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547299.0336568, "prompt_tokens": 1, "gen_tokens": 128, "prefill_ms": 4.906000000119093, "prefill_cuda_event_ms": 4.852735996246338, "kv_decode_ms": 513.740400000188, "kv_decode_cuda_event_ms": 513.7141723632812, "gpu_peak_mb": 258.55517578125, "params_millions_measured": 96.08832, "latency_ms": 518.646400000307, "cuda_event_ms": 518.5669083595276, "tokens_total": 129, "tokens_per_s": 248.72437174908305}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547299.553069, "prompt_tokens": 99, "prefill_ms": 924.154, "prefill_cuda_event_ms": null, "kv_decode_ms": 6208.7436, "kv_decode_ms_equiv": 6208.7436, "kv_decode_ms_per_token": 48.505809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 18213.196400000015, "ollama_total_duration_ms": 18070.7428, "ollama_load_ms": 10816.1909, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 924.154, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 107.12500297569453}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547299.553069, "prompt_tokens": 99, "prefill_ms": 924.154, "prefill_cuda_event_ms": null, "kv_decode_ms": 6208.7436, "kv_decode_ms_equiv": 6208.7436, "kv_decode_ms_per_token": 48.505809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 18213.196400000015, "ollama_total_duration_ms": 18070.7428, "ollama_load_ms": 10816.1909, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6208.7436, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 20.616087286967367}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547299.553069, "prompt_tokens": 99, "prefill_ms": 924.154, "prefill_cuda_event_ms": null, "kv_decode_ms": 6208.7436, "kv_decode_ms_equiv": 6208.7436, "kv_decode_ms_per_token": 48.505809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 18213.196400000015, "ollama_total_duration_ms": 18070.7428, "ollama_load_ms": 10816.1909, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7132.8976, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 31.824373870164628}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547317.7664418, "prompt_tokens": 99, "prefill_ms": 60.5473, "prefill_cuda_event_ms": null, "kv_decode_ms": 7384.5906, "kv_decode_ms_equiv": 7384.5906, "kv_decode_ms_per_token": 57.6921140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7728.388400000313, "ollama_total_duration_ms": 7713.5277, "ollama_load_ms": 241.7128, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 60.5473, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1635.0852969496575}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547317.7664418, "prompt_tokens": 99, "prefill_ms": 60.5473, "prefill_cuda_event_ms": null, "kv_decode_ms": 7384.5906, "kv_decode_ms_equiv": 7384.5906, "kv_decode_ms_per_token": 57.6921140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7728.388400000313, "ollama_total_duration_ms": 7713.5277, "ollama_load_ms": 241.7128, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7384.5906, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.333391508528585}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547317.7664418, "prompt_tokens": 99, "prefill_ms": 60.5473, "prefill_cuda_event_ms": null, "kv_decode_ms": 7384.5906, "kv_decode_ms_equiv": 7384.5906, "kv_decode_ms_per_token": 57.6921140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7728.388400000313, "ollama_total_duration_ms": 7713.5277, "ollama_load_ms": 241.7128, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7445.137900000001, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 30.48969717538744}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547325.4949987, "prompt_tokens": 99, "prefill_ms": 65.4757, "prefill_cuda_event_ms": null, "kv_decode_ms": 7360.8494, "kv_decode_ms_equiv": 7360.8494, "kv_decode_ms_per_token": 57.5066359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7698.196100000132, "ollama_total_duration_ms": 7677.2482, "ollama_load_ms": 230.4352, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 65.4757, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1512.01132633939}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547325.4949987, "prompt_tokens": 99, "prefill_ms": 65.4757, "prefill_cuda_event_ms": null, "kv_decode_ms": 7360.8494, "kv_decode_ms_equiv": 7360.8494, "kv_decode_ms_per_token": 57.5066359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7698.196100000132, "ollama_total_duration_ms": 7677.2482, "ollama_load_ms": 230.4352, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7360.8494, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.3892974905858}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547325.4949987, "prompt_tokens": 99, "prefill_ms": 65.4757, "prefill_cuda_event_ms": null, "kv_decode_ms": 7360.8494, "kv_decode_ms_equiv": 7360.8494, "kv_decode_ms_per_token": 57.5066359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7698.196100000132, "ollama_total_duration_ms": 7677.2482, "ollama_load_ms": 230.4352, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7426.3251, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 30.566935455061078}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547333.1954777, "prompt_tokens": 99, "prefill_ms": 64.3765, "prefill_cuda_event_ms": null, "kv_decode_ms": 7361.205, "kv_decode_ms_equiv": 7361.205, "kv_decode_ms_per_token": 57.5094140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7687.156400000276, "ollama_total_duration_ms": 7683.2631, "ollama_load_ms": 240.1003, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 64.3765, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1537.8282447787626}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547333.1954777, "prompt_tokens": 99, "prefill_ms": 64.3765, "prefill_cuda_event_ms": null, "kv_decode_ms": 7361.205, "kv_decode_ms_equiv": 7361.205, "kv_decode_ms_per_token": 57.5094140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7687.156400000276, "ollama_total_duration_ms": 7683.2631, "ollama_load_ms": 240.1003, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7361.205, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.388457460429372}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547333.1954777, "prompt_tokens": 99, "prefill_ms": 64.3765, "prefill_cuda_event_ms": null, "kv_decode_ms": 7361.205, "kv_decode_ms_equiv": 7361.205, "kv_decode_ms_per_token": 57.5094140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7687.156400000276, "ollama_total_duration_ms": 7683.2631, "ollama_load_ms": 240.1003, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7425.5815, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 30.569996437316053}
