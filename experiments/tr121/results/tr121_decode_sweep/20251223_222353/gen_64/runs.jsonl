{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546923.3893511, "prompt_tokens": 46, "prefill_ms": 23.1247, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.1346, "kv_decode_ms_equiv": 721.1346, "kv_decode_ms_per_token": 11.267728125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 7006.792900000164, "ollama_total_duration_ms": 6983.866, "ollama_load_ms": 6172.2444, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 23.1247, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 1989.2149952215595}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546923.3893511, "prompt_tokens": 46, "prefill_ms": 23.1247, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.1346, "kv_decode_ms_equiv": 721.1346, "kv_decode_ms_per_token": 11.267728125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 7006.792900000164, "ollama_total_duration_ms": 6983.866, "ollama_load_ms": 6172.2444, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 721.1346, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.74903520091812}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546923.3893511, "prompt_tokens": 46, "prefill_ms": 23.1247, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.1346, "kv_decode_ms_equiv": 721.1346, "kv_decode_ms_per_token": 11.267728125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 7006.792900000164, "ollama_total_duration_ms": 6983.866, "ollama_load_ms": 6172.2444, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 744.2592999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.7979516010079}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546930.396277, "prompt_tokens": 46, "prefill_ms": 11.5027, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9596, "kv_decode_ms_equiv": 730.9596, "kv_decode_ms_per_token": 11.42124375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.1238999998895, "ollama_total_duration_ms": 927.3721, "ollama_load_ms": 126.2847, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5027, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3999.061090004955}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546930.396277, "prompt_tokens": 46, "prefill_ms": 11.5027, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9596, "kv_decode_ms_equiv": 730.9596, "kv_decode_ms_per_token": 11.42124375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.1238999998895, "ollama_total_duration_ms": 927.3721, "ollama_load_ms": 126.2847, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 730.9596, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.55613853351129}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546930.396277, "prompt_tokens": 46, "prefill_ms": 11.5027, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9596, "kv_decode_ms_equiv": 730.9596, "kv_decode_ms_per_token": 11.42124375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.1238999998895, "ollama_total_duration_ms": 927.3721, "ollama_load_ms": 126.2847, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 742.4623, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 148.15567066502905}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546931.3365345, "prompt_tokens": 46, "prefill_ms": 11.5592, "prefill_cuda_event_ms": null, "kv_decode_ms": 715.209, "kv_decode_ms_equiv": 715.209, "kv_decode_ms_per_token": 11.175140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 928.0542000001333, "ollama_total_duration_ms": 914.7449, "ollama_load_ms": 124.4469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5592, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3979.514153228597}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546931.3365345, "prompt_tokens": 46, "prefill_ms": 11.5592, "prefill_cuda_event_ms": null, "kv_decode_ms": 715.209, "kv_decode_ms_equiv": 715.209, "kv_decode_ms_per_token": 11.175140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 928.0542000001333, "ollama_total_duration_ms": 914.7449, "ollama_load_ms": 124.4469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 715.209, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.48433255174362}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546931.3365345, "prompt_tokens": 46, "prefill_ms": 11.5592, "prefill_cuda_event_ms": null, "kv_decode_ms": 715.209, "kv_decode_ms_equiv": 715.209, "kv_decode_ms_per_token": 11.175140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 928.0542000001333, "ollama_total_duration_ms": 914.7449, "ollama_load_ms": 124.4469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 726.7682, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 151.35499874650543}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546932.2650025, "prompt_tokens": 46, "prefill_ms": 11.7929, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.318, "kv_decode_ms_equiv": 717.318, "kv_decode_ms_per_token": 11.20809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.4561000001195, "ollama_total_duration_ms": 925.072, "ollama_load_ms": 133.4086, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.7929, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3900.6520872728506}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546932.2650025, "prompt_tokens": 46, "prefill_ms": 11.7929, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.318, "kv_decode_ms_equiv": 717.318, "kv_decode_ms_per_token": 11.20809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.4561000001195, "ollama_total_duration_ms": 925.072, "ollama_load_ms": 133.4086, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 717.318, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.22123800044052}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546932.2650025, "prompt_tokens": 46, "prefill_ms": 11.7929, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.318, "kv_decode_ms_equiv": 717.318, "kv_decode_ms_per_token": 11.20809375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 940.4561000001195, "ollama_total_duration_ms": 925.072, "ollama_load_ms": 133.4086, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 729.1109, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 150.86868129388822}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546933.2055624, "prompt_tokens": 30, "prefill_ms": 68.9387, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.9189, "kv_decode_ms_equiv": 706.0279172413793, "kv_decode_ms_per_token": 11.031686206896552, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 550.5094000000099, "ollama_total_duration_ms": 536.2145, "ollama_load_ms": 119.941, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 68.9387, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 435.16921554946646}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546933.2055624, "prompt_tokens": 30, "prefill_ms": 68.9387, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.9189, "kv_decode_ms_equiv": 706.0279172413793, "kv_decode_ms_per_token": 11.031686206896552, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 550.5094000000099, "ollama_total_duration_ms": 536.2145, "ollama_load_ms": 119.941, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 706.0279172413793, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.64797359580818}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546933.2055624, "prompt_tokens": 30, "prefill_ms": 68.9387, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.9189, "kv_decode_ms_equiv": 706.0279172413793, "kv_decode_ms_per_token": 11.031686206896552, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 550.5094000000099, "ollama_total_duration_ms": 536.2145, "ollama_load_ms": 119.941, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 774.9666172413794, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 121.29554732900418}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546933.7561905, "prompt_tokens": 30, "prefill_ms": 12.258, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.5546, "kv_decode_ms_equiv": 714.0515310344828, "kv_decode_ms_per_token": 11.157055172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.2620000000643, "ollama_total_duration_ms": 505.6913, "ollama_load_ms": 141.8917, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.258, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2447.381302006853}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546933.7561905, "prompt_tokens": 30, "prefill_ms": 12.258, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.5546, "kv_decode_ms_equiv": 714.0515310344828, "kv_decode_ms_per_token": 11.157055172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.2620000000643, "ollama_total_duration_ms": 505.6913, "ollama_load_ms": 141.8917, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 714.0515310344828, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.62938558128984}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546933.7561905, "prompt_tokens": 30, "prefill_ms": 12.258, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.5546, "kv_decode_ms_equiv": 714.0515310344828, "kv_decode_ms_per_token": 11.157055172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.2620000000643, "ollama_total_duration_ms": 505.6913, "ollama_load_ms": 141.8917, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 726.3095310344828, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 129.42140503941312}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546934.2837603, "prompt_tokens": 30, "prefill_ms": 12.5294, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.3413, "kv_decode_ms_equiv": 715.7876965517241, "kv_decode_ms_per_token": 11.18418275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.7425999997067, "ollama_total_duration_ms": 491.7204, "ollama_load_ms": 127.8292, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.5294, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2394.3684454163804}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546934.2837603, "prompt_tokens": 30, "prefill_ms": 12.5294, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.3413, "kv_decode_ms_equiv": 715.7876965517241, "kv_decode_ms_per_token": 11.18418275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.7425999997067, "ollama_total_duration_ms": 491.7204, "ollama_load_ms": 127.8292, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 715.7876965517241, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.4119866942631}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546934.2837603, "prompt_tokens": 30, "prefill_ms": 12.5294, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.3413, "kv_decode_ms_equiv": 715.7876965517241, "kv_decode_ms_per_token": 11.18418275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.7425999997067, "ollama_total_duration_ms": 491.7204, "ollama_load_ms": 127.8292, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 728.3170965517241, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 129.06466214379773}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546934.7958233, "prompt_tokens": 30, "prefill_ms": 12.2308, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.2419, "kv_decode_ms_equiv": 708.9476413793103, "kv_decode_ms_per_token": 11.077306896551724, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.5962999997282, "ollama_total_duration_ms": 503.6235, "ollama_load_ms": 137.6578, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.2308, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2452.8240180527846}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546934.7958233, "prompt_tokens": 30, "prefill_ms": 12.2308, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.2419, "kv_decode_ms_equiv": 708.9476413793103, "kv_decode_ms_per_token": 11.077306896551724, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.5962999997282, "ollama_total_duration_ms": 503.6235, "ollama_load_ms": 137.6578, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 708.9476413793103, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.27464972657677}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546934.7958233, "prompt_tokens": 30, "prefill_ms": 12.2308, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.2419, "kv_decode_ms_equiv": 708.9476413793103, "kv_decode_ms_per_token": 11.077306896551724, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.5962999997282, "ollama_total_duration_ms": 503.6235, "ollama_load_ms": 137.6578, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 721.1784413793104, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.34222129576924}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546935.302521, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 45.217999999749736, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.3641000000498, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8398.287799999707, "params_millions_measured": 25.016064, "latency_ms": 45.217999999749736, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 375.9564775110374}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546935.302521, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 45.217999999749736, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.3641000000498, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8398.287799999707, "params_millions_measured": 25.016064, "latency_ms": 972.3641000000498, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.81896637277818}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546935.302521, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 45.217999999749736, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.3641000000498, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8398.287799999707, "params_millions_measured": 25.016064, "latency_ms": 1017.5820999997995, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 79.60045680836559}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546944.7325342, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 24.004100000183826, "prefill_cuda_event_ms": null, "kv_decode_ms": 1016.6567999999643, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 24.004100000183826, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 708.2123470519541}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546944.7325342, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.004100000183826, "prefill_cuda_event_ms": null, "kv_decode_ms": 1016.6567999999643, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1016.6567999999643, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.95143061060748}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546944.7325342, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.004100000183826, "prefill_cuda_event_ms": null, "kv_decode_ms": 1016.6567999999643, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1040.660900000148, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 77.83515264193021}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546945.7738848, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.40960000017367, "prefill_cuda_event_ms": null, "kv_decode_ms": 936.6101999999046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.40960000017367, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 758.6034556559803}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546945.7738848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.40960000017367, "prefill_cuda_event_ms": null, "kv_decode_ms": 936.6101999999046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 936.6101999999046, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.33152148034104}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546945.7738848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.40960000017367, "prefill_cuda_event_ms": null, "kv_decode_ms": 936.6101999999046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 959.0198000000782, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 84.46123844366237}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546946.7334049, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 16.572399999859044, "prefill_cuda_event_ms": null, "kv_decode_ms": 823.4916999999768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.572399999859044, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1025.801935757319}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546946.7334049, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.572399999859044, "prefill_cuda_event_ms": null, "kv_decode_ms": 823.4916999999768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 823.4916999999768, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.71784463644478}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546946.7334049, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.572399999859044, "prefill_cuda_event_ms": null, "kv_decode_ms": 823.4916999999768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 840.0640999998359, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 96.42121357169748}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546947.5739212, "prompt_tokens": 10, "prefill_ms": 8.1152, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.9633, "kv_decode_ms_equiv": 139.42283636363638, "kv_decode_ms_per_token": 2.1784818181818184, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1547.813799999858, "ollama_total_duration_ms": 1480.2835, "ollama_load_ms": 1410.2046, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.1152, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1232.255520504732}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546947.5739212, "prompt_tokens": 10, "prefill_ms": 8.1152, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.9633, "kv_decode_ms_equiv": 139.42283636363638, "kv_decode_ms_per_token": 2.1784818181818184, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1547.813799999858, "ollama_total_duration_ms": 1480.2835, "ollama_load_ms": 1410.2046, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.42283636363638, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 459.03527477434244}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546947.5739212, "prompt_tokens": 10, "prefill_ms": 8.1152, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.9633, "kv_decode_ms_equiv": 139.42283636363638, "kv_decode_ms_per_token": 2.1784818181818184, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1547.813799999858, "ollama_total_duration_ms": 1480.2835, "ollama_load_ms": 1410.2046, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 147.53803636363637, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 501.5655747078843}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546949.1218958, "prompt_tokens": 10, "prefill_ms": 3.3052, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2316, "kv_decode_ms_equiv": 123.5293090909091, "kv_decode_ms_per_token": 1.9301454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 226.32949999979246, "ollama_total_duration_ms": 193.4433, "ollama_load_ms": 158.1856, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.3052, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3025.535519787002}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546949.1218958, "prompt_tokens": 10, "prefill_ms": 3.3052, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2316, "kv_decode_ms_equiv": 123.5293090909091, "kv_decode_ms_per_token": 1.9301454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 226.32949999979246, "ollama_total_duration_ms": 193.4433, "ollama_load_ms": 158.1856, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.5293090909091, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 518.0956687202095}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546949.1218958, "prompt_tokens": 10, "prefill_ms": 3.3052, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2316, "kv_decode_ms_equiv": 123.5293090909091, "kv_decode_ms_per_token": 1.9301454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 226.32949999979246, "ollama_total_duration_ms": 193.4433, "ollama_load_ms": 158.1856, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.8345090909091, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 583.4374298477414}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546949.3483841, "prompt_tokens": 10, "prefill_ms": 3.1284, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2376, "kv_decode_ms_equiv": 135.20058181818183, "kv_decode_ms_per_token": 2.112509090909091, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.974800000291, "ollama_total_duration_ms": 191.7519, "ollama_load_ms": 153.7189, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.1284, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3196.522183863956}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546949.3483841, "prompt_tokens": 10, "prefill_ms": 3.1284, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2376, "kv_decode_ms_equiv": 135.20058181818183, "kv_decode_ms_per_token": 2.112509090909091, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.974800000291, "ollama_total_duration_ms": 191.7519, "ollama_load_ms": 153.7189, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.20058181818183, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 473.37074396667464}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546949.3483841, "prompt_tokens": 10, "prefill_ms": 3.1284, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2376, "kv_decode_ms_equiv": 135.20058181818183, "kv_decode_ms_per_token": 2.112509090909091, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.974800000291, "ollama_total_duration_ms": 191.7519, "ollama_load_ms": 153.7189, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 138.32898181818183, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 534.9565870242928}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546949.5485773, "prompt_tokens": 10, "prefill_ms": 2.6471, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7151, "kv_decode_ms_equiv": 126.3424, "kv_decode_ms_per_token": 1.9741, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 225.81450000006953, "ollama_total_duration_ms": 193.7363, "ollama_load_ms": 158.9596, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6471, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3777.719013259794}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546949.5485773, "prompt_tokens": 10, "prefill_ms": 2.6471, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7151, "kv_decode_ms_equiv": 126.3424, "kv_decode_ms_per_token": 1.9741, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 225.81450000006953, "ollama_total_duration_ms": 193.7363, "ollama_load_ms": 158.9596, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.3424, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 506.5599513702447}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546949.5485773, "prompt_tokens": 10, "prefill_ms": 2.6471, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7151, "kv_decode_ms_equiv": 126.3424, "kv_decode_ms_per_token": 1.9741, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 225.81450000006953, "ollama_total_duration_ms": 193.7363, "ollama_load_ms": 158.9596, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 128.9895, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 573.6901065590611}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546949.7745643, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 189.45790000043417, "prefill_cuda_event_ms": null, "kv_decode_ms": 1853.3105999999862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 207.91630000030636, "params_millions_measured": 96.08832, "latency_ms": 189.45790000043417, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 47.50395734344873}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546949.7745643, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 189.45790000043417, "prefill_cuda_event_ms": null, "kv_decode_ms": 1853.3105999999862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 207.91630000030636, "params_millions_measured": 96.08832, "latency_ms": 1853.3105999999862, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 34.532797686475476}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546949.7745643, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 189.45790000043417, "prefill_cuda_event_ms": null, "kv_decode_ms": 1853.3105999999862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 207.91630000030636, "params_millions_measured": 96.08832, "latency_ms": 2042.7685000004203, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 35.73581636880781}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546952.026159, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 40.0139999997009, "prefill_cuda_event_ms": null, "kv_decode_ms": 1958.9756000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 40.0139999997009, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 224.92127755453777}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546952.026159, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 40.0139999997009, "prefill_cuda_event_ms": null, "kv_decode_ms": 1958.9756000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1958.9756000000307, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.67013637127435}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546952.026159, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 40.0139999997009, "prefill_cuda_event_ms": null, "kv_decode_ms": 1958.9756000000307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1998.9895999997316, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 36.51844912050058}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546954.0256782, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 32.98919999997452, "prefill_cuda_event_ms": null, "kv_decode_ms": 1886.4246000002822, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 32.98919999997452, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 272.81655814651316}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546954.0256782, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 32.98919999997452, "prefill_cuda_event_ms": null, "kv_decode_ms": 1886.4246000002822, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1886.4246000002822, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.926614400591696}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546954.0256782, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 32.98919999997452, "prefill_cuda_event_ms": null, "kv_decode_ms": 1886.4246000002822, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1919.4138000002567, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 38.03244511422719}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546955.9458039, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.80410000020129, "prefill_cuda_event_ms": null, "kv_decode_ms": 2122.002600000087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.80410000020129, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 258.59022356411884}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546955.9458039, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.80410000020129, "prefill_cuda_event_ms": null, "kv_decode_ms": 2122.002600000087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2122.002600000087, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.160189247646244}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546955.9458039, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.80410000020129, "prefill_cuda_event_ms": null, "kv_decode_ms": 2122.002600000087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2156.8067000002884, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 33.846334027055015}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546958.1032305, "prompt_tokens": 92, "prefill_ms": 832.6303, "prefill_cuda_event_ms": null, "kv_decode_ms": 2939.048, "kv_decode_ms_equiv": 2939.048, "kv_decode_ms_per_token": 45.922625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 13999.761600000056, "ollama_total_duration_ms": 13867.0797, "ollama_load_ms": 10004.0023, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 832.6303, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 110.49321649716566}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546958.1032305, "prompt_tokens": 92, "prefill_ms": 832.6303, "prefill_cuda_event_ms": null, "kv_decode_ms": 2939.048, "kv_decode_ms_equiv": 2939.048, "kv_decode_ms_per_token": 45.922625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13999.761600000056, "ollama_total_duration_ms": 13867.0797, "ollama_load_ms": 10004.0023, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2939.048, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.775758681042298}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546958.1032305, "prompt_tokens": 92, "prefill_ms": 832.6303, "prefill_cuda_event_ms": null, "kv_decode_ms": 2939.048, "kv_decode_ms_equiv": 2939.048, "kv_decode_ms_per_token": 45.922625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13999.761600000056, "ollama_total_duration_ms": 13867.0797, "ollama_load_ms": 10004.0023, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3771.6782999999996, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 41.36089761420003}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546972.1032376, "prompt_tokens": 92, "prefill_ms": 56.1501, "prefill_cuda_event_ms": null, "kv_decode_ms": 3205.0349, "kv_decode_ms_equiv": 3205.0349, "kv_decode_ms_per_token": 50.0786703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3547.306899999967, "ollama_total_duration_ms": 3530.6271, "ollama_load_ms": 242.4978, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.1501, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1638.4654702306852}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546972.1032376, "prompt_tokens": 92, "prefill_ms": 56.1501, "prefill_cuda_event_ms": null, "kv_decode_ms": 3205.0349, "kv_decode_ms_equiv": 3205.0349, "kv_decode_ms_per_token": 50.0786703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3547.306899999967, "ollama_total_duration_ms": 3530.6271, "ollama_load_ms": 242.4978, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3205.0349, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.96858130936421}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546972.1032376, "prompt_tokens": 92, "prefill_ms": 56.1501, "prefill_cuda_event_ms": null, "kv_decode_ms": 3205.0349, "kv_decode_ms_equiv": 3205.0349, "kv_decode_ms_per_token": 50.0786703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3547.306899999967, "ollama_total_duration_ms": 3530.6271, "ollama_load_ms": 242.4978, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3261.185, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 47.835372724945074}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546975.6530073, "prompt_tokens": 92, "prefill_ms": 64.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 3650.5804, "kv_decode_ms_equiv": 3650.5804, "kv_decode_ms_per_token": 57.04031875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4004.1284999997515, "ollama_total_duration_ms": 3979.8286, "ollama_load_ms": 243.3741, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 64.4481, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1427.5052328928239}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546975.6530073, "prompt_tokens": 92, "prefill_ms": 64.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 3650.5804, "kv_decode_ms_equiv": 3650.5804, "kv_decode_ms_per_token": 57.04031875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4004.1284999997515, "ollama_total_duration_ms": 3979.8286, "ollama_load_ms": 243.3741, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3650.5804, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.531458833231014}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546975.6530073, "prompt_tokens": 92, "prefill_ms": 64.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 3650.5804, "kv_decode_ms_equiv": 3650.5804, "kv_decode_ms_per_token": 57.04031875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4004.1284999997515, "ollama_total_duration_ms": 3979.8286, "ollama_load_ms": 243.3741, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3715.0285, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 41.99160248703341}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546979.657254, "prompt_tokens": 92, "prefill_ms": 64.4605, "prefill_cuda_event_ms": null, "kv_decode_ms": 3540.6077, "kv_decode_ms_equiv": 3540.6077, "kv_decode_ms_per_token": 55.3219953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3914.8462000002837, "ollama_total_duration_ms": 3895.125, "ollama_load_ms": 267.135, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 64.4605, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1427.2306296103818}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546979.657254, "prompt_tokens": 92, "prefill_ms": 64.4605, "prefill_cuda_event_ms": null, "kv_decode_ms": 3540.6077, "kv_decode_ms_equiv": 3540.6077, "kv_decode_ms_per_token": 55.3219953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3914.8462000002837, "ollama_total_duration_ms": 3895.125, "ollama_load_ms": 267.135, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3540.6077, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.075992999732787}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546979.657254, "prompt_tokens": 92, "prefill_ms": 64.4605, "prefill_cuda_event_ms": null, "kv_decode_ms": 3540.6077, "kv_decode_ms_equiv": 3540.6077, "kv_decode_ms_per_token": 55.3219953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3914.8462000002837, "ollama_total_duration_ms": 3895.125, "ollama_load_ms": 267.135, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3605.0682, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 43.272412987915175}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546983.5722725, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 963.700599999811, "prefill_cuda_event_ms": 786.5142211914062, "kv_decode_ms": 440.52389999978914, "kv_decode_cuda_event_ms": 440.4356994628906, "gpu_peak_mb": 62.00732421875, "hf_load_ms": 411.53940000003786, "params_millions_measured": 25.016064, "latency_ms": 963.700599999811, "cuda_event_ms": 786.5142211914062, "tokens_total": 17, "tokens_per_s": 17.640333522676375}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546983.5722725, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 963.700599999811, "prefill_cuda_event_ms": 786.5142211914062, "kv_decode_ms": 440.52389999978914, "kv_decode_cuda_event_ms": 440.4356994628906, "gpu_peak_mb": 62.00732421875, "hf_load_ms": 411.53940000003786, "params_millions_measured": 25.016064, "latency_ms": 440.52389999978914, "cuda_event_ms": 440.4356994628906, "tokens_total": 64, "tokens_per_s": 145.28156134100928}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546983.5722725, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 963.700599999811, "prefill_cuda_event_ms": 786.5142211914062, "kv_decode_ms": 440.52389999978914, "kv_decode_cuda_event_ms": 440.4356994628906, "gpu_peak_mb": 62.00732421875, "hf_load_ms": 411.53940000003786, "params_millions_measured": 25.016064, "latency_ms": 1404.2244999996, "cuda_event_ms": 1226.9499206542969, "tokens_total": 81, "tokens_per_s": 57.683084150734494}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546985.3925054, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.428500000358326, "prefill_cuda_event_ms": 4.334464073181152, "kv_decode_ms": 227.2784000001593, "kv_decode_cuda_event_ms": 227.2030792236328, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 4.428500000358326, "cuda_event_ms": 4.334464073181152, "tokens_total": 17, "tokens_per_s": 3838.771592779602}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546985.3925054, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.428500000358326, "prefill_cuda_event_ms": 4.334464073181152, "kv_decode_ms": 227.2784000001593, "kv_decode_cuda_event_ms": 227.2030792236328, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 227.2784000001593, "cuda_event_ms": 227.2030792236328, "tokens_total": 64, "tokens_per_s": 281.5929714392355}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546985.3925054, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.428500000358326, "prefill_cuda_event_ms": 4.334464073181152, "kv_decode_ms": 227.2784000001593, "kv_decode_cuda_event_ms": 227.2030792236328, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 231.70690000051763, "cuda_event_ms": 231.53754329681396, "tokens_total": 81, "tokens_per_s": 349.579576610878}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546985.62513, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.718900000170834, "prefill_cuda_event_ms": 4.628479957580566, "kv_decode_ms": 220.2222000000802, "kv_decode_cuda_event_ms": 220.16799926757812, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 4.718900000170834, "cuda_event_ms": 4.628479957580566, "tokens_total": 17, "tokens_per_s": 3602.534488839468}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546985.62513, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.718900000170834, "prefill_cuda_event_ms": 4.628479957580566, "kv_decode_ms": 220.2222000000802, "kv_decode_cuda_event_ms": 220.16799926757812, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 220.2222000000802, "cuda_event_ms": 220.16799926757812, "tokens_total": 64, "tokens_per_s": 290.615569184109}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546985.62513, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.718900000170834, "prefill_cuda_event_ms": 4.628479957580566, "kv_decode_ms": 220.2222000000802, "kv_decode_cuda_event_ms": 220.16799926757812, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 224.94110000025103, "cuda_event_ms": 224.7964792251587, "tokens_total": 81, "tokens_per_s": 360.09426467599565}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546985.850989, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 1.9672000003083667, "prefill_cuda_event_ms": 1.921023964881897, "kv_decode_ms": 204.58899999994173, "kv_decode_cuda_event_ms": 204.53683471679688, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 1.9672000003083667, "cuda_event_ms": 1.921023964881897, "tokens_total": 17, "tokens_per_s": 8641.724276807228}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546985.850989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1.9672000003083667, "prefill_cuda_event_ms": 1.921023964881897, "kv_decode_ms": 204.58899999994173, "kv_decode_cuda_event_ms": 204.53683471679688, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 204.58899999994173, "cuda_event_ms": 204.53683471679688, "tokens_total": 64, "tokens_per_s": 312.8222924987083}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546985.850989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1.9672000003083667, "prefill_cuda_event_ms": 1.921023964881897, "kv_decode_ms": 204.58899999994173, "kv_decode_cuda_event_ms": 204.53683471679688, "gpu_peak_mb": 62.00732421875, "params_millions_measured": 25.016064, "latency_ms": 206.5562000002501, "cuda_event_ms": 206.45785868167877, "tokens_total": 81, "tokens_per_s": 392.14509174695274}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546986.0582545, "prompt_tokens": 25, "prefill_ms": 7.2646, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.954, "kv_decode_ms_equiv": 137.954, "kv_decode_ms_per_token": 2.15553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2665.01849999986, "ollama_total_duration_ms": 2586.3172, "ollama_load_ms": 2370.5126, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.2646, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3441.3457038240235}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546986.0582545, "prompt_tokens": 25, "prefill_ms": 7.2646, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.954, "kv_decode_ms_equiv": 137.954, "kv_decode_ms_per_token": 2.15553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2665.01849999986, "ollama_total_duration_ms": 2586.3172, "ollama_load_ms": 2370.5126, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.954, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 463.9227568609826}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546986.0582545, "prompt_tokens": 25, "prefill_ms": 7.2646, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.954, "kv_decode_ms_equiv": 137.954, "kv_decode_ms_per_token": 2.15553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2665.01849999986, "ollama_total_duration_ms": 2586.3172, "ollama_load_ms": 2370.5126, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 145.2186, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 612.8691503705448}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546988.7234921, "prompt_tokens": 25, "prefill_ms": 2.7081, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.7639, "kv_decode_ms_equiv": 138.7639, "kv_decode_ms_per_token": 2.1681859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 380.2397999997993, "ollama_total_duration_ms": 333.9068, "ollama_load_ms": 159.6913, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.7081, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9231.564565562572}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546988.7234921, "prompt_tokens": 25, "prefill_ms": 2.7081, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.7639, "kv_decode_ms_equiv": 138.7639, "kv_decode_ms_per_token": 2.1681859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 380.2397999997993, "ollama_total_duration_ms": 333.9068, "ollama_load_ms": 159.6913, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 138.7639, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 461.2150566537839}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546988.7234921, "prompt_tokens": 25, "prefill_ms": 2.7081, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.7639, "kv_decode_ms_equiv": 138.7639, "kv_decode_ms_per_token": 2.1681859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 380.2397999997993, "ollama_total_duration_ms": 333.9068, "ollama_load_ms": 159.6913, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 141.472, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 629.099751187514}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546989.1038787, "prompt_tokens": 25, "prefill_ms": 2.6053, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0989, "kv_decode_ms_equiv": 128.0989, "kv_decode_ms_per_token": 2.0015453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.26949999989665, "ollama_total_duration_ms": 317.1533, "ollama_load_ms": 146.7375, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6053, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9595.823897439834}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546989.1038787, "prompt_tokens": 25, "prefill_ms": 2.6053, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0989, "kv_decode_ms_equiv": 128.0989, "kv_decode_ms_per_token": 2.0015453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.26949999989665, "ollama_total_duration_ms": 317.1533, "ollama_load_ms": 146.7375, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 128.0989, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 499.61397014338144}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546989.1038787, "prompt_tokens": 25, "prefill_ms": 2.6053, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0989, "kv_decode_ms_equiv": 128.0989, "kv_decode_ms_per_token": 2.0015453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.26949999989665, "ollama_total_duration_ms": 317.1533, "ollama_load_ms": 146.7375, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 130.7042, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 680.926856214261}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546989.45739, "prompt_tokens": 25, "prefill_ms": 3.2393, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.9471, "kv_decode_ms_equiv": 136.9471, "kv_decode_ms_per_token": 2.1397984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 356.63179999983186, "ollama_total_duration_ms": 316.5955, "ollama_load_ms": 151.5612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.2393, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 7717.71679066465}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546989.45739, "prompt_tokens": 25, "prefill_ms": 3.2393, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.9471, "kv_decode_ms_equiv": 136.9471, "kv_decode_ms_per_token": 2.1397984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 356.63179999983186, "ollama_total_duration_ms": 316.5955, "ollama_load_ms": 151.5612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.9471, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 467.33373689548733}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546989.45739, "prompt_tokens": 25, "prefill_ms": 3.2393, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.9471, "kv_decode_ms_equiv": 136.9471, "kv_decode_ms_per_token": 2.1397984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 356.63179999983186, "ollama_total_duration_ms": 316.5955, "ollama_load_ms": 151.5612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 140.1864, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 634.8690029845977}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546989.8141496, "prompt_tokens": 84, "prefill_ms": 947.8216, "prefill_cuda_event_ms": null, "kv_decode_ms": 2037.3451, "kv_decode_ms_equiv": 3180.2460097560975, "kv_decode_ms_per_token": 49.69134390243902, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 13082.895999999892, "ollama_total_duration_ms": 12936.4172, "ollama_load_ms": 9860.7658, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 947.8216, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 88.62427275343798}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546989.8141496, "prompt_tokens": 84, "prefill_ms": 947.8216, "prefill_cuda_event_ms": null, "kv_decode_ms": 2037.3451, "kv_decode_ms_equiv": 3180.2460097560975, "kv_decode_ms_per_token": 49.69134390243902, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13082.895999999892, "ollama_total_duration_ms": 12936.4172, "ollama_load_ms": 9860.7658, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3180.2460097560975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.12422932177764}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546989.8141496, "prompt_tokens": 84, "prefill_ms": 947.8216, "prefill_cuda_event_ms": null, "kv_decode_ms": 2037.3451, "kv_decode_ms_equiv": 3180.2460097560975, "kv_decode_ms_per_token": 49.69134390243902, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13082.895999999892, "ollama_total_duration_ms": 12936.4172, "ollama_load_ms": 9860.7658, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 4128.067609756097, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 35.85212598025846}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547002.8971775, "prompt_tokens": 84, "prefill_ms": 59.9063, "prefill_cuda_event_ms": null, "kv_decode_ms": 2140.482, "kv_decode_ms_equiv": 3341.2401951219513, "kv_decode_ms_per_token": 52.20687804878049, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2489.089199999853, "ollama_total_duration_ms": 2486.6971, "ollama_load_ms": 271.19, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 59.9063, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1402.189752997598}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547002.8971775, "prompt_tokens": 84, "prefill_ms": 59.9063, "prefill_cuda_event_ms": null, "kv_decode_ms": 2140.482, "kv_decode_ms_equiv": 3341.2401951219513, "kv_decode_ms_per_token": 52.20687804878049, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2489.089199999853, "ollama_total_duration_ms": 2486.6971, "ollama_load_ms": 271.19, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3341.2401951219513, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.15456425235064}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547002.8971775, "prompt_tokens": 84, "prefill_ms": 59.9063, "prefill_cuda_event_ms": null, "kv_decode_ms": 2140.482, "kv_decode_ms_equiv": 3341.2401951219513, "kv_decode_ms_per_token": 52.20687804878049, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2489.089199999853, "ollama_total_duration_ms": 2486.6971, "ollama_load_ms": 271.19, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3401.1464951219514, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 43.51473840137936}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547005.387067, "prompt_tokens": 84, "prefill_ms": 62.8829, "prefill_cuda_event_ms": null, "kv_decode_ms": 2313.3073, "kv_decode_ms_equiv": 3611.0162731707314, "kv_decode_ms_per_token": 56.42212926829268, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2639.5637999999053, "ollama_total_duration_ms": 2635.7671, "ollama_load_ms": 243.7929, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 62.8829, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1335.8162552935694}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547005.387067, "prompt_tokens": 84, "prefill_ms": 62.8829, "prefill_cuda_event_ms": null, "kv_decode_ms": 2313.3073, "kv_decode_ms_equiv": 3611.0162731707314, "kv_decode_ms_per_token": 56.42212926829268, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2639.5637999999053, "ollama_total_duration_ms": 2635.7671, "ollama_load_ms": 243.7929, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3611.0162731707314, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.723542393178807}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547005.387067, "prompt_tokens": 84, "prefill_ms": 62.8829, "prefill_cuda_event_ms": null, "kv_decode_ms": 2313.3073, "kv_decode_ms_equiv": 3611.0162731707314, "kv_decode_ms_per_token": 56.42212926829268, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2639.5637999999053, "ollama_total_duration_ms": 2635.7671, "ollama_load_ms": 243.7929, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3673.8991731707315, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 40.284175755501124}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547008.0267897, "prompt_tokens": 84, "prefill_ms": 70.3974, "prefill_cuda_event_ms": null, "kv_decode_ms": 2449.8616, "kv_decode_ms_equiv": 3824.174204878049, "kv_decode_ms_per_token": 59.75272195121951, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2783.0587000003106, "ollama_total_duration_ms": 2780.1438, "ollama_load_ms": 248.238, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 70.3974, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1193.2258861832966}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547008.0267897, "prompt_tokens": 84, "prefill_ms": 70.3974, "prefill_cuda_event_ms": null, "kv_decode_ms": 2449.8616, "kv_decode_ms_equiv": 3824.174204878049, "kv_decode_ms_per_token": 59.75272195121951, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2783.0587000003106, "ollama_total_duration_ms": 2780.1438, "ollama_load_ms": 248.238, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3824.174204878049, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.735639270397968}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547008.0267897, "prompt_tokens": 84, "prefill_ms": 70.3974, "prefill_cuda_event_ms": null, "kv_decode_ms": 2449.8616, "kv_decode_ms_equiv": 3824.174204878049, "kv_decode_ms_per_token": 59.75272195121951, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2783.0587000003106, "ollama_total_duration_ms": 2780.1438, "ollama_load_ms": 248.238, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3894.5716048780487, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 38.001612247833954}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547010.8100429, "prompt_tokens": 38, "prefill_ms": 24.0222, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.5664, "kv_decode_ms_equiv": 719.6642742857142, "kv_decode_ms_per_token": 11.244754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 6110.946899999817, "ollama_total_duration_ms": 6095.7388, "ollama_load_ms": 5637.2455, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 24.0222, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1581.8701034876074}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547010.8100429, "prompt_tokens": 38, "prefill_ms": 24.0222, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.5664, "kv_decode_ms_equiv": 719.6642742857142, "kv_decode_ms_per_token": 11.244754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6110.946899999817, "ollama_total_duration_ms": 6095.7388, "ollama_load_ms": 5637.2455, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 719.6642742857142, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.93035584338502}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547010.8100429, "prompt_tokens": 38, "prefill_ms": 24.0222, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.5664, "kv_decode_ms_equiv": 719.6642742857142, "kv_decode_ms_per_token": 11.244754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6110.946899999817, "ollama_total_duration_ms": 6095.7388, "ollama_load_ms": 5637.2455, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 743.6864742857142, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 137.1545718886007}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547016.9211886, "prompt_tokens": 38, "prefill_ms": 12.042, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.2324, "kv_decode_ms_equiv": 719.0535314285714, "kv_decode_ms_per_token": 11.235211428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 577.5871000000734, "ollama_total_duration_ms": 559.0681, "ollama_load_ms": 123.2924, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.042, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3155.621989702707}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547016.9211886, "prompt_tokens": 38, "prefill_ms": 12.042, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.2324, "kv_decode_ms_equiv": 719.0535314285714, "kv_decode_ms_per_token": 11.235211428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 577.5871000000734, "ollama_total_duration_ms": 559.0681, "ollama_load_ms": 123.2924, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 719.0535314285714, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.0058906641467}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547016.9211886, "prompt_tokens": 38, "prefill_ms": 12.042, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.2324, "kv_decode_ms_equiv": 719.0535314285714, "kv_decode_ms_per_token": 11.235211428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 577.5871000000734, "ollama_total_duration_ms": 559.0681, "ollama_load_ms": 123.2924, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 731.0955314285715, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 139.5166508550401}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547017.4988966, "prompt_tokens": 38, "prefill_ms": 12.1894, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.8026, "kv_decode_ms_equiv": 709.1247542857143, "kv_decode_ms_per_token": 11.080074285714286, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.6572999999371, "ollama_total_duration_ms": 566.2529, "ollama_load_ms": 125.7762, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.1894, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3117.462713505177}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547017.4988966, "prompt_tokens": 38, "prefill_ms": 12.1894, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.8026, "kv_decode_ms_equiv": 709.1247542857143, "kv_decode_ms_per_token": 11.080074285714286, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.6572999999371, "ollama_total_duration_ms": 566.2529, "ollama_load_ms": 125.7762, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 709.1247542857143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.25210248719323}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547017.4988966, "prompt_tokens": 38, "prefill_ms": 12.1894, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.8026, "kv_decode_ms_equiv": 709.1247542857143, "kv_decode_ms_per_token": 11.080074285714286, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.6572999999371, "ollama_total_duration_ms": 566.2529, "ollama_load_ms": 125.7762, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 721.3141542857143, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.4085657323696}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547018.0816743, "prompt_tokens": 38, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.2361, "kv_decode_ms_equiv": 704.4317257142858, "kv_decode_ms_per_token": 11.006745714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.4636000002101, "ollama_total_duration_ms": 561.5451, "ollama_load_ms": 124.0543, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.513, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3300.6166941718056}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547018.0816743, "prompt_tokens": 38, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.2361, "kv_decode_ms_equiv": 704.4317257142858, "kv_decode_ms_per_token": 11.006745714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.4636000002101, "ollama_total_duration_ms": 561.5451, "ollama_load_ms": 124.0543, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 704.4317257142858, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.85337537162275}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547018.0816743, "prompt_tokens": 38, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.2361, "kv_decode_ms_equiv": 704.4317257142858, "kv_decode_ms_per_token": 11.006745714285715, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 582.4636000002101, "ollama_total_duration_ms": 561.5451, "ollama_load_ms": 124.0543, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 715.9447257142858, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 142.4690989911775}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547018.66438, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 60.69290000004912, "prefill_cuda_event_ms": 60.6104621887207, "kv_decode_ms": 627.8434000000743, "kv_decode_cuda_event_ms": 627.7732543945312, "gpu_peak_mb": 259.333984375, "hf_load_ms": 388.34899999983463, "params_millions_measured": 96.08832, "latency_ms": 60.69290000004912, "cuda_event_ms": 60.6104621887207, "tokens_total": 17, "tokens_per_s": 280.0986606338837}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547018.66438, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.69290000004912, "prefill_cuda_event_ms": 60.6104621887207, "kv_decode_ms": 627.8434000000743, "kv_decode_cuda_event_ms": 627.7732543945312, "gpu_peak_mb": 259.333984375, "hf_load_ms": 388.34899999983463, "params_millions_measured": 96.08832, "latency_ms": 627.8434000000743, "cuda_event_ms": 627.7732543945312, "tokens_total": 64, "tokens_per_s": 101.93624715970961}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547018.66438, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.69290000004912, "prefill_cuda_event_ms": 60.6104621887207, "kv_decode_ms": 627.8434000000743, "kv_decode_cuda_event_ms": 627.7732543945312, "gpu_peak_mb": 259.333984375, "hf_load_ms": 388.34899999983463, "params_millions_measured": 96.08832, "latency_ms": 688.5363000001234, "cuda_event_ms": 688.383716583252, "tokens_total": 81, "tokens_per_s": 117.64085640798528}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547019.7425578, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.658600000169827, "prefill_cuda_event_ms": 10.466303825378418, "kv_decode_ms": 543.2837000003019, "kv_decode_cuda_event_ms": 543.1983032226562, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 10.658600000169827, "cuda_event_ms": 10.466303825378418, "tokens_total": 17, "tokens_per_s": 1594.9561855899587}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547019.7425578, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.658600000169827, "prefill_cuda_event_ms": 10.466303825378418, "kv_decode_ms": 543.2837000003019, "kv_decode_cuda_event_ms": 543.1983032226562, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 543.2837000003019, "cuda_event_ms": 543.1983032226562, "tokens_total": 64, "tokens_per_s": 117.80217223517738}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547019.7425578, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.658600000169827, "prefill_cuda_event_ms": 10.466303825378418, "kv_decode_ms": 543.2837000003019, "kv_decode_cuda_event_ms": 543.1983032226562, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 553.9423000004717, "cuda_event_ms": 553.6646070480347, "tokens_total": 81, "tokens_per_s": 146.22461581274985}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547020.297154, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.705699999813078, "prefill_cuda_event_ms": 9.632672309875488, "kv_decode_ms": 626.6906000000745, "kv_decode_cuda_event_ms": 626.6355590820312, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 9.705699999813078, "cuda_event_ms": 9.632672309875488, "tokens_total": 17, "tokens_per_s": 1751.5480594215155}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547020.297154, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.705699999813078, "prefill_cuda_event_ms": 9.632672309875488, "kv_decode_ms": 626.6906000000745, "kv_decode_cuda_event_ms": 626.6355590820312, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 626.6906000000745, "cuda_event_ms": 626.6355590820312, "tokens_total": 64, "tokens_per_s": 102.12375931598845}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547020.297154, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.705699999813078, "prefill_cuda_event_ms": 9.632672309875488, "kv_decode_ms": 626.6906000000745, "kv_decode_cuda_event_ms": 626.6355590820312, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 636.3962999998876, "cuda_event_ms": 636.2682313919067, "tokens_total": 81, "tokens_per_s": 127.27918122719178}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547020.934179, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.681700000281126, "prefill_cuda_event_ms": 11.619327545166016, "kv_decode_ms": 646.6912999999295, "kv_decode_cuda_event_ms": 646.6427001953125, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 11.681700000281126, "cuda_event_ms": 11.619327545166016, "tokens_total": 17, "tokens_per_s": 1455.2676408049244}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547020.934179, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.681700000281126, "prefill_cuda_event_ms": 11.619327545166016, "kv_decode_ms": 646.6912999999295, "kv_decode_cuda_event_ms": 646.6427001953125, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 646.6912999999295, "cuda_event_ms": 646.6427001953125, "tokens_total": 64, "tokens_per_s": 98.96530230112417}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547020.934179, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.681700000281126, "prefill_cuda_event_ms": 11.619327545166016, "kv_decode_ms": 646.6912999999295, "kv_decode_cuda_event_ms": 646.6427001953125, "gpu_peak_mb": 259.333984375, "params_millions_measured": 96.08832, "latency_ms": 658.3730000002106, "cuda_event_ms": 658.2620277404785, "tokens_total": 81, "tokens_per_s": 123.03056170282512}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547021.5932167, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 74.96260000016264, "prefill_cuda_event_ms": 74.89740753173828, "kv_decode_ms": 669.1003000000819, "kv_decode_cuda_event_ms": 669.0785522460938, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 74.96260000016264, "cuda_event_ms": 74.89740753173828, "tokens_total": 9, "tokens_per_s": 120.0598698548406}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547021.5932167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 74.96260000016264, "prefill_cuda_event_ms": 74.89740753173828, "kv_decode_ms": 669.1003000000819, "kv_decode_cuda_event_ms": 669.0785522460938, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 669.1003000000819, "cuda_event_ms": 669.0785522460938, "tokens_total": 64, "tokens_per_s": 95.65083142242227}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547021.5932167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 74.96260000016264, "prefill_cuda_event_ms": 74.89740753173828, "kv_decode_ms": 669.1003000000819, "kv_decode_cuda_event_ms": 669.0785522460938, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 744.0629000002446, "cuda_event_ms": 743.975959777832, "tokens_total": 73, "tokens_per_s": 98.10998505633866}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547022.3379931, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.86409999991156, "prefill_cuda_event_ms": 11.794431686401367, "kv_decode_ms": 508.15160000001924, "kv_decode_cuda_event_ms": 508.1077880859375, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 11.86409999991156, "cuda_event_ms": 11.794431686401367, "tokens_total": 9, "tokens_per_s": 758.5910435740672}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547022.3379931, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.86409999991156, "prefill_cuda_event_ms": 11.794431686401367, "kv_decode_ms": 508.15160000001924, "kv_decode_cuda_event_ms": 508.1077880859375, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 508.15160000001924, "cuda_event_ms": 508.1077880859375, "tokens_total": 64, "tokens_per_s": 125.94666630981301}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547022.3379931, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.86409999991156, "prefill_cuda_event_ms": 11.794431686401367, "kv_decode_ms": 508.15160000001924, "kv_decode_cuda_event_ms": 508.1077880859375, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 520.0156999999308, "cuda_event_ms": 519.9022197723389, "tokens_total": 73, "tokens_per_s": 140.38037697709842}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547022.8585851, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.205799999908777, "prefill_cuda_event_ms": 4.163584232330322, "kv_decode_ms": 201.92130000032193, "kv_decode_cuda_event_ms": 201.88671875, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 4.205799999908777, "cuda_event_ms": 4.163584232330322, "tokens_total": 9, "tokens_per_s": 2139.9020400863587}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547022.8585851, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.205799999908777, "prefill_cuda_event_ms": 4.163584232330322, "kv_decode_ms": 201.92130000032193, "kv_decode_cuda_event_ms": 201.88671875, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 201.92130000032193, "cuda_event_ms": 201.88671875, "tokens_total": 64, "tokens_per_s": 316.955170157373}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547022.8585851, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.205799999908777, "prefill_cuda_event_ms": 4.163584232330322, "kv_decode_ms": 201.92130000032193, "kv_decode_cuda_event_ms": 201.88671875, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 206.1271000002307, "cuda_event_ms": 206.05030298233032, "tokens_total": 73, "tokens_per_s": 354.15042466477377}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.065399, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.710100000011153, "prefill_cuda_event_ms": 4.652031898498535, "kv_decode_ms": 216.84570000024905, "kv_decode_cuda_event_ms": 216.81459045410156, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 4.710100000011153, "cuda_event_ms": 4.652031898498535, "tokens_total": 9, "tokens_per_s": 1910.7874567373708}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.065399, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.710100000011153, "prefill_cuda_event_ms": 4.652031898498535, "kv_decode_ms": 216.84570000024905, "kv_decode_cuda_event_ms": 216.81459045410156, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 216.84570000024905, "cuda_event_ms": 216.81459045410156, "tokens_total": 64, "tokens_per_s": 295.1407383218874}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.065399, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.710100000011153, "prefill_cuda_event_ms": 4.652031898498535, "kv_decode_ms": 216.84570000024905, "kv_decode_cuda_event_ms": 216.81459045410156, "gpu_peak_mb": 258.00927734375, "params_millions_measured": 96.08832, "latency_ms": 221.5558000002602, "cuda_event_ms": 221.4666223526001, "tokens_total": 73, "tokens_per_s": 329.48810186830707}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.2876148, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.575400000154332, "prefill_cuda_event_ms": 8.487936019897461, "kv_decode_ms": 102.58070000008956, "kv_decode_cuda_event_ms": 102.5239028930664, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 8.575400000154332, "cuda_event_ms": 8.487936019897461, "tokens_total": 1, "tokens_per_s": 116.61263614315402}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.2876148, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.575400000154332, "prefill_cuda_event_ms": 8.487936019897461, "kv_decode_ms": 102.58070000008956, "kv_decode_cuda_event_ms": 102.5239028930664, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 102.58070000008956, "cuda_event_ms": 102.5239028930664, "tokens_total": 64, "tokens_per_s": 623.8990375377057}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.2876148, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.575400000154332, "prefill_cuda_event_ms": 8.487936019897461, "kv_decode_ms": 102.58070000008956, "kv_decode_cuda_event_ms": 102.5239028930664, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 111.1561000002439, "cuda_event_ms": 111.01183891296387, "tokens_total": 65, "tokens_per_s": 584.7632293671456}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.3992646, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.0349999999780266, "prefill_cuda_event_ms": 1.9936959743499756, "kv_decode_ms": 103.57810000004974, "kv_decode_cuda_event_ms": 103.54483032226562, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 2.0349999999780266, "cuda_event_ms": 1.9936959743499756, "tokens_total": 1, "tokens_per_s": 491.4004914057974}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.3992646, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0349999999780266, "prefill_cuda_event_ms": 1.9936959743499756, "kv_decode_ms": 103.57810000004974, "kv_decode_cuda_event_ms": 103.54483032226562, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 103.57810000004974, "cuda_event_ms": 103.54483032226562, "tokens_total": 64, "tokens_per_s": 617.8912337643698}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.3992646, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0349999999780266, "prefill_cuda_event_ms": 1.9936959743499756, "kv_decode_ms": 103.57810000004974, "kv_decode_cuda_event_ms": 103.54483032226562, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 105.61310000002777, "cuda_event_ms": 105.5385262966156, "tokens_total": 65, "tokens_per_s": 615.4539541021228}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.5054276, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.173699999730161, "prefill_cuda_event_ms": 2.1084160804748535, "kv_decode_ms": 100.00489999993079, "kv_decode_cuda_event_ms": 99.94035339355469, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 2.173699999730161, "cuda_event_ms": 2.1084160804748535, "tokens_total": 1, "tokens_per_s": 460.0450844753821}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.5054276, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.173699999730161, "prefill_cuda_event_ms": 2.1084160804748535, "kv_decode_ms": 100.00489999993079, "kv_decode_cuda_event_ms": 99.94035339355469, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 100.00489999993079, "cuda_event_ms": 99.94035339355469, "tokens_total": 64, "tokens_per_s": 639.9686415370077}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.5054276, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.173699999730161, "prefill_cuda_event_ms": 2.1084160804748535, "kv_decode_ms": 100.00489999993079, "kv_decode_cuda_event_ms": 99.94035339355469, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 102.17859999966095, "cuda_event_ms": 102.04876947402954, "tokens_total": 65, "tokens_per_s": 636.1410314901133}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.6081805, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.118999999765947, "prefill_cuda_event_ms": 2.0787200927734375, "kv_decode_ms": 99.87460000002102, "kv_decode_cuda_event_ms": 99.8348159790039, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 2.118999999765947, "cuda_event_ms": 2.0787200927734375, "tokens_total": 1, "tokens_per_s": 471.9207173716161}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.6081805, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.118999999765947, "prefill_cuda_event_ms": 2.0787200927734375, "kv_decode_ms": 99.87460000002102, "kv_decode_cuda_event_ms": 99.8348159790039, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 99.87460000002102, "cuda_event_ms": 99.8348159790039, "tokens_total": 64, "tokens_per_s": 640.8035676737281}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.6081805, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.118999999765947, "prefill_cuda_event_ms": 2.0787200927734375, "kv_decode_ms": 99.87460000002102, "kv_decode_cuda_event_ms": 99.8348159790039, "gpu_peak_mb": 255.8125, "params_millions_measured": 25.016064, "latency_ms": 101.99359999978697, "cuda_event_ms": 101.91353607177734, "tokens_total": 65, "tokens_per_s": 637.2948890924114}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547023.710746, "prompt_tokens": 17, "prefill_ms": 8.5213, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.7393, "kv_decode_ms_equiv": 156.95446486486486, "kv_decode_ms_per_token": 2.4524135135135134, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1608.4031000000323, "ollama_total_duration_ms": 1549.7056, "ollama_load_ms": 1397.6403, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.5213, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1995.0007627944092}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547023.710746, "prompt_tokens": 17, "prefill_ms": 8.5213, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.7393, "kv_decode_ms_equiv": 156.95446486486486, "kv_decode_ms_per_token": 2.4524135135135134, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1608.4031000000323, "ollama_total_duration_ms": 1549.7056, "ollama_load_ms": 1397.6403, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 156.95446486486486, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 407.76157629604813}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547023.710746, "prompt_tokens": 17, "prefill_ms": 8.5213, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.7393, "kv_decode_ms_equiv": 156.95446486486486, "kv_decode_ms_per_token": 2.4524135135135134, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1608.4031000000323, "ollama_total_duration_ms": 1549.7056, "ollama_load_ms": 1397.6403, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 165.47576486486486, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 489.49766188510046}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547025.3193989, "prompt_tokens": 17, "prefill_ms": 2.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.8388, "kv_decode_ms_equiv": 142.45173333333335, "kv_decode_ms_per_token": 2.2258083333333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 337.2729000002437, "ollama_total_duration_ms": 302.0999, "ollama_load_ms": 159.0605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4683, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6887.331361665923}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547025.3193989, "prompt_tokens": 17, "prefill_ms": 2.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.8388, "kv_decode_ms_equiv": 142.45173333333335, "kv_decode_ms_per_token": 2.2258083333333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 337.2729000002437, "ollama_total_duration_ms": 302.0999, "ollama_load_ms": 159.0605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.45173333333335, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 449.2749824969954}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547025.3193989, "prompt_tokens": 17, "prefill_ms": 2.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.8388, "kv_decode_ms_equiv": 142.45173333333335, "kv_decode_ms_per_token": 2.2258083333333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 337.2729000002437, "ollama_total_duration_ms": 302.0999, "ollama_load_ms": 159.0605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 144.92003333333335, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 558.9289357510038}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547025.656814, "prompt_tokens": 17, "prefill_ms": 2.8173, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5638, "kv_decode_ms_equiv": 132.75173333333333, "kv_decode_ms_per_token": 2.0742458333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.69860000010885, "ollama_total_duration_ms": 284.4085, "ollama_load_ms": 145.3709, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.8173, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6034.146168317183}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547025.656814, "prompt_tokens": 17, "prefill_ms": 2.8173, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5638, "kv_decode_ms_equiv": 132.75173333333333, "kv_decode_ms_per_token": 2.0742458333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.69860000010885, "ollama_total_duration_ms": 284.4085, "ollama_load_ms": 145.3709, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 132.75173333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 482.1029329937186}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547025.656814, "prompt_tokens": 17, "prefill_ms": 2.8173, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.5638, "kv_decode_ms_equiv": 132.75173333333333, "kv_decode_ms_per_token": 2.0742458333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.69860000010885, "ollama_total_duration_ms": 284.4085, "ollama_load_ms": 145.3709, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.56903333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 597.4815782660298}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547025.973646, "prompt_tokens": 17, "prefill_ms": 3.0202, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.1572, "kv_decode_ms_equiv": 138.87626666666668, "kv_decode_ms_per_token": 2.169941666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 299.43109999976514, "ollama_total_duration_ms": 290.3378, "ollama_load_ms": 151.1228, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.0202, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5628.766306867095}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547025.973646, "prompt_tokens": 17, "prefill_ms": 3.0202, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.1572, "kv_decode_ms_equiv": 138.87626666666668, "kv_decode_ms_per_token": 2.169941666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 299.43109999976514, "ollama_total_duration_ms": 290.3378, "ollama_load_ms": 151.1228, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 138.87626666666668, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 460.8418813101734}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547025.973646, "prompt_tokens": 17, "prefill_ms": 3.0202, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.1572, "kv_decode_ms_equiv": 138.87626666666668, "kv_decode_ms_per_token": 2.169941666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 299.43109999976514, "ollama_total_duration_ms": 290.3378, "ollama_load_ms": 151.1228, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 141.89646666666667, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 570.8387382913457}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547026.273334, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 59.23669999992853, "prefill_cuda_event_ms": null, "kv_decode_ms": 3362.6773999999386, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 59.23669999992853, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 16.881426548089387}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547026.273334, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 59.23669999992853, "prefill_cuda_event_ms": null, "kv_decode_ms": 3362.6773999999386, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3362.6773999999386, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.032453127975096}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547026.273334, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 59.23669999992853, "prefill_cuda_event_ms": null, "kv_decode_ms": 3362.6773999999386, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 3421.914099999867, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 18.995216741414556}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547029.6958537, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 36.06919999992897, "prefill_cuda_event_ms": null, "kv_decode_ms": 2211.9260999998005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 36.06919999992897, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 27.724485156365244}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547029.6958537, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.06919999992897, "prefill_cuda_event_ms": null, "kv_decode_ms": 2211.9260999998005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2211.9260999998005, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.93405887294597}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547029.6958537, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.06919999992897, "prefill_cuda_event_ms": null, "kv_decode_ms": 2211.9260999998005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2247.9952999997295, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 28.914651200564265}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547031.9444954, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 40.719999999964784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2380.327899999884, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 40.719999999964784, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 24.55795677801731}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547031.9444954, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 40.719999999964784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2380.327899999884, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2380.327899999884, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.887051989771294}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547031.9444954, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 40.719999999964784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2380.327899999884, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2421.0478999998486, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.847878557051295}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547034.3660738, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 35.417600000073435, "prefill_cuda_event_ms": null, "kv_decode_ms": 2108.4219999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 35.417600000073435, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.234550054151796}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547034.3660738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 35.417600000073435, "prefill_cuda_event_ms": null, "kv_decode_ms": 2108.4219999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2108.4219999997913, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.354454658510647}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547034.3660738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 35.417600000073435, "prefill_cuda_event_ms": null, "kv_decode_ms": 2108.4219999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2143.8395999998647, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 30.319432479931848}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547036.5104942, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 54.72379999991972, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.011499999957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 54.72379999991972, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 310.6509416382806}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547036.5104942, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 54.72379999991972, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.011499999957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2253.011499999957, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.4064240240235}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547036.5104942, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 54.72379999991972, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.011499999957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2307.7352999998766, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 35.09934609918405}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547038.819129, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 56.89230000007228, "prefill_cuda_event_ms": null, "kv_decode_ms": 2518.4385999996266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 56.89230000007228, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 298.8102080594105}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547038.819129, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 56.89230000007228, "prefill_cuda_event_ms": null, "kv_decode_ms": 2518.4385999996266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2518.4385999996266, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.412571106561618}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547038.819129, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 56.89230000007228, "prefill_cuda_event_ms": null, "kv_decode_ms": 2518.4385999996266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2575.330899999699, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 31.452268910379427}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547041.395114, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 58.04249999982858, "prefill_cuda_event_ms": null, "kv_decode_ms": 2164.735700000165, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 58.04249999982858, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 292.8888314605713}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547041.395114, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 58.04249999982858, "prefill_cuda_event_ms": null, "kv_decode_ms": 2164.735700000165, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2164.735700000165, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.564810152110084}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547041.395114, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 58.04249999982858, "prefill_cuda_event_ms": null, "kv_decode_ms": 2164.735700000165, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2222.7781999999934, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 36.44088285551849}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547043.6184235, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 41.56360000024506, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.683300000066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 41.56360000024506, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 409.0117314164261}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547043.6184235, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.56360000024506, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.683300000066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2468.683300000066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.924751060615304}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547043.6184235, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.56360000024506, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.683300000066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2510.246900000311, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 32.26774226869475}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547046.12908, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.789800000173273, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.0210000000552, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.789800000173273, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 505.90787979136024}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547046.12908, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.789800000173273, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.0210000000552, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 856.0210000000552, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.76452096385003}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547046.12908, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.789800000173273, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.0210000000552, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 873.8108000002285, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.5421123199449}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547047.003752, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.450000000186265, "prefill_cuda_event_ms": null, "kv_decode_ms": 862.3035999999047, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.450000000186265, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 419.5804195767761}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547047.003752, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.450000000186265, "prefill_cuda_event_ms": null, "kv_decode_ms": 862.3035999999047, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 862.3035999999047, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.21979915195422}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547047.003752, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.450000000186265, "prefill_cuda_event_ms": null, "kv_decode_ms": 862.3035999999047, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 883.753600000091, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 82.60220948462613}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547047.8880146, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.153799999912735, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.235000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.153799999912735, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 469.88065031696084}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547047.8880146, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.153799999912735, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.235000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 882.235000000037, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.54302991832938}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547047.8880146, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.153799999912735, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.235000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 901.3887999999497, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 80.98614049786737}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547048.7901561, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.291400000132853, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.3005999995657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 20.291400000132853, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 443.5376563441199}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547048.7901561, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.291400000132853, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.3005999995657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 843.3005999995657, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.89227376339227}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547048.7901561, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.291400000132853, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.3005999995657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 863.5919999996986, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 84.53065799593497}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547049.654242, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.521399999655841, "prefill_cuda_event_ms": 5.951488018035889, "kv_decode_ms": 182.9222999999729, "kv_decode_cuda_event_ms": 182.7993621826172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 6.521399999655841, "cuda_event_ms": 5.951488018035889, "tokens_total": 9, "tokens_per_s": 1380.0717638045455}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547049.654242, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.521399999655841, "prefill_cuda_event_ms": 5.951488018035889, "kv_decode_ms": 182.9222999999729, "kv_decode_cuda_event_ms": 182.7993621826172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 182.9222999999729, "cuda_event_ms": 182.7993621826172, "tokens_total": 64, "tokens_per_s": 349.8753295798789}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547049.654242, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.521399999655841, "prefill_cuda_event_ms": 5.951488018035889, "kv_decode_ms": 182.9222999999729, "kv_decode_cuda_event_ms": 182.7993621826172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 189.44369999962873, "cuda_event_ms": 188.75085020065308, "tokens_total": 73, "tokens_per_s": 385.3387576369289}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547049.8458302, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.5311000001456705, "prefill_cuda_event_ms": 3.468287944793701, "kv_decode_ms": 114.87690000012662, "kv_decode_cuda_event_ms": 114.81938934326172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 3.5311000001456705, "cuda_event_ms": 3.468287944793701, "tokens_total": 9, "tokens_per_s": 2548.780833062988}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547049.8458302, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5311000001456705, "prefill_cuda_event_ms": 3.468287944793701, "kv_decode_ms": 114.87690000012662, "kv_decode_cuda_event_ms": 114.81938934326172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 114.87690000012662, "cuda_event_ms": 114.81938934326172, "tokens_total": 64, "tokens_per_s": 557.1180977196412}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547049.8458302, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5311000001456705, "prefill_cuda_event_ms": 3.468287944793701, "kv_decode_ms": 114.87690000012662, "kv_decode_cuda_event_ms": 114.81938934326172, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 118.4080000002723, "cuda_event_ms": 118.28767728805542, "tokens_total": 73, "tokens_per_s": 616.512397809541}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547049.964968, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.318700000159879, "prefill_cuda_event_ms": 2.271199941635132, "kv_decode_ms": 110.872499999914, "kv_decode_cuda_event_ms": 110.80089569091797, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 2.318700000159879, "cuda_event_ms": 2.271199941635132, "tokens_total": 9, "tokens_per_s": 3881.4853147795884}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547049.964968, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.318700000159879, "prefill_cuda_event_ms": 2.271199941635132, "kv_decode_ms": 110.872499999914, "kv_decode_cuda_event_ms": 110.80089569091797, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 110.872499999914, "cuda_event_ms": 110.80089569091797, "tokens_total": 64, "tokens_per_s": 577.2396220888827}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547049.964968, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.318700000159879, "prefill_cuda_event_ms": 2.271199941635132, "kv_decode_ms": 110.872499999914, "kv_decode_cuda_event_ms": 110.80089569091797, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 113.19120000007388, "cuda_event_ms": 113.0720956325531, "tokens_total": 73, "tokens_per_s": 644.9264607138396}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547050.0787954, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.412499999991269, "prefill_cuda_event_ms": 2.3421759605407715, "kv_decode_ms": 112.74779999985185, "kv_decode_cuda_event_ms": 112.70051574707031, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 2.412499999991269, "cuda_event_ms": 2.3421759605407715, "tokens_total": 9, "tokens_per_s": 3730.56994820003}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547050.0787954, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.412499999991269, "prefill_cuda_event_ms": 2.3421759605407715, "kv_decode_ms": 112.74779999985185, "kv_decode_cuda_event_ms": 112.70051574707031, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 112.74779999985185, "cuda_event_ms": 112.70051574707031, "tokens_total": 64, "tokens_per_s": 567.6385703320517}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547050.0787954, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.412499999991269, "prefill_cuda_event_ms": 2.3421759605407715, "kv_decode_ms": 112.74779999985185, "kv_decode_cuda_event_ms": 112.70051574707031, "gpu_peak_mb": 256.6142578125, "params_millions_measured": 25.016064, "latency_ms": 115.16029999984312, "cuda_event_ms": 115.04269170761108, "tokens_total": 73, "tokens_per_s": 633.8990086001812}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547050.1946013, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.710100000072998, "prefill_cuda_event_ms": null, "kv_decode_ms": 839.5715999999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.710100000072998, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 85.39636723800533}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547050.1946013, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.710100000072998, "prefill_cuda_event_ms": null, "kv_decode_ms": 839.5715999999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 839.5715999999993, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.22935316058815}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547050.1946013, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.710100000072998, "prefill_cuda_event_ms": null, "kv_decode_ms": 839.5715999999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 851.2817000000723, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 76.35545319486427}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547051.0463152, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.040799999998853, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.6800999998777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.040799999998853, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 76.68241212196246}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547051.0463152, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.040799999998853, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.6800999998777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 859.6800999998777, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.4462969423267}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547051.0463152, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.040799999998853, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.6800999998777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 872.7208999998766, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 74.47971052373009}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547051.9195364, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.589000000185479, "prefill_cuda_event_ms": null, "kv_decode_ms": 830.7915000000321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.589000000185479, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 86.2887220626452}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547051.9195364, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.589000000185479, "prefill_cuda_event_ms": null, "kv_decode_ms": 830.7915000000321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 830.7915000000321, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.03497207181047}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547051.9195364, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.589000000185479, "prefill_cuda_event_ms": null, "kv_decode_ms": 830.7915000000321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 842.3805000002176, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 77.16227999102925}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547052.7622702, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.108299999861629, "prefill_cuda_event_ms": null, "kv_decode_ms": 683.9814999998453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.108299999861629, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 70.88026197414344}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547052.7622702, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.108299999861629, "prefill_cuda_event_ms": null, "kv_decode_ms": 683.9814999998453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 683.9814999998453, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 93.56978222366318}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547052.7622702, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.108299999861629, "prefill_cuda_event_ms": null, "kv_decode_ms": 683.9814999998453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 698.0897999997069, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 93.11123010252734}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547053.4608307, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.422800000076677, "prefill_cuda_event_ms": 17.289215087890625, "kv_decode_ms": 613.6130000004414, "kv_decode_cuda_event_ms": 613.48046875, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 17.422800000076677, "cuda_event_ms": 17.289215087890625, "tokens_total": 1, "tokens_per_s": 57.39605574279674}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547053.4608307, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.422800000076677, "prefill_cuda_event_ms": 17.289215087890625, "kv_decode_ms": 613.6130000004414, "kv_decode_cuda_event_ms": 613.48046875, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 613.6130000004414, "cuda_event_ms": 613.48046875, "tokens_total": 64, "tokens_per_s": 104.30026743232943}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547053.4608307, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.422800000076677, "prefill_cuda_event_ms": 17.289215087890625, "kv_decode_ms": 613.6130000004414, "kv_decode_cuda_event_ms": 613.48046875, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 631.0358000005181, "cuda_event_ms": 630.7696838378906, "tokens_total": 65, "tokens_per_s": 103.00524946436737}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547054.0939019, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.507400000074995, "prefill_cuda_event_ms": 9.4453763961792, "kv_decode_ms": 547.4546999998893, "kv_decode_cuda_event_ms": 547.2593994140625, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 9.507400000074995, "cuda_event_ms": 9.4453763961792, "tokens_total": 1, "tokens_per_s": 105.18122725372993}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547054.0939019, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.507400000074995, "prefill_cuda_event_ms": 9.4453763961792, "kv_decode_ms": 547.4546999998893, "kv_decode_cuda_event_ms": 547.2593994140625, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 547.4546999998893, "cuda_event_ms": 547.2593994140625, "tokens_total": 64, "tokens_per_s": 116.90464982767149}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547054.0939019, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.507400000074995, "prefill_cuda_event_ms": 9.4453763961792, "kv_decode_ms": 547.4546999998893, "kv_decode_cuda_event_ms": 547.2593994140625, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 556.9620999999643, "cuda_event_ms": 556.7047758102417, "tokens_total": 65, "tokens_per_s": 116.70452980553644}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547054.6515033, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.808800000224437, "prefill_cuda_event_ms": 8.762368202209473, "kv_decode_ms": 554.7391999998581, "kv_decode_cuda_event_ms": 554.6892700195312, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 8.808800000224437, "cuda_event_ms": 8.762368202209473, "tokens_total": 1, "tokens_per_s": 113.52284079267565}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547054.6515033, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.808800000224437, "prefill_cuda_event_ms": 8.762368202209473, "kv_decode_ms": 554.7391999998581, "kv_decode_cuda_event_ms": 554.6892700195312, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 554.7391999998581, "cuda_event_ms": 554.6892700195312, "tokens_total": 64, "tokens_per_s": 115.36952860013565}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547054.6515033, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.808800000224437, "prefill_cuda_event_ms": 8.762368202209473, "kv_decode_ms": 554.7391999998581, "kv_decode_cuda_event_ms": 554.6892700195312, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 563.5480000000825, "cuda_event_ms": 563.4516382217407, "tokens_total": 65, "tokens_per_s": 115.34066308458281}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547055.2156641, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.709299999689392, "prefill_cuda_event_ms": 10.625951766967773, "kv_decode_ms": 475.21569999980784, "kv_decode_cuda_event_ms": 475.1769714355469, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 10.709299999689392, "cuda_event_ms": 10.625951766967773, "tokens_total": 1, "tokens_per_s": 93.37678466650515}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547055.2156641, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.709299999689392, "prefill_cuda_event_ms": 10.625951766967773, "kv_decode_ms": 475.21569999980784, "kv_decode_cuda_event_ms": 475.1769714355469, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 475.21569999980784, "cuda_event_ms": 475.1769714355469, "tokens_total": 64, "tokens_per_s": 134.67568516786352}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547055.2156641, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.709299999689392, "prefill_cuda_event_ms": 10.625951766967773, "kv_decode_ms": 475.21569999980784, "kv_decode_cuda_event_ms": 475.1769714355469, "gpu_peak_mb": 257.05517578125, "params_millions_measured": 96.08832, "latency_ms": 485.92499999949723, "cuda_event_ms": 485.80292320251465, "tokens_total": 65, "tokens_per_s": 133.76549879110408}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547055.702479, "prompt_tokens": 99, "prefill_ms": 928.8641, "prefill_cuda_event_ms": null, "kv_decode_ms": 3119.3174, "kv_decode_ms_equiv": 3119.3174, "kv_decode_ms_per_token": 48.739334375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 14681.720600000062, "ollama_total_duration_ms": 14538.5787, "ollama_load_ms": 10396.0624, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 928.8641, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 106.58179167436873}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547055.702479, "prompt_tokens": 99, "prefill_ms": 928.8641, "prefill_cuda_event_ms": null, "kv_decode_ms": 3119.3174, "kv_decode_ms_equiv": 3119.3174, "kv_decode_ms_per_token": 48.739334375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14681.720600000062, "ollama_total_duration_ms": 14538.5787, "ollama_load_ms": 10396.0624, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3119.3174, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.517309331842924}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547055.702479, "prompt_tokens": 99, "prefill_ms": 928.8641, "prefill_cuda_event_ms": null, "kv_decode_ms": 3119.3174, "kv_decode_ms_equiv": 3119.3174, "kv_decode_ms_per_token": 48.739334375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14681.720600000062, "ollama_total_duration_ms": 14538.5787, "ollama_load_ms": 10396.0624, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 4048.1814999999997, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 40.264993059229184}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547070.385166, "prompt_tokens": 99, "prefill_ms": 61.8268, "prefill_cuda_event_ms": null, "kv_decode_ms": 3405.2314, "kv_decode_ms_equiv": 3405.2314, "kv_decode_ms_per_token": 53.206740625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3759.723699999995, "ollama_total_duration_ms": 3756.1586, "ollama_load_ms": 265.4426, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.8268, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1601.247355515731}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547070.385166, "prompt_tokens": 99, "prefill_ms": 61.8268, "prefill_cuda_event_ms": null, "kv_decode_ms": 3405.2314, "kv_decode_ms_equiv": 3405.2314, "kv_decode_ms_per_token": 53.206740625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3759.723699999995, "ollama_total_duration_ms": 3756.1586, "ollama_load_ms": 265.4426, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3405.2314, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.794611138614545}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547070.385166, "prompt_tokens": 99, "prefill_ms": 61.8268, "prefill_cuda_event_ms": null, "kv_decode_ms": 3405.2314, "kv_decode_ms_equiv": 3405.2314, "kv_decode_ms_per_token": 53.206740625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3759.723699999995, "ollama_total_duration_ms": 3756.1586, "ollama_load_ms": 265.4426, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3467.0582, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 47.013920908509704}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547074.1450634, "prompt_tokens": 99, "prefill_ms": 68.9087, "prefill_cuda_event_ms": null, "kv_decode_ms": 3817.3078, "kv_decode_ms_equiv": 3817.3078, "kv_decode_ms_per_token": 59.645434375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4175.359000000299, "ollama_total_duration_ms": 4153.3912, "ollama_load_ms": 244.4831, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 68.9087, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1436.6836117935763}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547074.1450634, "prompt_tokens": 99, "prefill_ms": 68.9087, "prefill_cuda_event_ms": null, "kv_decode_ms": 3817.3078, "kv_decode_ms_equiv": 3817.3078, "kv_decode_ms_per_token": 59.645434375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4175.359000000299, "ollama_total_duration_ms": 4153.3912, "ollama_load_ms": 244.4831, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3817.3078, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.765742600059653}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547074.1450634, "prompt_tokens": 99, "prefill_ms": 68.9087, "prefill_cuda_event_ms": null, "kv_decode_ms": 3817.3078, "kv_decode_ms_equiv": 3817.3078, "kv_decode_ms_per_token": 59.645434375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4175.359000000299, "ollama_total_duration_ms": 4153.3912, "ollama_load_ms": 244.4831, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3886.2165, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 41.94310841920413}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547078.3206487, "prompt_tokens": 99, "prefill_ms": 69.5254, "prefill_cuda_event_ms": null, "kv_decode_ms": 3672.5099, "kv_decode_ms_equiv": 3672.5099, "kv_decode_ms_per_token": 57.3829671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4035.4632999997193, "ollama_total_duration_ms": 4016.5228, "ollama_load_ms": 251.5655, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 69.5254, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1423.9400276733395}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547078.3206487, "prompt_tokens": 99, "prefill_ms": 69.5254, "prefill_cuda_event_ms": null, "kv_decode_ms": 3672.5099, "kv_decode_ms_equiv": 3672.5099, "kv_decode_ms_per_token": 57.3829671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4035.4632999997193, "ollama_total_duration_ms": 4016.5228, "ollama_load_ms": 251.5655, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3672.5099, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.42677398909122}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547078.3206487, "prompt_tokens": 99, "prefill_ms": 69.5254, "prefill_cuda_event_ms": null, "kv_decode_ms": 3672.5099, "kv_decode_ms_equiv": 3672.5099, "kv_decode_ms_per_token": 57.3829671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4035.4632999997193, "ollama_total_duration_ms": 4016.5228, "ollama_load_ms": 251.5655, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3742.0353, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 43.559182886382715}
