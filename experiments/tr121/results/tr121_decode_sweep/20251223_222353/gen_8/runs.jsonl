{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546639.5353823, "prompt_tokens": 46, "prefill_ms": 23.7272, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.1841, "kv_decode_ms_equiv": 84.1841, "kv_decode_ms_per_token": 10.5230125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 7330.050699999902, "ollama_total_duration_ms": 7308.7089, "ollama_load_ms": 7191.2057, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 23.7272, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 1938.7032603931352}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546639.5353823, "prompt_tokens": 46, "prefill_ms": 23.7272, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.1841, "kv_decode_ms_equiv": 84.1841, "kv_decode_ms_per_token": 10.5230125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 7330.050699999902, "ollama_total_duration_ms": 7308.7089, "ollama_load_ms": 7191.2057, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 84.1841, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 95.02982154587387}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546639.5353823, "prompt_tokens": 46, "prefill_ms": 23.7272, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.1841, "kv_decode_ms_equiv": 84.1841, "kv_decode_ms_per_token": 10.5230125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 7330.050699999902, "ollama_total_duration_ms": 7308.7089, "ollama_load_ms": 7191.2057, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 107.9113, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 500.4109856891725}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546646.8655362, "prompt_tokens": 46, "prefill_ms": 11.75, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2889, "kv_decode_ms_equiv": 79.2889, "kv_decode_ms_per_token": 9.9111125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 349.9317999999221, "ollama_total_duration_ms": 346.5961, "ollama_load_ms": 244.6654, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.75, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3914.8936170212764}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546646.8655362, "prompt_tokens": 46, "prefill_ms": 11.75, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2889, "kv_decode_ms_equiv": 79.2889, "kv_decode_ms_per_token": 9.9111125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 349.9317999999221, "ollama_total_duration_ms": 346.5961, "ollama_load_ms": 244.6654, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.2889, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.89684684741496}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546646.8655362, "prompt_tokens": 46, "prefill_ms": 11.75, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2889, "kv_decode_ms_equiv": 79.2889, "kv_decode_ms_per_token": 9.9111125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 349.9317999999221, "ollama_total_duration_ms": 346.5961, "ollama_load_ms": 244.6654, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.0389, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 593.1530367787836}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546647.2155542, "prompt_tokens": 46, "prefill_ms": 11.6414, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.9401, "kv_decode_ms_equiv": 78.9401, "kv_decode_ms_per_token": 9.8675125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.1144999999233, "ollama_total_duration_ms": 350.7482, "ollama_load_ms": 250.1351, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6414, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3951.4147782912705}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546647.2155542, "prompt_tokens": 46, "prefill_ms": 11.6414, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.9401, "kv_decode_ms_equiv": 78.9401, "kv_decode_ms_per_token": 9.8675125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.1144999999233, "ollama_total_duration_ms": 350.7482, "ollama_load_ms": 250.1351, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 78.9401, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 101.34266361456345}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546647.2155542, "prompt_tokens": 46, "prefill_ms": 11.6414, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.9401, "kv_decode_ms_equiv": 78.9401, "kv_decode_ms_per_token": 9.8675125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.1144999999233, "ollama_total_duration_ms": 350.7482, "ollama_load_ms": 250.1351, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 90.5815, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 596.1482201111705}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546647.570027, "prompt_tokens": 46, "prefill_ms": 11.3169, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.2175, "kv_decode_ms_equiv": 81.2175, "kv_decode_ms_per_token": 10.1521875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 361.9570000000749, "ollama_total_duration_ms": 346.6229, "ollama_load_ms": 244.3237, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.3169, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4064.7173695976808}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546647.570027, "prompt_tokens": 46, "prefill_ms": 11.3169, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.2175, "kv_decode_ms_equiv": 81.2175, "kv_decode_ms_per_token": 10.1521875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 361.9570000000749, "ollama_total_duration_ms": 346.6229, "ollama_load_ms": 244.3237, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 81.2175, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 98.50093883707329}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546647.570027, "prompt_tokens": 46, "prefill_ms": 11.3169, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.2175, "kv_decode_ms_equiv": 81.2175, "kv_decode_ms_per_token": 10.1521875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 361.9570000000749, "ollama_total_duration_ms": 346.6229, "ollama_load_ms": 244.3237, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.5344, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 583.5667600373482}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546647.9320683, "prompt_tokens": 30, "prefill_ms": 15126.6585, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.5114, "kv_decode_ms_equiv": 83.5114, "kv_decode_ms_per_token": 10.438925, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 15469.263299999966, "ollama_total_duration_ms": 15466.3457, "ollama_load_ms": 246.7845, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 15126.6585, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 1.9832536048857057}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546647.9320683, "prompt_tokens": 30, "prefill_ms": 15126.6585, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.5114, "kv_decode_ms_equiv": 83.5114, "kv_decode_ms_per_token": 10.438925, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 15469.263299999966, "ollama_total_duration_ms": 15466.3457, "ollama_load_ms": 246.7845, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 83.5114, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 95.79530459314537}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546647.9320683, "prompt_tokens": 30, "prefill_ms": 15126.6585, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.5114, "kv_decode_ms_equiv": 83.5114, "kv_decode_ms_per_token": 10.438925, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 15469.263299999966, "ollama_total_duration_ms": 15466.3457, "ollama_load_ms": 246.7845, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 15210.169899999999, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 2.498328437475245}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546663.4014175, "prompt_tokens": 30, "prefill_ms": 11.2219, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.2556, "kv_decode_ms_equiv": 80.2556, "kv_decode_ms_per_token": 10.03195, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.960800000219, "ollama_total_duration_ms": 341.4141, "ollama_load_ms": 239.7712, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.2219, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2673.344086117324}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546663.4014175, "prompt_tokens": 30, "prefill_ms": 11.2219, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.2556, "kv_decode_ms_equiv": 80.2556, "kv_decode_ms_per_token": 10.03195, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.960800000219, "ollama_total_duration_ms": 341.4141, "ollama_load_ms": 239.7712, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.2556, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.68151755142321}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546663.4014175, "prompt_tokens": 30, "prefill_ms": 11.2219, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.2556, "kv_decode_ms_equiv": 80.2556, "kv_decode_ms_per_token": 10.03195, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.960800000219, "ollama_total_duration_ms": 341.4141, "ollama_load_ms": 239.7712, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.4775, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 415.4026946516903}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546663.7695372, "prompt_tokens": 30, "prefill_ms": 11.3454, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2194, "kv_decode_ms_equiv": 79.2194, "kv_decode_ms_per_token": 9.902425, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 348.7398999996003, "ollama_total_duration_ms": 345.5594, "ollama_load_ms": 242.9826, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.3454, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2644.243481939817}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546663.7695372, "prompt_tokens": 30, "prefill_ms": 11.3454, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2194, "kv_decode_ms_equiv": 79.2194, "kv_decode_ms_per_token": 9.902425, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 348.7398999996003, "ollama_total_duration_ms": 345.5594, "ollama_load_ms": 242.9826, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.2194, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.98536469602143}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546663.7695372, "prompt_tokens": 30, "prefill_ms": 11.3454, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.2194, "kv_decode_ms_equiv": 79.2194, "kv_decode_ms_per_token": 9.902425, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 348.7398999996003, "ollama_total_duration_ms": 345.5594, "ollama_load_ms": 242.9826, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 90.56479999999999, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 419.58906771725884}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546664.1183574, "prompt_tokens": 30, "prefill_ms": 11.6387, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.7155, "kv_decode_ms_equiv": 79.7155, "kv_decode_ms_per_token": 9.9644375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 350.1424999999472, "ollama_total_duration_ms": 347.3525, "ollama_load_ms": 246.5916, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6387, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2577.607464751218}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546664.1183574, "prompt_tokens": 30, "prefill_ms": 11.6387, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.7155, "kv_decode_ms_equiv": 79.7155, "kv_decode_ms_per_token": 9.9644375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 350.1424999999472, "ollama_total_duration_ms": 347.3525, "ollama_load_ms": 246.5916, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.7155, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.35689420501657}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546664.1183574, "prompt_tokens": 30, "prefill_ms": 11.6387, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.7155, "kv_decode_ms_equiv": 79.7155, "kv_decode_ms_per_token": 9.9644375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 350.1424999999472, "ollama_total_duration_ms": 347.3525, "ollama_load_ms": 246.5916, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.3542, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 415.96336019580923}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546664.4685829, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 116.68319999989762, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.03189999972892, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 6013.509999999769, "params_millions_measured": 25.016064, "latency_ms": 116.68319999989762, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 145.6936388444516}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546664.4685829, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 116.68319999989762, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.03189999972892, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 6013.509999999769, "params_millions_measured": 25.016064, "latency_ms": 72.03189999972892, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 111.0619045177221}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546664.4685829, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 116.68319999989762, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.03189999972892, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 6013.509999999769, "params_millions_measured": 25.016064, "latency_ms": 188.71509999962655, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 132.4748258091137}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546670.6748662, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 12.291499999719235, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.46919999982492, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.291499999719235, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1383.069600975334}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546670.6748662, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.291499999719235, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.46919999982492, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 65.46919999982492, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 122.19486415018655}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546670.6748662, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.291499999719235, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.46919999982492, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 77.76069999954416, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 321.49916346106136}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546670.753067, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.824600000229111, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.902999999754684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.824600000229111, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1730.3503450118637}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546670.753067, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.824600000229111, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.902999999754684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 63.902999999754684, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 125.18974070123016}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546670.753067, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.824600000229111, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.902999999754684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 73.7275999999838, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 339.0860410484743}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546670.8271728, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 12.355700000171055, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.77369999988514, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.355700000171055, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1375.8831955910753}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546670.8271728, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.355700000171055, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.77369999988514, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 66.77369999988514, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 119.80764882002586}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546670.8271728, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.355700000171055, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.77369999988514, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 79.1294000000562, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 315.9381974333465}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546670.906696, "prompt_tokens": 10, "prefill_ms": 7.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 18.4855, "kv_decode_ms_equiv": 18.4855, "kv_decode_ms_per_token": 2.3106875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 1954.5020999998997, "ollama_total_duration_ms": 1889.0672, "ollama_load_ms": 1825.2161, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.8946, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1266.6886226027918}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546670.906696, "prompt_tokens": 10, "prefill_ms": 7.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 18.4855, "kv_decode_ms_equiv": 18.4855, "kv_decode_ms_per_token": 2.3106875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1954.5020999998997, "ollama_total_duration_ms": 1889.0672, "ollama_load_ms": 1825.2161, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 18.4855, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 432.77163181953426}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546670.906696, "prompt_tokens": 10, "prefill_ms": 7.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 18.4855, "kv_decode_ms_equiv": 18.4855, "kv_decode_ms_per_token": 2.3106875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1954.5020999998997, "ollama_total_duration_ms": 1889.0672, "ollama_load_ms": 1825.2161, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 26.3801, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 682.332515797893}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546672.8613625, "prompt_tokens": 10, "prefill_ms": 2.3784, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.2577, "kv_decode_ms_equiv": 13.2577, "kv_decode_ms_per_token": 1.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 364.55849999993006, "ollama_total_duration_ms": 342.3137, "ollama_load_ms": 313.833, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3784, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4204.507231752438}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546672.8613625, "prompt_tokens": 10, "prefill_ms": 2.3784, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.2577, "kv_decode_ms_equiv": 13.2577, "kv_decode_ms_per_token": 1.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 364.55849999993006, "ollama_total_duration_ms": 342.3137, "ollama_load_ms": 313.833, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.2577, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 603.4229164938112}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546672.8613625, "prompt_tokens": 10, "prefill_ms": 2.3784, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.2577, "kv_decode_ms_equiv": 13.2577, "kv_decode_ms_per_token": 1.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 364.55849999993006, "ollama_total_duration_ms": 342.3137, "ollama_load_ms": 313.833, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 15.636099999999999, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1151.1822001650028}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546673.2259972, "prompt_tokens": 10, "prefill_ms": 2.6987, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.7796, "kv_decode_ms_equiv": 10.7796, "kv_decode_ms_per_token": 1.34745, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 334.5686000002388, "ollama_total_duration_ms": 322.7562, "ollama_load_ms": 296.0185, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6987, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3705.4878274724865}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546673.2259972, "prompt_tokens": 10, "prefill_ms": 2.6987, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.7796, "kv_decode_ms_equiv": 10.7796, "kv_decode_ms_per_token": 1.34745, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 334.5686000002388, "ollama_total_duration_ms": 322.7562, "ollama_load_ms": 296.0185, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 10.7796, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 742.1425655868492}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546673.2259972, "prompt_tokens": 10, "prefill_ms": 2.6987, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.7796, "kv_decode_ms_equiv": 10.7796, "kv_decode_ms_per_token": 1.34745, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 334.5686000002388, "ollama_total_duration_ms": 322.7562, "ollama_load_ms": 296.0185, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.4783, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1335.47999376776}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546673.5606582, "prompt_tokens": 10, "prefill_ms": 2.5339, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.9952, "kv_decode_ms_equiv": 10.9952, "kv_decode_ms_per_token": 1.3744, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 342.8521000000728, "ollama_total_duration_ms": 331.8041, "ollama_load_ms": 304.6459, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5339, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3946.4856545246457}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546673.5606582, "prompt_tokens": 10, "prefill_ms": 2.5339, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.9952, "kv_decode_ms_equiv": 10.9952, "kv_decode_ms_per_token": 1.3744, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 342.8521000000728, "ollama_total_duration_ms": 331.8041, "ollama_load_ms": 304.6459, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 10.9952, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 727.5902211874272}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546673.5606582, "prompt_tokens": 10, "prefill_ms": 2.5339, "prefill_cuda_event_ms": null, "kv_decode_ms": 10.9952, "kv_decode_ms_equiv": 10.9952, "kv_decode_ms_per_token": 1.3744, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 342.8521000000728, "ollama_total_duration_ms": 331.8041, "ollama_load_ms": 304.6459, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.5291, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1330.4654411601657}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546673.9037223, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 770.6796000002214, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.3779000001632, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 152.25580000014816, "params_millions_measured": 96.08832, "latency_ms": 770.6796000002214, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 11.678004711682279}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546673.9037223, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 770.6796000002214, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.3779000001632, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 152.25580000014816, "params_millions_measured": 96.08832, "latency_ms": 209.3779000001632, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 38.20842600863684}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546673.9037223, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 770.6796000002214, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.3779000001632, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 152.25580000014816, "params_millions_measured": 96.08832, "latency_ms": 980.0575000003846, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 17.345921030136832}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546675.036583, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.63260000000446, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.82510000021648, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 39.63260000000446, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 227.0857829160587}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546675.036583, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.63260000000446, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.82510000021648, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 209.82510000021648, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 38.12699243318243}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546675.036583, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.63260000000446, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.82510000021648, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 249.45770000022094, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 68.1478262646731}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546675.2864187, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 40.938699999969685, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.28679999967426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 40.938699999969685, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 219.84088405363786}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546675.2864187, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 40.938699999969685, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.28679999967426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 212.28679999967426, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 37.68486782980513}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546675.2864187, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 40.938699999969685, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.28679999967426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 253.22549999964394, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 67.1338392066514}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546675.540072, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.99570000021413, "prefill_cuda_event_ms": null, "kv_decode_ms": 195.16059999978097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 38.99570000021413, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 230.79467736059567}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546675.540072, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 38.99570000021413, "prefill_cuda_event_ms": null, "kv_decode_ms": 195.16059999978097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 195.16059999978097, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 40.99188053330938}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546675.540072, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 38.99570000021413, "prefill_cuda_event_ms": null, "kv_decode_ms": 195.16059999978097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 234.1562999999951, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 72.60107885203325}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546675.7747054, "prompt_tokens": 92, "prefill_ms": 819.2509, "prefill_cuda_event_ms": null, "kv_decode_ms": 289.6124, "kv_decode_ms_equiv": 289.6124, "kv_decode_ms_per_token": 36.20155, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 12547.21840000002, "ollama_total_duration_ms": 12377.2332, "ollama_load_ms": 11192.8215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 819.2509, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 112.29771001777355}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546675.7747054, "prompt_tokens": 92, "prefill_ms": 819.2509, "prefill_cuda_event_ms": null, "kv_decode_ms": 289.6124, "kv_decode_ms_equiv": 289.6124, "kv_decode_ms_per_token": 36.20155, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 12547.21840000002, "ollama_total_duration_ms": 12377.2332, "ollama_load_ms": 11192.8215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 289.6124, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.623126634080585}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546675.7747054, "prompt_tokens": 92, "prefill_ms": 819.2509, "prefill_cuda_event_ms": null, "kv_decode_ms": 289.6124, "kv_decode_ms_equiv": 289.6124, "kv_decode_ms_per_token": 36.20155, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 12547.21840000002, "ollama_total_duration_ms": 12377.2332, "ollama_load_ms": 11192.8215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1108.8633, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 90.18244178520473}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546688.322036, "prompt_tokens": 92, "prefill_ms": 56.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.5424, "kv_decode_ms_equiv": 281.5424, "kv_decode_ms_per_token": 35.1928, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 789.1436999998405, "ollama_total_duration_ms": 787.0221, "ollama_load_ms": 443.1334, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.2721, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1634.9132163185664}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546688.322036, "prompt_tokens": 92, "prefill_ms": 56.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.5424, "kv_decode_ms_equiv": 281.5424, "kv_decode_ms_per_token": 35.1928, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 789.1436999998405, "ollama_total_duration_ms": 787.0221, "ollama_load_ms": 443.1334, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 281.5424, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 28.4149030483508}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546688.322036, "prompt_tokens": 92, "prefill_ms": 56.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.5424, "kv_decode_ms_equiv": 281.5424, "kv_decode_ms_per_token": 35.1928, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 789.1436999998405, "ollama_total_duration_ms": 787.0221, "ollama_load_ms": 443.1334, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 337.8145, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 296.0204490926233}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546689.1395788, "prompt_tokens": 92, "prefill_ms": 56.8777, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.0102, "kv_decode_ms_equiv": 307.0102, "kv_decode_ms_per_token": 38.376275, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 848.9041000002544, "ollama_total_duration_ms": 829.0042, "ollama_load_ms": 459.6395, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.8777, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1617.5056305019368}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546689.1395788, "prompt_tokens": 92, "prefill_ms": 56.8777, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.0102, "kv_decode_ms_equiv": 307.0102, "kv_decode_ms_per_token": 38.376275, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 848.9041000002544, "ollama_total_duration_ms": 829.0042, "ollama_load_ms": 459.6395, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 307.0102, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.057766158909377}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546689.1395788, "prompt_tokens": 92, "prefill_ms": 56.8777, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.0102, "kv_decode_ms_equiv": 307.0102, "kv_decode_ms_per_token": 38.376275, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 848.9041000002544, "ollama_total_duration_ms": 829.0042, "ollama_load_ms": 459.6395, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 363.8879, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 274.8099071169995}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546689.988608, "prompt_tokens": 92, "prefill_ms": 60.4465, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3633, "kv_decode_ms_equiv": 316.3633, "kv_decode_ms_per_token": 39.5454125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 812.3872999999548, "ollama_total_duration_ms": 810.1639, "ollama_load_ms": 432.4035, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 60.4465, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1522.007064098004}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546689.988608, "prompt_tokens": 92, "prefill_ms": 60.4465, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3633, "kv_decode_ms_equiv": 316.3633, "kv_decode_ms_per_token": 39.5454125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 812.3872999999548, "ollama_total_duration_ms": 810.1639, "ollama_load_ms": 432.4035, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 316.3633, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.287383207849967}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546689.988608, "prompt_tokens": 92, "prefill_ms": 60.4465, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.3633, "kv_decode_ms_equiv": 316.3633, "kv_decode_ms_per_token": 39.5454125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 812.3872999999548, "ollama_total_duration_ms": 810.1639, "ollama_load_ms": 432.4035, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 376.8098, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 265.38587902968555}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546690.801085, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 628.0643000000055, "prefill_cuda_event_ms": 537.4044189453125, "kv_decode_ms": 155.49300000020594, "kv_decode_cuda_event_ms": 155.36842346191406, "gpu_peak_mb": 61.76123046875, "hf_load_ms": 347.3759999997128, "params_millions_measured": 25.016064, "latency_ms": 628.0643000000055, "cuda_event_ms": 537.4044189453125, "tokens_total": 17, "tokens_per_s": 27.067292313859987}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546690.801085, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 628.0643000000055, "prefill_cuda_event_ms": 537.4044189453125, "kv_decode_ms": 155.49300000020594, "kv_decode_cuda_event_ms": 155.36842346191406, "gpu_peak_mb": 61.76123046875, "hf_load_ms": 347.3759999997128, "params_millions_measured": 25.016064, "latency_ms": 155.49300000020594, "cuda_event_ms": 155.36842346191406, "tokens_total": 8, "tokens_per_s": 51.44926138147315}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546690.801085, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 628.0643000000055, "prefill_cuda_event_ms": 537.4044189453125, "kv_decode_ms": 155.49300000020594, "kv_decode_cuda_event_ms": 155.36842346191406, "gpu_peak_mb": 61.76123046875, "hf_load_ms": 347.3759999997128, "params_millions_measured": 25.016064, "latency_ms": 783.5573000002114, "cuda_event_ms": 692.7728424072266, "tokens_total": 25, "tokens_per_s": 31.905771281810857}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546691.9349866, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.6838999997380597, "prefill_cuda_event_ms": 2.6224639415740967, "kv_decode_ms": 13.845999999830383, "kv_decode_cuda_event_ms": 13.806495666503906, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 2.6838999997380597, "cuda_event_ms": 2.6224639415740967, "tokens_total": 17, "tokens_per_s": 6334.066098460877}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546691.9349866, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.6838999997380597, "prefill_cuda_event_ms": 2.6224639415740967, "kv_decode_ms": 13.845999999830383, "kv_decode_cuda_event_ms": 13.806495666503906, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 13.845999999830383, "cuda_event_ms": 13.806495666503906, "tokens_total": 8, "tokens_per_s": 577.7841976092736}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546691.9349866, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.6838999997380597, "prefill_cuda_event_ms": 2.6224639415740967, "kv_decode_ms": 13.845999999830383, "kv_decode_cuda_event_ms": 13.806495666503906, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 16.529899999568443, "cuda_event_ms": 16.428959608078003, "tokens_total": 25, "tokens_per_s": 1512.4108434202683}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546691.9521239, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.404500000011467, "prefill_cuda_event_ms": 2.34879994392395, "kv_decode_ms": 13.76269999991564, "kv_decode_cuda_event_ms": 13.718591690063477, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 2.404500000011467, "cuda_event_ms": 2.34879994392395, "tokens_total": 17, "tokens_per_s": 7070.076939038856}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546691.9521239, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.404500000011467, "prefill_cuda_event_ms": 2.34879994392395, "kv_decode_ms": 13.76269999991564, "kv_decode_cuda_event_ms": 13.718591690063477, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 13.76269999991564, "cuda_event_ms": 13.718591690063477, "tokens_total": 8, "tokens_per_s": 581.2812892854627}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546691.9521239, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.404500000011467, "prefill_cuda_event_ms": 2.34879994392395, "kv_decode_ms": 13.76269999991564, "kv_decode_cuda_event_ms": 13.718591690063477, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 16.167199999927107, "cuda_event_ms": 16.067391633987427, "tokens_total": 25, "tokens_per_s": 1546.3407392815527}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546691.968794, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.598799999759649, "prefill_cuda_event_ms": 2.531359910964966, "kv_decode_ms": 13.701200000014069, "kv_decode_cuda_event_ms": 13.64684772491455, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 2.598799999759649, "cuda_event_ms": 2.531359910964966, "tokens_total": 17, "tokens_per_s": 6541.480683997326}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546691.968794, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.598799999759649, "prefill_cuda_event_ms": 2.531359910964966, "kv_decode_ms": 13.701200000014069, "kv_decode_cuda_event_ms": 13.64684772491455, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 13.701200000014069, "cuda_event_ms": 13.64684772491455, "tokens_total": 8, "tokens_per_s": 583.8904621487012}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546691.968794, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 2.598799999759649, "prefill_cuda_event_ms": 2.531359910964966, "kv_decode_ms": 13.701200000014069, "kv_decode_cuda_event_ms": 13.64684772491455, "gpu_peak_mb": 61.76123046875, "params_millions_measured": 25.016064, "latency_ms": 16.299999999773718, "cuda_event_ms": 16.178207635879517, "tokens_total": 25, "tokens_per_s": 1533.7423313096356}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546691.9857078, "prompt_tokens": 25, "prefill_ms": 7.2974, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.9606, "kv_decode_ms_equiv": 16.9606, "kv_decode_ms_per_token": 2.120075, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 3135.6965999998465, "ollama_total_duration_ms": 3055.0669, "ollama_load_ms": 2994.711, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.2974, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3425.8777098692685}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546691.9857078, "prompt_tokens": 25, "prefill_ms": 7.2974, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.9606, "kv_decode_ms_equiv": 16.9606, "kv_decode_ms_per_token": 2.120075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 3135.6965999998465, "ollama_total_duration_ms": 3055.0669, "ollama_load_ms": 2994.711, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.9606, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 471.68142636463335}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546691.9857078, "prompt_tokens": 25, "prefill_ms": 7.2974, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.9606, "kv_decode_ms_equiv": 16.9606, "kv_decode_ms_per_token": 2.120075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 3135.6965999998465, "ollama_total_duration_ms": 3055.0669, "ollama_load_ms": 2994.711, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 24.258, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1360.3759584466982}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546695.1215765, "prompt_tokens": 25, "prefill_ms": 3.1866, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.7925, "kv_decode_ms_equiv": 13.7925, "kv_decode_ms_per_token": 1.7240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 373.5671000004004, "ollama_total_duration_ms": 339.017, "ollama_load_ms": 310.5804, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.1866, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 7845.352413230403}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546695.1215765, "prompt_tokens": 25, "prefill_ms": 3.1866, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.7925, "kv_decode_ms_equiv": 13.7925, "kv_decode_ms_per_token": 1.7240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 373.5671000004004, "ollama_total_duration_ms": 339.017, "ollama_load_ms": 310.5804, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.7925, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 580.0253761102048}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546695.1215765, "prompt_tokens": 25, "prefill_ms": 3.1866, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.7925, "kv_decode_ms_equiv": 13.7925, "kv_decode_ms_per_token": 1.7240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 373.5671000004004, "ollama_total_duration_ms": 339.017, "ollama_load_ms": 310.5804, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.9791, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1943.5659133876354}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546695.4952333, "prompt_tokens": 25, "prefill_ms": 2.9778, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.2805, "kv_decode_ms_equiv": 15.2805, "kv_decode_ms_per_token": 1.9100625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 366.57329999979993, "ollama_total_duration_ms": 351.7363, "ollama_load_ms": 324.5685, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.9778, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8395.45973537511}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546695.4952333, "prompt_tokens": 25, "prefill_ms": 2.9778, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.2805, "kv_decode_ms_equiv": 15.2805, "kv_decode_ms_per_token": 1.9100625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 366.57329999979993, "ollama_total_duration_ms": 351.7363, "ollama_load_ms": 324.5685, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 15.2805, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 523.5430777788685}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546695.4952333, "prompt_tokens": 25, "prefill_ms": 2.9778, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.2805, "kv_decode_ms_equiv": 15.2805, "kv_decode_ms_per_token": 1.9100625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 366.57329999979993, "ollama_total_duration_ms": 351.7363, "ollama_load_ms": 324.5685, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 18.2583, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1807.3971837465701}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546695.8618994, "prompt_tokens": 25, "prefill_ms": 3.0057, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.0611, "kv_decode_ms_equiv": 14.0611, "kv_decode_ms_per_token": 1.7576375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 351.62620000028255, "ollama_total_duration_ms": 335.8972, "ollama_load_ms": 308.362, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.0057, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8317.530026283395}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546695.8618994, "prompt_tokens": 25, "prefill_ms": 3.0057, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.0611, "kv_decode_ms_equiv": 14.0611, "kv_decode_ms_per_token": 1.7576375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 351.62620000028255, "ollama_total_duration_ms": 335.8972, "ollama_load_ms": 308.362, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.0611, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 568.9455305772664}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546695.8618994, "prompt_tokens": 25, "prefill_ms": 3.0057, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.0611, "kv_decode_ms_equiv": 14.0611, "kv_decode_ms_per_token": 1.7576375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 351.62620000028255, "ollama_total_duration_ms": 335.8972, "ollama_load_ms": 308.362, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 17.0668, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1933.5786439168444}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546696.2138417, "prompt_tokens": 84, "prefill_ms": 1011.4133, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.0316, "kv_decode_ms_equiv": 345.0316, "kv_decode_ms_per_token": 43.12895, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 11758.903500000088, "ollama_total_duration_ms": 11599.589, "ollama_load_ms": 10170.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1011.4133, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 83.05210145051484}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546696.2138417, "prompt_tokens": 84, "prefill_ms": 1011.4133, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.0316, "kv_decode_ms_equiv": 345.0316, "kv_decode_ms_per_token": 43.12895, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 11758.903500000088, "ollama_total_duration_ms": 11599.589, "ollama_load_ms": 10170.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 345.0316, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 23.18628206807724}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546696.2138417, "prompt_tokens": 84, "prefill_ms": 1011.4133, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.0316, "kv_decode_ms_equiv": 345.0316, "kv_decode_ms_per_token": 43.12895, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 11758.903500000088, "ollama_total_duration_ms": 11599.589, "ollama_load_ms": 10170.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1356.4449, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 67.82435467891102}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546707.9728634, "prompt_tokens": 84, "prefill_ms": 55.1086, "prefill_cuda_event_ms": null, "kv_decode_ms": 297.3072, "kv_decode_ms_equiv": 297.3072, "kv_decode_ms_per_token": 37.1634, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 887.5401000000238, "ollama_total_duration_ms": 885.2538, "ollama_load_ms": 525.3701, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.1086, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1524.2630006931768}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546707.9728634, "prompt_tokens": 84, "prefill_ms": 55.1086, "prefill_cuda_event_ms": null, "kv_decode_ms": 297.3072, "kv_decode_ms_equiv": 297.3072, "kv_decode_ms_per_token": 37.1634, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 887.5401000000238, "ollama_total_duration_ms": 885.2538, "ollama_load_ms": 525.3701, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 297.3072, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.908194621590056}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546707.9728634, "prompt_tokens": 84, "prefill_ms": 55.1086, "prefill_cuda_event_ms": null, "kv_decode_ms": 297.3072, "kv_decode_ms_equiv": 297.3072, "kv_decode_ms_per_token": 37.1634, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 887.5401000000238, "ollama_total_duration_ms": 885.2538, "ollama_load_ms": 525.3701, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 352.41580000000005, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 261.05526483205347}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546708.8605068, "prompt_tokens": 84, "prefill_ms": 53.4784, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.8574, "kv_decode_ms_equiv": 314.8574, "kv_decode_ms_per_token": 39.357175, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 837.7899000001889, "ollama_total_duration_ms": 834.8697, "ollama_load_ms": 458.8434, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.4784, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1570.72762087123}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546708.8605068, "prompt_tokens": 84, "prefill_ms": 53.4784, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.8574, "kv_decode_ms_equiv": 314.8574, "kv_decode_ms_per_token": 39.357175, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 837.7899000001889, "ollama_total_duration_ms": 834.8697, "ollama_load_ms": 458.8434, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 314.8574, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.408327706447427}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546708.8605068, "prompt_tokens": 84, "prefill_ms": 53.4784, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.8574, "kv_decode_ms_equiv": 314.8574, "kv_decode_ms_per_token": 39.357175, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 837.7899000001889, "ollama_total_duration_ms": 834.8697, "ollama_load_ms": 458.8434, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 368.3358, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 249.77208297428598}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546709.740438, "prompt_tokens": 84, "prefill_ms": 57.2203, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.2209, "kv_decode_ms_equiv": 313.2209, "kv_decode_ms_per_token": 39.1526125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 810.7767999999851, "ollama_total_duration_ms": 806.7291, "ollama_load_ms": 434.0506, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 57.2203, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1468.0104787986081}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546709.740438, "prompt_tokens": 84, "prefill_ms": 57.2203, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.2209, "kv_decode_ms_equiv": 313.2209, "kv_decode_ms_per_token": 39.1526125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 810.7767999999851, "ollama_total_duration_ms": 806.7291, "ollama_load_ms": 434.0506, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 313.2209, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.541079793845178}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546709.740438, "prompt_tokens": 84, "prefill_ms": 57.2203, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.2209, "kv_decode_ms_equiv": 313.2209, "kv_decode_ms_per_token": 39.1526125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 810.7767999999851, "ollama_total_duration_ms": 806.7291, "ollama_load_ms": 434.0506, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 370.4412, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 248.35250506693103}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546710.5514023, "prompt_tokens": 38, "prefill_ms": 22.6655, "prefill_cuda_event_ms": null, "kv_decode_ms": 85.9771, "kv_decode_ms_equiv": 85.9771, "kv_decode_ms_per_token": 10.7471375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 6485.967999999957, "ollama_total_duration_ms": 6482.921, "ollama_load_ms": 6365.0899, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.6655, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1676.5568816042}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546710.5514023, "prompt_tokens": 38, "prefill_ms": 22.6655, "prefill_cuda_event_ms": null, "kv_decode_ms": 85.9771, "kv_decode_ms_equiv": 85.9771, "kv_decode_ms_per_token": 10.7471375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 6485.967999999957, "ollama_total_duration_ms": 6482.921, "ollama_load_ms": 6365.0899, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 85.9771, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 93.0480325575066}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546710.5514023, "prompt_tokens": 38, "prefill_ms": 22.6655, "prefill_cuda_event_ms": null, "kv_decode_ms": 85.9771, "kv_decode_ms_equiv": 85.9771, "kv_decode_ms_per_token": 10.7471375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 6485.967999999957, "ollama_total_duration_ms": 6482.921, "ollama_load_ms": 6365.0899, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 108.64259999999999, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 423.4066563208171}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546717.0375707, "prompt_tokens": 38, "prefill_ms": 11.7108, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.1686, "kv_decode_ms_equiv": 80.1686, "kv_decode_ms_per_token": 10.021075, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 246.96959999982937, "ollama_total_duration_ms": 244.2366, "ollama_load_ms": 143.3969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.7108, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3244.8679851077636}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546717.0375707, "prompt_tokens": 38, "prefill_ms": 11.7108, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.1686, "kv_decode_ms_equiv": 80.1686, "kv_decode_ms_per_token": 10.021075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 246.96959999982937, "ollama_total_duration_ms": 244.2366, "ollama_load_ms": 143.3969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.1686, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.78969322153563}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546717.0375707, "prompt_tokens": 38, "prefill_ms": 11.7108, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.1686, "kv_decode_ms_equiv": 80.1686, "kv_decode_ms_per_token": 10.021075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 246.96959999982937, "ollama_total_duration_ms": 244.2366, "ollama_load_ms": 143.3969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.8794, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 500.65629509988094}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546717.2847338, "prompt_tokens": 38, "prefill_ms": 11.9101, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.6863, "kv_decode_ms_equiv": 81.6863, "kv_decode_ms_per_token": 10.2107875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 255.1101000003655, "ollama_total_duration_ms": 230.3273, "ollama_load_ms": 128.0469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9101, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3190.569348704041}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546717.2847338, "prompt_tokens": 38, "prefill_ms": 11.9101, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.6863, "kv_decode_ms_equiv": 81.6863, "kv_decode_ms_per_token": 10.2107875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 255.1101000003655, "ollama_total_duration_ms": 230.3273, "ollama_load_ms": 128.0469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 81.6863, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 97.935639146344}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546717.2847338, "prompt_tokens": 38, "prefill_ms": 11.9101, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.6863, "kv_decode_ms_equiv": 81.6863, "kv_decode_ms_per_token": 10.2107875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 255.1101000003655, "ollama_total_duration_ms": 230.3273, "ollama_load_ms": 128.0469, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 93.5964, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 491.4718942181537}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546717.5399761, "prompt_tokens": 38, "prefill_ms": 11.615, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9308, "kv_decode_ms_equiv": 79.9308, "kv_decode_ms_per_token": 9.99135, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 236.17120000017167, "ollama_total_duration_ms": 233.5001, "ollama_load_ms": 129.8682, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.615, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3271.6315109771845}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546717.5399761, "prompt_tokens": 38, "prefill_ms": 11.615, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9308, "kv_decode_ms_equiv": 79.9308, "kv_decode_ms_per_token": 9.99135, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 236.17120000017167, "ollama_total_duration_ms": 233.5001, "ollama_load_ms": 129.8682, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.9308, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.08657488727748}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546717.5399761, "prompt_tokens": 38, "prefill_ms": 11.615, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9308, "kv_decode_ms_equiv": 79.9308, "kv_decode_ms_per_token": 9.99135, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 236.17120000017167, "ollama_total_duration_ms": 233.5001, "ollama_load_ms": 129.8682, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.5458, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 502.4807254947797}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546717.776524, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 63.522500000090076, "prefill_cuda_event_ms": 63.40915298461914, "kv_decode_ms": 98.37999999990643, "kv_decode_cuda_event_ms": 98.29497528076172, "gpu_peak_mb": 258.021484375, "hf_load_ms": 469.72850000020117, "params_millions_measured": 96.08832, "latency_ms": 63.522500000090076, "cuda_event_ms": 63.40915298461914, "tokens_total": 17, "tokens_per_s": 267.62170884294375}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546717.776524, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 63.522500000090076, "prefill_cuda_event_ms": 63.40915298461914, "kv_decode_ms": 98.37999999990643, "kv_decode_cuda_event_ms": 98.29497528076172, "gpu_peak_mb": 258.021484375, "hf_load_ms": 469.72850000020117, "params_millions_measured": 96.08832, "latency_ms": 98.37999999990643, "cuda_event_ms": 98.29497528076172, "tokens_total": 8, "tokens_per_s": 81.31734092302916}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546717.776524, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 63.522500000090076, "prefill_cuda_event_ms": 63.40915298461914, "kv_decode_ms": 98.37999999990643, "kv_decode_cuda_event_ms": 98.29497528076172, "gpu_peak_mb": 258.021484375, "hf_load_ms": 469.72850000020117, "params_millions_measured": 96.08832, "latency_ms": 161.9024999999965, "cuda_event_ms": 161.70412826538086, "tokens_total": 25, "tokens_per_s": 154.4139219592072}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.4099705, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 12.441000000308122, "prefill_cuda_event_ms": 12.371968269348145, "kv_decode_ms": 91.14480000016556, "kv_decode_cuda_event_ms": 91.05705261230469, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 12.441000000308122, "cuda_event_ms": 12.371968269348145, "tokens_total": 17, "tokens_per_s": 1366.4496422778689}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.4099705, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.441000000308122, "prefill_cuda_event_ms": 12.371968269348145, "kv_decode_ms": 91.14480000016556, "kv_decode_cuda_event_ms": 91.05705261230469, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 91.14480000016556, "cuda_event_ms": 91.05705261230469, "tokens_total": 8, "tokens_per_s": 87.77242365977509}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.4099705, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 12.441000000308122, "prefill_cuda_event_ms": 12.371968269348145, "kv_decode_ms": 91.14480000016556, "kv_decode_cuda_event_ms": 91.05705261230469, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 103.58580000047368, "cuda_event_ms": 103.42902088165283, "tokens_total": 25, "tokens_per_s": 241.34582153041902}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.5142567, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.276000000132626, "prefill_cuda_event_ms": 13.162495613098145, "kv_decode_ms": 79.94920000010097, "kv_decode_cuda_event_ms": 79.8924789428711, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 13.276000000132626, "cuda_event_ms": 13.162495613098145, "tokens_total": 17, "tokens_per_s": 1280.5061765464125}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.5142567, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.276000000132626, "prefill_cuda_event_ms": 13.162495613098145, "kv_decode_ms": 79.94920000010097, "kv_decode_cuda_event_ms": 79.8924789428711, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 79.94920000010097, "cuda_event_ms": 79.8924789428711, "tokens_total": 8, "tokens_per_s": 100.06354034799467}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.5142567, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.276000000132626, "prefill_cuda_event_ms": 13.162495613098145, "kv_decode_ms": 79.94920000010097, "kv_decode_cuda_event_ms": 79.8924789428711, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 93.2252000002336, "cuda_event_ms": 93.05497455596924, "tokens_total": 25, "tokens_per_s": 268.16783444752446}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.608127, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.680499999925814, "prefill_cuda_event_ms": 5.637983798980713, "kv_decode_ms": 71.0188000002745, "kv_decode_cuda_event_ms": 70.97344207763672, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 5.680499999925814, "cuda_event_ms": 5.637983798980713, "tokens_total": 17, "tokens_per_s": 2992.694305117862}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.608127, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.680499999925814, "prefill_cuda_event_ms": 5.637983798980713, "kv_decode_ms": 71.0188000002745, "kv_decode_cuda_event_ms": 70.97344207763672, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 71.0188000002745, "cuda_event_ms": 70.97344207763672, "tokens_total": 8, "tokens_per_s": 112.64622888543708}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.608127, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.680499999925814, "prefill_cuda_event_ms": 5.637983798980713, "kv_decode_ms": 71.0188000002745, "kv_decode_cuda_event_ms": 70.97344207763672, "gpu_peak_mb": 258.021484375, "params_millions_measured": 96.08832, "latency_ms": 76.69930000020031, "cuda_event_ms": 76.61142587661743, "tokens_total": 25, "tokens_per_s": 325.9482159541835}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.6854823, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 87.35289999958695, "prefill_cuda_event_ms": 87.25103759765625, "kv_decode_ms": 74.6819000000869, "kv_decode_cuda_event_ms": 74.60848236083984, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 87.35289999958695, "cuda_event_ms": 87.25103759765625, "tokens_total": 9, "tokens_per_s": 103.03035159728591}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.6854823, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 87.35289999958695, "prefill_cuda_event_ms": 87.25103759765625, "kv_decode_ms": 74.6819000000869, "kv_decode_cuda_event_ms": 74.60848236083984, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 74.6819000000869, "cuda_event_ms": 74.60848236083984, "tokens_total": 8, "tokens_per_s": 107.12100254533819}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.6854823, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 87.35289999958695, "prefill_cuda_event_ms": 87.25103759765625, "kv_decode_ms": 74.6819000000869, "kv_decode_cuda_event_ms": 74.60848236083984, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 162.03479999967385, "cuda_event_ms": 161.8595199584961, "tokens_total": 17, "tokens_per_s": 104.91573415114665}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.8483295, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.657200000172452, "prefill_cuda_event_ms": 11.598848342895508, "kv_decode_ms": 79.37760000004346, "kv_decode_cuda_event_ms": 79.34528350830078, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 11.657200000172452, "cuda_event_ms": 11.598848342895508, "tokens_total": 9, "tokens_per_s": 772.0550389344661}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.8483295, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.657200000172452, "prefill_cuda_event_ms": 11.598848342895508, "kv_decode_ms": 79.37760000004346, "kv_decode_cuda_event_ms": 79.34528350830078, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 79.37760000004346, "cuda_event_ms": 79.34528350830078, "tokens_total": 8, "tokens_per_s": 100.78410030028144}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.8483295, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.657200000172452, "prefill_cuda_event_ms": 11.598848342895508, "kv_decode_ms": 79.37760000004346, "kv_decode_cuda_event_ms": 79.34528350830078, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 91.03480000021591, "cuda_event_ms": 90.94413185119629, "tokens_total": 17, "tokens_per_s": 186.74177347519498}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546718.9402094, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.190700000042852, "prefill_cuda_event_ms": 11.014143943786621, "kv_decode_ms": 87.0521000001645, "kv_decode_cuda_event_ms": 86.97046661376953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 11.190700000042852, "cuda_event_ms": 11.014143943786621, "tokens_total": 9, "tokens_per_s": 804.2392343611692}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546718.9402094, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.190700000042852, "prefill_cuda_event_ms": 11.014143943786621, "kv_decode_ms": 87.0521000001645, "kv_decode_cuda_event_ms": 86.97046661376953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 87.0521000001645, "cuda_event_ms": 86.97046661376953, "tokens_total": 8, "tokens_per_s": 91.8989892258186}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546718.9402094, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.190700000042852, "prefill_cuda_event_ms": 11.014143943786621, "kv_decode_ms": 87.0521000001645, "kv_decode_cuda_event_ms": 86.97046661376953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 98.24280000020735, "cuda_event_ms": 97.98461055755615, "tokens_total": 17, "tokens_per_s": 173.04067066455883}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.0393007, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.399600000004284, "prefill_cuda_event_ms": 11.33568000793457, "kv_decode_ms": 67.94140000010884, "kv_decode_cuda_event_ms": 67.84204864501953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 11.399600000004284, "cuda_event_ms": 11.33568000793457, "tokens_total": 9, "tokens_per_s": 789.5013860132476}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.0393007, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.399600000004284, "prefill_cuda_event_ms": 11.33568000793457, "kv_decode_ms": 67.94140000010884, "kv_decode_cuda_event_ms": 67.84204864501953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 67.94140000010884, "cuda_event_ms": 67.84204864501953, "tokens_total": 8, "tokens_per_s": 117.74853035096692}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.0393007, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 11.399600000004284, "prefill_cuda_event_ms": 11.33568000793457, "kv_decode_ms": 67.94140000010884, "kv_decode_cuda_event_ms": 67.84204864501953, "gpu_peak_mb": 256.69677734375, "params_millions_measured": 96.08832, "latency_ms": 79.34100000011313, "cuda_event_ms": 79.1777286529541, "tokens_total": 17, "tokens_per_s": 214.26500800312274}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.1195421, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.859000000233209, "prefill_cuda_event_ms": 9.594880104064941, "kv_decode_ms": 30.24520000008124, "kv_decode_cuda_event_ms": 30.130176544189453, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 9.859000000233209, "cuda_event_ms": 9.594880104064941, "tokens_total": 1, "tokens_per_s": 101.43016532877022}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.1195421, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 9.859000000233209, "prefill_cuda_event_ms": 9.594880104064941, "kv_decode_ms": 30.24520000008124, "kv_decode_cuda_event_ms": 30.130176544189453, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 30.24520000008124, "cuda_event_ms": 30.130176544189453, "tokens_total": 8, "tokens_per_s": 264.50478092320475}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.1195421, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 9.859000000233209, "prefill_cuda_event_ms": 9.594880104064941, "kv_decode_ms": 30.24520000008124, "kv_decode_cuda_event_ms": 30.130176544189453, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 40.10420000031445, "cuda_event_ms": 39.725056648254395, "tokens_total": 9, "tokens_per_s": 224.41539788674086}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.1605299, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.078000000139582, "prefill_cuda_event_ms": 4.9694719314575195, "kv_decode_ms": 34.644200000002456, "kv_decode_cuda_event_ms": 34.547454833984375, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 5.078000000139582, "cuda_event_ms": 4.9694719314575195, "tokens_total": 1, "tokens_per_s": 196.92792437426397}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.1605299, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.078000000139582, "prefill_cuda_event_ms": 4.9694719314575195, "kv_decode_ms": 34.644200000002456, "kv_decode_cuda_event_ms": 34.547454833984375, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 34.644200000002456, "cuda_event_ms": 34.547454833984375, "tokens_total": 8, "tokens_per_s": 230.9188839690174}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.1605299, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.078000000139582, "prefill_cuda_event_ms": 4.9694719314575195, "kv_decode_ms": 34.644200000002456, "kv_decode_cuda_event_ms": 34.547454833984375, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 39.72220000014204, "cuda_event_ms": 39.516926765441895, "tokens_total": 9, "tokens_per_s": 226.57355332705183}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.2010448, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.244200000106503, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 27.350599999863334, "kv_decode_cuda_event_ms": 27.292736053466797, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 5.244200000106503, "cuda_event_ms": 5.155839920043945, "tokens_total": 1, "tokens_per_s": 190.68685404440933}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.2010448, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.244200000106503, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 27.350599999863334, "kv_decode_cuda_event_ms": 27.292736053466797, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 27.350599999863334, "cuda_event_ms": 27.292736053466797, "tokens_total": 8, "tokens_per_s": 292.49815360686694}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.2010448, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.244200000106503, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 27.350599999863334, "kv_decode_cuda_event_ms": 27.292736053466797, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 32.59479999996984, "cuda_event_ms": 32.44857597351074, "tokens_total": 9, "tokens_per_s": 276.1176629403564}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.2345607, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.214600000272185, "prefill_cuda_event_ms": 5.125120162963867, "kv_decode_ms": 24.3294000001697, "kv_decode_cuda_event_ms": 24.266592025756836, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 5.214600000272185, "cuda_event_ms": 5.125120162963867, "tokens_total": 1, "tokens_per_s": 191.76926321248098}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.2345607, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.214600000272185, "prefill_cuda_event_ms": 5.125120162963867, "kv_decode_ms": 24.3294000001697, "kv_decode_cuda_event_ms": 24.266592025756836, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 24.3294000001697, "cuda_event_ms": 24.266592025756836, "tokens_total": 8, "tokens_per_s": 328.82027505586655}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.2345607, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.214600000272185, "prefill_cuda_event_ms": 5.125120162963867, "kv_decode_ms": 24.3294000001697, "kv_decode_cuda_event_ms": 24.266592025756836, "gpu_peak_mb": 255.56640625, "params_millions_measured": 25.016064, "latency_ms": 29.544000000441883, "cuda_event_ms": 29.391712188720703, "tokens_total": 9, "tokens_per_s": 304.6303817988556}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546719.2646985, "prompt_tokens": 17, "prefill_ms": 9.0168, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.4653, "kv_decode_ms_equiv": 16.4653, "kv_decode_ms_per_token": 2.0581625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 1519.175500000074, "ollama_total_duration_ms": 1445.3637, "ollama_load_ms": 1380.6742, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.0168, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1885.369532428356}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546719.2646985, "prompt_tokens": 17, "prefill_ms": 9.0168, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.4653, "kv_decode_ms_equiv": 16.4653, "kv_decode_ms_per_token": 2.0581625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1519.175500000074, "ollama_total_duration_ms": 1445.3637, "ollama_load_ms": 1380.6742, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.4653, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 485.8702847807207}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546719.2646985, "prompt_tokens": 17, "prefill_ms": 9.0168, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.4653, "kv_decode_ms_equiv": 16.4653, "kv_decode_ms_per_token": 2.0581625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1519.175500000074, "ollama_total_duration_ms": 1445.3637, "ollama_load_ms": 1380.6742, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 25.4821, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 981.0808371366567}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546720.7840414, "prompt_tokens": 17, "prefill_ms": 1.8986, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.4467, "kv_decode_ms_equiv": 15.4467, "kv_decode_ms_per_token": 1.9308375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 204.7565000002578, "ollama_total_duration_ms": 189.4756, "ollama_load_ms": 161.2464, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.8986, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 8953.966080269673}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546720.7840414, "prompt_tokens": 17, "prefill_ms": 1.8986, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.4467, "kv_decode_ms_equiv": 15.4467, "kv_decode_ms_per_token": 1.9308375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 204.7565000002578, "ollama_total_duration_ms": 189.4756, "ollama_load_ms": 161.2464, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 15.4467, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 517.9099742987175}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546720.7840414, "prompt_tokens": 17, "prefill_ms": 1.8986, "prefill_cuda_event_ms": null, "kv_decode_ms": 15.4467, "kv_decode_ms_equiv": 15.4467, "kv_decode_ms_per_token": 1.9308375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 204.7565000002578, "ollama_total_duration_ms": 189.4756, "ollama_load_ms": 161.2464, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 17.3453, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1441.312632240434}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546720.9890609, "prompt_tokens": 17, "prefill_ms": 2.5436, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2178, "kv_decode_ms_equiv": 12.2178, "kv_decode_ms_per_token": 1.527225, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 192.84330000027694, "ollama_total_duration_ms": 166.8052, "ollama_load_ms": 139.6371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5436, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6683.440792577449}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546720.9890609, "prompt_tokens": 17, "prefill_ms": 2.5436, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2178, "kv_decode_ms_equiv": 12.2178, "kv_decode_ms_per_token": 1.527225, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 192.84330000027694, "ollama_total_duration_ms": 166.8052, "ollama_load_ms": 139.6371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.2178, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 654.7823667108644}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546720.9890609, "prompt_tokens": 17, "prefill_ms": 2.5436, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2178, "kv_decode_ms_equiv": 12.2178, "kv_decode_ms_per_token": 1.527225, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 192.84330000027694, "ollama_total_duration_ms": 166.8052, "ollama_load_ms": 139.6371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.7614, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1693.6062975056566}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546721.1819863, "prompt_tokens": 17, "prefill_ms": 2.8245, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.6569, "kv_decode_ms_equiv": 13.6569, "kv_decode_ms_per_token": 1.7071125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 209.71700000018245, "ollama_total_duration_ms": 172.0053, "ollama_load_ms": 148.0608, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.8245, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6018.76438307665}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546721.1819863, "prompt_tokens": 17, "prefill_ms": 2.8245, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.6569, "kv_decode_ms_equiv": 13.6569, "kv_decode_ms_per_token": 1.7071125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 209.71700000018245, "ollama_total_duration_ms": 172.0053, "ollama_load_ms": 148.0608, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.6569, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 585.7844752469448}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546721.1819863, "prompt_tokens": 17, "prefill_ms": 2.8245, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.6569, "kv_decode_ms_equiv": 13.6569, "kv_decode_ms_per_token": 1.7071125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 209.71700000018245, "ollama_total_duration_ms": 172.0053, "ollama_load_ms": 148.0608, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.4814, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1516.8614316744936}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546721.3918521, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 58.90440000030139, "prefill_cuda_event_ms": null, "kv_decode_ms": 373.51470000021436, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 58.90440000030139, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 16.976660487075385}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546721.3918521, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 58.90440000030139, "prefill_cuda_event_ms": null, "kv_decode_ms": 373.51470000021436, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 373.51470000021436, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 21.418166406825243}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546721.3918521, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 58.90440000030139, "prefill_cuda_event_ms": null, "kv_decode_ms": 373.51470000021436, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 432.41910000051575, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 20.813141695150065}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546721.824949, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 50.09509999990769, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.81269999966025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 50.09509999990769, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 19.962032214764374}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546721.824949, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 50.09509999990769, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.81269999966025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 355.81269999966025, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 22.483739338162014}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546721.824949, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 50.09509999990769, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.81269999966025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 405.90779999956794, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 22.172522922716883}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546722.2312958, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 43.62919999994119, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.7392999997028, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 43.62919999994119, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 22.92042943719683}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546722.2312958, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 43.62919999994119, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.7392999997028, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 362.7392999997028, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 22.05440656693817}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546722.2312958, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 43.62919999994119, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.7392999997028, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 406.368499999644, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 22.147385931753778}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546722.6381135, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 62.10049999981493, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.35099999971135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 62.10049999981493, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 16.10292992814841}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546722.6381135, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 62.10049999981493, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.35099999971135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 393.35099999971135, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 20.338069561297342}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546722.6381135, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 62.10049999981493, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.35099999971135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 455.4514999995263, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 19.760611173767924}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546723.0940068, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 78.01329999983864, "prefill_cuda_event_ms": null, "kv_decode_ms": 464.5307999999204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 78.01329999983864, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 217.91156123424037}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546723.0940068, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 78.01329999983864, "prefill_cuda_event_ms": null, "kv_decode_ms": 464.5307999999204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 464.5307999999204, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 17.22167830421873}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546723.0940068, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 78.01329999983864, "prefill_cuda_event_ms": null, "kv_decode_ms": 464.5307999999204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 542.544099999759, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 46.0792035154582}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546723.6372566, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 93.97570000010091, "prefill_cuda_event_ms": null, "kv_decode_ms": 468.70209999997314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 93.97570000010091, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 180.8978278425353}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546723.6372566, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 93.97570000010091, "prefill_cuda_event_ms": null, "kv_decode_ms": 468.70209999997314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 468.70209999997314, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 17.068410830675727}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546723.6372566, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 93.97570000010091, "prefill_cuda_event_ms": null, "kv_decode_ms": 468.70209999997314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 562.6778000000741, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 44.43040048851529}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546724.200455, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 82.55889999963983, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.5963000000993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 82.55889999963983, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 205.91359623340625}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546724.200455, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 82.55889999963983, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.5963000000993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 423.5963000000993, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 18.885906227221827}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546724.200455, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 82.55889999963983, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.5963000000993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 506.1551999997391, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 49.39196515221593}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546724.7071395, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 99.63130000005549, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.7550000000483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 99.63130000005549, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 170.62910952673036}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546724.7071395, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 99.63130000005549, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.7550000000483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 451.7550000000483, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 17.708713793979356}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546724.7071395, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 99.63130000005549, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.7550000000483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 551.3863000001038, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 45.34026326006884}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.2592235, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.100199999793404, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.3563999999351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 24.100199999793404, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 373.4408843112153}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.2592235, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 24.100199999793404, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.3563999999351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 132.3563999999351, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.44286487093879}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.2592235, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 24.100199999793404, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.3563999999351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 156.4565999997285, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 108.65633025407365}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.4164577, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 23.195599999780825, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6109000001452, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 23.195599999780825, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 388.00462156982536}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.4164577, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 23.195599999780825, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6109000001452, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 132.6109000001452, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.32686604186564}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.4164577, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 23.195599999780825, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6109000001452, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 155.80649999992602, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 109.10969696391403}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.5727968, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.53580000024158, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.65299999975832, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.53580000024158, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 399.36456659641647}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.5727968, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.53580000024158, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.65299999975832, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 128.65299999975832, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 62.18277070892267}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.5727968, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.53580000024158, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.65299999975832, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 151.1887999999999, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 112.44219148508363}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.7244732, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 23.807799999758572, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.78130000010424, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 23.807799999758572, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 378.0273691853622}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.7244732, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 23.807799999758572, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.78130000010424, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 129.78130000010424, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 61.64216262276287}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.7244732, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 23.807799999758572, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.78130000010424, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 153.5890999998628, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 110.68493792863677}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.8787534, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.501400000004651, "prefill_cuda_event_ms": 3.4007039070129395, "kv_decode_ms": 13.80250000011074, "kv_decode_cuda_event_ms": 13.743231773376465, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 3.501400000004651, "cuda_event_ms": 3.4007039070129395, "tokens_total": 9, "tokens_per_s": 2570.4004112606517}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.8787534, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.501400000004651, "prefill_cuda_event_ms": 3.4007039070129395, "kv_decode_ms": 13.80250000011074, "kv_decode_cuda_event_ms": 13.743231773376465, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 13.80250000011074, "cuda_event_ms": 13.743231773376465, "tokens_total": 8, "tokens_per_s": 579.6051439910027}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.8787534, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.501400000004651, "prefill_cuda_event_ms": 3.4007039070129395, "kv_decode_ms": 13.80250000011074, "kv_decode_cuda_event_ms": 13.743231773376465, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 17.30390000011539, "cuda_event_ms": 17.143935680389404, "tokens_total": 17, "tokens_per_s": 982.4374851846483}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.8974853, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.6143000000047323, "prefill_cuda_event_ms": 2.5374720096588135, "kv_decode_ms": 13.618699999824457, "kv_decode_cuda_event_ms": 13.566975593566895, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 2.6143000000047323, "cuda_event_ms": 2.5374720096588135, "tokens_total": 9, "tokens_per_s": 3442.604138768966}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.8974853, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.6143000000047323, "prefill_cuda_event_ms": 2.5374720096588135, "kv_decode_ms": 13.618699999824457, "kv_decode_cuda_event_ms": 13.566975593566895, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 13.618699999824457, "cuda_event_ms": 13.566975593566895, "tokens_total": 8, "tokens_per_s": 587.4275812010778}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.8974853, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.6143000000047323, "prefill_cuda_event_ms": 2.5374720096588135, "kv_decode_ms": 13.618699999824457, "kv_decode_cuda_event_ms": 13.566975593566895, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 16.23299999982919, "cuda_event_ms": 16.104447603225708, "tokens_total": 17, "tokens_per_s": 1047.2494301841239}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.9142709, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.3689999998168787, "prefill_cuda_event_ms": 2.3174080848693848, "kv_decode_ms": 13.58589999972537, "kv_decode_cuda_event_ms": 13.531423568725586, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 2.3689999998168787, "cuda_event_ms": 2.3174080848693848, "tokens_total": 9, "tokens_per_s": 3799.0713384110136}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.9142709, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.3689999998168787, "prefill_cuda_event_ms": 2.3174080848693848, "kv_decode_ms": 13.58589999972537, "kv_decode_cuda_event_ms": 13.531423568725586, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 13.58589999972537, "cuda_event_ms": 13.531423568725586, "tokens_total": 8, "tokens_per_s": 588.8457886604284}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.9142709, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.3689999998168787, "prefill_cuda_event_ms": 2.3174080848693848, "kv_decode_ms": 13.58589999972537, "kv_decode_cuda_event_ms": 13.531423568725586, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 15.95489999954225, "cuda_event_ms": 15.84883165359497, "tokens_total": 17, "tokens_per_s": 1065.5033877045757}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.930944, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.4720999999772175, "prefill_cuda_event_ms": 2.382848024368286, "kv_decode_ms": 13.717599999836239, "kv_decode_cuda_event_ms": 13.674495697021484, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 2.4720999999772175, "cuda_event_ms": 2.382848024368286, "tokens_total": 9, "tokens_per_s": 3640.629424409588}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.930944, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.4720999999772175, "prefill_cuda_event_ms": 2.382848024368286, "kv_decode_ms": 13.717599999836239, "kv_decode_cuda_event_ms": 13.674495697021484, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 13.717599999836239, "cuda_event_ms": 13.674495697021484, "tokens_total": 8, "tokens_per_s": 583.1923951781291}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.930944, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.4720999999772175, "prefill_cuda_event_ms": 2.382848024368286, "kv_decode_ms": 13.717599999836239, "kv_decode_cuda_event_ms": 13.674495697021484, "gpu_peak_mb": 256.3681640625, "params_millions_measured": 25.016064, "latency_ms": 16.189699999813456, "cuda_event_ms": 16.05734372138977, "tokens_total": 17, "tokens_per_s": 1050.0503406607831}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546725.947819, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.778399999817339, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.01969999968787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.778399999817339, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 63.377782285376}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546725.947819, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 15.778399999817339, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.01969999968787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 126.01969999968787, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 63.482138110309855}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546725.947819, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 15.778399999817339, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.01969999968787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 141.7980999995052, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 63.47052605099366}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.0899546, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.525199999752658, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.78150000005917, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.525199999752658, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 68.84586787218272}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.0899546, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 14.525199999752658, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.78150000005917, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 128.78150000005917, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 62.12072386170626}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.0899546, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 14.525199999752658, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.78150000005917, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 143.30669999981183, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 62.802367230644606}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.233679, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.31640000000334, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.6319000002768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 18.31640000000334, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.59588128670577}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.233679, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 18.31640000000334, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.6319000002768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 129.6319000002768, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 61.71320485145183}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.233679, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 18.31640000000334, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.6319000002768, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 147.94830000028014, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 60.83206092927704}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.3824093, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.000999999960186, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.57890000002226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.000999999960186, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 62.49609399428087}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.3824093, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 16.000999999960186, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.57890000002226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 131.57890000002226, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.800021887997595}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.3824093, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 16.000999999960186, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.57890000002226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 147.57989999998244, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 60.983914476165594}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.5304732, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.641200000103709, "prefill_cuda_event_ms": 6.563839912414551, "kv_decode_ms": 38.50220000003901, "kv_decode_cuda_event_ms": 38.46428680419922, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 6.641200000103709, "cuda_event_ms": 6.563839912414551, "tokens_total": 1, "tokens_per_s": 150.57519725115702}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.5304732, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 6.641200000103709, "prefill_cuda_event_ms": 6.563839912414551, "kv_decode_ms": 38.50220000003901, "kv_decode_cuda_event_ms": 38.46428680419922, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 38.50220000003901, "cuda_event_ms": 38.46428680419922, "tokens_total": 8, "tokens_per_s": 207.7803346300184}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.5304732, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 6.641200000103709, "prefill_cuda_event_ms": 6.563839912414551, "kv_decode_ms": 38.50220000003901, "kv_decode_cuda_event_ms": 38.46428680419922, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 45.14340000014272, "cuda_event_ms": 45.02812671661377, "tokens_total": 9, "tokens_per_s": 199.36469118346307}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.5767646, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.1756000002569635, "prefill_cuda_event_ms": 5.135168075561523, "kv_decode_ms": 32.95040000011795, "kv_decode_cuda_event_ms": 32.90214538574219, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 5.1756000002569635, "cuda_event_ms": 5.135168075561523, "tokens_total": 1, "tokens_per_s": 193.21431330673758}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.5767646, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.1756000002569635, "prefill_cuda_event_ms": 5.135168075561523, "kv_decode_ms": 32.95040000011795, "kv_decode_cuda_event_ms": 32.90214538574219, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 32.95040000011795, "cuda_event_ms": 32.90214538574219, "tokens_total": 8, "tokens_per_s": 242.78916189094403}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.5767646, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.1756000002569635, "prefill_cuda_event_ms": 5.135168075561523, "kv_decode_ms": 32.95040000011795, "kv_decode_cuda_event_ms": 32.90214538574219, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 38.126000000374916, "cuda_event_ms": 38.03731346130371, "tokens_total": 9, "tokens_per_s": 236.05938204667413}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.6155813, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.25629999992816, "prefill_cuda_event_ms": 5.184512138366699, "kv_decode_ms": 32.81319999996413, "kv_decode_cuda_event_ms": 32.77104187011719, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 5.25629999992816, "cuda_event_ms": 5.184512138366699, "tokens_total": 1, "tokens_per_s": 190.24789300718516}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.6155813, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.25629999992816, "prefill_cuda_event_ms": 5.184512138366699, "kv_decode_ms": 32.81319999996413, "kv_decode_cuda_event_ms": 32.77104187011719, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 32.81319999996413, "cuda_event_ms": 32.77104187011719, "tokens_total": 8, "tokens_per_s": 243.80432265090712}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.6155813, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 5.25629999992816, "prefill_cuda_event_ms": 5.184512138366699, "kv_decode_ms": 32.81319999996413, "kv_decode_cuda_event_ms": 32.77104187011719, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 38.06949999989229, "cuda_event_ms": 37.95555400848389, "tokens_total": 9, "tokens_per_s": 236.40972432066258}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.6544018, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.96290000000954, "prefill_cuda_event_ms": 4.898687839508057, "kv_decode_ms": 32.45839999999589, "kv_decode_cuda_event_ms": 32.42496109008789, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 4.96290000000954, "cuda_event_ms": 4.898687839508057, "tokens_total": 1, "tokens_per_s": 201.49509359408364}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.6544018, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 4.96290000000954, "prefill_cuda_event_ms": 4.898687839508057, "kv_decode_ms": 32.45839999999589, "kv_decode_cuda_event_ms": 32.42496109008789, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 32.45839999999589, "cuda_event_ms": 32.42496109008789, "tokens_total": 8, "tokens_per_s": 246.46932689229945}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.6544018, "prompt_tokens": 1, "gen_tokens": 8, "prefill_ms": 4.96290000000954, "prefill_cuda_event_ms": 4.898687839508057, "kv_decode_ms": 32.45839999999589, "kv_decode_cuda_event_ms": 32.42496109008789, "gpu_peak_mb": 255.74267578125, "params_millions_measured": 96.08832, "latency_ms": 37.42130000000543, "cuda_event_ms": 37.32364892959595, "tokens_total": 9, "tokens_per_s": 240.50473927946635}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546726.6924896, "prompt_tokens": 99, "prefill_ms": 918.3474, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.7606, "kv_decode_ms_equiv": 300.7606, "kv_decode_ms_per_token": 37.595075, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 12245.836399999916, "ollama_total_duration_ms": 12092.774, "ollama_load_ms": 10802.077, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 918.3474, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 107.80234146685666}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546726.6924896, "prompt_tokens": 99, "prefill_ms": 918.3474, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.7606, "kv_decode_ms_equiv": 300.7606, "kv_decode_ms_per_token": 37.595075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 12245.836399999916, "ollama_total_duration_ms": 12092.774, "ollama_load_ms": 10802.077, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 300.7606, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.59922875536224}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546726.6924896, "prompt_tokens": 99, "prefill_ms": 918.3474, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.7606, "kv_decode_ms_equiv": 300.7606, "kv_decode_ms_per_token": 37.595075, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 12245.836399999916, "ollama_total_duration_ms": 12092.774, "ollama_load_ms": 10802.077, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1219.108, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 87.76909018725168}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546738.938504, "prompt_tokens": 99, "prefill_ms": 57.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.0827, "kv_decode_ms_equiv": 300.0827, "kv_decode_ms_per_token": 37.5103375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 631.7423999998937, "ollama_total_duration_ms": 604.8871, "ollama_load_ms": 242.0424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 57.0256, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1736.0624000448922}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546738.938504, "prompt_tokens": 99, "prefill_ms": 57.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.0827, "kv_decode_ms_equiv": 300.0827, "kv_decode_ms_per_token": 37.5103375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 631.7423999998937, "ollama_total_duration_ms": 604.8871, "ollama_load_ms": 242.0424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 300.0827, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.65931758145338}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546738.938504, "prompt_tokens": 99, "prefill_ms": 57.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 300.0827, "kv_decode_ms_equiv": 300.0827, "kv_decode_ms_per_token": 37.5103375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 631.7423999998937, "ollama_total_duration_ms": 604.8871, "ollama_load_ms": 242.0424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 357.1083, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 299.62899210127574}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546739.5710967, "prompt_tokens": 99, "prefill_ms": 54.9288, "prefill_cuda_event_ms": null, "kv_decode_ms": 298.5391, "kv_decode_ms_equiv": 298.5391, "kv_decode_ms_per_token": 37.3173875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 619.0903000001526, "ollama_total_duration_ms": 616.8325, "ollama_load_ms": 254.6257, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 54.9288, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1802.333202254555}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546739.5710967, "prompt_tokens": 99, "prefill_ms": 54.9288, "prefill_cuda_event_ms": null, "kv_decode_ms": 298.5391, "kv_decode_ms_equiv": 298.5391, "kv_decode_ms_per_token": 37.3173875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 619.0903000001526, "ollama_total_duration_ms": 616.8325, "ollama_load_ms": 254.6257, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 298.5391, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.797159903007678}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546739.5710967, "prompt_tokens": 99, "prefill_ms": 54.9288, "prefill_cuda_event_ms": null, "kv_decode_ms": 298.5391, "kv_decode_ms_equiv": 298.5391, "kv_decode_ms_per_token": 37.3173875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 619.0903000001526, "ollama_total_duration_ms": 616.8325, "ollama_load_ms": 254.6257, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 353.46790000000004, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 302.71489999516217}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546740.1903093, "prompt_tokens": 99, "prefill_ms": 61.5447, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.8436, "kv_decode_ms_equiv": 326.8436, "kv_decode_ms_per_token": 40.85545, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.1278000000166, "ollama_total_duration_ms": 636.2546, "ollama_load_ms": 244.1575, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.5447, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1608.5869295000218}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546740.1903093, "prompt_tokens": 99, "prefill_ms": 61.5447, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.8436, "kv_decode_ms_equiv": 326.8436, "kv_decode_ms_per_token": 40.85545, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.1278000000166, "ollama_total_duration_ms": 636.2546, "ollama_load_ms": 244.1575, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 326.8436, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 24.476538625813696}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546740.1903093, "prompt_tokens": 99, "prefill_ms": 61.5447, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.8436, "kv_decode_ms_equiv": 326.8436, "kv_decode_ms_per_token": 40.85545, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.1278000000166, "ollama_total_duration_ms": 636.2546, "ollama_load_ms": 244.1575, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 388.38829999999996, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 275.4974853773917}
