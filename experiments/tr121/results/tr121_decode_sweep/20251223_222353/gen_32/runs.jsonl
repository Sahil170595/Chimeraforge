{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546773.2028332, "prompt_tokens": 46, "prefill_ms": 22.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.2837, "kv_decode_ms_equiv": 358.2837, "kv_decode_ms_per_token": 11.196365625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 6970.548599999802, "ollama_total_duration_ms": 6967.4095, "ollama_load_ms": 6548.8437, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.2823, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2064.418843656176}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546773.2028332, "prompt_tokens": 46, "prefill_ms": 22.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.2837, "kv_decode_ms_equiv": 358.2837, "kv_decode_ms_per_token": 11.196365625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6970.548599999802, "ollama_total_duration_ms": 6967.4095, "ollama_load_ms": 6548.8437, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 358.2837, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.31469670543204}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546773.2028332, "prompt_tokens": 46, "prefill_ms": 22.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.2837, "kv_decode_ms_equiv": 358.2837, "kv_decode_ms_per_token": 11.196365625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6970.548599999802, "ollama_total_duration_ms": 6967.4095, "ollama_load_ms": 6548.8437, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 380.56600000000003, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 204.95787852829733}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546780.173544, "prompt_tokens": 46, "prefill_ms": 11.7892, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.5185, "kv_decode_ms_equiv": 358.5185, "kv_decode_ms_per_token": 11.203703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.5389999998297, "ollama_total_duration_ms": 524.142, "ollama_load_ms": 122.6279, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.7892, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3901.8762935568147}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546780.173544, "prompt_tokens": 46, "prefill_ms": 11.7892, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.5185, "kv_decode_ms_equiv": 358.5185, "kv_decode_ms_per_token": 11.203703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.5389999998297, "ollama_total_duration_ms": 524.142, "ollama_load_ms": 122.6279, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 358.5185, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.2562029574485}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546780.173544, "prompt_tokens": 46, "prefill_ms": 11.7892, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.5185, "kv_decode_ms_equiv": 358.5185, "kv_decode_ms_per_token": 11.203703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.5389999998297, "ollama_total_duration_ms": 524.142, "ollama_load_ms": 122.6279, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 370.3077, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 210.63564165692475}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546780.7151945, "prompt_tokens": 46, "prefill_ms": 12.0347, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.9841, "kv_decode_ms_equiv": 358.9841, "kv_decode_ms_per_token": 11.218253125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.2752999999575, "ollama_total_duration_ms": 525.7368, "ollama_load_ms": 122.6825, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.0347, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3822.2805720125966}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546780.7151945, "prompt_tokens": 46, "prefill_ms": 12.0347, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.9841, "kv_decode_ms_equiv": 358.9841, "kv_decode_ms_per_token": 11.218253125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.2752999999575, "ollama_total_duration_ms": 525.7368, "ollama_load_ms": 122.6825, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 358.9841, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.14043825339338}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546780.7151945, "prompt_tokens": 46, "prefill_ms": 12.0347, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.9841, "kv_decode_ms_equiv": 358.9841, "kv_decode_ms_per_token": 11.218253125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.2752999999575, "ollama_total_duration_ms": 525.7368, "ollama_load_ms": 122.6825, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 371.0188, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 210.2319343386373}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546781.2610233, "prompt_tokens": 46, "prefill_ms": 11.6372, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.5603, "kv_decode_ms_equiv": 359.5603, "kv_decode_ms_per_token": 11.236259375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.1214000001601, "ollama_total_duration_ms": 533.4573, "ollama_load_ms": 127.9428, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6372, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3952.840889561063}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546781.2610233, "prompt_tokens": 46, "prefill_ms": 11.6372, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.5603, "kv_decode_ms_equiv": 359.5603, "kv_decode_ms_per_token": 11.236259375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.1214000001601, "ollama_total_duration_ms": 533.4573, "ollama_load_ms": 127.9428, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 359.5603, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 88.99758955591037}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546781.2610233, "prompt_tokens": 46, "prefill_ms": 11.6372, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.5603, "kv_decode_ms_equiv": 359.5603, "kv_decode_ms_per_token": 11.236259375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.1214000001601, "ollama_total_duration_ms": 533.4573, "ollama_load_ms": 127.9428, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 371.1975, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 210.13072555714947}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546781.7983027, "prompt_tokens": 30, "prefill_ms": 60.148, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7189, "kv_decode_ms_equiv": 356.1036137931035, "kv_decode_ms_per_token": 11.128237931034484, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.7954999999165, "ollama_total_duration_ms": 542.8189, "ollama_load_ms": 133.4841, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 60.148, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 498.76970140320543}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546781.7983027, "prompt_tokens": 30, "prefill_ms": 60.148, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7189, "kv_decode_ms_equiv": 356.1036137931035, "kv_decode_ms_per_token": 11.128237931034484, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.7954999999165, "ollama_total_duration_ms": 542.8189, "ollama_load_ms": 133.4841, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 356.1036137931035, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.86148626560143}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546781.7983027, "prompt_tokens": 30, "prefill_ms": 60.148, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7189, "kv_decode_ms_equiv": 356.1036137931035, "kv_decode_ms_per_token": 11.128237931034484, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.7954999999165, "ollama_total_duration_ms": 542.8189, "ollama_load_ms": 133.4841, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 416.2516137931035, "cuda_event_ms": null, "tokens_total": 62, "tokens_per_s": 148.94837147902783}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546782.3452442, "prompt_tokens": 30, "prefill_ms": 13.5092, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.7743, "kv_decode_ms_equiv": 351.7509517241379, "kv_decode_ms_per_token": 10.99221724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 506.5732999996726, "ollama_total_duration_ms": 491.3307, "ollama_load_ms": 126.618, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 13.5092, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2220.7088502650045}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546782.3452442, "prompt_tokens": 30, "prefill_ms": 13.5092, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.7743, "kv_decode_ms_equiv": 351.7509517241379, "kv_decode_ms_per_token": 10.99221724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 506.5732999996726, "ollama_total_duration_ms": 491.3307, "ollama_load_ms": 126.618, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 351.7509517241379, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.9734567686291}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546782.3452442, "prompt_tokens": 30, "prefill_ms": 13.5092, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.7743, "kv_decode_ms_equiv": 351.7509517241379, "kv_decode_ms_per_token": 10.99221724137931, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 506.5732999996726, "ollama_total_duration_ms": 491.3307, "ollama_load_ms": 126.618, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 365.2601517241379, "cuda_event_ms": null, "tokens_total": 62, "tokens_per_s": 169.74203100814947}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546782.858119, "prompt_tokens": 30, "prefill_ms": 12.1023, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4569, "kv_decode_ms_equiv": 354.71106206896553, "kv_decode_ms_per_token": 11.084720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 498.781599999802, "ollama_total_duration_ms": 485.2961, "ollama_load_ms": 122.0805, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.1023, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2478.8676532559925}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546782.858119, "prompt_tokens": 30, "prefill_ms": 12.1023, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4569, "kv_decode_ms_equiv": 354.71106206896553, "kv_decode_ms_per_token": 11.084720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 498.781599999802, "ollama_total_duration_ms": 485.2961, "ollama_load_ms": 122.0805, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 354.71106206896553, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.21427133777499}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546782.858119, "prompt_tokens": 30, "prefill_ms": 12.1023, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4569, "kv_decode_ms_equiv": 354.71106206896553, "kv_decode_ms_per_token": 11.084720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 498.781599999802, "ollama_total_duration_ms": 485.2961, "ollama_load_ms": 122.0805, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 366.81336206896555, "cuda_event_ms": null, "tokens_total": 62, "tokens_per_s": 169.02328652995803}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546783.3571208, "prompt_tokens": 30, "prefill_ms": 11.9348, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.1911, "kv_decode_ms_equiv": 354.4177655172414, "kv_decode_ms_per_token": 11.075555172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 507.6162000000295, "ollama_total_duration_ms": 482.4488, "ollama_load_ms": 120.0473, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.9348, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2513.6575392968466}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546783.3571208, "prompt_tokens": 30, "prefill_ms": 11.9348, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.1911, "kv_decode_ms_equiv": 354.4177655172414, "kv_decode_ms_per_token": 11.075555172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 507.6162000000295, "ollama_total_duration_ms": 482.4488, "ollama_load_ms": 120.0473, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 354.4177655172414, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.28892768199368}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546783.3571208, "prompt_tokens": 30, "prefill_ms": 11.9348, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.1911, "kv_decode_ms_equiv": 354.4177655172414, "kv_decode_ms_per_token": 11.075555172413793, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 507.6162000000295, "ollama_total_duration_ms": 482.4488, "ollama_load_ms": 120.0473, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 366.3525655172414, "cuda_event_ms": null, "tokens_total": 62, "tokens_per_s": 169.23588323303863}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546783.8648686, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.28810000001249, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.55729999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 10586.39519999997, "params_millions_measured": 25.016064, "latency_ms": 49.28810000001249, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 344.910840547631}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546783.8648686, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 49.28810000001249, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.55729999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 10586.39519999997, "params_millions_measured": 25.016064, "latency_ms": 422.55729999988034, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 75.72937445408957}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546783.8648686, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 49.28810000001249, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.55729999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 10586.39519999997, "params_millions_measured": 25.016064, "latency_ms": 471.84539999989283, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 103.84757380279882}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546794.9347126, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.811699999991106, "prefill_cuda_event_ms": null, "kv_decode_ms": 401.5322999998716, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.811699999991106, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 858.0788120155077}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546794.9347126, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 19.811699999991106, "prefill_cuda_event_ms": null, "kv_decode_ms": 401.5322999998716, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 401.5322999998716, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 79.69470949164048}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546794.9347126, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 19.811699999991106, "prefill_cuda_event_ms": null, "kv_decode_ms": 401.5322999998716, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 421.3439999998627, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 116.29452418929893}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546795.3565679, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 14.536199999838573, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.2795999996415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.536199999838573, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1169.4940906281413}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546795.3565679, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 14.536199999838573, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.2795999996415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 318.2795999996415, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 100.54053103006301}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546795.3565679, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 14.536199999838573, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.2795999996415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 332.8157999994801, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 147.2285871045682}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546795.6898983, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 14.972599999964586, "prefill_cuda_event_ms": null, "kv_decode_ms": 336.11930000006396, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.972599999964586, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1135.4073440845416}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546795.6898983, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 14.972599999964586, "prefill_cuda_event_ms": null, "kv_decode_ms": 336.11930000006396, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 336.11930000006396, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 95.20429204747812}
{"task_idx": 2, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546795.6898983, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 14.972599999964586, "prefill_cuda_event_ms": null, "kv_decode_ms": 336.11930000006396, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 351.09190000002855, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 139.56459832880228}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546796.0416553, "prompt_tokens": 10, "prefill_ms": 8.4505, "prefill_cuda_event_ms": null, "kv_decode_ms": 24.5595, "kv_decode_ms_equiv": 71.44581818181818, "kv_decode_ms_per_token": 2.232681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1536.8555000000015, "ollama_total_duration_ms": 1464.279, "ollama_load_ms": 1393.4048, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.4505, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1183.361931246672}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546796.0416553, "prompt_tokens": 10, "prefill_ms": 8.4505, "prefill_cuda_event_ms": null, "kv_decode_ms": 24.5595, "kv_decode_ms_equiv": 71.44581818181818, "kv_decode_ms_per_token": 2.232681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1536.8555000000015, "ollama_total_duration_ms": 1464.279, "ollama_load_ms": 1393.4048, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 71.44581818181818, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 447.89185447586476}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546796.0416553, "prompt_tokens": 10, "prefill_ms": 8.4505, "prefill_cuda_event_ms": null, "kv_decode_ms": 24.5595, "kv_decode_ms_equiv": 71.44581818181818, "kv_decode_ms_per_token": 2.232681818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1536.8555000000015, "ollama_total_duration_ms": 1464.279, "ollama_load_ms": 1393.4048, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 79.89631818181819, "cuda_event_ms": null, "tokens_total": 42, "tokens_per_s": 525.6812949054996}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546797.5786483, "prompt_tokens": 10, "prefill_ms": 2.9004, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.5809, "kv_decode_ms_equiv": 68.59898181818181, "kv_decode_ms_per_token": 2.1437181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 223.64040000002205, "ollama_total_duration_ms": 194.1767, "ollama_load_ms": 154.1763, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9004, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3447.800303406427}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546797.5786483, "prompt_tokens": 10, "prefill_ms": 2.9004, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.5809, "kv_decode_ms_equiv": 68.59898181818181, "kv_decode_ms_per_token": 2.1437181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 223.64040000002205, "ollama_total_duration_ms": 194.1767, "ollama_load_ms": 154.1763, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 68.59898181818181, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 466.4792268318851}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546797.5786483, "prompt_tokens": 10, "prefill_ms": 2.9004, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.5809, "kv_decode_ms_equiv": 68.59898181818181, "kv_decode_ms_per_token": 2.1437181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 223.64040000002205, "ollama_total_duration_ms": 194.1767, "ollama_load_ms": 154.1763, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 71.49938181818182, "cuda_event_ms": null, "tokens_total": 42, "tokens_per_s": 587.4176661667259}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546797.8024118, "prompt_tokens": 10, "prefill_ms": 2.7405, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2291, "kv_decode_ms_equiv": 61.75738181818181, "kv_decode_ms_per_token": 1.9299181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 185.8012000002418, "ollama_total_duration_ms": 178.0731, "ollama_load_ms": 144.13, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.7405, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3648.9691662105456}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546797.8024118, "prompt_tokens": 10, "prefill_ms": 2.7405, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2291, "kv_decode_ms_equiv": 61.75738181818181, "kv_decode_ms_per_token": 1.9299181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 185.8012000002418, "ollama_total_duration_ms": 178.0731, "ollama_load_ms": 144.13, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 61.75738181818181, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 518.156681159352}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546797.8024118, "prompt_tokens": 10, "prefill_ms": 2.7405, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2291, "kv_decode_ms_equiv": 61.75738181818181, "kv_decode_ms_per_token": 1.9299181818181816, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 185.8012000002418, "ollama_total_duration_ms": 178.0731, "ollama_load_ms": 144.13, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 64.49788181818181, "cuda_event_ms": null, "tokens_total": 42, "tokens_per_s": 651.1841756043575}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546797.9883916, "prompt_tokens": 10, "prefill_ms": 2.9761, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.5429, "kv_decode_ms_equiv": 62.67025454545455, "kv_decode_ms_per_token": 1.9584454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 221.14209999972445, "ollama_total_duration_ms": 190.1579, "ollama_load_ms": 152.7806, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9761, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3360.102147105272}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546797.9883916, "prompt_tokens": 10, "prefill_ms": 2.9761, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.5429, "kv_decode_ms_equiv": 62.67025454545455, "kv_decode_ms_per_token": 1.9584454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 221.14209999972445, "ollama_total_duration_ms": 190.1579, "ollama_load_ms": 152.7806, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 62.67025454545455, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 510.609063775072}
{"task_idx": 3, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546797.9883916, "prompt_tokens": 10, "prefill_ms": 2.9761, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.5429, "kv_decode_ms_equiv": 62.67025454545455, "kv_decode_ms_per_token": 1.9584454545454546, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 221.14209999972445, "ollama_total_duration_ms": 190.1579, "ollama_load_ms": 152.7806, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 65.64635454545454, "cuda_event_ms": null, "tokens_total": 42, "tokens_per_s": 639.791810083202}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546798.2096794, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 186.0225000000355, "prefill_cuda_event_ms": null, "kv_decode_ms": 1034.614799999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 201.99760000014066, "params_millions_measured": 96.08832, "latency_ms": 186.0225000000355, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 48.381244204320886}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546798.2096794, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 186.0225000000355, "prefill_cuda_event_ms": null, "kv_decode_ms": 1034.614799999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 201.99760000014066, "params_millions_measured": 96.08832, "latency_ms": 1034.614799999872, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 30.9293855065711}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546798.2096794, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 186.0225000000355, "prefill_cuda_event_ms": null, "kv_decode_ms": 1034.614799999872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 201.99760000014066, "params_millions_measured": 96.08832, "latency_ms": 1220.6372999999076, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 33.58901124847086}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546799.6334274, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.92660000005344, "prefill_cuda_event_ms": null, "kv_decode_ms": 1118.280600000162, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 44.92660000005344, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 200.32675519601514}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546799.6334274, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 44.92660000005344, "prefill_cuda_event_ms": null, "kv_decode_ms": 1118.280600000162, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1118.280600000162, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 28.615358256233154}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546799.6334274, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 44.92660000005344, "prefill_cuda_event_ms": null, "kv_decode_ms": 1118.280600000162, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1163.2072000002154, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 35.24737467236483}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546800.7972178, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 50.157300000137184, "prefill_cuda_event_ms": null, "kv_decode_ms": 995.5934999998135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 50.157300000137184, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 179.43549592931407}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546800.7972178, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 50.157300000137184, "prefill_cuda_event_ms": null, "kv_decode_ms": 995.5934999998135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 995.5934999998135, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 32.141632101862854}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546800.7972178, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 50.157300000137184, "prefill_cuda_event_ms": null, "kv_decode_ms": 995.5934999998135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1045.7507999999507, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 39.20628126701116}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546801.843504, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 37.94449999986682, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.0186999998841, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 37.94449999986682, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 237.18852534706187}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546801.843504, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 37.94449999986682, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.0186999998841, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 988.0186999998841, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 32.388050954909815}
{"task_idx": 4, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546801.843504, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 37.94449999986682, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.0186999998841, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1025.963199999751, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 39.96244699615927}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546802.8699589, "prompt_tokens": 92, "prefill_ms": 794.1791, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.9006, "kv_decode_ms_equiv": 1315.9006, "kv_decode_ms_per_token": 41.12189375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 12347.535100000187, "ollama_total_duration_ms": 12212.487, "ollama_load_ms": 10019.5032, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 794.1791, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 115.84288732856356}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546802.8699589, "prompt_tokens": 92, "prefill_ms": 794.1791, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.9006, "kv_decode_ms_equiv": 1315.9006, "kv_decode_ms_per_token": 41.12189375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12347.535100000187, "ollama_total_duration_ms": 12212.487, "ollama_load_ms": 10019.5032, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1315.9006, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 24.317946203535435}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546802.8699589, "prompt_tokens": 92, "prefill_ms": 794.1791, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.9006, "kv_decode_ms_equiv": 1315.9006, "kv_decode_ms_per_token": 41.12189375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12347.535100000187, "ollama_total_duration_ms": 12212.487, "ollama_load_ms": 10019.5032, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2110.0797, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 58.765552789309346}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546815.217627, "prompt_tokens": 92, "prefill_ms": 54.2901, "prefill_cuda_event_ms": null, "kv_decode_ms": 1472.2939, "kv_decode_ms_equiv": 1472.2939, "kv_decode_ms_per_token": 46.009184375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1783.433000000059, "ollama_total_duration_ms": 1780.8134, "ollama_load_ms": 240.57, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 54.2901, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1694.5999362683067}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546815.217627, "prompt_tokens": 92, "prefill_ms": 54.2901, "prefill_cuda_event_ms": null, "kv_decode_ms": 1472.2939, "kv_decode_ms_equiv": 1472.2939, "kv_decode_ms_per_token": 46.009184375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1783.433000000059, "ollama_total_duration_ms": 1780.8134, "ollama_load_ms": 240.57, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1472.2939, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.7347908593522}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546815.217627, "prompt_tokens": 92, "prefill_ms": 54.2901, "prefill_cuda_event_ms": null, "kv_decode_ms": 1472.2939, "kv_decode_ms_equiv": 1472.2939, "kv_decode_ms_per_token": 46.009184375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1783.433000000059, "ollama_total_duration_ms": 1780.8134, "ollama_load_ms": 240.57, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1526.5839999999998, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 81.22710574721077}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546817.0020645, "prompt_tokens": 92, "prefill_ms": 55.9469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.0976, "kv_decode_ms_equiv": 1524.0976, "kv_decode_ms_per_token": 47.62805, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1856.6983999999138, "ollama_total_duration_ms": 1852.4714, "ollama_load_ms": 259.945, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.9469, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1644.4164019811642}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546817.0020645, "prompt_tokens": 92, "prefill_ms": 55.9469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.0976, "kv_decode_ms_equiv": 1524.0976, "kv_decode_ms_per_token": 47.62805, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1856.6983999999138, "ollama_total_duration_ms": 1852.4714, "ollama_load_ms": 259.945, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1524.0976, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.996030700396087}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546817.0020645, "prompt_tokens": 92, "prefill_ms": 55.9469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.0976, "kv_decode_ms_equiv": 1524.0976, "kv_decode_ms_per_token": 47.62805, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1856.6983999999138, "ollama_total_duration_ms": 1852.4714, "ollama_load_ms": 259.945, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1580.0445, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 78.4788023375291}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546818.8589602, "prompt_tokens": 92, "prefill_ms": 58.5745, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4253, "kv_decode_ms_equiv": 1638.4253, "kv_decode_ms_per_token": 51.200790625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1966.0737000003792, "ollama_total_duration_ms": 1962.7088, "ollama_load_ms": 255.7191, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.5745, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1570.6493439978146}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546818.8589602, "prompt_tokens": 92, "prefill_ms": 58.5745, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4253, "kv_decode_ms_equiv": 1638.4253, "kv_decode_ms_per_token": 51.200790625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1966.0737000003792, "ollama_total_duration_ms": 1962.7088, "ollama_load_ms": 255.7191, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1638.4253, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 19.53094840515463}
{"task_idx": 5, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546818.8589602, "prompt_tokens": 92, "prefill_ms": 58.5745, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4253, "kv_decode_ms_equiv": 1638.4253, "kv_decode_ms_per_token": 51.200790625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1966.0737000003792, "ollama_total_duration_ms": 1962.7088, "ollama_load_ms": 255.7191, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1696.9998, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 73.07013235947346}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546820.8251815, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 1140.784000000167, "prefill_cuda_event_ms": 918.2258911132812, "kv_decode_ms": 396.013000000039, "kv_decode_cuda_event_ms": 395.85791015625, "gpu_peak_mb": 61.86669921875, "hf_load_ms": 439.84550000004674, "params_millions_measured": 25.016064, "latency_ms": 1140.784000000167, "cuda_event_ms": 918.2258911132812, "tokens_total": 17, "tokens_per_s": 14.902032286565653}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546820.8251815, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 1140.784000000167, "prefill_cuda_event_ms": 918.2258911132812, "kv_decode_ms": 396.013000000039, "kv_decode_cuda_event_ms": 395.85791015625, "gpu_peak_mb": 61.86669921875, "hf_load_ms": 439.84550000004674, "params_millions_measured": 25.016064, "latency_ms": 396.013000000039, "cuda_event_ms": 395.85791015625, "tokens_total": 32, "tokens_per_s": 80.80542810462497}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546820.8251815, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 1140.784000000167, "prefill_cuda_event_ms": 918.2258911132812, "kv_decode_ms": 396.013000000039, "kv_decode_cuda_event_ms": 395.85791015625, "gpu_peak_mb": 61.86669921875, "hf_load_ms": 439.84550000004674, "params_millions_measured": 25.016064, "latency_ms": 1536.797000000206, "cuda_event_ms": 1314.0838012695312, "tokens_total": 49, "tokens_per_s": 31.884497431992273}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546822.8095152, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.1513000003069465, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 135.15610000013112, "kv_decode_cuda_event_ms": 135.03692626953125, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 5.1513000003069465, "cuda_event_ms": 5.01964807510376, "tokens_total": 17, "tokens_per_s": 3300.1378290891685}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546822.8095152, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.1513000003069465, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 135.15610000013112, "kv_decode_cuda_event_ms": 135.03692626953125, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 135.15610000013112, "cuda_event_ms": 135.03692626953125, "tokens_total": 32, "tokens_per_s": 236.763268546288}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546822.8095152, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.1513000003069465, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 135.15610000013112, "kv_decode_cuda_event_ms": 135.03692626953125, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 140.30740000043807, "cuda_event_ms": 140.056574344635, "tokens_total": 49, "tokens_per_s": 349.23318370839326}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546822.951227, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.276700000194978, "prefill_cuda_event_ms": 5.17087984085083, "kv_decode_ms": 139.69589999987875, "kv_decode_cuda_event_ms": 139.62474060058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 5.276700000194978, "cuda_event_ms": 5.17087984085083, "tokens_total": 17, "tokens_per_s": 3221.710538664665}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546822.951227, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.276700000194978, "prefill_cuda_event_ms": 5.17087984085083, "kv_decode_ms": 139.69589999987875, "kv_decode_cuda_event_ms": 139.62474060058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 139.69589999987875, "cuda_event_ms": 139.62474060058594, "tokens_total": 32, "tokens_per_s": 229.06899916194945}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546822.951227, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.276700000194978, "prefill_cuda_event_ms": 5.17087984085083, "kv_decode_ms": 139.69589999987875, "kv_decode_cuda_event_ms": 139.62474060058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 144.97260000007373, "cuda_event_ms": 144.79562044143677, "tokens_total": 49, "tokens_per_s": 337.99490386442045}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546823.0969915, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.079399999729503, "prefill_cuda_event_ms": 2.040832042694092, "kv_decode_ms": 142.11950000026263, "kv_decode_cuda_event_ms": 142.05052185058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 2.079399999729503, "cuda_event_ms": 2.040832042694092, "tokens_total": 17, "tokens_per_s": 8175.435222762061}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546823.0969915, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.079399999729503, "prefill_cuda_event_ms": 2.040832042694092, "kv_decode_ms": 142.11950000026263, "kv_decode_cuda_event_ms": 142.05052185058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 142.11950000026263, "cuda_event_ms": 142.05052185058594, "tokens_total": 32, "tokens_per_s": 225.1626272252637}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546823.0969915, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.079399999729503, "prefill_cuda_event_ms": 2.040832042694092, "kv_decode_ms": 142.11950000026263, "kv_decode_cuda_event_ms": 142.05052185058594, "gpu_peak_mb": 61.86669921875, "params_millions_measured": 25.016064, "latency_ms": 144.19889999999214, "cuda_event_ms": 144.09135389328003, "tokens_total": 49, "tokens_per_s": 339.80841740126084}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546823.2423115, "prompt_tokens": 25, "prefill_ms": 9.5214, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.0458, "kv_decode_ms_equiv": 71.0458, "kv_decode_ms_per_token": 2.22018125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 2634.084399999665, "ollama_total_duration_ms": 2559.446, "ollama_load_ms": 2422.9955, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.5214, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2625.664293066146}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546823.2423115, "prompt_tokens": 25, "prefill_ms": 9.5214, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.0458, "kv_decode_ms_equiv": 71.0458, "kv_decode_ms_per_token": 2.22018125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2634.084399999665, "ollama_total_duration_ms": 2559.446, "ollama_load_ms": 2422.9955, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 71.0458, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 450.41367681129634}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546823.2423115, "prompt_tokens": 25, "prefill_ms": 9.5214, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.0458, "kv_decode_ms_equiv": 71.0458, "kv_decode_ms_per_token": 2.22018125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2634.084399999665, "ollama_total_duration_ms": 2559.446, "ollama_load_ms": 2422.9955, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 80.5672, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 707.4839388733876}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546825.8765564, "prompt_tokens": 25, "prefill_ms": 3.9975, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2427, "kv_decode_ms_equiv": 69.2427, "kv_decode_ms_per_token": 2.163834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 291.91099999979997, "ollama_total_duration_ms": 259.178, "ollama_load_ms": 155.7611, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.9975, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 6253.908692933083}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546825.8765564, "prompt_tokens": 25, "prefill_ms": 3.9975, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2427, "kv_decode_ms_equiv": 69.2427, "kv_decode_ms_per_token": 2.163834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 291.91099999979997, "ollama_total_duration_ms": 259.178, "ollama_load_ms": 155.7611, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 69.2427, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 462.1425796509957}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546825.8765564, "prompt_tokens": 25, "prefill_ms": 3.9975, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2427, "kv_decode_ms_equiv": 69.2427, "kv_decode_ms_per_token": 2.163834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 291.91099999979997, "ollama_total_duration_ms": 259.178, "ollama_load_ms": 155.7611, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 73.2402, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 778.2611188937223}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546826.1686175, "prompt_tokens": 25, "prefill_ms": 1.9726, "prefill_cuda_event_ms": null, "kv_decode_ms": 59.3265, "kv_decode_ms_equiv": 59.3265, "kv_decode_ms_per_token": 1.853953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 287.7920000000813, "ollama_total_duration_ms": 263.581, "ollama_load_ms": 172.0416, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.9726, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 12673.628713373215}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546826.1686175, "prompt_tokens": 25, "prefill_ms": 1.9726, "prefill_cuda_event_ms": null, "kv_decode_ms": 59.3265, "kv_decode_ms_equiv": 59.3265, "kv_decode_ms_per_token": 1.853953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 287.7920000000813, "ollama_total_duration_ms": 263.581, "ollama_load_ms": 172.0416, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 59.3265, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 539.3879632204832}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546826.1686175, "prompt_tokens": 25, "prefill_ms": 1.9726, "prefill_cuda_event_ms": null, "kv_decode_ms": 59.3265, "kv_decode_ms_equiv": 59.3265, "kv_decode_ms_per_token": 1.853953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 287.7920000000813, "ollama_total_duration_ms": 263.581, "ollama_load_ms": 172.0416, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 61.2991, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 929.8668332814021}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546826.4566295, "prompt_tokens": 25, "prefill_ms": 2.8574, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.8041, "kv_decode_ms_equiv": 65.8041, "kv_decode_ms_per_token": 2.056378125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.4758999999467, "ollama_total_duration_ms": 244.6393, "ollama_load_ms": 153.7152, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.8574, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8749.212570868622}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546826.4566295, "prompt_tokens": 25, "prefill_ms": 2.8574, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.8041, "kv_decode_ms_equiv": 65.8041, "kv_decode_ms_per_token": 2.056378125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.4758999999467, "ollama_total_duration_ms": 244.6393, "ollama_load_ms": 153.7152, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 65.8041, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 486.2918875875515}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546826.4566295, "prompt_tokens": 25, "prefill_ms": 2.8574, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.8041, "kv_decode_ms_equiv": 65.8041, "kv_decode_ms_per_token": 2.056378125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.4758999999467, "ollama_total_duration_ms": 244.6393, "ollama_load_ms": 153.7152, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 68.6615, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 830.1595508399904}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546826.7322338, "prompt_tokens": 84, "prefill_ms": 947.5123, "prefill_cuda_event_ms": null, "kv_decode_ms": 1420.8922, "kv_decode_ms_equiv": 1420.8922, "kv_decode_ms_per_token": 44.40288125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 11592.093000000204, "ollama_total_duration_ms": 11456.5764, "ollama_load_ms": 8998.8773, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 947.5123, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 88.65320270776432}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546826.7322338, "prompt_tokens": 84, "prefill_ms": 947.5123, "prefill_cuda_event_ms": null, "kv_decode_ms": 1420.8922, "kv_decode_ms_equiv": 1420.8922, "kv_decode_ms_per_token": 44.40288125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 11592.093000000204, "ollama_total_duration_ms": 11456.5764, "ollama_load_ms": 8998.8773, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1420.8922, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 22.521061062901182}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546826.7322338, "prompt_tokens": 84, "prefill_ms": 947.5123, "prefill_cuda_event_ms": null, "kv_decode_ms": 1420.8922, "kv_decode_ms_equiv": 1420.8922, "kv_decode_ms_per_token": 44.40288125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 11592.093000000204, "ollama_total_duration_ms": 11456.5764, "ollama_load_ms": 8998.8773, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2368.4045, "cuda_event_ms": null, "tokens_total": 116, "tokens_per_s": 48.97812008041701}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546838.3244636, "prompt_tokens": 84, "prefill_ms": 57.6806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1468.8472, "kv_decode_ms_equiv": 1468.8472, "kv_decode_ms_per_token": 45.901475, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1818.572000000131, "ollama_total_duration_ms": 1799.9017, "ollama_load_ms": 258.8552, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 57.6806, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1456.2955309064052}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546838.3244636, "prompt_tokens": 84, "prefill_ms": 57.6806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1468.8472, "kv_decode_ms_equiv": 1468.8472, "kv_decode_ms_per_token": 45.901475, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1818.572000000131, "ollama_total_duration_ms": 1799.9017, "ollama_load_ms": 258.8552, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1468.8472, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.785792286631313}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546838.3244636, "prompt_tokens": 84, "prefill_ms": 57.6806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1468.8472, "kv_decode_ms_equiv": 1468.8472, "kv_decode_ms_per_token": 45.901475, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1818.572000000131, "ollama_total_duration_ms": 1799.9017, "ollama_load_ms": 258.8552, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1526.5277999999998, "cuda_event_ms": null, "tokens_total": 116, "tokens_per_s": 75.98944480408416}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546840.1450403, "prompt_tokens": 84, "prefill_ms": 56.7002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1514.9583, "kv_decode_ms_equiv": 1514.9583, "kv_decode_ms_per_token": 47.342446875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1840.4319999999643, "ollama_total_duration_ms": 1816.9492, "ollama_load_ms": 235.8458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.7002, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1481.4762558156763}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546840.1450403, "prompt_tokens": 84, "prefill_ms": 56.7002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1514.9583, "kv_decode_ms_equiv": 1514.9583, "kv_decode_ms_per_token": 47.342446875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1840.4319999999643, "ollama_total_duration_ms": 1816.9492, "ollama_load_ms": 235.8458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1514.9583, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.1226936081343}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546840.1450403, "prompt_tokens": 84, "prefill_ms": 56.7002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1514.9583, "kv_decode_ms_equiv": 1514.9583, "kv_decode_ms_per_token": 47.342446875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1840.4319999999643, "ollama_total_duration_ms": 1816.9492, "ollama_load_ms": 235.8458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1571.6585, "cuda_event_ms": null, "tokens_total": 116, "tokens_per_s": 73.80738245617607}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546841.9856288, "prompt_tokens": 84, "prefill_ms": 58.084, "prefill_cuda_event_ms": null, "kv_decode_ms": 1670.969, "kv_decode_ms_equiv": 1670.969, "kv_decode_ms_per_token": 52.21778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1978.979999999865, "ollama_total_duration_ms": 1976.3974, "ollama_load_ms": 237.6879, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.084, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1446.1813924660835}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546841.9856288, "prompt_tokens": 84, "prefill_ms": 58.084, "prefill_cuda_event_ms": null, "kv_decode_ms": 1670.969, "kv_decode_ms_equiv": 1670.969, "kv_decode_ms_per_token": 52.21778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1978.979999999865, "ollama_total_duration_ms": 1976.3974, "ollama_load_ms": 237.6879, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1670.969, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 19.150564732200298}
{"task_idx": 8, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546841.9856288, "prompt_tokens": 84, "prefill_ms": 58.084, "prefill_cuda_event_ms": null, "kv_decode_ms": 1670.969, "kv_decode_ms_equiv": 1670.969, "kv_decode_ms_per_token": 52.21778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1978.979999999865, "ollama_total_duration_ms": 1976.3974, "ollama_load_ms": 237.6879, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1729.053, "cuda_event_ms": null, "tokens_total": 116, "tokens_per_s": 67.08874742416802}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546843.964757, "prompt_tokens": 38, "prefill_ms": 22.2177, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.817, "kv_decode_ms_equiv": 358.817, "kv_decode_ms_per_token": 11.21303125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 6078.984699999637, "ollama_total_duration_ms": 6074.9615, "ollama_load_ms": 5656.4222, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.2177, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1710.3480558293613}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546843.964757, "prompt_tokens": 38, "prefill_ms": 22.2177, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.817, "kv_decode_ms_equiv": 358.817, "kv_decode_ms_per_token": 11.21303125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6078.984699999637, "ollama_total_duration_ms": 6074.9615, "ollama_load_ms": 5656.4222, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 358.817, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.18195068795514}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546843.964757, "prompt_tokens": 38, "prefill_ms": 22.2177, "prefill_cuda_event_ms": null, "kv_decode_ms": 358.817, "kv_decode_ms_equiv": 358.817, "kv_decode_ms_per_token": 11.21303125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6078.984699999637, "ollama_total_duration_ms": 6074.9615, "ollama_load_ms": 5656.4222, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 381.0347, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 183.71030250000854}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546850.043942, "prompt_tokens": 38, "prefill_ms": 12.4638, "prefill_cuda_event_ms": null, "kv_decode_ms": 360.9712, "kv_decode_ms_equiv": 360.9712, "kv_decode_ms_per_token": 11.28035, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 540.9105999997337, "ollama_total_duration_ms": 537.7807, "ollama_load_ms": 134.3419, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.4638, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3048.8294099712766}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546850.043942, "prompt_tokens": 38, "prefill_ms": 12.4638, "prefill_cuda_event_ms": null, "kv_decode_ms": 360.9712, "kv_decode_ms_equiv": 360.9712, "kv_decode_ms_per_token": 11.28035, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 540.9105999997337, "ollama_total_duration_ms": 537.7807, "ollama_load_ms": 134.3419, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 360.9712, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 88.64973161293754}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546850.043942, "prompt_tokens": 38, "prefill_ms": 12.4638, "prefill_cuda_event_ms": null, "kv_decode_ms": 360.9712, "kv_decode_ms_equiv": 360.9712, "kv_decode_ms_per_token": 11.28035, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 540.9105999997337, "ollama_total_duration_ms": 537.7807, "ollama_load_ms": 134.3419, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 373.435, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 187.44895363316238}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546850.5849779, "prompt_tokens": 38, "prefill_ms": 12.1526, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.1287, "kv_decode_ms_equiv": 355.1287, "kv_decode_ms_per_token": 11.097771875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.1600999999755, "ollama_total_duration_ms": 521.0337, "ollama_load_ms": 120.3819, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.1526, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3126.9028849793463}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546850.5849779, "prompt_tokens": 38, "prefill_ms": 12.1526, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.1287, "kv_decode_ms_equiv": 355.1287, "kv_decode_ms_per_token": 11.097771875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.1600999999755, "ollama_total_duration_ms": 521.0337, "ollama_load_ms": 120.3819, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 355.1287, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.10817768318923}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546850.5849779, "prompt_tokens": 38, "prefill_ms": 12.1526, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.1287, "kv_decode_ms_equiv": 355.1287, "kv_decode_ms_per_token": 11.097771875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 545.1600999999755, "ollama_total_duration_ms": 521.0337, "ollama_load_ms": 120.3819, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 367.2813, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 190.58961074250175}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546851.1302798, "prompt_tokens": 38, "prefill_ms": 12.2318, "prefill_cuda_event_ms": null, "kv_decode_ms": 350.3328, "kv_decode_ms_equiv": 350.3328, "kv_decode_ms_per_token": 10.9479, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.1074999998345, "ollama_total_duration_ms": 522.8352, "ollama_load_ms": 127.6757, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.2318, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3106.656420150755}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546851.1302798, "prompt_tokens": 38, "prefill_ms": 12.2318, "prefill_cuda_event_ms": null, "kv_decode_ms": 350.3328, "kv_decode_ms_equiv": 350.3328, "kv_decode_ms_per_token": 10.9479, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.1074999998345, "ollama_total_duration_ms": 522.8352, "ollama_load_ms": 127.6757, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 350.3328, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 91.34171850309191}
{"task_idx": 9, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546851.1302798, "prompt_tokens": 38, "prefill_ms": 12.2318, "prefill_cuda_event_ms": null, "kv_decode_ms": 350.3328, "kv_decode_ms_equiv": 350.3328, "kv_decode_ms_per_token": 10.9479, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.1074999998345, "ollama_total_duration_ms": 522.8352, "ollama_load_ms": 127.6757, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 362.56460000000004, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 193.0690420410597}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546851.6766033, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.75299999998606, "prefill_cuda_event_ms": 50.681854248046875, "kv_decode_ms": 271.68110000002343, "kv_decode_cuda_event_ms": 271.63201904296875, "gpu_peak_mb": 258.583984375, "hf_load_ms": 391.1944000001313, "params_millions_measured": 96.08832, "latency_ms": 50.75299999998606, "cuda_event_ms": 50.681854248046875, "tokens_total": 17, "tokens_per_s": 334.9555691290105}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546851.6766033, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.75299999998606, "prefill_cuda_event_ms": 50.681854248046875, "kv_decode_ms": 271.68110000002343, "kv_decode_cuda_event_ms": 271.63201904296875, "gpu_peak_mb": 258.583984375, "hf_load_ms": 391.1944000001313, "params_millions_measured": 96.08832, "latency_ms": 271.68110000002343, "cuda_event_ms": 271.63201904296875, "tokens_total": 32, "tokens_per_s": 117.78515325503776}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546851.6766033, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.75299999998606, "prefill_cuda_event_ms": 50.681854248046875, "kv_decode_ms": 271.68110000002343, "kv_decode_cuda_event_ms": 271.63201904296875, "gpu_peak_mb": 258.583984375, "hf_load_ms": 391.1944000001313, "params_millions_measured": 96.08832, "latency_ms": 322.4341000000095, "cuda_event_ms": 322.3138732910156, "tokens_total": 49, "tokens_per_s": 151.96903801427504}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546852.391324, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.741000000256463, "prefill_cuda_event_ms": 11.682815551757812, "kv_decode_ms": 276.8100999996932, "kv_decode_cuda_event_ms": 276.738037109375, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 11.741000000256463, "cuda_event_ms": 11.682815551757812, "tokens_total": 17, "tokens_per_s": 1447.9175538394227}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546852.391324, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.741000000256463, "prefill_cuda_event_ms": 11.682815551757812, "kv_decode_ms": 276.8100999996932, "kv_decode_cuda_event_ms": 276.738037109375, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 276.8100999996932, "cuda_event_ms": 276.738037109375, "tokens_total": 32, "tokens_per_s": 115.60271825354447}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546852.391324, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.741000000256463, "prefill_cuda_event_ms": 11.682815551757812, "kv_decode_ms": 276.8100999996932, "kv_decode_cuda_event_ms": 276.738037109375, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 288.55109999994966, "cuda_event_ms": 288.4208526611328, "tokens_total": 49, "tokens_per_s": 169.8139428337253}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546852.6805167, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.605600000166305, "prefill_cuda_event_ms": 11.494400024414062, "kv_decode_ms": 316.4097000003494, "kv_decode_cuda_event_ms": 316.357666015625, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 11.605600000166305, "cuda_event_ms": 11.494400024414062, "tokens_total": 17, "tokens_per_s": 1464.8100916588883}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546852.6805167, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.605600000166305, "prefill_cuda_event_ms": 11.494400024414062, "kv_decode_ms": 316.4097000003494, "kv_decode_cuda_event_ms": 316.357666015625, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 316.4097000003494, "cuda_event_ms": 316.357666015625, "tokens_total": 32, "tokens_per_s": 101.13469972622414}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546852.6805167, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.605600000166305, "prefill_cuda_event_ms": 11.494400024414062, "kv_decode_ms": 316.4097000003494, "kv_decode_cuda_event_ms": 316.357666015625, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 328.0153000005157, "cuda_event_ms": 327.85206604003906, "tokens_total": 49, "tokens_per_s": 149.38327571891605}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546853.0093727, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.298299999703886, "prefill_cuda_event_ms": 10.187775611877441, "kv_decode_ms": 303.80700000023353, "kv_decode_cuda_event_ms": 303.7470703125, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 10.298299999703886, "cuda_event_ms": 10.187775611877441, "tokens_total": 17, "tokens_per_s": 1650.7578921267407}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546853.0093727, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 10.298299999703886, "prefill_cuda_event_ms": 10.187775611877441, "kv_decode_ms": 303.80700000023353, "kv_decode_cuda_event_ms": 303.7470703125, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 303.80700000023353, "cuda_event_ms": 303.7470703125, "tokens_total": 32, "tokens_per_s": 105.33002860360493}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546853.0093727, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 10.298299999703886, "prefill_cuda_event_ms": 10.187775611877441, "kv_decode_ms": 303.80700000023353, "kv_decode_cuda_event_ms": 303.7470703125, "gpu_peak_mb": 258.583984375, "params_millions_measured": 96.08832, "latency_ms": 314.1052999999374, "cuda_event_ms": 313.93484592437744, "tokens_total": 49, "tokens_per_s": 155.9986412200296}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546853.3243356, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 89.43519999957061, "prefill_cuda_event_ms": 89.34297943115234, "kv_decode_ms": 291.7114000001675, "kv_decode_cuda_event_ms": 291.6812744140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 89.43519999957061, "cuda_event_ms": 89.34297943115234, "tokens_total": 9, "tokens_per_s": 100.63151868663803}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546853.3243356, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 89.43519999957061, "prefill_cuda_event_ms": 89.34297943115234, "kv_decode_ms": 291.7114000001675, "kv_decode_cuda_event_ms": 291.6812744140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 291.7114000001675, "cuda_event_ms": 291.6812744140625, "tokens_total": 32, "tokens_per_s": 109.69746125787894}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546853.3243356, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 89.43519999957061, "prefill_cuda_event_ms": 89.34297943115234, "kv_decode_ms": 291.7114000001675, "kv_decode_cuda_event_ms": 291.6812744140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 381.1465999997381, "cuda_event_ms": 381.02425384521484, "tokens_total": 41, "tokens_per_s": 107.57015804424904}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546853.7062786, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.592200000042794, "prefill_cuda_event_ms": 9.354240417480469, "kv_decode_ms": 276.53559999998834, "kv_decode_cuda_event_ms": 276.453369140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 9.592200000042794, "cuda_event_ms": 9.354240417480469, "tokens_total": 9, "tokens_per_s": 938.2623381455608}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546853.7062786, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 9.592200000042794, "prefill_cuda_event_ms": 9.354240417480469, "kv_decode_ms": 276.53559999998834, "kv_decode_cuda_event_ms": 276.453369140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 276.53559999998834, "cuda_event_ms": 276.453369140625, "tokens_total": 32, "tokens_per_s": 115.7174700110993}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546853.7062786, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 9.592200000042794, "prefill_cuda_event_ms": 9.354240417480469, "kv_decode_ms": 276.53559999998834, "kv_decode_cuda_event_ms": 276.453369140625, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 286.12780000003113, "cuda_event_ms": 285.80760955810547, "tokens_total": 41, "tokens_per_s": 143.29261260176585}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546853.9932904, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.253900000407157, "prefill_cuda_event_ms": 10.173439979553223, "kv_decode_ms": 279.3927999996413, "kv_decode_cuda_event_ms": 279.3287658691406, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 10.253900000407157, "cuda_event_ms": 10.173439979553223, "tokens_total": 9, "tokens_per_s": 877.714820667515}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546853.9932904, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 10.253900000407157, "prefill_cuda_event_ms": 10.173439979553223, "kv_decode_ms": 279.3927999996413, "kv_decode_cuda_event_ms": 279.3287658691406, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 279.3927999996413, "cuda_event_ms": 279.3287658691406, "tokens_total": 32, "tokens_per_s": 114.5340896402523}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546853.9932904, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 10.253900000407157, "prefill_cuda_event_ms": 10.173439979553223, "kv_decode_ms": 279.3927999996413, "kv_decode_cuda_event_ms": 279.3287658691406, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 289.64670000004844, "cuda_event_ms": 289.50220584869385, "tokens_total": 41, "tokens_per_s": 141.55175943655888}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546854.2839482, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.591199999955279, "prefill_cuda_event_ms": 8.523776054382324, "kv_decode_ms": 261.34929999989254, "kv_decode_cuda_event_ms": 261.3074035644531, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 8.591199999955279, "cuda_event_ms": 8.523776054382324, "tokens_total": 9, "tokens_per_s": 1047.5835738950145}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546854.2839482, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 8.591199999955279, "prefill_cuda_event_ms": 8.523776054382324, "kv_decode_ms": 261.34929999989254, "kv_decode_cuda_event_ms": 261.3074035644531, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 261.34929999989254, "cuda_event_ms": 261.3074035644531, "tokens_total": 32, "tokens_per_s": 122.44149879113186}
{"task_idx": 11, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546854.2839482, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 8.591199999955279, "prefill_cuda_event_ms": 8.523776054382324, "kv_decode_ms": 261.34929999989254, "kv_decode_cuda_event_ms": 261.3074035644531, "gpu_peak_mb": 257.25927734375, "params_millions_measured": 96.08832, "latency_ms": 269.9404999998478, "cuda_event_ms": 269.83117961883545, "tokens_total": 41, "tokens_per_s": 151.88532287679365}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546854.5548136, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.404299999772775, "prefill_cuda_event_ms": 15.303680419921875, "kv_decode_ms": 136.17509999994581, "kv_decode_cuda_event_ms": 136.0957489013672, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 15.404299999772775, "cuda_event_ms": 15.303680419921875, "tokens_total": 1, "tokens_per_s": 64.91693877779261}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546854.5548136, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 15.404299999772775, "prefill_cuda_event_ms": 15.303680419921875, "kv_decode_ms": 136.17509999994581, "kv_decode_cuda_event_ms": 136.0957489013672, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 136.17509999994581, "cuda_event_ms": 136.0957489013672, "tokens_total": 32, "tokens_per_s": 234.99156600591982}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546854.5548136, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 15.404299999772775, "prefill_cuda_event_ms": 15.303680419921875, "kv_decode_ms": 136.17509999994581, "kv_decode_cuda_event_ms": 136.0957489013672, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 151.5793999997186, "cuda_event_ms": 151.39942932128906, "tokens_total": 33, "tokens_per_s": 217.70768323440564}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546854.7071095, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.776900000000751, "prefill_cuda_event_ms": 5.7139201164245605, "kv_decode_ms": 142.98079999980473, "kv_decode_cuda_event_ms": 142.91148376464844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 5.776900000000751, "cuda_event_ms": 5.7139201164245605, "tokens_total": 1, "tokens_per_s": 173.10322145092871}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546854.7071095, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.776900000000751, "prefill_cuda_event_ms": 5.7139201164245605, "kv_decode_ms": 142.98079999980473, "kv_decode_cuda_event_ms": 142.91148376464844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 142.98079999980473, "cuda_event_ms": 142.91148376464844, "tokens_total": 32, "tokens_per_s": 223.80627329014598}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546854.7071095, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.776900000000751, "prefill_cuda_event_ms": 5.7139201164245605, "kv_decode_ms": 142.98079999980473, "kv_decode_cuda_event_ms": 142.91148376464844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 148.75769999980548, "cuda_event_ms": 148.625403881073, "tokens_total": 33, "tokens_per_s": 221.83725615576975}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546854.8566725, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.37850000000617, "prefill_cuda_event_ms": 5.312511920928955, "kv_decode_ms": 142.4869000002218, "kv_decode_cuda_event_ms": 142.4127960205078, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 5.37850000000617, "cuda_event_ms": 5.312511920928955, "tokens_total": 1, "tokens_per_s": 185.92544389678403}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546854.8566725, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.37850000000617, "prefill_cuda_event_ms": 5.312511920928955, "kv_decode_ms": 142.4869000002218, "kv_decode_cuda_event_ms": 142.4127960205078, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 142.4869000002218, "cuda_event_ms": 142.4127960205078, "tokens_total": 32, "tokens_per_s": 224.58204929681386}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546854.8566725, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.37850000000617, "prefill_cuda_event_ms": 5.312511920928955, "kv_decode_ms": 142.4869000002218, "kv_decode_cuda_event_ms": 142.4127960205078, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 147.86540000022796, "cuda_event_ms": 147.72530794143677, "tokens_total": 33, "tokens_per_s": 223.17594244460923}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546855.0052886, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.4258000000118045, "prefill_cuda_event_ms": 5.345151901245117, "kv_decode_ms": 139.83809999990626, "kv_decode_cuda_event_ms": 139.79136657714844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 5.4258000000118045, "cuda_event_ms": 5.345151901245117, "tokens_total": 1, "tokens_per_s": 184.304618673343}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546855.0052886, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.4258000000118045, "prefill_cuda_event_ms": 5.345151901245117, "kv_decode_ms": 139.83809999990626, "kv_decode_cuda_event_ms": 139.79136657714844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 139.83809999990626, "cuda_event_ms": 139.79136657714844, "tokens_total": 32, "tokens_per_s": 228.83606113084667}
{"task_idx": 12, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546855.0052886, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 5.4258000000118045, "prefill_cuda_event_ms": 5.345151901245117, "kv_decode_ms": 139.83809999990626, "kv_decode_cuda_event_ms": 139.79136657714844, "gpu_peak_mb": 255.671875, "params_millions_measured": 25.016064, "latency_ms": 145.26389999991807, "cuda_event_ms": 145.13651847839355, "tokens_total": 33, "tokens_per_s": 227.17275248715347}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546855.1511874, "prompt_tokens": 17, "prefill_ms": 10.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.2755, "kv_decode_ms_equiv": 72.2755, "kv_decode_ms_per_token": 2.258609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1596.4224999997896, "ollama_total_duration_ms": 1538.3413, "ollama_load_ms": 1401.2214, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 10.5373, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1613.3165042278383}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546855.1511874, "prompt_tokens": 17, "prefill_ms": 10.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.2755, "kv_decode_ms_equiv": 72.2755, "kv_decode_ms_per_token": 2.258609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1596.4224999997896, "ollama_total_duration_ms": 1538.3413, "ollama_load_ms": 1401.2214, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 72.2755, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 442.75030957931807}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546855.1511874, "prompt_tokens": 17, "prefill_ms": 10.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.2755, "kv_decode_ms_equiv": 72.2755, "kv_decode_ms_per_token": 2.258609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1596.4224999997896, "ollama_total_duration_ms": 1538.3413, "ollama_load_ms": 1401.2214, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 82.8128, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 591.6959697051664}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546856.747809, "prompt_tokens": 17, "prefill_ms": 2.3267, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.129, "kv_decode_ms_equiv": 67.129, "kv_decode_ms_per_token": 2.09778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 280.84450000005745, "ollama_total_duration_ms": 250.0489, "ollama_load_ms": 157.2935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3267, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7306.48558043581}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546856.747809, "prompt_tokens": 17, "prefill_ms": 2.3267, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.129, "kv_decode_ms_equiv": 67.129, "kv_decode_ms_per_token": 2.09778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 280.84450000005745, "ollama_total_duration_ms": 250.0489, "ollama_load_ms": 157.2935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 67.129, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 476.69412623456327}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546856.747809, "prompt_tokens": 17, "prefill_ms": 2.3267, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.129, "kv_decode_ms_equiv": 67.129, "kv_decode_ms_per_token": 2.09778125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 280.84450000005745, "ollama_total_duration_ms": 250.0489, "ollama_load_ms": 157.2935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 69.45570000000001, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 705.4856548850562}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546857.0287762, "prompt_tokens": 17, "prefill_ms": 2.7572, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.1721, "kv_decode_ms_equiv": 66.1721, "kv_decode_ms_per_token": 2.067878125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.92210000044724, "ollama_total_duration_ms": 241.0033, "ollama_load_ms": 148.0345, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.7572, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6165.675322791238}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546857.0287762, "prompt_tokens": 17, "prefill_ms": 2.7572, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.1721, "kv_decode_ms_equiv": 66.1721, "kv_decode_ms_per_token": 2.067878125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.92210000044724, "ollama_total_duration_ms": 241.0033, "ollama_load_ms": 148.0345, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 66.1721, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 483.5874938229254}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546857.0287762, "prompt_tokens": 17, "prefill_ms": 2.7572, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.1721, "kv_decode_ms_equiv": 66.1721, "kv_decode_ms_per_token": 2.067878125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.92210000044724, "ollama_total_duration_ms": 241.0033, "ollama_load_ms": 148.0345, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 68.9293, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 710.873315121436}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546857.2908988, "prompt_tokens": 17, "prefill_ms": 2.766, "prefill_cuda_event_ms": null, "kv_decode_ms": 68.1179, "kv_decode_ms_equiv": 68.1179, "kv_decode_ms_per_token": 2.128684375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.10339999980715, "ollama_total_duration_ms": 244.5372, "ollama_load_ms": 151.2691, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.766, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6146.059291395516}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546857.2908988, "prompt_tokens": 17, "prefill_ms": 2.766, "prefill_cuda_event_ms": null, "kv_decode_ms": 68.1179, "kv_decode_ms_equiv": 68.1179, "kv_decode_ms_per_token": 2.128684375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.10339999980715, "ollama_total_duration_ms": 244.5372, "ollama_load_ms": 151.2691, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 68.1179, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 469.77373054659637}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546857.2908988, "prompt_tokens": 17, "prefill_ms": 2.766, "prefill_cuda_event_ms": null, "kv_decode_ms": 68.1179, "kv_decode_ms_equiv": 68.1179, "kv_decode_ms_per_token": 2.128684375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 275.10339999980715, "ollama_total_duration_ms": 244.5372, "ollama_load_ms": 151.2691, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 70.88390000000001, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 691.2712195576145}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546857.566396, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 57.83460000020568, "prefill_cuda_event_ms": null, "kv_decode_ms": 1512.4892999997428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 57.83460000020568, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 17.290687581420872}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546857.566396, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 57.83460000020568, "prefill_cuda_event_ms": null, "kv_decode_ms": 1512.4892999997428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1512.4892999997428, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.157174467287433}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546857.566396, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 57.83460000020568, "prefill_cuda_event_ms": null, "kv_decode_ms": 1512.4892999997428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1570.3238999999485, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 21.014772812157467}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546859.137473, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 55.704099999729806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1506.0277000002316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 55.704099999729806, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 17.951999942640676}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546859.137473, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 55.704099999729806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1506.0277000002316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1506.0277000002316, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.24794915790399}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546859.137473, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 55.704099999729806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1506.0277000002316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1561.7317999999614, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 21.130388713350666}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546860.6998022, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 57.37790000011955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1696.9567999999526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 57.37790000011955, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 17.42831299155104}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546860.6998022, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 57.37790000011955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1696.9567999999526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1696.9567999999526, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 18.857286172518293}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546860.6998022, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 57.37790000011955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1696.9567999999526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1754.3347000000722, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 18.81054966307093}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546862.4546747, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 33.537299999807146, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.989500000098, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 33.537299999807146, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 29.817546433545647}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546862.4546747, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 33.537299999807146, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.989500000098, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1133.989500000098, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 28.21895617199034}
{"task_idx": 14, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546862.4546747, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 33.537299999807146, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.989500000098, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1167.526799999905, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 28.264875804138015}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546863.6227329, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 57.33250000002954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.7654999995793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 57.33250000002954, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 296.5159377315003}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546863.6227329, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 57.33250000002954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.7654999995793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1150.7654999995793, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 27.807576782595323}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546863.6227329, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 57.33250000002954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.7654999995793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1208.0979999996089, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 40.5596234742677}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546864.831696, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 53.25360000006185, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.994900000158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 53.25360000006185, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 319.22724473050187}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546864.831696, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 53.25360000006185, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.994900000158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1177.994900000158, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 27.164803514850284}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546864.831696, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 53.25360000006185, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.994900000158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1231.2485000002198, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 39.79700279837194}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546866.063597, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 56.70490000011341, "prefill_cuda_event_ms": null, "kv_decode_ms": 1032.98470000027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 56.70490000011341, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 299.79772471102143}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546866.063597, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 56.70490000011341, "prefill_cuda_event_ms": null, "kv_decode_ms": 1032.98470000027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1032.98470000027, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 30.978193578270456}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546866.063597, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 56.70490000011341, "prefill_cuda_event_ms": null, "kv_decode_ms": 1032.98470000027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1089.6896000003835, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 44.966933702939585}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546867.1538777, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.04080000026079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1200.8231999998316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 50.04080000026079, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 339.72278620468506}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546867.1538777, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.04080000026079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1200.8231999998316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1200.8231999998316, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 26.64838587396087}
{"task_idx": 15, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546867.1538777, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.04080000026079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1200.8231999998316, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1250.8640000000923, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 39.17292367515284}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546868.4051814, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.501899999748275, "prefill_cuda_event_ms": null, "kv_decode_ms": 434.69750000031127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.501899999748275, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 418.5676614673756}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546868.4051814, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.501899999748275, "prefill_cuda_event_ms": null, "kv_decode_ms": 434.69750000031127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 434.69750000031127, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 73.61441002070885}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546868.4051814, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.501899999748275, "prefill_cuda_event_ms": null, "kv_decode_ms": 434.69750000031127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 456.19940000005954, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 89.87298098155028}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546868.8618274, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.1690000001945, "prefill_cuda_event_ms": null, "kv_decode_ms": 418.783699999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.1690000001945, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 524.2005940880682}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546868.8618274, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.1690000001945, "prefill_cuda_event_ms": null, "kv_decode_ms": 418.783699999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 418.783699999949, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 76.41176101172012}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546868.8618274, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.1690000001945, "prefill_cuda_event_ms": null, "kv_decode_ms": 418.783699999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 435.9527000001435, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 94.04690004210664}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546869.298244, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 18.498100000215345, "prefill_cuda_event_ms": null, "kv_decode_ms": 428.9030000004459, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 18.498100000215345, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 486.5364550897242}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546869.298244, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 18.498100000215345, "prefill_cuda_event_ms": null, "kv_decode_ms": 428.9030000004459, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 428.9030000004459, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 74.60894421341592}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546869.298244, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 18.498100000215345, "prefill_cuda_event_ms": null, "kv_decode_ms": 428.9030000004459, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 447.40110000066124, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 91.64036476427842}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546869.7463129, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.726300000049378, "prefill_cuda_event_ms": null, "kv_decode_ms": 411.8402000003698, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.726300000049378, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 507.72016720776077}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546869.7463129, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.726300000049378, "prefill_cuda_event_ms": null, "kv_decode_ms": 411.8402000003698, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 411.8402000003698, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 77.70003996688828}
{"task_idx": 16, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546869.7463129, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.726300000049378, "prefill_cuda_event_ms": null, "kv_decode_ms": 411.8402000003698, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 429.5665000004192, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 95.44505914674443}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.1765738, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.085700000061479, "prefill_cuda_event_ms": 6.939648151397705, "kv_decode_ms": 96.32779999992636, "kv_decode_cuda_event_ms": 96.18431854248047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 7.085700000061479, "cuda_event_ms": 6.939648151397705, "tokens_total": 9, "tokens_per_s": 1270.163851125776}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.1765738, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 7.085700000061479, "prefill_cuda_event_ms": 6.939648151397705, "kv_decode_ms": 96.32779999992636, "kv_decode_cuda_event_ms": 96.18431854248047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 96.32779999992636, "cuda_event_ms": 96.18431854248047, "tokens_total": 32, "tokens_per_s": 332.19901212344166}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.1765738, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 7.085700000061479, "prefill_cuda_event_ms": 6.939648151397705, "kv_decode_ms": 96.32779999992636, "kv_decode_cuda_event_ms": 96.18431854248047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 103.41349999998783, "cuda_event_ms": 103.12396669387817, "tokens_total": 41, "tokens_per_s": 396.46661219284545}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.282159, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.656899999896268, "prefill_cuda_event_ms": 3.5983359813690186, "kv_decode_ms": 87.38100000027771, "kv_decode_cuda_event_ms": 87.34105682373047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 3.656899999896268, "cuda_event_ms": 3.5983359813690186, "tokens_total": 9, "tokens_per_s": 2461.1009325536097}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.282159, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.656899999896268, "prefill_cuda_event_ms": 3.5983359813690186, "kv_decode_ms": 87.38100000027771, "kv_decode_cuda_event_ms": 87.34105682373047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 87.38100000027771, "cuda_event_ms": 87.34105682373047, "tokens_total": 32, "tokens_per_s": 366.2123344880271}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.282159, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.656899999896268, "prefill_cuda_event_ms": 3.5983359813690186, "kv_decode_ms": 87.38100000027771, "kv_decode_cuda_event_ms": 87.34105682373047, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 91.03790000017398, "cuda_event_ms": 90.93939280509949, "tokens_total": 41, "tokens_per_s": 450.3618822481807}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.37424, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.313899999942805, "prefill_cuda_event_ms": 4.249599933624268, "kv_decode_ms": 62.41679999993721, "kv_decode_cuda_event_ms": 62.3636474609375, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 4.313899999942805, "cuda_event_ms": 4.249599933624268, "tokens_total": 9, "tokens_per_s": 2086.2792369130775}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.37424, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.313899999942805, "prefill_cuda_event_ms": 4.249599933624268, "kv_decode_ms": 62.41679999993721, "kv_decode_cuda_event_ms": 62.3636474609375, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 62.41679999993721, "cuda_event_ms": 62.3636474609375, "tokens_total": 32, "tokens_per_s": 512.6824829217805}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.37424, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.313899999942805, "prefill_cuda_event_ms": 4.249599933624268, "kv_decode_ms": 62.41679999993721, "kv_decode_cuda_event_ms": 62.3636474609375, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 66.73069999988002, "cuda_event_ms": 66.61324739456177, "tokens_total": 41, "tokens_per_s": 614.4098593312182}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.442295, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.2754999999961, "prefill_cuda_event_ms": 2.2157440185546875, "kv_decode_ms": 54.71410000018295, "kv_decode_cuda_event_ms": 54.65087890625, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 2.2754999999961, "cuda_event_ms": 2.2157440185546875, "tokens_total": 9, "tokens_per_s": 3955.1746868887826}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.442295, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 2.2754999999961, "prefill_cuda_event_ms": 2.2157440185546875, "kv_decode_ms": 54.71410000018295, "kv_decode_cuda_event_ms": 54.65087890625, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 54.71410000018295, "cuda_event_ms": 54.65087890625, "tokens_total": 32, "tokens_per_s": 584.8583820238841}
{"task_idx": 17, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.442295, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 2.2754999999961, "prefill_cuda_event_ms": 2.2157440185546875, "kv_decode_ms": 54.71410000018295, "kv_decode_cuda_event_ms": 54.65087890625, "gpu_peak_mb": 256.4736328125, "params_millions_measured": 25.016064, "latency_ms": 56.98960000017905, "cuda_event_ms": 56.86662292480469, "tokens_total": 41, "tokens_per_s": 719.4295099434139}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.5000875, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.004599999978382, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.55620000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.004599999978382, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 90.87109027151959}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.5000875, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 11.004599999978382, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.55620000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 426.55620000005, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 75.01942299747665}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.5000875, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 11.004599999978382, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.55620000005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 437.5608000000284, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 75.41809046879396}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546870.9380755, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.048000000002503, "prefill_cuda_event_ms": null, "kv_decode_ms": 409.27609999971537, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.048000000002503, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 71.1845102505568}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546870.9380755, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 14.048000000002503, "prefill_cuda_event_ms": null, "kv_decode_ms": 409.27609999971537, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 409.27609999971537, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 78.18682791402247}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546870.9380755, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 14.048000000002503, "prefill_cuda_event_ms": null, "kv_decode_ms": 409.27609999971537, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 423.3240999997179, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 77.95445617204878}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546871.3619463, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.059700000008888, "prefill_cuda_event_ms": null, "kv_decode_ms": 437.4606000001222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.059700000008888, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 82.92080234162235}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546871.3619463, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 12.059700000008888, "prefill_cuda_event_ms": null, "kv_decode_ms": 437.4606000001222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 437.4606000001222, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 73.14944477283454}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546871.3619463, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 12.059700000008888, "prefill_cuda_event_ms": null, "kv_decode_ms": 437.4606000001222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 449.5203000001311, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 73.41159008834612}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546871.811856, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.55459999967934, "prefill_cuda_event_ms": null, "kv_decode_ms": 447.0277000000351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.55459999967934, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 73.77569238661833}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546871.811856, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 13.55459999967934, "prefill_cuda_event_ms": null, "kv_decode_ms": 447.0277000000351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 447.0277000000351, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 71.58393092865943}
{"task_idx": 18, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546871.811856, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 13.55459999967934, "prefill_cuda_event_ms": null, "kv_decode_ms": 447.0277000000351, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 460.58229999971445, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 71.64843286426868}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546872.2728407, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.366199999931268, "prefill_cuda_event_ms": 9.146368026733398, "kv_decode_ms": 240.39319999974396, "kv_decode_cuda_event_ms": 240.17202758789062, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 9.366199999931268, "cuda_event_ms": 9.146368026733398, "tokens_total": 1, "tokens_per_s": 106.76688518367516}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546872.2728407, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 9.366199999931268, "prefill_cuda_event_ms": 9.146368026733398, "kv_decode_ms": 240.39319999974396, "kv_decode_cuda_event_ms": 240.17202758789062, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 240.39319999974396, "cuda_event_ms": 240.17202758789062, "tokens_total": 32, "tokens_per_s": 133.1152461884699}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546872.2728407, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 9.366199999931268, "prefill_cuda_event_ms": 9.146368026733398, "kv_decode_ms": 240.39319999974396, "kv_decode_cuda_event_ms": 240.17202758789062, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 249.75939999967522, "cuda_event_ms": 249.31839561462402, "tokens_total": 33, "tokens_per_s": 132.1271591781647}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546872.5234494, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.736800000202493, "prefill_cuda_event_ms": 7.688191890716553, "kv_decode_ms": 168.70960000005653, "kv_decode_cuda_event_ms": 168.6579132080078, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 7.736800000202493, "cuda_event_ms": 7.688191890716553, "tokens_total": 1, "tokens_per_s": 129.2524040913333}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546872.5234494, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 7.736800000202493, "prefill_cuda_event_ms": 7.688191890716553, "kv_decode_ms": 168.70960000005653, "kv_decode_cuda_event_ms": 168.6579132080078, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 168.70960000005653, "cuda_event_ms": 168.6579132080078, "tokens_total": 32, "tokens_per_s": 189.67503923896018}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546872.5234494, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 7.736800000202493, "prefill_cuda_event_ms": 7.688191890716553, "kv_decode_ms": 168.70960000005653, "kv_decode_cuda_event_ms": 168.6579132080078, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 176.44640000025902, "cuda_event_ms": 176.34610509872437, "tokens_total": 33, "tokens_per_s": 187.02563498009343}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546872.7006266, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.981999999927211, "prefill_cuda_event_ms": 4.924384117126465, "kv_decode_ms": 127.86110000024564, "kv_decode_cuda_event_ms": 127.82182312011719, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 4.981999999927211, "cuda_event_ms": 4.924384117126465, "tokens_total": 1, "tokens_per_s": 200.72260136784632}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546872.7006266, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 4.981999999927211, "prefill_cuda_event_ms": 4.924384117126465, "kv_decode_ms": 127.86110000024564, "kv_decode_cuda_event_ms": 127.82182312011719, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 127.86110000024564, "cuda_event_ms": 127.82182312011719, "tokens_total": 32, "tokens_per_s": 250.2715837728482}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546872.7006266, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 4.981999999927211, "prefill_cuda_event_ms": 4.924384117126465, "kv_decode_ms": 127.86110000024564, "kv_decode_cuda_event_ms": 127.82182312011719, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 132.84310000017285, "cuda_event_ms": 132.74620723724365, "tokens_total": 33, "tokens_per_s": 248.41335379825568}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546872.834191, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.748699999709061, "prefill_cuda_event_ms": 4.699135780334473, "kv_decode_ms": 128.1989999997677, "kv_decode_cuda_event_ms": 128.16383361816406, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 4.748699999709061, "cuda_event_ms": 4.699135780334473, "tokens_total": 1, "tokens_per_s": 210.5839493042869}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546872.834191, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 4.748699999709061, "prefill_cuda_event_ms": 4.699135780334473, "kv_decode_ms": 128.1989999997677, "kv_decode_cuda_event_ms": 128.16383361816406, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 128.1989999997677, "cuda_event_ms": 128.16383361816406, "tokens_total": 32, "tokens_per_s": 249.61193145077564}
{"task_idx": 19, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546872.834191, "prompt_tokens": 1, "gen_tokens": 32, "prefill_ms": 4.748699999709061, "prefill_cuda_event_ms": 4.699135780334473, "kv_decode_ms": 128.1989999997677, "kv_decode_cuda_event_ms": 128.16383361816406, "gpu_peak_mb": 256.30517578125, "params_millions_measured": 96.08832, "latency_ms": 132.94769999947675, "cuda_event_ms": 132.86296939849854, "tokens_total": 33, "tokens_per_s": 248.2179082460989}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546872.9677713, "prompt_tokens": 99, "prefill_ms": 911.4508, "prefill_cuda_event_ms": null, "kv_decode_ms": 1447.6512, "kv_decode_ms_equiv": 1447.6512, "kv_decode_ms_per_token": 45.2391, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 12895.786099999896, "ollama_total_duration_ms": 12755.4682, "ollama_load_ms": 10315.9651, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 911.4508, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 108.61804060076528}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546872.9677713, "prompt_tokens": 99, "prefill_ms": 911.4508, "prefill_cuda_event_ms": null, "kv_decode_ms": 1447.6512, "kv_decode_ms_equiv": 1447.6512, "kv_decode_ms_per_token": 45.2391, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12895.786099999896, "ollama_total_duration_ms": 12755.4682, "ollama_load_ms": 10315.9651, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1447.6512, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 22.1047721992701}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546872.9677713, "prompt_tokens": 99, "prefill_ms": 911.4508, "prefill_cuda_event_ms": null, "kv_decode_ms": 1447.6512, "kv_decode_ms_equiv": 1447.6512, "kv_decode_ms_per_token": 45.2391, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12895.786099999896, "ollama_total_duration_ms": 12755.4682, "ollama_load_ms": 10315.9651, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2359.102, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 55.52960406120634}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546885.8644865, "prompt_tokens": 99, "prefill_ms": 60.4869, "prefill_cuda_event_ms": null, "kv_decode_ms": 1560.1616, "kv_decode_ms_equiv": 1560.1616, "kv_decode_ms_per_token": 48.75505, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1881.942699999854, "ollama_total_duration_ms": 1878.9569, "ollama_load_ms": 242.8492, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 60.4869, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1636.7180331608995}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546885.8644865, "prompt_tokens": 99, "prefill_ms": 60.4869, "prefill_cuda_event_ms": null, "kv_decode_ms": 1560.1616, "kv_decode_ms_equiv": 1560.1616, "kv_decode_ms_per_token": 48.75505, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1881.942699999854, "ollama_total_duration_ms": 1878.9569, "ollama_load_ms": 242.8492, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1560.1616, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.51069581510018}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546885.8644865, "prompt_tokens": 99, "prefill_ms": 60.4869, "prefill_cuda_event_ms": null, "kv_decode_ms": 1560.1616, "kv_decode_ms_equiv": 1560.1616, "kv_decode_ms_per_token": 48.75505, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1881.942699999854, "ollama_total_duration_ms": 1878.9569, "ollama_load_ms": 242.8492, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1620.6485, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 80.83183984682675}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546887.746598, "prompt_tokens": 99, "prefill_ms": 63.5742, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.0724, "kv_decode_ms_equiv": 1656.0724, "kv_decode_ms_per_token": 51.7522625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1984.1609000000062, "ollama_total_duration_ms": 1980.9347, "ollama_load_ms": 249.0588, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 63.5742, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1557.235482318299}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546887.746598, "prompt_tokens": 99, "prefill_ms": 63.5742, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.0724, "kv_decode_ms_equiv": 1656.0724, "kv_decode_ms_per_token": 51.7522625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1984.1609000000062, "ollama_total_duration_ms": 1980.9347, "ollama_load_ms": 249.0588, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1656.0724, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 19.322826707334777}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546887.746598, "prompt_tokens": 99, "prefill_ms": 63.5742, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.0724, "kv_decode_ms_equiv": 1656.0724, "kv_decode_ms_per_token": 51.7522625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1984.1609000000062, "ollama_total_duration_ms": 1980.9347, "ollama_load_ms": 249.0588, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1719.6466, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 76.17844271026384}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766546889.7308743, "prompt_tokens": 99, "prefill_ms": 69.319, "prefill_cuda_event_ms": null, "kv_decode_ms": 1785.7299, "kv_decode_ms_equiv": 1785.7299, "kv_decode_ms_per_token": 55.804059375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 2114.6598000000267, "ollama_total_duration_ms": 2112.1848, "ollama_load_ms": 242.63, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 69.319, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1428.179864106522}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766546889.7308743, "prompt_tokens": 99, "prefill_ms": 69.319, "prefill_cuda_event_ms": null, "kv_decode_ms": 1785.7299, "kv_decode_ms_equiv": 1785.7299, "kv_decode_ms_per_token": 55.804059375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2114.6598000000267, "ollama_total_duration_ms": 2112.1848, "ollama_load_ms": 242.63, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1785.7299, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 17.91984330889011}
{"task_idx": 20, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766546889.7308743, "prompt_tokens": 99, "prefill_ms": 69.319, "prefill_cuda_event_ms": null, "kv_decode_ms": 1785.7299, "kv_decode_ms_equiv": 1785.7299, "kv_decode_ms_per_token": 55.804059375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2114.6598000000267, "ollama_total_duration_ms": 2112.1848, "ollama_load_ms": 242.63, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1855.0489, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 70.61808451518448}
