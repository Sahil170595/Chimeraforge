{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554417.8493376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 90.92819999932544, "prefill_cuda_event_ms": null, "kv_decode_ms": 650.5293999998685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5547.731600001498, "params_millions_measured": 45.1712, "latency_ms": 90.92819999932544, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 186.96070086206606, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554417.8493376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 90.92819999932544, "prefill_cuda_event_ms": null, "kv_decode_ms": 650.5293999998685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5547.731600001498, "params_millions_measured": 45.1712, "latency_ms": 650.5293999998685, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 98.38141058653605, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554417.8493376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 90.92819999932544, "prefill_cuda_event_ms": null, "kv_decode_ms": 650.5293999998685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5547.731600001498, "params_millions_measured": 45.1712, "latency_ms": 741.4575999991939, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 109.24427775787592, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554424.1417475, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.710200001194607, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.4824999996636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 15.710200001194607, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1082.0995276130996, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554424.1417475, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.710200001194607, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.4824999996636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 643.4824999996636, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.45880424103757, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554424.1417475, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.710200001194607, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.4824999996636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 659.1927000008582, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 122.87757434191026, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554424.8013945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.668800001047202, "prefill_cuda_event_ms": null, "kv_decode_ms": 633.8605000000825, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 17.668800001047202, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 962.1479669809175, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554424.8013945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.668800001047202, "prefill_cuda_event_ms": null, "kv_decode_ms": 633.8605000000825, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 633.8605000000825, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 100.96858851433662, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554424.8013945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.668800001047202, "prefill_cuda_event_ms": null, "kv_decode_ms": 633.8605000000825, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 651.5293000011297, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 124.32288156474245, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554425.4535332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.093300000648014, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.6779000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.093300000648014, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1056.340216072246, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554425.4535332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.093300000648014, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.6779000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 645.6779000000097, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.1206296514083, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554425.4535332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.093300000648014, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.6779000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 661.7712000006577, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 122.39879885966555, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554426.1157818, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 876.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2438.2267, "kv_decode_ms_equiv": 2438.2267, "kv_decode_ms_per_token": 38.0972921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14330.592599999363, "ollama_total_duration_ms": 14207.5476, "ollama_load_ms": 10815.1668, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 876.7628, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 104.93145922705662, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554426.1157818, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 876.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2438.2267, "kv_decode_ms_equiv": 2438.2267, "kv_decode_ms_per_token": 38.0972921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14330.592599999363, "ollama_total_duration_ms": 14207.5476, "ollama_load_ms": 10815.1668, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2438.2267, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.248584678364814, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554426.1157818, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 876.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2438.2267, "kv_decode_ms_equiv": 2438.2267, "kv_decode_ms_per_token": 38.0972921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14330.592599999363, "ollama_total_duration_ms": 14207.5476, "ollama_load_ms": 10815.1668, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3314.9895, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 47.058972584981035, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554440.446524, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.6284, "prefill_cuda_event_ms": null, "kv_decode_ms": 2649.5874, "kv_decode_ms_equiv": 2649.5874, "kv_decode_ms_per_token": 41.399803125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3226.8003999997745, "ollama_total_duration_ms": 3202.9041, "ollama_load_ms": 490.4389, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 49.6284, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1853.777272690637, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554440.446524, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.6284, "prefill_cuda_event_ms": null, "kv_decode_ms": 2649.5874, "kv_decode_ms_equiv": 2649.5874, "kv_decode_ms_per_token": 41.399803125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3226.8003999997745, "ollama_total_duration_ms": 3202.9041, "ollama_load_ms": 490.4389, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2649.5874, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.154704238101377, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554440.446524, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.6284, "prefill_cuda_event_ms": null, "kv_decode_ms": 2649.5874, "kv_decode_ms_equiv": 2649.5874, "kv_decode_ms_per_token": 41.399803125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3226.8003999997745, "ollama_total_duration_ms": 3202.9041, "ollama_load_ms": 490.4389, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2699.2158, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 57.79456388777807, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554443.6744366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.5915, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8472, "kv_decode_ms_equiv": 2962.8472, "kv_decode_ms_per_token": 46.2944875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3454.1360000002896, "ollama_total_duration_ms": 3450.4848, "ollama_load_ms": 429.2215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 50.5915, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1818.4872952966405, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554443.6744366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.5915, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8472, "kv_decode_ms_equiv": 2962.8472, "kv_decode_ms_per_token": 46.2944875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3454.1360000002896, "ollama_total_duration_ms": 3450.4848, "ollama_load_ms": 429.2215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2962.8472, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.60084394497293, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554443.6744366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.5915, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8472, "kv_decode_ms_equiv": 2962.8472, "kv_decode_ms_per_token": 46.2944875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3454.1360000002896, "ollama_total_duration_ms": 3450.4848, "ollama_load_ms": 429.2215, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3013.4387, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 51.76810133884588, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554447.1287746, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 57.1897, "prefill_cuda_event_ms": null, "kv_decode_ms": 3188.5337, "kv_decode_ms_equiv": 3188.5337, "kv_decode_ms_per_token": 49.8208390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3703.2622000006086, "ollama_total_duration_ms": 3698.925, "ollama_load_ms": 438.9958, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 57.1897, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1608.6812835178362, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554447.1287746, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 57.1897, "prefill_cuda_event_ms": null, "kv_decode_ms": 3188.5337, "kv_decode_ms_equiv": 3188.5337, "kv_decode_ms_per_token": 49.8208390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3703.2622000006086, "ollama_total_duration_ms": 3698.925, "ollama_load_ms": 438.9958, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3188.5337, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.07192208757273, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554447.1287746, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 57.1897, "prefill_cuda_event_ms": null, "kv_decode_ms": 3188.5337, "kv_decode_ms_equiv": 3188.5337, "kv_decode_ms_per_token": 49.8208390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3703.2622000006086, "ollama_total_duration_ms": 3698.925, "ollama_load_ms": 438.9958, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3245.7234, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 48.06324531535867, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554450.8321698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 666.3520000001881, "prefill_cuda_event_ms": null, "kv_decode_ms": 1176.192799999626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 307.20840000140015, "params_millions_measured": 96.08832, "latency_ms": 666.3520000001881, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 13.506375009000436, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554450.8321698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 666.3520000001881, "prefill_cuda_event_ms": null, "kv_decode_ms": 1176.192799999626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 307.20840000140015, "params_millions_measured": 96.08832, "latency_ms": 1176.192799999626, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.41284796167801, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554450.8321698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 666.3520000001881, "prefill_cuda_event_ms": null, "kv_decode_ms": 1176.192799999626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 307.20840000140015, "params_millions_measured": 96.08832, "latency_ms": 1842.5447999998141, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 39.619118080606434, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554452.9825761, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.782099999356433, "prefill_cuda_event_ms": null, "kv_decode_ms": 1206.1408999998093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 24.782099999356433, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 363.1653491929143, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554452.9825761, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.782099999356433, "prefill_cuda_event_ms": null, "kv_decode_ms": 1206.1408999998093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1206.1408999998093, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 53.06179402423889, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554452.9825761, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.782099999356433, "prefill_cuda_event_ms": null, "kv_decode_ms": 1206.1408999998093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1230.9229999991658, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 59.30509057028707, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554454.2139316, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.355700000640354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.8219000007084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 21.355700000640354, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 421.433153665304, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554454.2139316, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.355700000640354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.8219000007084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1177.8219000007084, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.337587032438016, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554454.2139316, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.355700000640354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1177.8219000007084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1199.1776000013488, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 60.875053036279105, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554455.4135213, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.35079999991285, "prefill_cuda_event_ms": null, "kv_decode_ms": 1173.672800001441, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 22.35079999991285, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 402.6701505107241, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554455.4135213, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.35079999991285, "prefill_cuda_event_ms": null, "kv_decode_ms": 1173.672800001441, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1173.672800001441, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.52967811805933, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554455.4135213, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.35079999991285, "prefill_cuda_event_ms": null, "kv_decode_ms": 1173.672800001441, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1196.0236000013538, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 61.035584916482726, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554456.6100307, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 497.3525999994308, "prefill_cuda_event_ms": 495.6201477050781, "kv_decode_ms": 406.0444999995525, "kv_decode_cuda_event_ms": 405.9689025878906, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 363.45000000073924, "params_millions_measured": 96.08832, "latency_ms": 497.3525999994308, "cuda_event_ms": 495.6201477050781, "tokens_total": 17, "tokens_per_s": 34.180981460676904, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554456.6100307, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 497.3525999994308, "prefill_cuda_event_ms": 495.6201477050781, "kv_decode_ms": 406.0444999995525, "kv_decode_cuda_event_ms": 405.9689025878906, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 363.45000000073924, "params_millions_measured": 96.08832, "latency_ms": 406.0444999995525, "cuda_event_ms": 405.9689025878906, "tokens_total": 64, "tokens_per_s": 157.61819209488255, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554456.6100307, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 497.3525999994308, "prefill_cuda_event_ms": 495.6201477050781, "kv_decode_ms": 406.0444999995525, "kv_decode_cuda_event_ms": 405.9689025878906, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 363.45000000073924, "params_millions_measured": 96.08832, "latency_ms": 903.3970999989833, "cuda_event_ms": 901.5890502929688, "tokens_total": 81, "tokens_per_s": 89.66156743262864, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554458.0111222, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.556100000831066, "prefill_cuda_event_ms": 5.487167835235596, "kv_decode_ms": 278.66790000007313, "kv_decode_cuda_event_ms": 278.640625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 5.556100000831066, "cuda_event_ms": 5.487167835235596, "tokens_total": 17, "tokens_per_s": 3059.700148927699, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554458.0111222, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.556100000831066, "prefill_cuda_event_ms": 5.487167835235596, "kv_decode_ms": 278.66790000007313, "kv_decode_cuda_event_ms": 278.640625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 278.66790000007313, "cuda_event_ms": 278.640625, "tokens_total": 64, "tokens_per_s": 229.66405531452745, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554458.0111222, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.556100000831066, "prefill_cuda_event_ms": 5.487167835235596, "kv_decode_ms": 278.66790000007313, "kv_decode_cuda_event_ms": 278.640625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 284.2240000009042, "cuda_event_ms": 284.1277928352356, "tokens_total": 81, "tokens_per_s": 284.98648952847867, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554458.2964094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.237100000158534, "prefill_cuda_event_ms": 5.193920135498047, "kv_decode_ms": 278.4584000000905, "kv_decode_cuda_event_ms": 278.43585205078125, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 5.237100000158534, "cuda_event_ms": 5.193920135498047, "tokens_total": 17, "tokens_per_s": 3246.0712989030926, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554458.2964094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.237100000158534, "prefill_cuda_event_ms": 5.193920135498047, "kv_decode_ms": 278.4584000000905, "kv_decode_cuda_event_ms": 278.43585205078125, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 278.4584000000905, "cuda_event_ms": 278.43585205078125, "tokens_total": 64, "tokens_per_s": 229.8368445698862, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554458.2964094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.237100000158534, "prefill_cuda_event_ms": 5.193920135498047, "kv_decode_ms": 278.4584000000905, "kv_decode_cuda_event_ms": 278.43585205078125, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 283.695500000249, "cuda_event_ms": 283.6297721862793, "tokens_total": 81, "tokens_per_s": 285.51739453015256, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554458.580819, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.239000000277883, "prefill_cuda_event_ms": 6.196191787719727, "kv_decode_ms": 278.4339000008913, "kv_decode_cuda_event_ms": 278.40087890625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 6.239000000277883, "cuda_event_ms": 6.196191787719727, "tokens_total": 17, "tokens_per_s": 2724.795640205614, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554458.580819, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.239000000277883, "prefill_cuda_event_ms": 6.196191787719727, "kv_decode_ms": 278.4339000008913, "kv_decode_cuda_event_ms": 278.40087890625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 278.4339000008913, "cuda_event_ms": 278.40087890625, "tokens_total": 64, "tokens_per_s": 229.85706840939673, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554458.580819, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.239000000277883, "prefill_cuda_event_ms": 6.196191787719727, "kv_decode_ms": 278.4339000008913, "kv_decode_cuda_event_ms": 278.40087890625, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 284.6729000011692, "cuda_event_ms": 284.5970706939697, "tokens_total": 81, "tokens_per_s": 284.5370950296545, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554458.8664324, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.08330000108981, "prefill_cuda_event_ms": 8.02883243560791, "kv_decode_ms": 315.5408999991778, "kv_decode_cuda_event_ms": 315.5074157714844, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 292.0106000001397, "params_millions_measured": 51.475968, "latency_ms": 8.08330000108981, "cuda_event_ms": 8.02883243560791, "tokens_total": 17, "tokens_per_s": 2103.1014558049333, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554458.8664324, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.08330000108981, "prefill_cuda_event_ms": 8.02883243560791, "kv_decode_ms": 315.5408999991778, "kv_decode_cuda_event_ms": 315.5074157714844, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 292.0106000001397, "params_millions_measured": 51.475968, "latency_ms": 315.5408999991778, "cuda_event_ms": 315.5074157714844, "tokens_total": 64, "tokens_per_s": 202.82632140608956, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554458.8664324, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.08330000108981, "prefill_cuda_event_ms": 8.02883243560791, "kv_decode_ms": 315.5408999991778, "kv_decode_cuda_event_ms": 315.5074157714844, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 292.0106000001397, "params_millions_measured": 51.475968, "latency_ms": 323.6242000002676, "cuda_event_ms": 323.5362482070923, "tokens_total": 81, "tokens_per_s": 250.2903058545468, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554459.4834185, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.901099999187863, "prefill_cuda_event_ms": 4.848959922790527, "kv_decode_ms": 295.2607999995962, "kv_decode_cuda_event_ms": 295.193603515625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 4.901099999187863, "cuda_event_ms": 4.848959922790527, "tokens_total": 17, "tokens_per_s": 3468.6090883305756, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554459.4834185, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.901099999187863, "prefill_cuda_event_ms": 4.848959922790527, "kv_decode_ms": 295.2607999995962, "kv_decode_cuda_event_ms": 295.193603515625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 295.2607999995962, "cuda_event_ms": 295.193603515625, "tokens_total": 64, "tokens_per_s": 216.75752419585507, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554459.4834185, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.901099999187863, "prefill_cuda_event_ms": 4.848959922790527, "kv_decode_ms": 295.2607999995962, "kv_decode_cuda_event_ms": 295.193603515625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 300.16189999878407, "cuda_event_ms": 300.0425634384155, "tokens_total": 81, "tokens_per_s": 269.85436859350943, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554459.784945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.02210000033665, "prefill_cuda_event_ms": 5.970143795013428, "kv_decode_ms": 291.85489999872516, "kv_decode_cuda_event_ms": 291.8041687011719, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 6.02210000033665, "cuda_event_ms": 5.970143795013428, "tokens_total": 17, "tokens_per_s": 2822.9355206737946, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554459.784945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.02210000033665, "prefill_cuda_event_ms": 5.970143795013428, "kv_decode_ms": 291.85489999872516, "kv_decode_cuda_event_ms": 291.8041687011719, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 291.85489999872516, "cuda_event_ms": 291.8041687011719, "tokens_total": 64, "tokens_per_s": 219.28704983291203, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554459.784945, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.02210000033665, "prefill_cuda_event_ms": 5.970143795013428, "kv_decode_ms": 291.85489999872516, "kv_decode_cuda_event_ms": 291.8041687011719, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 297.8769999990618, "cuda_event_ms": 297.7743124961853, "tokens_total": 81, "tokens_per_s": 271.92431775617155, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554460.0837352, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.342599999494269, "prefill_cuda_event_ms": 5.29036808013916, "kv_decode_ms": 289.9258999987069, "kv_decode_cuda_event_ms": 289.8831481933594, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.342599999494269, "cuda_event_ms": 5.29036808013916, "tokens_total": 17, "tokens_per_s": 3181.9713251243256, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554460.0837352, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.342599999494269, "prefill_cuda_event_ms": 5.29036808013916, "kv_decode_ms": 289.9258999987069, "kv_decode_cuda_event_ms": 289.8831481933594, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 289.9258999987069, "cuda_event_ms": 289.8831481933594, "tokens_total": 64, "tokens_per_s": 220.74605959759182, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554460.0837352, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.342599999494269, "prefill_cuda_event_ms": 5.29036808013916, "kv_decode_ms": 289.9258999987069, "kv_decode_cuda_event_ms": 289.8831481933594, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 295.26849999820115, "cuda_event_ms": 295.17351627349854, "tokens_total": 81, "tokens_per_s": 274.326587497459, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554460.3799818, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 94.00800000003073, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.5155000007508, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 122.69229999947129, "params_millions_measured": 51.475968, "latency_ms": 94.00800000003073, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 95.73653306098478, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554460.3799818, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 94.00800000003073, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.5155000007508, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 122.69229999947129, "params_millions_measured": 51.475968, "latency_ms": 781.5155000007508, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 81.89216976494838, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554460.3799818, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 94.00800000003073, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.5155000007508, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 122.69229999947129, "params_millions_measured": 51.475968, "latency_ms": 875.5235000007815, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.37868715109856, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554461.378874, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.670200000182376, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.6762999999482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.670200000182376, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 613.4885686553772, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554461.378874, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.670200000182376, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.6762999999482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 767.6762999999482, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 83.3684718415878, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554461.378874, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.670200000182376, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.6762999999482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 782.3465000001306, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 93.30903889771068, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554462.1616757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.033899999049027, "prefill_cuda_event_ms": null, "kv_decode_ms": 764.4908000002033, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.033899999049027, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 641.3042704173369, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554462.1616757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.033899999049027, "prefill_cuda_event_ms": null, "kv_decode_ms": 764.4908000002033, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 764.4908000002033, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 83.71585374210257, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554462.1616757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.033899999049027, "prefill_cuda_event_ms": null, "kv_decode_ms": 764.4908000002033, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 778.5246999992523, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 93.76709563623365, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554462.9406219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.392299999599345, "prefill_cuda_event_ms": null, "kv_decode_ms": 759.0459999992163, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.392299999599345, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 625.334380206815, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554462.9406219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.392299999599345, "prefill_cuda_event_ms": null, "kv_decode_ms": 759.0459999992163, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 759.0459999992163, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 84.31636554315033, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554462.9406219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.392299999599345, "prefill_cuda_event_ms": null, "kv_decode_ms": 759.0459999992163, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 773.4382999988156, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 94.38374075878035, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554463.7147405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 45.301000000108615, "prefill_cuda_event_ms": 45.223838806152344, "kv_decode_ms": 221.56549999999697, "kv_decode_cuda_event_ms": 221.517822265625, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 230.81710000042222, "params_millions_measured": 45.1712, "latency_ms": 45.301000000108615, "cuda_event_ms": 45.223838806152344, "tokens_total": 9, "tokens_per_s": 198.67111101252559, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554463.7147405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 45.301000000108615, "prefill_cuda_event_ms": 45.223838806152344, "kv_decode_ms": 221.56549999999697, "kv_decode_cuda_event_ms": 221.517822265625, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 230.81710000042222, "params_millions_measured": 45.1712, "latency_ms": 221.56549999999697, "cuda_event_ms": 221.517822265625, "tokens_total": 64, "tokens_per_s": 288.8536347039628, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554463.7147405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 45.301000000108615, "prefill_cuda_event_ms": 45.223838806152344, "kv_decode_ms": 221.56549999999697, "kv_decode_cuda_event_ms": 221.517822265625, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 230.81710000042222, "params_millions_measured": 45.1712, "latency_ms": 266.8665000001056, "cuda_event_ms": 266.74166107177734, "tokens_total": 73, "tokens_per_s": 273.54501220636956, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554464.213871, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.0430999997624895, "prefill_cuda_event_ms": 3.9987521171569824, "kv_decode_ms": 218.43540000008943, "kv_decode_cuda_event_ms": 218.380615234375, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 4.0430999997624895, "cuda_event_ms": 3.9987521171569824, "tokens_total": 9, "tokens_per_s": 2226.014691827732, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554464.213871, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.0430999997624895, "prefill_cuda_event_ms": 3.9987521171569824, "kv_decode_ms": 218.43540000008943, "kv_decode_cuda_event_ms": 218.380615234375, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 218.43540000008943, "cuda_event_ms": 218.380615234375, "tokens_total": 64, "tokens_per_s": 292.9928024485674, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554464.213871, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.0430999997624895, "prefill_cuda_event_ms": 3.9987521171569824, "kv_decode_ms": 218.43540000008943, "kv_decode_cuda_event_ms": 218.380615234375, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 222.47849999985192, "cuda_event_ms": 222.37936735153198, "tokens_total": 73, "tokens_per_s": 328.12159377220087, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554464.43707, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.84940000003553, "prefill_cuda_event_ms": 3.8023040294647217, "kv_decode_ms": 215.63849999984086, "kv_decode_cuda_event_ms": 215.6112060546875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 3.84940000003553, "cuda_event_ms": 3.8023040294647217, "tokens_total": 9, "tokens_per_s": 2338.0267054390115, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554464.43707, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.84940000003553, "prefill_cuda_event_ms": 3.8023040294647217, "kv_decode_ms": 215.63849999984086, "kv_decode_cuda_event_ms": 215.6112060546875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 215.63849999984086, "cuda_event_ms": 215.6112060546875, "tokens_total": 64, "tokens_per_s": 296.7930123797338, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554464.43707, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.84940000003553, "prefill_cuda_event_ms": 3.8023040294647217, "kv_decode_ms": 215.63849999984086, "kv_decode_cuda_event_ms": 215.6112060546875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 219.4878999998764, "cuda_event_ms": 219.41351008415222, "tokens_total": 73, "tokens_per_s": 332.59236613973303, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554464.6572711, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.101100001207669, "prefill_cuda_event_ms": 4.0590081214904785, "kv_decode_ms": 213.93150000039896, "kv_decode_cuda_event_ms": 213.8736572265625, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 4.101100001207669, "cuda_event_ms": 4.0590081214904785, "tokens_total": 9, "tokens_per_s": 2194.533173380247, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554464.6572711, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.101100001207669, "prefill_cuda_event_ms": 4.0590081214904785, "kv_decode_ms": 213.93150000039896, "kv_decode_cuda_event_ms": 213.8736572265625, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 213.93150000039896, "cuda_event_ms": 213.8736572265625, "tokens_total": 64, "tokens_per_s": 299.16118009680974, "gen_tokens": 64}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554464.6572711, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.101100001207669, "prefill_cuda_event_ms": 4.0590081214904785, "kv_decode_ms": 213.93150000039896, "kv_decode_cuda_event_ms": 213.8736572265625, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 218.03260000160662, "cuda_event_ms": 217.93266534805298, "tokens_total": 73, "tokens_per_s": 334.81231705470685, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554464.876104, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.8349, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7219, "kv_decode_ms_equiv": 143.08652972972973, "kv_decode_ms_per_token": 2.235727027027027, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 3151.7125999998825, "ollama_total_duration_ms": 3105.0392, "ollama_load_ms": 2977.5069, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 7.8349, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2169.7788101954075, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554464.876104, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.8349, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7219, "kv_decode_ms_equiv": 143.08652972972973, "kv_decode_ms_per_token": 2.235727027027027, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 3151.7125999998825, "ollama_total_duration_ms": 3105.0392, "ollama_load_ms": 2977.5069, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 143.08652972972973, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 447.28179599356395, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554464.876104, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.8349, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7219, "kv_decode_ms_equiv": 143.08652972972973, "kv_decode_ms_per_token": 2.235727027027027, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 3151.7125999998825, "ollama_total_duration_ms": 3105.0392, "ollama_load_ms": 2977.5069, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 150.92142972972974, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 536.7031053512737, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554468.0279307, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.181, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.3774, "kv_decode_ms_equiv": 127.16986666666666, "kv_decode_ms_per_token": 1.9870291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 442.547900000136, "ollama_total_duration_ms": 429.5265, "ollama_load_ms": 296.2218, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.181, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7794.589637780834, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554468.0279307, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.181, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.3774, "kv_decode_ms_equiv": 127.16986666666666, "kv_decode_ms_per_token": 1.9870291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 442.547900000136, "ollama_total_duration_ms": 429.5265, "ollama_load_ms": 296.2218, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 127.16986666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 503.26387592867917, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554468.0279307, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.181, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.3774, "kv_decode_ms_equiv": 127.16986666666666, "kv_decode_ms_per_token": 1.9870291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 442.547900000136, "ollama_total_duration_ms": 429.5265, "ollama_load_ms": 296.2218, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 129.35086666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 626.2037672212477, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554468.470618, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.723, "kv_decode_ms_equiv": 127.63066666666667, "kv_decode_ms_per_token": 1.9942291666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7705000006681, "ollama_total_duration_ms": 425.2397, "ollama_load_ms": 289.7159, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4896, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6828.40616966581, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554468.470618, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.723, "kv_decode_ms_equiv": 127.63066666666667, "kv_decode_ms_per_token": 1.9942291666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7705000006681, "ollama_total_duration_ms": 425.2397, "ollama_load_ms": 289.7159, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 127.63066666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 501.44688319421664, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554468.470618, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4896, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.723, "kv_decode_ms_equiv": 127.63066666666667, "kv_decode_ms_per_token": 1.9942291666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7705000006681, "ollama_total_duration_ms": 425.2397, "ollama_load_ms": 289.7159, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 130.12026666666668, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 622.5010298165184, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554468.9354808, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2137, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3009, "kv_decode_ms_equiv": 133.73453333333333, "kv_decode_ms_per_token": 2.0896020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 458.4092999994027, "ollama_total_duration_ms": 424.6017, "ollama_load_ms": 292.0951, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2137, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7679.450693409225, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554468.9354808, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2137, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3009, "kv_decode_ms_equiv": 133.73453333333333, "kv_decode_ms_per_token": 2.0896020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 458.4092999994027, "ollama_total_duration_ms": 424.6017, "ollama_load_ms": 292.0951, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 133.73453333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 478.5600129211204, "gen_tokens": 64}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554468.9354808, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2137, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3009, "kv_decode_ms_equiv": 133.73453333333333, "kv_decode_ms_per_token": 2.0896020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 458.4092999994027, "ollama_total_duration_ms": 424.6017, "ollama_load_ms": 292.0951, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.94823333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 595.8150246895449, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554469.3940046, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.336700000043493, "prefill_cuda_event_ms": 5.268127918243408, "kv_decode_ms": 270.5543000010948, "kv_decode_cuda_event_ms": 270.5234069824219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 5.336700000043493, "cuda_event_ms": 5.268127918243408, "tokens_total": 9, "tokens_per_s": 1686.435437616252, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554469.3940046, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.336700000043493, "prefill_cuda_event_ms": 5.268127918243408, "kv_decode_ms": 270.5543000010948, "kv_decode_cuda_event_ms": 270.5234069824219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 270.5543000010948, "cuda_event_ms": 270.5234069824219, "tokens_total": 64, "tokens_per_s": 236.55140576121326, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554469.3940046, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.336700000043493, "prefill_cuda_event_ms": 5.268127918243408, "kv_decode_ms": 270.5543000010948, "kv_decode_cuda_event_ms": 270.5234069824219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 275.8910000011383, "cuda_event_ms": 275.7915349006653, "tokens_total": 73, "tokens_per_s": 264.59725036227644, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554469.6711373, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.789200000232086, "prefill_cuda_event_ms": 4.743648052215576, "kv_decode_ms": 278.27590000015334, "kv_decode_cuda_event_ms": 278.2470703125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 4.789200000232086, "cuda_event_ms": 4.743648052215576, "tokens_total": 9, "tokens_per_s": 1879.2282635020167, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554469.6711373, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.789200000232086, "prefill_cuda_event_ms": 4.743648052215576, "kv_decode_ms": 278.27590000015334, "kv_decode_cuda_event_ms": 278.2470703125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 278.27590000015334, "cuda_event_ms": 278.2470703125, "tokens_total": 64, "tokens_per_s": 229.9875770771552, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554469.6711373, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.789200000232086, "prefill_cuda_event_ms": 4.743648052215576, "kv_decode_ms": 278.27590000015334, "kv_decode_cuda_event_ms": 278.2470703125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 283.0651000003854, "cuda_event_ms": 282.9907183647156, "tokens_total": 73, "tokens_per_s": 257.89120594485365, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554469.9548774, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.348000000594766, "prefill_cuda_event_ms": 5.304224014282227, "kv_decode_ms": 276.4728999991348, "kv_decode_cuda_event_ms": 276.4472351074219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 5.348000000594766, "cuda_event_ms": 5.304224014282227, "tokens_total": 9, "tokens_per_s": 1682.8721015331123, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554469.9548774, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.348000000594766, "prefill_cuda_event_ms": 5.304224014282227, "kv_decode_ms": 276.4728999991348, "kv_decode_cuda_event_ms": 276.4472351074219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 276.4728999991348, "cuda_event_ms": 276.4472351074219, "tokens_total": 64, "tokens_per_s": 231.48742607394894, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554469.9548774, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.348000000594766, "prefill_cuda_event_ms": 5.304224014282227, "kv_decode_ms": 276.4728999991348, "kv_decode_cuda_event_ms": 276.4472351074219, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 281.8208999997296, "cuda_event_ms": 281.7514591217041, "tokens_total": 73, "tokens_per_s": 259.02975968095353, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554470.2373405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.286899999191519, "prefill_cuda_event_ms": 5.239999771118164, "kv_decode_ms": 273.5484999993787, "kv_decode_cuda_event_ms": 273.5206298828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 5.286899999191519, "cuda_event_ms": 5.239999771118164, "tokens_total": 9, "tokens_per_s": 1702.320830992887, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554470.2373405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.286899999191519, "prefill_cuda_event_ms": 5.239999771118164, "kv_decode_ms": 273.5484999993787, "kv_decode_cuda_event_ms": 273.5206298828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 273.5484999993787, "cuda_event_ms": 273.5206298828125, "tokens_total": 64, "tokens_per_s": 233.9621675869009, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554470.2373405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.286899999191519, "prefill_cuda_event_ms": 5.239999771118164, "kv_decode_ms": 273.5484999993787, "kv_decode_cuda_event_ms": 273.5206298828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 278.8353999985702, "cuda_event_ms": 278.76062965393066, "tokens_total": 73, "tokens_per_s": 261.8032000254427, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554470.5172942, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.28420000070764, "prefill_cuda_event_ms": null, "kv_decode_ms": 635.8893999986321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.13989999936894, "params_millions_measured": 5.03672, "latency_ms": 33.28420000070764, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 510.75284968959954, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554470.5172942, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.28420000070764, "prefill_cuda_event_ms": null, "kv_decode_ms": 635.8893999986321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.13989999936894, "params_millions_measured": 5.03672, "latency_ms": 635.8893999986321, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 100.64643316925502, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554470.5172942, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.28420000070764, "prefill_cuda_event_ms": null, "kv_decode_ms": 635.8893999986321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.13989999936894, "params_millions_measured": 5.03672, "latency_ms": 669.1735999993398, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 121.04482304753193, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554471.3306527, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.654599999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 610.4107000010117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.654599999426864, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1245.0016844663012, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554471.3306527, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.654599999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 610.4107000010117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 610.4107000010117, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 104.84744123897882, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554471.3306527, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.654599999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 610.4107000010117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 624.0653000004386, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 129.79410968682777, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554471.9551563, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.876499999241787, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.3538999992743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.876499999241787, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1225.092782829163, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554471.9551563, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.876499999241787, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.3538999992743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 653.3538999992743, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 97.95610005553053, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554471.9551563, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.876499999241787, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.3538999992743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 667.2303999985161, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 121.39734640415087, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554472.6230156, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.206899999408051, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.5301000000909, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.206899999408051, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1196.6016513601367, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554472.6230156, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.206899999408051, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.5301000000909, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 616.5301000000909, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 103.80677277555559, "gen_tokens": 64}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554472.6230156, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.206899999408051, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.5301000000909, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 630.736999999499, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 128.42119615634462, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554473.2546222, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.567399999359623, "prefill_cuda_event_ms": 4.500288009643555, "kv_decode_ms": 209.78049999939685, "kv_decode_cuda_event_ms": 209.73362731933594, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 349.16940000039176, "params_millions_measured": 74.824704, "latency_ms": 4.567399999359623, "cuda_event_ms": 4.500288009643555, "tokens_total": 9, "tokens_per_s": 1970.4864914966622, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554473.2546222, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.567399999359623, "prefill_cuda_event_ms": 4.500288009643555, "kv_decode_ms": 209.78049999939685, "kv_decode_cuda_event_ms": 209.73362731933594, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 349.16940000039176, "params_millions_measured": 74.824704, "latency_ms": 209.78049999939685, "cuda_event_ms": 209.73362731933594, "tokens_total": 64, "tokens_per_s": 305.0807868232939, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554473.2546222, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.567399999359623, "prefill_cuda_event_ms": 4.500288009643555, "kv_decode_ms": 209.78049999939685, "kv_decode_cuda_event_ms": 209.73362731933594, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 349.16940000039176, "params_millions_measured": 74.824704, "latency_ms": 214.34789999875647, "cuda_event_ms": 214.2339153289795, "tokens_total": 73, "tokens_per_s": 340.5678338832501, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554473.8197632, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.035399999338551, "prefill_cuda_event_ms": 3.9846079349517822, "kv_decode_ms": 200.0313999997161, "kv_decode_cuda_event_ms": 199.98822021484375, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 4.035399999338551, "cuda_event_ms": 3.9846079349517822, "tokens_total": 9, "tokens_per_s": 2230.262180075136, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554473.8197632, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.035399999338551, "prefill_cuda_event_ms": 3.9846079349517822, "kv_decode_ms": 200.0313999997161, "kv_decode_cuda_event_ms": 199.98822021484375, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 200.0313999997161, "cuda_event_ms": 199.98822021484375, "tokens_total": 64, "tokens_per_s": 319.9497678868959, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554473.8197632, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.035399999338551, "prefill_cuda_event_ms": 3.9846079349517822, "kv_decode_ms": 200.0313999997161, "kv_decode_cuda_event_ms": 199.98822021484375, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 204.06679999905464, "cuda_event_ms": 203.97282814979553, "tokens_total": 73, "tokens_per_s": 357.72599952730275, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554474.0252142, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.397499998958665, "prefill_cuda_event_ms": 4.337696075439453, "kv_decode_ms": 212.22670000133803, "kv_decode_cuda_event_ms": 212.1912384033203, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 4.397499998958665, "cuda_event_ms": 4.337696075439453, "tokens_total": 9, "tokens_per_s": 2046.6173967325103, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554474.0252142, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.397499998958665, "prefill_cuda_event_ms": 4.337696075439453, "kv_decode_ms": 212.22670000133803, "kv_decode_cuda_event_ms": 212.1912384033203, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 212.22670000133803, "cuda_event_ms": 212.1912384033203, "tokens_total": 64, "tokens_per_s": 301.5643177771529, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554474.0252142, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.397499998958665, "prefill_cuda_event_ms": 4.337696075439453, "kv_decode_ms": 212.22670000133803, "kv_decode_cuda_event_ms": 212.1912384033203, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 216.6242000002967, "cuda_event_ms": 216.52893447875977, "tokens_total": 73, "tokens_per_s": 336.9891267914666, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554474.2431638, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.373799998575123, "prefill_cuda_event_ms": 4.304096221923828, "kv_decode_ms": 210.10349999960454, "kv_decode_cuda_event_ms": 210.0674591064453, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 4.373799998575123, "cuda_event_ms": 4.304096221923828, "tokens_total": 9, "tokens_per_s": 2057.707257517943, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554474.2431638, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.373799998575123, "prefill_cuda_event_ms": 4.304096221923828, "kv_decode_ms": 210.10349999960454, "kv_decode_cuda_event_ms": 210.0674591064453, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 210.10349999960454, "cuda_event_ms": 210.0674591064453, "tokens_total": 64, "tokens_per_s": 304.6117746735321, "gen_tokens": 64}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554474.2431638, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.373799998575123, "prefill_cuda_event_ms": 4.304096221923828, "kv_decode_ms": 210.10349999960454, "kv_decode_cuda_event_ms": 210.0674591064453, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 214.47729999817966, "cuda_event_ms": 214.37155532836914, "tokens_total": 73, "tokens_per_s": 340.3623600288682, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554474.4585514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 41.64579999996931, "prefill_cuda_event_ms": null, "kv_decode_ms": 1416.333900000609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 41.64579999996931, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 408.2044287782328, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554474.4585514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 41.64579999996931, "prefill_cuda_event_ms": null, "kv_decode_ms": 1416.333900000609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1416.333900000609, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 45.18708476862164, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554474.4585514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 41.64579999996931, "prefill_cuda_event_ms": null, "kv_decode_ms": 1416.333900000609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1457.9797000005783, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 55.55632907643904, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554475.9172506, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.403100000010454, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.3253999986337, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 32.403100000010454, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 524.6411608764136, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554475.9172506, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.403100000010454, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.3253999986337, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1349.3253999986337, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.43110890824764, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554475.9172506, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.403100000010454, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.3253999986337, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1381.728499998644, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 58.62222571227233, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554477.299477, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.84390000003623, "prefill_cuda_event_ms": null, "kv_decode_ms": 1301.1791999997513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 30.84390000003623, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 551.1624664838115, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554477.299477, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.84390000003623, "prefill_cuda_event_ms": null, "kv_decode_ms": 1301.1791999997513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1301.1791999997513, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.186153605907805, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554477.299477, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.84390000003623, "prefill_cuda_event_ms": null, "kv_decode_ms": 1301.1791999997513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1332.0230999997875, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 60.809756227210265, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554478.6319726, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 29.341899999053567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.1918000009318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 29.341899999053567, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 579.3762503637577, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554478.6319726, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 29.341899999053567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.1918000009318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1318.1918000009318, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 48.55135648693518, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554478.6319726, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 29.341899999053567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.1918000009318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1347.5336999999854, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 60.10981395122131, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554479.9802303, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 962.9263, "prefill_cuda_event_ms": null, "kv_decode_ms": 2710.9213, "kv_decode_ms_equiv": 2710.9213, "kv_decode_ms_per_token": 42.3581453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14372.311799999807, "ollama_total_duration_ms": 14218.6403, "ollama_load_ms": 10446.8591, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 962.9263, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 102.81160666190134, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554479.9802303, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 962.9263, "prefill_cuda_event_ms": null, "kv_decode_ms": 2710.9213, "kv_decode_ms_equiv": 2710.9213, "kv_decode_ms_per_token": 42.3581453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14372.311799999807, "ollama_total_duration_ms": 14218.6403, "ollama_load_ms": 10446.8591, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2710.9213, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.608210242031003, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554479.9802303, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 962.9263, "prefill_cuda_event_ms": null, "kv_decode_ms": 2710.9213, "kv_decode_ms_equiv": 2710.9213, "kv_decode_ms_per_token": 42.3581453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14372.311799999807, "ollama_total_duration_ms": 14218.6403, "ollama_load_ms": 10446.8591, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3673.8476, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 44.367654227137784, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554494.352663, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.1555, "prefill_cuda_event_ms": null, "kv_decode_ms": 3060.4585, "kv_decode_ms_equiv": 3060.4585, "kv_decode_ms_per_token": 47.8196640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3664.5828999990044, "ollama_total_duration_ms": 3640.1363, "ollama_load_ms": 493.5063, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.1555, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1794.9252567740298, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554494.352663, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.1555, "prefill_cuda_event_ms": null, "kv_decode_ms": 3060.4585, "kv_decode_ms_equiv": 3060.4585, "kv_decode_ms_per_token": 47.8196640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3664.5828999990044, "ollama_total_duration_ms": 3640.1363, "ollama_load_ms": 493.5063, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3060.4585, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.9118993118188, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554494.352663, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.1555, "prefill_cuda_event_ms": null, "kv_decode_ms": 3060.4585, "kv_decode_ms_equiv": 3060.4585, "kv_decode_ms_per_token": 47.8196640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3664.5828999990044, "ollama_total_duration_ms": 3640.1363, "ollama_load_ms": 493.5063, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3115.614, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 52.317135563006204, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554498.0174627, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.6379, "prefill_cuda_event_ms": null, "kv_decode_ms": 3399.1448, "kv_decode_ms_equiv": 3399.1448, "kv_decode_ms_per_token": 53.1116375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3958.5444000003918, "ollama_total_duration_ms": 3931.3534, "ollama_load_ms": 460.501, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.6379, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1688.3278562158605, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554498.0174627, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.6379, "prefill_cuda_event_ms": null, "kv_decode_ms": 3399.1448, "kv_decode_ms_equiv": 3399.1448, "kv_decode_ms_per_token": 53.1116375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3958.5444000003918, "ollama_total_duration_ms": 3931.3534, "ollama_load_ms": 460.501, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3399.1448, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.82826527425369, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554498.0174627, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.6379, "prefill_cuda_event_ms": null, "kv_decode_ms": 3399.1448, "kv_decode_ms_equiv": 3399.1448, "kv_decode_ms_per_token": 53.1116375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3958.5444000003918, "ollama_total_duration_ms": 3931.3534, "ollama_load_ms": 460.501, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3457.7827, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 47.14003572289259, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554501.9780712, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.2113, "prefill_cuda_event_ms": null, "kv_decode_ms": 3311.3928, "kv_decode_ms_equiv": 3311.3928, "kv_decode_ms_per_token": 51.7405125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3869.5599999991828, "ollama_total_duration_ms": 3850.5243, "ollama_load_ms": 464.1351, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.2113, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1700.700723055489, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554501.9780712, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.2113, "prefill_cuda_event_ms": null, "kv_decode_ms": 3311.3928, "kv_decode_ms_equiv": 3311.3928, "kv_decode_ms_per_token": 51.7405125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3869.5599999991828, "ollama_total_duration_ms": 3850.5243, "ollama_load_ms": 464.1351, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3311.3928, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.327214820301595, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554501.9780712, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 58.2113, "prefill_cuda_event_ms": null, "kv_decode_ms": 3311.3928, "kv_decode_ms_equiv": 3311.3928, "kv_decode_ms_per_token": 51.7405125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3869.5599999991828, "ollama_total_duration_ms": 3850.5243, "ollama_load_ms": 464.1351, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3369.6041, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 48.37363534784398, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554505.8477826, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.488500000108615, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.6263000006147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 37.488500000108615, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 453.4723982008015, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554505.8477826, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.488500000108615, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.6263000006147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 773.6263000006147, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.7272806003999, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554505.8477826, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.488500000108615, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.6263000006147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 811.1148000007233, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 99.86255952909227, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554506.6601717, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.040299999687704, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.171600000438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 19.040299999687704, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 892.8430749661944, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554506.6601717, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.040299999687704, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.171600000438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 767.171600000438, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 83.42331754716083, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554506.6601717, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.040299999687704, "prefill_cuda_event_ms": null, "kv_decode_ms": 767.171600000438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 786.2119000001258, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 103.0256601305412, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554507.4468462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.176599999103928, "prefill_cuda_event_ms": null, "kv_decode_ms": 780.246199999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.176599999103928, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 842.5601935288896, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554507.4468462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.176599999103928, "prefill_cuda_event_ms": null, "kv_decode_ms": 780.246199999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 780.246199999965, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.02539147259272, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554507.4468462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.176599999103928, "prefill_cuda_event_ms": null, "kv_decode_ms": 780.246199999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 800.422799999069, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 101.19651764054474, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554508.2477071, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.733299999847077, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.3838999997242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.733299999847077, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 819.9370095510791, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554508.2477071, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.733299999847077, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.3838999997242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 775.3838999997242, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.53975869246545, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554508.2477071, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.733299999847077, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.3838999997242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 796.1171999995713, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 101.74381359935902, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554509.0443113, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.460300001592259, "prefill_cuda_event_ms": 9.380576133728027, "kv_decode_ms": 364.56530000032217, "kv_decode_cuda_event_ms": 364.5030517578125, "gpu_peak_mb": 579.873046875, "hf_load_ms": 165.19400000106543, "params_millions_measured": 5.03672, "latency_ms": 9.460300001592259, "cuda_event_ms": 9.380576133728027, "tokens_total": 17, "tokens_per_s": 1796.9831820490615, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554509.0443113, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.460300001592259, "prefill_cuda_event_ms": 9.380576133728027, "kv_decode_ms": 364.56530000032217, "kv_decode_cuda_event_ms": 364.5030517578125, "gpu_peak_mb": 579.873046875, "hf_load_ms": 165.19400000106543, "params_millions_measured": 5.03672, "latency_ms": 364.56530000032217, "cuda_event_ms": 364.5030517578125, "tokens_total": 64, "tokens_per_s": 175.5515404234672, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554509.0443113, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.460300001592259, "prefill_cuda_event_ms": 9.380576133728027, "kv_decode_ms": 364.56530000032217, "kv_decode_cuda_event_ms": 364.5030517578125, "gpu_peak_mb": 579.873046875, "hf_load_ms": 165.19400000106543, "params_millions_measured": 5.03672, "latency_ms": 374.0256000019144, "cuda_event_ms": 373.8836278915405, "tokens_total": 81, "tokens_per_s": 216.5627165616081, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554509.5847576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.825599999501719, "prefill_cuda_event_ms": 6.775360107421875, "kv_decode_ms": 384.90660000024945, "kv_decode_cuda_event_ms": 384.869384765625, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 6.825599999501719, "cuda_event_ms": 6.775360107421875, "tokens_total": 17, "tokens_per_s": 2490.6235351091523, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554509.5847576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.825599999501719, "prefill_cuda_event_ms": 6.775360107421875, "kv_decode_ms": 384.90660000024945, "kv_decode_cuda_event_ms": 384.869384765625, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 384.90660000024945, "cuda_event_ms": 384.869384765625, "tokens_total": 64, "tokens_per_s": 166.2741038993837, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554509.5847576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.825599999501719, "prefill_cuda_event_ms": 6.775360107421875, "kv_decode_ms": 384.90660000024945, "kv_decode_cuda_event_ms": 384.869384765625, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 391.7321999997512, "cuda_event_ms": 391.6447448730469, "tokens_total": 81, "tokens_per_s": 206.7739134032164, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554509.9781115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.437000000005355, "prefill_cuda_event_ms": 6.37011194229126, "kv_decode_ms": 370.98879999939527, "kv_decode_cuda_event_ms": 370.9542541503906, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 6.437000000005355, "cuda_event_ms": 6.37011194229126, "tokens_total": 17, "tokens_per_s": 2640.98182382878, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554509.9781115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.437000000005355, "prefill_cuda_event_ms": 6.37011194229126, "kv_decode_ms": 370.98879999939527, "kv_decode_cuda_event_ms": 370.9542541503906, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 370.98879999939527, "cuda_event_ms": 370.9542541503906, "tokens_total": 64, "tokens_per_s": 172.51194645257303, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554509.9781115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.437000000005355, "prefill_cuda_event_ms": 6.37011194229126, "kv_decode_ms": 370.98879999939527, "kv_decode_cuda_event_ms": 370.9542541503906, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 377.4257999994006, "cuda_event_ms": 377.3243660926819, "tokens_total": 81, "tokens_per_s": 214.61171970789658, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554510.3562586, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.641699999818229, "prefill_cuda_event_ms": 6.590208053588867, "kv_decode_ms": 367.1431000002485, "kv_decode_cuda_event_ms": 367.1050109863281, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 6.641699999818229, "cuda_event_ms": 6.590208053588867, "tokens_total": 17, "tokens_per_s": 2559.5856483227576, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554510.3562586, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.641699999818229, "prefill_cuda_event_ms": 6.590208053588867, "kv_decode_ms": 367.1431000002485, "kv_decode_cuda_event_ms": 367.1050109863281, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 367.1431000002485, "cuda_event_ms": 367.1050109863281, "tokens_total": 64, "tokens_per_s": 174.31895083948652, "gen_tokens": 64}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554510.3562586, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.641699999818229, "prefill_cuda_event_ms": 6.590208053588867, "kv_decode_ms": 367.1431000002485, "kv_decode_cuda_event_ms": 367.1050109863281, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 373.78480000006675, "cuda_event_ms": 373.695219039917, "tokens_total": 81, "tokens_per_s": 216.7022308022839, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554510.7307804, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.0944, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4303, "kv_decode_ms_equiv": 136.4303, "kv_decode_ms_per_token": 2.1317234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2045.697300000029, "ollama_total_duration_ms": 1991.195, "ollama_load_ms": 1787.1059, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.0944, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2748.944405348346, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554510.7307804, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.0944, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4303, "kv_decode_ms_equiv": 136.4303, "kv_decode_ms_per_token": 2.1317234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2045.697300000029, "ollama_total_duration_ms": 1991.195, "ollama_load_ms": 1787.1059, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.4303, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 469.1040040225669, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554510.7307804, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.0944, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4303, "kv_decode_ms_equiv": 136.4303, "kv_decode_ms_per_token": 2.1317234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2045.697300000029, "ollama_total_duration_ms": 1991.195, "ollama_load_ms": 1787.1059, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 145.5247, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 611.5800273080789, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554512.7765918, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3235, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.7139, "kv_decode_ms_equiv": 126.7139, "kv_decode_ms_per_token": 1.9799046875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 516.5085000007821, "ollama_total_duration_ms": 463.7852, "ollama_load_ms": 292.0631, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3235, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10759.629868732516, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554512.7765918, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3235, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.7139, "kv_decode_ms_equiv": 126.7139, "kv_decode_ms_per_token": 1.9799046875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 516.5085000007821, "ollama_total_duration_ms": 463.7852, "ollama_load_ms": 292.0631, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 126.7139, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 505.074818153336, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554512.7765918, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3235, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.7139, "kv_decode_ms_equiv": 126.7139, "kv_decode_ms_per_token": 1.9799046875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 516.5085000007821, "ollama_total_duration_ms": 463.7852, "ollama_load_ms": 292.0631, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.0374, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 689.7225145577949, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554513.2932673, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2984, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7015, "kv_decode_ms_equiv": 123.7015, "kv_decode_ms_per_token": 1.9328359375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.0091999990982, "ollama_total_duration_ms": 471.7357, "ollama_load_ms": 306.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2984, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10877.1319178559, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554513.2932673, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2984, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7015, "kv_decode_ms_equiv": 123.7015, "kv_decode_ms_per_token": 1.9328359375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.0091999990982, "ollama_total_duration_ms": 471.7357, "ollama_load_ms": 306.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 123.7015, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 517.3744861622536, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554513.2932673, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2984, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7015, "kv_decode_ms_equiv": 123.7015, "kv_decode_ms_per_token": 1.9328359375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.0091999990982, "ollama_total_duration_ms": 471.7357, "ollama_load_ms": 306.06, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 125.9999, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 706.3497669442595, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554513.7844985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2485, "kv_decode_ms_equiv": 129.2485, "kv_decode_ms_per_token": 2.0195078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 556.365000000369, "ollama_total_duration_ms": 502.5373, "ollama_load_ms": 298.6779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3132, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10807.539339443196, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554513.7844985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2485, "kv_decode_ms_equiv": 129.2485, "kv_decode_ms_per_token": 2.0195078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 556.365000000369, "ollama_total_duration_ms": 502.5373, "ollama_load_ms": 298.6779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.2485, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 495.17015671361753, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554513.7844985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2485, "kv_decode_ms_equiv": 129.2485, "kv_decode_ms_per_token": 2.0195078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 556.365000000369, "ollama_total_duration_ms": 502.5373, "ollama_load_ms": 298.6779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.5617, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 676.4886741354056, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554514.3409667, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.437300001489348, "prefill_cuda_event_ms": 11.32307243347168, "kv_decode_ms": 427.7524999997695, "kv_decode_cuda_event_ms": 427.7158203125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 11.437300001489348, "cuda_event_ms": 11.32307243347168, "tokens_total": 9, "tokens_per_s": 786.8990057817871, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554514.3409667, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.437300001489348, "prefill_cuda_event_ms": 11.32307243347168, "kv_decode_ms": 427.7524999997695, "kv_decode_cuda_event_ms": 427.7158203125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 427.7524999997695, "cuda_event_ms": 427.7158203125, "tokens_total": 64, "tokens_per_s": 149.61923074683256, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554514.3409667, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.437300001489348, "prefill_cuda_event_ms": 11.32307243347168, "kv_decode_ms": 427.7524999997695, "kv_decode_cuda_event_ms": 427.7158203125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 439.18980000125885, "cuda_event_ms": 439.0388927459717, "tokens_total": 73, "tokens_per_s": 166.2151534479871, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554514.7814784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.4838999989879085, "prefill_cuda_event_ms": 7.408703804016113, "kv_decode_ms": 420.5390000006446, "kv_decode_cuda_event_ms": 420.50048828125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 7.4838999989879085, "cuda_event_ms": 7.408703804016113, "tokens_total": 9, "tokens_per_s": 1202.5815418721686, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554514.7814784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.4838999989879085, "prefill_cuda_event_ms": 7.408703804016113, "kv_decode_ms": 420.5390000006446, "kv_decode_cuda_event_ms": 420.50048828125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 420.5390000006446, "cuda_event_ms": 420.50048828125, "tokens_total": 64, "tokens_per_s": 152.1856474664702, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554514.7814784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.4838999989879085, "prefill_cuda_event_ms": 7.408703804016113, "kv_decode_ms": 420.5390000006446, "kv_decode_cuda_event_ms": 420.50048828125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 428.0228999996325, "cuda_event_ms": 427.9091920852661, "tokens_total": 73, "tokens_per_s": 170.55162235493165, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554515.2102935, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.242100000439677, "prefill_cuda_event_ms": 7.17628812789917, "kv_decode_ms": 417.0905999999377, "kv_decode_cuda_event_ms": 417.0513000488281, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 7.242100000439677, "cuda_event_ms": 7.17628812789917, "tokens_total": 9, "tokens_per_s": 1242.7334612134048, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554515.2102935, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.242100000439677, "prefill_cuda_event_ms": 7.17628812789917, "kv_decode_ms": 417.0905999999377, "kv_decode_cuda_event_ms": 417.0513000488281, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 417.0905999999377, "cuda_event_ms": 417.0513000488281, "tokens_total": 64, "tokens_per_s": 153.44388005869604, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554515.2102935, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.242100000439677, "prefill_cuda_event_ms": 7.17628812789917, "kv_decode_ms": 417.0905999999377, "kv_decode_cuda_event_ms": 417.0513000488281, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 424.3327000003774, "cuda_event_ms": 424.2275881767273, "tokens_total": 73, "tokens_per_s": 172.03482079023152, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554515.6352613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.262200000695884, "prefill_cuda_event_ms": 7.20252799987793, "kv_decode_ms": 414.2555000016728, "kv_decode_cuda_event_ms": 414.2080078125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 7.262200000695884, "cuda_event_ms": 7.20252799987793, "tokens_total": 9, "tokens_per_s": 1239.2938777694906, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554515.6352613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.262200000695884, "prefill_cuda_event_ms": 7.20252799987793, "kv_decode_ms": 414.2555000016728, "kv_decode_cuda_event_ms": 414.2080078125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 414.2555000016728, "cuda_event_ms": 414.2080078125, "tokens_total": 64, "tokens_per_s": 154.49402602920557, "gen_tokens": 64}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554515.6352613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 7.262200000695884, "prefill_cuda_event_ms": 7.20252799987793, "kv_decode_ms": 414.2555000016728, "kv_decode_cuda_event_ms": 414.2080078125, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 421.51770000236866, "cuda_event_ms": 421.41053581237793, "tokens_total": 73, "tokens_per_s": 173.18371209462802, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554516.0575135, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.4039, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.01, "kv_decode_ms_equiv": 716.8182857142857, "kv_decode_ms_per_token": 11.200285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 7360.4717000016535, "ollama_total_duration_ms": 7356.5056, "ollama_load_ms": 6894.423, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 21.4039, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1775.377384495349, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554516.0575135, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.4039, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.01, "kv_decode_ms_equiv": 716.8182857142857, "kv_decode_ms_per_token": 11.200285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 7360.4717000016535, "ollama_total_duration_ms": 7356.5056, "ollama_load_ms": 6894.423, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 716.8182857142857, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.2834366470243, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554516.0575135, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.4039, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.01, "kv_decode_ms_equiv": 716.8182857142857, "kv_decode_ms_per_token": 11.200285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 7360.4717000016535, "ollama_total_duration_ms": 7356.5056, "ollama_load_ms": 6894.423, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 738.2221857142857, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 138.16978407565372, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554523.4181354, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.9371, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.89, "kv_decode_ms_equiv": 714.7702857142857, "kv_decode_ms_per_token": 11.168285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 688.1465000005846, "ollama_total_duration_ms": 685.3152, "ollama_load_ms": 248.1182, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.9371, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3183.3527406153926, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554523.4181354, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.9371, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.89, "kv_decode_ms_equiv": 714.7702857142857, "kv_decode_ms_per_token": 11.168285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 688.1465000005846, "ollama_total_duration_ms": 685.3152, "ollama_load_ms": 248.1182, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 714.7702857142857, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.53925656834404, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554523.4181354, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.9371, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.89, "kv_decode_ms_equiv": 714.7702857142857, "kv_decode_ms_per_token": 11.168285714285714, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 688.1465000005846, "ollama_total_duration_ms": 685.3152, "ollama_load_ms": 248.1182, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 726.7073857142857, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 140.35910740021376, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554524.106431, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.8433, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1055, "kv_decode_ms_equiv": 715.1643428571429, "kv_decode_ms_per_token": 11.174442857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.5358000007254, "ollama_total_duration_ms": 662.7787, "ollama_load_ms": 223.191, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.8433, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3208.56518031292, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554524.106431, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.8433, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1055, "kv_decode_ms_equiv": 715.1643428571429, "kv_decode_ms_per_token": 11.174442857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.5358000007254, "ollama_total_duration_ms": 662.7787, "ollama_load_ms": 223.191, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 715.1643428571429, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.48992023891252, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554524.106431, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.8433, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1055, "kv_decode_ms_equiv": 715.1643428571429, "kv_decode_ms_per_token": 11.174442857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.5358000007254, "ollama_total_duration_ms": 662.7787, "ollama_load_ms": 223.191, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 727.0076428571429, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 140.30113851229902, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554524.7720783, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.776, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.777, "kv_decode_ms_equiv": 718.2207999999999, "kv_decode_ms_per_token": 11.222199999999999, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.079599999488, "ollama_total_duration_ms": 678.132, "ollama_load_ms": 237.4653, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.776, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3226.9021739130435, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554524.7720783, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.776, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.777, "kv_decode_ms_equiv": 718.2207999999999, "kv_decode_ms_per_token": 11.222199999999999, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.079599999488, "ollama_total_duration_ms": 678.132, "ollama_load_ms": 237.4653, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 718.2207999999999, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.10908734472741, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554524.7720783, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.776, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.777, "kv_decode_ms_equiv": 718.2207999999999, "kv_decode_ms_per_token": 11.222199999999999, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.079599999488, "ollama_total_duration_ms": 678.132, "ollama_load_ms": 237.4653, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 729.9967999999999, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 139.72663989759957, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554525.464267, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.1790000011824304, "prefill_cuda_event_ms": 3.1022720336914062, "kv_decode_ms": 136.8321999998443, "kv_decode_cuda_event_ms": 136.78009033203125, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 195.63650000054622, "params_millions_measured": 25.016064, "latency_ms": 3.1790000011824304, "cuda_event_ms": 3.1022720336914062, "tokens_total": 17, "tokens_per_s": 5347.593580898661, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554525.464267, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.1790000011824304, "prefill_cuda_event_ms": 3.1022720336914062, "kv_decode_ms": 136.8321999998443, "kv_decode_cuda_event_ms": 136.78009033203125, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 195.63650000054622, "params_millions_measured": 25.016064, "latency_ms": 136.8321999998443, "cuda_event_ms": 136.78009033203125, "tokens_total": 64, "tokens_per_s": 467.726163871317, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554525.464267, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.1790000011824304, "prefill_cuda_event_ms": 3.1022720336914062, "kv_decode_ms": 136.8321999998443, "kv_decode_cuda_event_ms": 136.78009033203125, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 195.63650000054622, "params_millions_measured": 25.016064, "latency_ms": 140.01120000102674, "cuda_event_ms": 139.88236236572266, "tokens_total": 81, "tokens_per_s": 578.5251465554613, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554525.8008733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.381500000410597, "prefill_cuda_event_ms": 2.319648027420044, "kv_decode_ms": 121.50990000009187, "kv_decode_cuda_event_ms": 121.46278381347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 2.381500000410597, "cuda_event_ms": 2.319648027420044, "tokens_total": 17, "tokens_per_s": 7138.358176388416, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554525.8008733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.381500000410597, "prefill_cuda_event_ms": 2.319648027420044, "kv_decode_ms": 121.50990000009187, "kv_decode_cuda_event_ms": 121.46278381347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 121.50990000009187, "cuda_event_ms": 121.46278381347656, "tokens_total": 64, "tokens_per_s": 526.7060544033993, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554525.8008733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.381500000410597, "prefill_cuda_event_ms": 2.319648027420044, "kv_decode_ms": 121.50990000009187, "kv_decode_cuda_event_ms": 121.46278381347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 123.89140000050247, "cuda_event_ms": 123.7824318408966, "tokens_total": 81, "tokens_per_s": 653.7984073121418, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554525.9254072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.421300001515192, "prefill_cuda_event_ms": 2.37062406539917, "kv_decode_ms": 121.94780000027095, "kv_decode_cuda_event_ms": 121.90281677246094, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 2.421300001515192, "cuda_event_ms": 2.37062406539917, "tokens_total": 17, "tokens_per_s": 7021.021760773883, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554525.9254072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.421300001515192, "prefill_cuda_event_ms": 2.37062406539917, "kv_decode_ms": 121.94780000027095, "kv_decode_cuda_event_ms": 121.90281677246094, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 121.94780000027095, "cuda_event_ms": 121.90281677246094, "tokens_total": 64, "tokens_per_s": 524.8147158034651, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554525.9254072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.421300001515192, "prefill_cuda_event_ms": 2.37062406539917, "kv_decode_ms": 121.94780000027095, "kv_decode_cuda_event_ms": 121.90281677246094, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 124.36910000178614, "cuda_event_ms": 124.27344083786011, "tokens_total": 81, "tokens_per_s": 651.2871766285734, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554526.0504787, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.6722000002337154, "prefill_cuda_event_ms": 2.6077120304107666, "kv_decode_ms": 123.63840000034543, "kv_decode_cuda_event_ms": 123.5947494506836, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 2.6722000002337154, "cuda_event_ms": 2.6077120304107666, "tokens_total": 17, "tokens_per_s": 6361.799265965553, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554526.0504787, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.6722000002337154, "prefill_cuda_event_ms": 2.6077120304107666, "kv_decode_ms": 123.63840000034543, "kv_decode_cuda_event_ms": 123.5947494506836, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 123.63840000034543, "cuda_event_ms": 123.5947494506836, "tokens_total": 64, "tokens_per_s": 517.6385330109513, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554526.0504787, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 2.6722000002337154, "prefill_cuda_event_ms": 2.6077120304107666, "kv_decode_ms": 123.63840000034543, "kv_decode_cuda_event_ms": 123.5947494506836, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 126.31060000057914, "cuda_event_ms": 126.20246148109436, "tokens_total": 81, "tokens_per_s": 641.2763457669317, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554526.1776161, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.87959999902523, "prefill_cuda_event_ms": null, "kv_decode_ms": 415.0052999993932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 95.71759999926144, "params_millions_measured": 25.016064, "latency_ms": 30.87959999902523, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 291.45455252931066, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554526.1776161, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.87959999902523, "prefill_cuda_event_ms": null, "kv_decode_ms": 415.0052999993932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 95.71759999926144, "params_millions_measured": 25.016064, "latency_ms": 415.0052999993932, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 154.21489797863686, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554526.1776161, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.87959999902523, "prefill_cuda_event_ms": null, "kv_decode_ms": 415.0052999993932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 95.71759999926144, "params_millions_measured": 25.016064, "latency_ms": 445.8848999984184, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 163.71938139250497, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554526.719872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.360399998811772, "prefill_cuda_event_ms": null, "kv_decode_ms": 372.31880000035744, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 6.360399998811772, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1415.0053458400964, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554526.719872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.360399998811772, "prefill_cuda_event_ms": null, "kv_decode_ms": 372.31880000035744, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 372.31880000035744, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 171.89569798768838, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554526.719872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.360399998811772, "prefill_cuda_event_ms": null, "kv_decode_ms": 372.31880000035744, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 378.6791999991692, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 192.77530955003644, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554527.0989032, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.044199999858392, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.685300001569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.044199999858392, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1118.818527654513, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554527.0989032, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.044199999858392, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.685300001569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 385.685300001569, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 165.93839588840862, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554527.0989032, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.044199999858392, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.685300001569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 393.72950000142737, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 185.40647830486503, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554527.4930124, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.18849999854865, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.622800001438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.18849999854865, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 883.3488738560188, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554527.4930124, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.18849999854865, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.622800001438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 390.622800001438, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 163.84092275147378, "gen_tokens": 64}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554527.4930124, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.18849999854865, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.622800001438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 400.81129999998666, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 182.13059362348923, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554527.8941946, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.349499999298132, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.92230000079144, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.349499999298132, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1184.7102687084225, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554527.8941946, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.349499999298132, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.92230000079144, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 407.92230000079144, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 156.89262391361254, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554527.8941946, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 14.349499999298132, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.92230000079144, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 422.27180000008957, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 191.8195816059297, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554528.3169322, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.743099999264814, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.1115999991307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.743099999264814, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1944.3904337625659, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554528.3169322, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.743099999264814, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.1115999991307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 396.1115999991307, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 161.57062807587673, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554528.3169322, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.743099999264814, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.1115999991307, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 404.8546999983955, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 200.07177883897856, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554528.7222302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.266399999934947, "prefill_cuda_event_ms": null, "kv_decode_ms": 452.64850000057777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 7.266399999934947, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2339.535395815286, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554528.7222302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.266399999934947, "prefill_cuda_event_ms": null, "kv_decode_ms": 452.64850000057777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 452.64850000057777, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 141.3900631503657, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554528.7222302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.266399999934947, "prefill_cuda_event_ms": null, "kv_decode_ms": 452.64850000057777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 459.9149000005127, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 176.11953863619053, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554529.182581, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.235400001169182, "prefill_cuda_event_ms": null, "kv_decode_ms": 499.4311000009475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.235400001169182, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1513.074745734993, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554529.182581, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.235400001169182, "prefill_cuda_event_ms": null, "kv_decode_ms": 499.4311000009475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 499.4311000009475, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 128.14580429588503, "gen_tokens": 64}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554529.182581, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.235400001169182, "prefill_cuda_event_ms": null, "kv_decode_ms": 499.4311000009475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 510.66650000211666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 158.61623975660095, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554529.6936636, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 416.3248, "prefill_cuda_event_ms": null, "kv_decode_ms": 731.6916, "kv_decode_ms_equiv": 731.6916, "kv_decode_ms_per_token": 11.43268125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1467.879100000573, "ollama_total_duration_ms": 1445.4547, "ollama_load_ms": 245.0517, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 416.3248, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 110.49065537292037, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554529.6936636, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 416.3248, "prefill_cuda_event_ms": null, "kv_decode_ms": 731.6916, "kv_decode_ms_equiv": 731.6916, "kv_decode_ms_per_token": 11.43268125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1467.879100000573, "ollama_total_duration_ms": 1445.4547, "ollama_load_ms": 245.0517, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 731.6916, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.46854549102382, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554529.6936636, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 416.3248, "prefill_cuda_event_ms": null, "kv_decode_ms": 731.6916, "kv_decode_ms_equiv": 731.6916, "kv_decode_ms_per_token": 11.43268125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1467.879100000573, "ollama_total_duration_ms": 1445.4547, "ollama_load_ms": 245.0517, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1148.0164, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 95.81744651034603, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554531.1616879, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.708, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.2882, "kv_decode_ms_equiv": 722.2882, "kv_decode_ms_per_token": 11.285753125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1047.4747000007483, "ollama_total_duration_ms": 1033.4742, "ollama_load_ms": 242.7374, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.708, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3619.7670758577274, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554531.1616879, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.708, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.2882, "kv_decode_ms_equiv": 722.2882, "kv_decode_ms_per_token": 11.285753125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1047.4747000007483, "ollama_total_duration_ms": 1033.4742, "ollama_load_ms": 242.7374, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 722.2882, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.60728999864598, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554531.1616879, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.708, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.2882, "kv_decode_ms_equiv": 722.2882, "kv_decode_ms_per_token": 11.285753125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1047.4747000007483, "ollama_total_duration_ms": 1033.4742, "ollama_load_ms": 242.7374, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 734.9961999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.66063770125618, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554532.2092535, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.4679, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.013, "kv_decode_ms_equiv": 724.013, "kv_decode_ms_per_token": 11.312703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1035.7229999990523, "ollama_total_duration_ms": 1018.8023, "ollama_load_ms": 229.8769, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.4679, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4011.1964701471065, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554532.2092535, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.4679, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.013, "kv_decode_ms_equiv": 724.013, "kv_decode_ms_per_token": 11.312703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1035.7229999990523, "ollama_total_duration_ms": 1018.8023, "ollama_load_ms": 229.8769, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 724.013, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.39620283061215, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554532.2092535, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.4679, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.013, "kv_decode_ms_equiv": 724.013, "kv_decode_ms_per_token": 11.312703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1035.7229999990523, "ollama_total_duration_ms": 1018.8023, "ollama_load_ms": 229.8769, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 735.4809, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.56200766056602, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554533.2450688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3004, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.0163, "kv_decode_ms_equiv": 727.0163, "kv_decode_ms_per_token": 11.3596296875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1041.6332000004331, "ollama_total_duration_ms": 1027.3092, "ollama_load_ms": 237.4723, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.3004, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4070.652366287919, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554533.2450688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3004, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.0163, "kv_decode_ms_equiv": 727.0163, "kv_decode_ms_per_token": 11.3596296875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1041.6332000004331, "ollama_total_duration_ms": 1027.3092, "ollama_load_ms": 237.4723, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 727.0163, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.03103864383785, "gen_tokens": 64}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554533.2450688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3004, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.0163, "kv_decode_ms_equiv": 727.0163, "kv_decode_ms_per_token": 11.3596296875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1041.6332000004331, "ollama_total_duration_ms": 1027.3092, "ollama_load_ms": 237.4723, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 738.3167, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 148.98755506952506, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554534.2868457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 184.08569999883184, "prefill_cuda_event_ms": null, "kv_decode_ms": 693.2496000008541, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 184.08569999883184, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 48.89027230283021, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554534.2868457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 184.08569999883184, "prefill_cuda_event_ms": null, "kv_decode_ms": 693.2496000008541, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 693.2496000008541, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 92.31884158306207, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554534.2868457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 184.08569999883184, "prefill_cuda_event_ms": null, "kv_decode_ms": 693.2496000008541, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 877.335299999686, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.2065004109901, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554535.1654217, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.501199999358505, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.8631999996869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.501199999358505, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 620.6382920308758, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554535.1654217, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.501199999358505, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.8631999996869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 673.8631999996869, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.97476639179841, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554535.1654217, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.501199999358505, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.8631999996869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 688.3643999990454, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 106.04848246089024, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554535.8541481, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.28300000022864, "prefill_cuda_event_ms": null, "kv_decode_ms": 674.6078999985912, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 13.28300000022864, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 677.5577806101848, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554535.8541481, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.28300000022864, "prefill_cuda_event_ms": null, "kv_decode_ms": 674.6078999985912, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 674.6078999985912, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.86992369957964, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554535.8541481, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.28300000022864, "prefill_cuda_event_ms": null, "kv_decode_ms": 674.6078999985912, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 687.8908999988198, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 106.1214794382732, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554536.5428782, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.245799999320298, "prefill_cuda_event_ms": null, "kv_decode_ms": 670.3821999999491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 15.245799999320298, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 590.3265161815875, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554536.5428782, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.245799999320298, "prefill_cuda_event_ms": null, "kv_decode_ms": 670.3821999999491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 670.3821999999491, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 95.46792859357672, "gen_tokens": 64}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554536.5428782, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.245799999320298, "prefill_cuda_event_ms": null, "kv_decode_ms": 670.3821999999491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 685.6279999992694, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 106.47173102626758, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554537.2289205, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.769399998622248, "prefill_cuda_event_ms": 4.701024055480957, "kv_decode_ms": 156.39879999980622, "kv_decode_cuda_event_ms": 156.3504638671875, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 4.769399998622248, "cuda_event_ms": 4.701024055480957, "tokens_total": 17, "tokens_per_s": 3564.389651719471, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554537.2289205, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.769399998622248, "prefill_cuda_event_ms": 4.701024055480957, "kv_decode_ms": 156.39879999980622, "kv_decode_cuda_event_ms": 156.3504638671875, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 156.39879999980622, "cuda_event_ms": 156.3504638671875, "tokens_total": 64, "tokens_per_s": 409.21030084680507, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554537.2289205, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.769399998622248, "prefill_cuda_event_ms": 4.701024055480957, "kv_decode_ms": 156.39879999980622, "kv_decode_cuda_event_ms": 156.3504638671875, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 161.16819999842846, "cuda_event_ms": 161.05148792266846, "tokens_total": 81, "tokens_per_s": 502.58053388193093, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554537.392258, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.125999999814667, "prefill_cuda_event_ms": 4.064864158630371, "kv_decode_ms": 160.12819999923522, "kv_decode_cuda_event_ms": 160.09933471679688, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 4.125999999814667, "cuda_event_ms": 4.064864158630371, "tokens_total": 17, "tokens_per_s": 4120.21328181377, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554537.392258, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.125999999814667, "prefill_cuda_event_ms": 4.064864158630371, "kv_decode_ms": 160.12819999923522, "kv_decode_cuda_event_ms": 160.09933471679688, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 160.12819999923522, "cuda_event_ms": 160.09933471679688, "tokens_total": 64, "tokens_per_s": 399.6797565969371, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554537.392258, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.125999999814667, "prefill_cuda_event_ms": 4.064864158630371, "kv_decode_ms": 160.12819999923522, "kv_decode_cuda_event_ms": 160.09933471679688, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 164.2541999990499, "cuda_event_ms": 164.16419887542725, "tokens_total": 81, "tokens_per_s": 493.1380750109802, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554537.557957, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.870800001095631, "prefill_cuda_event_ms": 3.821536064147949, "kv_decode_ms": 181.6422000010789, "kv_decode_cuda_event_ms": 181.6051788330078, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 3.870800001095631, "cuda_event_ms": 3.821536064147949, "tokens_total": 17, "tokens_per_s": 4391.856979226037, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554537.557957, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.870800001095631, "prefill_cuda_event_ms": 3.821536064147949, "kv_decode_ms": 181.6422000010789, "kv_decode_cuda_event_ms": 181.6051788330078, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 181.6422000010789, "cuda_event_ms": 181.6051788330078, "tokens_total": 64, "tokens_per_s": 352.3410308816996, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554537.557957, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.870800001095631, "prefill_cuda_event_ms": 3.821536064147949, "kv_decode_ms": 181.6422000010789, "kv_decode_cuda_event_ms": 181.6051788330078, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 185.51300000217452, "cuda_event_ms": 185.42671489715576, "tokens_total": 81, "tokens_per_s": 436.62708273301894, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554537.744278, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.811900000073365, "prefill_cuda_event_ms": 3.759903907775879, "kv_decode_ms": 192.1688999991602, "kv_decode_cuda_event_ms": 192.13722229003906, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 3.811900000073365, "cuda_event_ms": 3.759903907775879, "tokens_total": 17, "tokens_per_s": 4459.718250655267, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554537.744278, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.811900000073365, "prefill_cuda_event_ms": 3.759903907775879, "kv_decode_ms": 192.1688999991602, "kv_decode_cuda_event_ms": 192.13722229003906, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 192.1688999991602, "cuda_event_ms": 192.13722229003906, "tokens_total": 64, "tokens_per_s": 333.04036189143864, "gen_tokens": 64}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554537.744278, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.811900000073365, "prefill_cuda_event_ms": 3.759903907775879, "kv_decode_ms": 192.1688999991602, "kv_decode_cuda_event_ms": 192.13722229003906, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 195.98079999923357, "cuda_event_ms": 195.89712619781494, "tokens_total": 81, "tokens_per_s": 413.3057932221767, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554537.9411757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.750600001192652, "prefill_cuda_event_ms": 5.703551769256592, "kv_decode_ms": 302.4723999988055, "kv_decode_cuda_event_ms": 302.4486389160156, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.750600001192652, "cuda_event_ms": 5.703551769256592, "tokens_total": 9, "tokens_per_s": 1565.0540809886681, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554537.9411757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.750600001192652, "prefill_cuda_event_ms": 5.703551769256592, "kv_decode_ms": 302.4723999988055, "kv_decode_cuda_event_ms": 302.4486389160156, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 302.4723999988055, "cuda_event_ms": 302.4486389160156, "tokens_total": 64, "tokens_per_s": 211.58955329561556, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554537.9411757, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.750600001192652, "prefill_cuda_event_ms": 5.703551769256592, "kv_decode_ms": 302.4723999988055, "kv_decode_cuda_event_ms": 302.4486389160156, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 308.22299999999814, "cuda_event_ms": 308.1521906852722, "tokens_total": 73, "tokens_per_s": 236.84150760975152, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554538.2501678, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.38819999928819, "prefill_cuda_event_ms": 5.320608139038086, "kv_decode_ms": 296.9857000007323, "kv_decode_cuda_event_ms": 296.9559020996094, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.38819999928819, "cuda_event_ms": 5.320608139038086, "tokens_total": 9, "tokens_per_s": 1670.3166180150974, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554538.2501678, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.38819999928819, "prefill_cuda_event_ms": 5.320608139038086, "kv_decode_ms": 296.9857000007323, "kv_decode_cuda_event_ms": 296.9559020996094, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 296.9857000007323, "cuda_event_ms": 296.9559020996094, "tokens_total": 64, "tokens_per_s": 215.49859134578597, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554538.2501678, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.38819999928819, "prefill_cuda_event_ms": 5.320608139038086, "kv_decode_ms": 296.9857000007323, "kv_decode_cuda_event_ms": 296.9559020996094, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 302.3739000000205, "cuda_event_ms": 302.27651023864746, "tokens_total": 73, "tokens_per_s": 241.4229535022535, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554538.5531592, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.5656999993516365, "prefill_cuda_event_ms": 5.509088039398193, "kv_decode_ms": 299.1191000000981, "kv_decode_cuda_event_ms": 299.0712585449219, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.5656999993516365, "cuda_event_ms": 5.509088039398193, "tokens_total": 9, "tokens_per_s": 1617.047271870283, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554538.5531592, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.5656999993516365, "prefill_cuda_event_ms": 5.509088039398193, "kv_decode_ms": 299.1191000000981, "kv_decode_cuda_event_ms": 299.0712585449219, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 299.1191000000981, "cuda_event_ms": 299.0712585449219, "tokens_total": 64, "tokens_per_s": 213.96159589935584, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554538.5531592, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.5656999993516365, "prefill_cuda_event_ms": 5.509088039398193, "kv_decode_ms": 299.1191000000981, "kv_decode_cuda_event_ms": 299.0712585449219, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 304.68479999944975, "cuda_event_ms": 304.58034658432007, "tokens_total": 73, "tokens_per_s": 239.59186674271848, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554538.8585715, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.248999999821535, "prefill_cuda_event_ms": 5.177984237670898, "kv_decode_ms": 298.7192000000505, "kv_decode_cuda_event_ms": 298.6454162597656, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.248999999821535, "cuda_event_ms": 5.177984237670898, "tokens_total": 9, "tokens_per_s": 1714.6123071644117, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554538.8585715, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.248999999821535, "prefill_cuda_event_ms": 5.177984237670898, "kv_decode_ms": 298.7192000000505, "kv_decode_cuda_event_ms": 298.6454162597656, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 298.7192000000505, "cuda_event_ms": 298.6454162597656, "tokens_total": 64, "tokens_per_s": 214.24802958761668, "gen_tokens": 64}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554538.8585715, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.248999999821535, "prefill_cuda_event_ms": 5.177984237670898, "kv_decode_ms": 298.7192000000505, "kv_decode_cuda_event_ms": 298.6454162597656, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 303.968199999872, "cuda_event_ms": 303.8234004974365, "tokens_total": 73, "tokens_per_s": 240.15670060233518, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554539.1634166, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 471.02690000065195, "prefill_cuda_event_ms": null, "kv_decode_ms": 1066.982399999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 116.89649999971152, "params_millions_measured": 74.824704, "latency_ms": 471.02690000065195, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 19.10718899491206, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554539.1634166, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 471.02690000065195, "prefill_cuda_event_ms": null, "kv_decode_ms": 1066.982399999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 116.89649999971152, "params_millions_measured": 74.824704, "latency_ms": 1066.982399999688, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.982245255421944, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554539.1634166, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 471.02690000065195, "prefill_cuda_event_ms": null, "kv_decode_ms": 1066.982399999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 116.89649999971152, "params_millions_measured": 74.824704, "latency_ms": 1538.0093000003399, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 47.46395226607789, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554540.8189807, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.51280000078259, "prefill_cuda_event_ms": null, "kv_decode_ms": 1011.5208000006533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 16.51280000078259, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 545.0317329328439, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554540.8189807, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.51280000078259, "prefill_cuda_event_ms": null, "kv_decode_ms": 1011.5208000006533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1011.5208000006533, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.27106669478143, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554540.8189807, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.51280000078259, "prefill_cuda_event_ms": null, "kv_decode_ms": 1011.5208000006533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1028.0336000014358, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 71.00935222340792, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554541.847405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.81049999933748, "prefill_cuda_event_ms": null, "kv_decode_ms": 987.0998000005784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 16.81049999933748, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 535.3796734394991, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554541.847405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.81049999933748, "prefill_cuda_event_ms": null, "kv_decode_ms": 987.0998000005784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 987.0998000005784, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.8364025602705, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554541.847405, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.81049999933748, "prefill_cuda_event_ms": null, "kv_decode_ms": 987.0998000005784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1003.9102999999159, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 72.71565995488453, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554542.8516934, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.89729999998235, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.8926999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 16.89729999998235, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 532.6294733483693, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554542.8516934, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.89729999998235, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.8926999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 950.8926999988034, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.30517544206674, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554542.8516934, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.89729999998235, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.8926999988034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 967.7899999987858, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 75.42958699727377, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554543.8201394, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.173000000795582, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.3423000005569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 20.173000000795582, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 842.7105536771702, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554543.8201394, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.173000000795582, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.3423000005569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 950.3423000005569, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.3441558898962, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554543.8201394, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.173000000795582, "prefill_cuda_event_ms": null, "kv_decode_ms": 950.3423000005569, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 970.5153000013524, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 83.46081715547105, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554544.791144, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.117299999081297, "prefill_cuda_event_ms": null, "kv_decode_ms": 895.2853000009782, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.117299999081297, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 768.6290822435896, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554544.791144, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.117299999081297, "prefill_cuda_event_ms": null, "kv_decode_ms": 895.2853000009782, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 895.2853000009782, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.48559235802271, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554544.791144, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.117299999081297, "prefill_cuda_event_ms": null, "kv_decode_ms": 895.2853000009782, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 917.4026000000595, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 88.29275173189475, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554545.7089252, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.032199998851866, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.2215999991167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 23.032199998851866, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 738.0970988810202, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554545.7089252, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.032199998851866, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.2215999991167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 856.2215999991167, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.74700474744624, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554545.7089252, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.032199998851866, "prefill_cuda_event_ms": null, "kv_decode_ms": 856.2215999991167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 879.2537999979686, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 92.12357114656444, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554546.5886633, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.274000000223168, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.9726000005321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.274000000223168, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 882.0172252673634, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554546.5886633, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.274000000223168, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.9726000005321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 865.9726000005321, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.90534065391985, "gen_tokens": 64}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554546.5886633, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.274000000223168, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.9726000005321, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 885.2466000007553, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 91.49992781664554, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554547.47433, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.99559999986377, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.0445999988733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 11.99559999986377, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 750.2751008788397, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554547.47433, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.99559999986377, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.0445999988733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 640.0445999988733, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.99303173577695, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554547.47433, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.99559999986377, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.0445999988733, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 652.0401999987371, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 111.95628735795951, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554548.127101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.808900000891299, "prefill_cuda_event_ms": null, "kv_decode_ms": 630.867699999726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.808900000891299, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 702.6364480457917, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554548.127101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.808900000891299, "prefill_cuda_event_ms": null, "kv_decode_ms": 630.867699999726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 630.867699999726, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 101.44757767758247, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554548.127101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.808900000891299, "prefill_cuda_event_ms": null, "kv_decode_ms": 630.867699999726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 643.6766000006173, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 113.41098930725458, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554548.7712424, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.122399999701884, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.8017000004475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.122399999701884, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 685.8501493785027, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554548.7712424, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.122399999701884, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.8017000004475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 645.8017000004475, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.1016282551682, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554548.7712424, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.122399999701884, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.8017000004475, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 658.9241000001493, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 110.78665964711786, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554549.4307115, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.063799999144976, "prefill_cuda_event_ms": null, "kv_decode_ms": 620.4577999997127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.063799999144976, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 688.9266523208445, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554549.4307115, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.063799999144976, "prefill_cuda_event_ms": null, "kv_decode_ms": 620.4577999997127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 620.4577999997127, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 103.14964208690685, "gen_tokens": 64}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554549.4307115, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.063799999144976, "prefill_cuda_event_ms": null, "kv_decode_ms": 620.4577999997127, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 633.5215999988577, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 115.22890458688642, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.064613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.227199999557342, "prefill_cuda_event_ms": 6.137983798980713, "kv_decode_ms": 173.51330000019516, "kv_decode_cuda_event_ms": 173.4297637939453, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 6.227199999557342, "cuda_event_ms": 6.137983798980713, "tokens_total": 9, "tokens_per_s": 1445.2723536484714, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.064613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.227199999557342, "prefill_cuda_event_ms": 6.137983798980713, "kv_decode_ms": 173.51330000019516, "kv_decode_cuda_event_ms": 173.4297637939453, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 173.51330000019516, "cuda_event_ms": 173.4297637939453, "tokens_total": 64, "tokens_per_s": 368.8478059026485, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.064613, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.227199999557342, "prefill_cuda_event_ms": 6.137983798980713, "kv_decode_ms": 173.51330000019516, "kv_decode_cuda_event_ms": 173.4297637939453, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 179.7404999997525, "cuda_event_ms": 179.56774759292603, "tokens_total": 73, "tokens_per_s": 406.1410756067804, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.245899, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.886800001055235, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 126.36220000058529, "kv_decode_cuda_event_ms": 126.2919692993164, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 3.886800001055235, "cuda_event_ms": 3.8377599716186523, "tokens_total": 9, "tokens_per_s": 2315.5294837801202, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.245899, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.886800001055235, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 126.36220000058529, "kv_decode_cuda_event_ms": 126.2919692993164, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 126.36220000058529, "cuda_event_ms": 126.2919692993164, "tokens_total": 64, "tokens_per_s": 506.48057725889197, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.245899, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.886800001055235, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 126.36220000058529, "kv_decode_cuda_event_ms": 126.2919692993164, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 130.24900000164052, "cuda_event_ms": 130.12972927093506, "tokens_total": 73, "tokens_per_s": 560.4649555780125, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.3773565, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.311300000656047, "prefill_cuda_event_ms": 2.2610878944396973, "kv_decode_ms": 125.63539999973727, "kv_decode_cuda_event_ms": 125.591552734375, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 2.311300000656047, "cuda_event_ms": 2.2610878944396973, "tokens_total": 9, "tokens_per_s": 3893.912515660196, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.3773565, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.311300000656047, "prefill_cuda_event_ms": 2.2610878944396973, "kv_decode_ms": 125.63539999973727, "kv_decode_cuda_event_ms": 125.591552734375, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 125.63539999973727, "cuda_event_ms": 125.591552734375, "tokens_total": 64, "tokens_per_s": 509.41056422102236, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.3773565, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.311300000656047, "prefill_cuda_event_ms": 2.2610878944396973, "kv_decode_ms": 125.63539999973727, "kv_decode_cuda_event_ms": 125.591552734375, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 127.94670000039332, "cuda_event_ms": 127.8526406288147, "tokens_total": 73, "tokens_per_s": 570.550080617754, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.5060434, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.5112999992416007, "prefill_cuda_event_ms": 2.4505600929260254, "kv_decode_ms": 116.37080000036804, "kv_decode_cuda_event_ms": 116.3141098022461, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 2.5112999992416007, "cuda_event_ms": 2.4505600929260254, "tokens_total": 9, "tokens_per_s": 3583.8012195747033, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.5060434, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.5112999992416007, "prefill_cuda_event_ms": 2.4505600929260254, "kv_decode_ms": 116.37080000036804, "kv_decode_cuda_event_ms": 116.3141098022461, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 116.37080000036804, "cuda_event_ms": 116.3141098022461, "tokens_total": 64, "tokens_per_s": 549.9661427076002, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.5060434, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.5112999992416007, "prefill_cuda_event_ms": 2.4505600929260254, "kv_decode_ms": 116.37080000036804, "kv_decode_cuda_event_ms": 116.3141098022461, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 118.88209999960964, "cuda_event_ms": 118.76466989517212, "tokens_total": 73, "tokens_per_s": 614.0537557819024, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.6257994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.193299999315059, "prefill_cuda_event_ms": 4.13862419128418, "kv_decode_ms": 218.73449999839067, "kv_decode_cuda_event_ms": 218.70700073242188, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.193299999315059, "cuda_event_ms": 4.13862419128418, "tokens_total": 17, "tokens_per_s": 4054.0862811572774, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.6257994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.193299999315059, "prefill_cuda_event_ms": 4.13862419128418, "kv_decode_ms": 218.73449999839067, "kv_decode_cuda_event_ms": 218.70700073242188, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 218.73449999839067, "cuda_event_ms": 218.70700073242188, "tokens_total": 64, "tokens_per_s": 292.5921608181191, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.6257994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.193299999315059, "prefill_cuda_event_ms": 4.13862419128418, "kv_decode_ms": 218.73449999839067, "kv_decode_cuda_event_ms": 218.70700073242188, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 222.92779999770573, "cuda_event_ms": 222.84562492370605, "tokens_total": 81, "tokens_per_s": 363.3463390426569, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554550.8496997, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.017999999632593, "prefill_cuda_event_ms": 3.9410240650177, "kv_decode_ms": 219.31519999998272, "kv_decode_cuda_event_ms": 219.28448486328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.017999999632593, "cuda_event_ms": 3.9410240650177, "tokens_total": 17, "tokens_per_s": 4230.9606773405885, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554550.8496997, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.017999999632593, "prefill_cuda_event_ms": 3.9410240650177, "kv_decode_ms": 219.31519999998272, "kv_decode_cuda_event_ms": 219.28448486328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 219.31519999998272, "cuda_event_ms": 219.28448486328125, "tokens_total": 64, "tokens_per_s": 291.81743901017825, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554550.8496997, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.017999999632593, "prefill_cuda_event_ms": 3.9410240650177, "kv_decode_ms": 219.31519999998272, "kv_decode_cuda_event_ms": 219.28448486328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 223.33319999961532, "cuda_event_ms": 223.22550892829895, "tokens_total": 81, "tokens_per_s": 362.6867836942269, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554551.0736759, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.105699999854551, "prefill_cuda_event_ms": 4.0542402267456055, "kv_decode_ms": 214.73839999998745, "kv_decode_cuda_event_ms": 214.71026611328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.105699999854551, "cuda_event_ms": 4.0542402267456055, "tokens_total": 17, "tokens_per_s": 4140.585040456498, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554551.0736759, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.105699999854551, "prefill_cuda_event_ms": 4.0542402267456055, "kv_decode_ms": 214.73839999998745, "kv_decode_cuda_event_ms": 214.71026611328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 214.73839999998745, "cuda_event_ms": 214.71026611328125, "tokens_total": 64, "tokens_per_s": 298.0370534566884, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554551.0736759, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.105699999854551, "prefill_cuda_event_ms": 4.0542402267456055, "kv_decode_ms": 214.73839999998745, "kv_decode_cuda_event_ms": 214.71026611328125, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 218.844099999842, "cuda_event_ms": 218.76450634002686, "tokens_total": 81, "tokens_per_s": 370.1264964422549, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554551.293233, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.147600000578677, "prefill_cuda_event_ms": 4.092959880828857, "kv_decode_ms": 216.26180000021122, "kv_decode_cuda_event_ms": 216.2176055908203, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.147600000578677, "cuda_event_ms": 4.092959880828857, "tokens_total": 17, "tokens_per_s": 4098.755906458709, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554551.293233, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.147600000578677, "prefill_cuda_event_ms": 4.092959880828857, "kv_decode_ms": 216.26180000021122, "kv_decode_cuda_event_ms": 216.2176055908203, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 216.26180000021122, "cuda_event_ms": 216.2176055908203, "tokens_total": 64, "tokens_per_s": 295.9376089533033, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554551.293233, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.147600000578677, "prefill_cuda_event_ms": 4.092959880828857, "kv_decode_ms": 216.26180000021122, "kv_decode_cuda_event_ms": 216.2176055908203, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 220.4094000007899, "cuda_event_ms": 220.31056547164917, "tokens_total": 81, "tokens_per_s": 367.49793792692014, "gen_tokens": 64}
