{
  "argv": [
    "C:\\Users\\sahil\\OneDrive\\Documents\\GitHub\\Banterhearts\\scripts\\tr121\\run_scaling.py",
    "--config",
    "scripts\\tr121\\configs\\scaling.yaml",
    "--out-dir",
    "scripts\\tr121\\results\\tr121_decode_sweep\\20251224_002955\\gen_8",
    "--gen-tokens",
    "8",
    "--repetitions",
    "3",
    "--warmup-repetitions",
    "1",
    "--seed",
    "42",
    "--models",
    "models/gpt2-5m,models/gpt2-25m,models/gpt2-45m,models/gpt2-50m,models/gpt2-75m,models/gpt2-100m,gemma3:270m,qwen2.5:7b,gpt-oss-20b:latest",
    "--backends",
    "hf_cpu_fp32,hf_gpu_fp16,ollama",
    "--scenarios",
    "short,medium",
    "--ollama-timeout-s",
    "900.0"
  ],
  "config_path": "scripts\\tr121\\configs\\scaling.yaml",
  "config_sha256": "5291e18bc612158e476a7c57d18b20e8950e75405581f418d0d4232cbc5c1ccd",
  "cuda_available": true,
  "gen_tokens": 8,
  "git_head": "dbf0ac00357fa86c5043bf452786942e8c002bbd",
  "measured_params": {
    "hf_params_millions": [
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-100m",
        "params_millions_measured": 96.08832
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-100m",
        "params_millions_measured": 96.08832
      },
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-25m",
        "params_millions_measured": 25.016064
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-25m",
        "params_millions_measured": 25.016064
      },
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-45m",
        "params_millions_measured": 45.1712
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-45m",
        "params_millions_measured": 45.1712
      },
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-50m",
        "params_millions_measured": 51.475968
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-50m",
        "params_millions_measured": 51.475968
      },
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-5m",
        "params_millions_measured": 5.03672
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-5m",
        "params_millions_measured": 5.03672
      },
      {
        "device": "cpu",
        "dtype": "fp32",
        "model": "models/gpt2-75m",
        "params_millions_measured": 74.824704
      },
      {
        "device": "cuda",
        "dtype": "fp16",
        "model": "models/gpt2-75m",
        "params_millions_measured": 74.824704
      }
    ],
    "ollama_params_millions": [
      {
        "family": "gemma3",
        "model": "gemma3:270m",
        "parameter_size_raw": "268.10M",
        "params_millions_measured": 268.1,
        "quantization_level": "Q8_0"
      },
      {
        "family": "qwen2",
        "model": "qwen2.5:7b",
        "parameter_size_raw": "7.6B",
        "params_millions_measured": 7600.0,
        "quantization_level": "Q4_K_M"
      },
      {
        "family": "gpt-oss",
        "model": "gpt-oss-20b:latest",
        "parameter_size_raw": "20.9B",
        "params_millions_measured": 20900.0,
        "quantization_level": "Q4_K_M"
      }
    ]
  },
  "modes": [
    "prefill",
    "kv_decode",
    "e2e_kv"
  ],
  "nvml": {
    "memory_total_mb": 12282.0,
    "name": "NVIDIA GeForce RTX 4080 Laptop GPU"
  },
  "ollama_timeout_s": 900.0,
  "platform": {
    "machine": "AMD64",
    "os": "Windows-11-10.0.26200-SP0",
    "processor": "Intel64 Family 6 Model 183 Stepping 1, GenuineIntel",
    "python": "3.13.1"
  },
  "records_expected": 360,
  "repetitions": 3,
  "resolved": {
    "backends": [
      {
        "batch_size": null,
        "device": "cpu",
        "dtype": "fp32",
        "kind": "hf",
        "name": "hf_cpu_fp32",
        "options": null,
        "url": null
      },
      {
        "batch_size": null,
        "device": "cuda",
        "dtype": "fp16",
        "kind": "hf",
        "name": "hf_gpu_fp16",
        "options": null,
        "url": null
      },
      {
        "batch_size": null,
        "device": null,
        "dtype": null,
        "kind": "ollama",
        "name": "ollama",
        "options": {
          "temperature": 0,
          "top_p": 1
        },
        "url": "http://localhost:11434"
      }
    ],
    "filters": {
      "backends_allowlist": [
        "hf_cpu_fp32",
        "hf_gpu_fp16",
        "ollama"
      ],
      "models_allowlist": [
        "gemma3:270m",
        "gpt-oss-20b:latest",
        "models/gpt2-100m",
        "models/gpt2-25m",
        "models/gpt2-45m",
        "models/gpt2-50m",
        "models/gpt2-5m",
        "models/gpt2-75m",
        "qwen2.5:7b"
      ],
      "scenarios_allowlist": [
        "medium",
        "short"
      ]
    },
    "models": [
      {
        "kind": "hf",
        "name": "models/gpt2-5m",
        "params_millions": 5.037
      },
      {
        "kind": "hf",
        "name": "models/gpt2-25m",
        "params_millions": 25.016
      },
      {
        "kind": "hf",
        "name": "models/gpt2-45m",
        "params_millions": 45.171
      },
      {
        "kind": "hf",
        "name": "models/gpt2-50m",
        "params_millions": 51.476
      },
      {
        "kind": "hf",
        "name": "models/gpt2-75m",
        "params_millions": 74.825
      },
      {
        "kind": "hf",
        "name": "models/gpt2-100m",
        "params_millions": 96.088
      },
      {
        "kind": "ollama",
        "name": "gemma3:270m",
        "params_millions": 268.1
      },
      {
        "kind": "ollama",
        "name": "qwen2.5:7b",
        "params_millions": 7600
      },
      {
        "kind": "ollama",
        "name": "gpt-oss-20b:latest",
        "params_millions": 20900
      }
    ],
    "scenarios": [
      {
        "name": "short",
        "prompt": "Summarize RLHF in one sentence."
      },
      {
        "name": "medium",
        "prompt": "Explain how backpressure works in an inference service and when to enable queueing."
      }
    ]
  },
  "results": {
    "hf_load_ms_csv": "scripts\\tr121\\results\\tr121_decode_sweep\\20251224_002955\\gen_8\\hf_load_ms.csv",
    "metrics_csv": "scripts\\tr121\\results\\tr121_decode_sweep\\20251224_002955\\gen_8\\metrics.csv",
    "resolved_model_params_csv": "scripts\\tr121\\results\\tr121_decode_sweep\\20251224_002955\\gen_8\\resolved_model_params.csv",
    "runs_jsonl": "scripts\\tr121\\results\\tr121_decode_sweep\\20251224_002955\\gen_8\\runs.jsonl"
  },
  "run_id": "20251224_002955",
  "run_name": "tr121_scaling_v0",
  "seed": 42,
  "task_count": 30,
  "torch": {
    "cuda": "12.8",
    "torch": "2.8.0+cu128",
    "transformers": "4.57.0"
  },
  "warmup_repetitions": 1
}