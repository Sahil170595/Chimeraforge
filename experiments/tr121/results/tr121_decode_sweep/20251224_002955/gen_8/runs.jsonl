{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554200.0051112, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 282.568500000707, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.2590000001801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7266.599099999439, "params_millions_measured": 45.1712, "latency_ms": 282.568500000707, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 60.162403098567125, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554200.0051112, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 282.568500000707, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.2590000001801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7266.599099999439, "params_millions_measured": 45.1712, "latency_ms": 133.2590000001801, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.03346865869614, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554200.0051112, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 282.568500000707, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.2590000001801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7266.599099999439, "params_millions_measured": 45.1712, "latency_ms": 415.8275000008871, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 60.12108386277163, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554207.695061, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 28.904000000693486, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.43669999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 28.904000000693486, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 588.1538887210118, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554207.695061, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 28.904000000693486, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.43669999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 131.43669999954, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.865800800141805, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554207.695061, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 28.904000000693486, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.43669999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 160.34070000023348, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 155.91799212529068, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554207.8559918, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 30.530699999872013, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.49700000030862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 30.530699999872013, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 556.8165813450482, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554207.8559918, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 30.530699999872013, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.49700000030862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 131.49700000030862, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 60.83788983764819, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554207.8559918, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 30.530699999872013, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.49700000030862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 162.02770000018063, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 154.29460518153456, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554208.0188591, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 29.283400001077098, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.5444999995525, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 29.283400001077098, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 580.5336811768684, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554208.0188591, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 29.283400001077098, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.5444999995525, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 93.5444999995525, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 85.52079491619786, "gen_tokens": 8}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554208.0188591, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 29.283400001077098, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.5444999995525, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 122.8279000006296, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 203.53681858821858, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554208.1422706, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 649.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 218.5711, "kv_decode_ms_equiv": 218.5711, "kv_decode_ms_per_token": 27.3213875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10921.831100000418, "ollama_total_duration_ms": 10850.0888, "ollama_load_ms": 9928.337, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 649.6472, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 141.61532598000883, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554208.1422706, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 649.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 218.5711, "kv_decode_ms_equiv": 218.5711, "kv_decode_ms_per_token": 27.3213875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10921.831100000418, "ollama_total_duration_ms": 10850.0888, "ollama_load_ms": 9928.337, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 218.5711, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 36.601362211198094, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554208.1422706, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 649.6472, "prefill_cuda_event_ms": null, "kv_decode_ms": 218.5711, "kv_decode_ms_equiv": 218.5711, "kv_decode_ms_per_token": 27.3213875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10921.831100000418, "ollama_total_duration_ms": 10850.0888, "ollama_load_ms": 9928.337, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 868.2183, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 115.17840616812614, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554219.064249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 43.5542, "prefill_cuda_event_ms": null, "kv_decode_ms": 229.9545, "kv_decode_ms_equiv": 229.9545, "kv_decode_ms_per_token": 28.7443125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 514.3685999992158, "ollama_total_duration_ms": 491.3036, "ollama_load_ms": 212.7454, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 43.5542, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2112.310638239251, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554219.064249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 43.5542, "prefill_cuda_event_ms": null, "kv_decode_ms": 229.9545, "kv_decode_ms_equiv": 229.9545, "kv_decode_ms_per_token": 28.7443125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 514.3685999992158, "ollama_total_duration_ms": 491.3036, "ollama_load_ms": 212.7454, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 229.9545, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 34.789490964516894, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554219.064249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 43.5542, "prefill_cuda_event_ms": null, "kv_decode_ms": 229.9545, "kv_decode_ms_equiv": 229.9545, "kv_decode_ms_per_token": 28.7443125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 514.3685999992158, "ollama_total_duration_ms": 491.3036, "ollama_load_ms": 212.7454, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 273.5087, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 365.6190826836587, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554219.579551, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 39.5913, "prefill_cuda_event_ms": null, "kv_decode_ms": 217.2498, "kv_decode_ms_equiv": 217.2498, "kv_decode_ms_per_token": 27.156225, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 533.7470000013127, "ollama_total_duration_ms": 518.2429, "ollama_load_ms": 257.1452, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 39.5913, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2323.742842493174, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554219.579551, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 39.5913, "prefill_cuda_event_ms": null, "kv_decode_ms": 217.2498, "kv_decode_ms_equiv": 217.2498, "kv_decode_ms_per_token": 27.156225, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 533.7470000013127, "ollama_total_duration_ms": 518.2429, "ollama_load_ms": 257.1452, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 217.2498, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 36.823969458199734, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554219.579551, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 39.5913, "prefill_cuda_event_ms": null, "kv_decode_ms": 217.2498, "kv_decode_ms_equiv": 217.2498, "kv_decode_ms_per_token": 27.156225, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 533.7470000013127, "ollama_total_duration_ms": 518.2429, "ollama_load_ms": 257.1452, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 256.8411, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 389.34578616895817, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554220.1133974, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 41.3328, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.7555, "kv_decode_ms_equiv": 233.7555, "kv_decode_ms_per_token": 29.2194375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 496.4858000003005, "ollama_total_duration_ms": 494.2755, "ollama_load_ms": 216.1627, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 41.3328, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2225.8351720667365, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554220.1133974, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 41.3328, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.7555, "kv_decode_ms_equiv": 233.7555, "kv_decode_ms_per_token": 29.2194375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 496.4858000003005, "ollama_total_duration_ms": 494.2755, "ollama_load_ms": 216.1627, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 233.7555, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 34.223793664747994, "gen_tokens": 8}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554220.1133974, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 41.3328, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.7555, "kv_decode_ms_equiv": 233.7555, "kv_decode_ms_per_token": 29.2194375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 496.4858000003005, "ollama_total_duration_ms": 494.2755, "ollama_load_ms": 216.1627, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 275.0883, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 363.5196407844318, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554220.610027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 780.3051999999298, "prefill_cuda_event_ms": null, "kv_decode_ms": 236.42499999914435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 331.7600999998831, "params_millions_measured": 96.08832, "latency_ms": 780.3051999999298, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 11.533948511429644, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554220.610027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 780.3051999999298, "prefill_cuda_event_ms": null, "kv_decode_ms": 236.42499999914435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 331.7600999998831, "params_millions_measured": 96.08832, "latency_ms": 236.42499999914435, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 33.83736914467147, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554220.610027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 780.3051999999298, "prefill_cuda_event_ms": null, "kv_decode_ms": 236.42499999914435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 331.7600999998831, "params_millions_measured": 96.08832, "latency_ms": 1016.7301999990741, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 16.72026659581419, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554221.959446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 31.081699999049306, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.6808999990026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 31.081699999049306, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 289.55945139021617, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554221.959446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 31.081699999049306, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.6808999990026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 269.6808999990026, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 29.664688897247036, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554221.959446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 31.081699999049306, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.6808999990026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 300.7625999980519, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 56.5229852385573, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554222.2607427, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 36.116499999479856, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.67260000051465, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 36.116499999479856, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 249.19358188444662, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554222.2607427, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 36.116499999479856, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.67260000051465, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 292.67260000051465, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.334297778425217, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554222.2607427, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 36.116499999479856, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.67260000051465, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 328.7890999999945, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 51.70487707773854, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554222.5900245, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 42.4431999999797, "prefill_cuda_event_ms": null, "kv_decode_ms": 284.9130000013247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 42.4431999999797, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 212.04810193398012, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554222.5900245, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 42.4431999999797, "prefill_cuda_event_ms": null, "kv_decode_ms": 284.9130000013247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 284.9130000013247, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 28.078746845397735, "gen_tokens": 8}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554222.5900245, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 42.4431999999797, "prefill_cuda_event_ms": null, "kv_decode_ms": 284.9130000013247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 327.3562000013044, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 51.931199103399486, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554222.9179072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 587.3155999997834, "prefill_cuda_event_ms": 585.0922241210938, "kv_decode_ms": 143.58530000026803, "kv_decode_cuda_event_ms": 143.47366333007812, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 476.1285000004136, "params_millions_measured": 96.08832, "latency_ms": 587.3155999997834, "cuda_event_ms": 585.0922241210938, "tokens_total": 17, "tokens_per_s": 28.94525532781058, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554222.9179072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 587.3155999997834, "prefill_cuda_event_ms": 585.0922241210938, "kv_decode_ms": 143.58530000026803, "kv_decode_cuda_event_ms": 143.47366333007812, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 476.1285000004136, "params_millions_measured": 96.08832, "latency_ms": 143.58530000026803, "cuda_event_ms": 143.47366333007812, "tokens_total": 8, "tokens_per_s": 55.71600992570316, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554222.9179072, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 587.3155999997834, "prefill_cuda_event_ms": 585.0922241210938, "kv_decode_ms": 143.58530000026803, "kv_decode_cuda_event_ms": 143.47366333007812, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 476.1285000004136, "params_millions_measured": 96.08832, "latency_ms": 730.9009000000515, "cuda_event_ms": 728.5658874511719, "tokens_total": 25, "tokens_per_s": 34.20436340959252, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.2708316, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.330800000592717, "prefill_cuda_event_ms": 5.262656211853027, "kv_decode_ms": 37.21330000007583, "kv_decode_cuda_event_ms": 37.173248291015625, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 5.330800000592717, "cuda_event_ms": 5.262656211853027, "tokens_total": 17, "tokens_per_s": 3189.0147816668823, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.2708316, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.330800000592717, "prefill_cuda_event_ms": 5.262656211853027, "kv_decode_ms": 37.21330000007583, "kv_decode_cuda_event_ms": 37.173248291015625, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 37.21330000007583, "cuda_event_ms": 37.173248291015625, "tokens_total": 8, "tokens_per_s": 214.97690341850085, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.2708316, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.330800000592717, "prefill_cuda_event_ms": 5.262656211853027, "kv_decode_ms": 37.21330000007583, "kv_decode_cuda_event_ms": 37.173248291015625, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 42.54410000066855, "cuda_event_ms": 42.43590450286865, "tokens_total": 25, "tokens_per_s": 587.625546188711, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.314529, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.678199999238132, "prefill_cuda_event_ms": 5.620287895202637, "kv_decode_ms": 35.62710000005609, "kv_decode_cuda_event_ms": 35.59116744995117, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 5.678199999238132, "cuda_event_ms": 5.620287895202637, "tokens_total": 17, "tokens_per_s": 2993.906520073432, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.314529, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.678199999238132, "prefill_cuda_event_ms": 5.620287895202637, "kv_decode_ms": 35.62710000005609, "kv_decode_cuda_event_ms": 35.59116744995117, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 35.62710000005609, "cuda_event_ms": 35.59116744995117, "tokens_total": 8, "tokens_per_s": 224.54816698489086, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.314529, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.678199999238132, "prefill_cuda_event_ms": 5.620287895202637, "kv_decode_ms": 35.62710000005609, "kv_decode_cuda_event_ms": 35.59116744995117, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 41.30529999929422, "cuda_event_ms": 41.21145534515381, "tokens_total": 25, "tokens_per_s": 605.2492053181353, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.3566191, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.580700000791694, "prefill_cuda_event_ms": 5.521279811859131, "kv_decode_ms": 35.229099999924074, "kv_decode_cuda_event_ms": 35.19795227050781, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 5.580700000791694, "cuda_event_ms": 5.521279811859131, "tokens_total": 17, "tokens_per_s": 3046.212840250924, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.3566191, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.580700000791694, "prefill_cuda_event_ms": 5.521279811859131, "kv_decode_ms": 35.229099999924074, "kv_decode_cuda_event_ms": 35.19795227050781, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 35.229099999924074, "cuda_event_ms": 35.19795227050781, "tokens_total": 8, "tokens_per_s": 227.08499507558358, "gen_tokens": 8}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.3566191, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.580700000791694, "prefill_cuda_event_ms": 5.521279811859131, "kv_decode_ms": 35.229099999924074, "kv_decode_cuda_event_ms": 35.19795227050781, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 40.80980000071577, "cuda_event_ms": 40.71923208236694, "tokens_total": 25, "tokens_per_s": 612.5979544021662, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.3984115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.394400001227041, "prefill_cuda_event_ms": 6.351263999938965, "kv_decode_ms": 50.29850000028091, "kv_decode_cuda_event_ms": 50.25312042236328, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 312.9549000004772, "params_millions_measured": 51.475968, "latency_ms": 6.394400001227041, "cuda_event_ms": 6.351263999938965, "tokens_total": 17, "tokens_per_s": 2658.5762537122823, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.3984115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.394400001227041, "prefill_cuda_event_ms": 6.351263999938965, "kv_decode_ms": 50.29850000028091, "kv_decode_cuda_event_ms": 50.25312042236328, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 312.9549000004772, "params_millions_measured": 51.475968, "latency_ms": 50.29850000028091, "cuda_event_ms": 50.25312042236328, "tokens_total": 8, "tokens_per_s": 159.0504687009617, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.3984115, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.394400001227041, "prefill_cuda_event_ms": 6.351263999938965, "kv_decode_ms": 50.29850000028091, "kv_decode_cuda_event_ms": 50.25312042236328, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 312.9549000004772, "params_millions_measured": 51.475968, "latency_ms": 56.69290000150795, "cuda_event_ms": 56.604384422302246, "tokens_total": 25, "tokens_per_s": 440.9723263289589, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.7692428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.253300001641037, "prefill_cuda_event_ms": 5.211999893188477, "kv_decode_ms": 36.34380000039528, "kv_decode_cuda_event_ms": 36.311710357666016, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 5.253300001641037, "cuda_event_ms": 5.211999893188477, "tokens_total": 17, "tokens_per_s": 3236.061141509052, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.7692428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.253300001641037, "prefill_cuda_event_ms": 5.211999893188477, "kv_decode_ms": 36.34380000039528, "kv_decode_cuda_event_ms": 36.311710357666016, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 36.34380000039528, "cuda_event_ms": 36.311710357666016, "tokens_total": 8, "tokens_per_s": 220.12007549879186, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.7692428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.253300001641037, "prefill_cuda_event_ms": 5.211999893188477, "kv_decode_ms": 36.34380000039528, "kv_decode_cuda_event_ms": 36.311710357666016, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 41.59710000203631, "cuda_event_ms": 41.52371025085449, "tokens_total": 25, "tokens_per_s": 601.0034353062154, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.8115246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.400800000643358, "prefill_cuda_event_ms": 5.360223770141602, "kv_decode_ms": 39.568700000018, "kv_decode_cuda_event_ms": 39.530494689941406, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 5.400800000643358, "cuda_event_ms": 5.360223770141602, "tokens_total": 17, "tokens_per_s": 3147.681824539867, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.8115246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.400800000643358, "prefill_cuda_event_ms": 5.360223770141602, "kv_decode_ms": 39.568700000018, "kv_decode_cuda_event_ms": 39.530494689941406, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 39.568700000018, "cuda_event_ms": 39.530494689941406, "tokens_total": 8, "tokens_per_s": 202.1800059136732, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.8115246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.400800000643358, "prefill_cuda_event_ms": 5.360223770141602, "kv_decode_ms": 39.568700000018, "kv_decode_cuda_event_ms": 39.530494689941406, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 44.969500000661355, "cuda_event_ms": 44.89071846008301, "tokens_total": 25, "tokens_per_s": 555.9323541429709, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.8572557, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.868700000064564, "prefill_cuda_event_ms": 5.822656154632568, "kv_decode_ms": 38.96059999897261, "kv_decode_cuda_event_ms": 38.92019271850586, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 5.868700000064564, "cuda_event_ms": 5.822656154632568, "tokens_total": 17, "tokens_per_s": 2896.7232947352863, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.8572557, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.868700000064564, "prefill_cuda_event_ms": 5.822656154632568, "kv_decode_ms": 38.96059999897261, "kv_decode_cuda_event_ms": 38.92019271850586, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 38.96059999897261, "cuda_event_ms": 38.92019271850586, "tokens_total": 8, "tokens_per_s": 205.3356467870351, "gen_tokens": 8}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.8572557, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.868700000064564, "prefill_cuda_event_ms": 5.822656154632568, "kv_decode_ms": 38.96059999897261, "kv_decode_cuda_event_ms": 38.92019271850586, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 44.829299999037175, "cuda_event_ms": 44.74284887313843, "tokens_total": 25, "tokens_per_s": 557.6709875134552, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554224.9027646, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 107.91999999855761, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.78190000042378, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 119.58260000028531, "params_millions_measured": 51.475968, "latency_ms": 107.91999999855761, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 83.39510748814203, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554224.9027646, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 107.91999999855761, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.78190000042378, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 119.58260000028531, "params_millions_measured": 51.475968, "latency_ms": 126.78190000042378, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 63.10048989621752, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554224.9027646, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 107.91999999855761, "prefill_cuda_event_ms": null, "kv_decode_ms": 126.78190000042378, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 119.58260000028531, "params_millions_measured": 51.475968, "latency_ms": 234.7018999989814, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 72.43230668381372, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.2580776, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 18.592500000522705, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.65250000056403, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 18.592500000522705, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 484.06615569433785, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.2580776, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 18.592500000522705, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.65250000056403, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 106.65250000056403, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 75.00996226021604, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.2580776, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 18.592500000522705, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.65250000056403, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 125.24500000108674, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 135.7339614344085, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.3839028, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.2747000008967, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.57189999980619, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.2747000008967, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 630.4861047471851, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.3839028, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.2747000008967, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.57189999980619, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 88.57189999980619, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 90.32209989869818, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.3839028, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.2747000008967, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.57189999980619, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 102.84660000070289, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 165.29472048549798, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.4872599, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.217400001143687, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.9225999997434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.217400001143687, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 633.0271357122972, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.4872599, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.217400001143687, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.9225999997434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 96.9225999997434, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 82.54008868954382, "gen_tokens": 8}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.4872599, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 14.217400001143687, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.9225999997434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 111.14000000088708, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 152.96023033889068, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.598756, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.87530000085826, "prefill_cuda_event_ms": 39.812896728515625, "kv_decode_ms": 31.643999998777872, "kv_decode_cuda_event_ms": 31.613792419433594, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 261.0053999997035, "params_millions_measured": 45.1712, "latency_ms": 39.87530000085826, "cuda_event_ms": 39.812896728515625, "tokens_total": 9, "tokens_per_s": 225.70363106500233, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.598756, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.87530000085826, "prefill_cuda_event_ms": 39.812896728515625, "kv_decode_ms": 31.643999998777872, "kv_decode_cuda_event_ms": 31.613792419433594, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 261.0053999997035, "params_millions_measured": 45.1712, "latency_ms": 31.643999998777872, "cuda_event_ms": 31.613792419433594, "tokens_total": 8, "tokens_per_s": 252.8125395117232, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.598756, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.87530000085826, "prefill_cuda_event_ms": 39.812896728515625, "kv_decode_ms": 31.643999998777872, "kv_decode_cuda_event_ms": 31.613792419433594, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 261.0053999997035, "params_millions_measured": 45.1712, "latency_ms": 71.51929999963613, "cuda_event_ms": 71.42668914794922, "tokens_total": 17, "tokens_per_s": 237.69807590519608, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.932336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.479999999603024, "prefill_cuda_event_ms": 4.404064178466797, "kv_decode_ms": 29.238600000098813, "kv_decode_cuda_event_ms": 29.206527709960938, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 4.479999999603024, "cuda_event_ms": 4.404064178466797, "tokens_total": 9, "tokens_per_s": 2008.9285716065842, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.932336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.479999999603024, "prefill_cuda_event_ms": 4.404064178466797, "kv_decode_ms": 29.238600000098813, "kv_decode_cuda_event_ms": 29.206527709960938, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 29.238600000098813, "cuda_event_ms": 29.206527709960938, "tokens_total": 8, "tokens_per_s": 273.61091160223003, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.932336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.479999999603024, "prefill_cuda_event_ms": 4.404064178466797, "kv_decode_ms": 29.238600000098813, "kv_decode_cuda_event_ms": 29.206527709960938, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 33.71859999970184, "cuda_event_ms": 33.610591888427734, "tokens_total": 17, "tokens_per_s": 504.1727711159516, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554225.9666831, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.41560000035679, "prefill_cuda_event_ms": 4.359231948852539, "kv_decode_ms": 29.784599999402417, "kv_decode_cuda_event_ms": 29.725696563720703, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 4.41560000035679, "cuda_event_ms": 4.359231948852539, "tokens_total": 9, "tokens_per_s": 2038.2281002067173, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554225.9666831, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.41560000035679, "prefill_cuda_event_ms": 4.359231948852539, "kv_decode_ms": 29.784599999402417, "kv_decode_cuda_event_ms": 29.725696563720703, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 29.784599999402417, "cuda_event_ms": 29.725696563720703, "tokens_total": 8, "tokens_per_s": 268.5951800648828, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554225.9666831, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.41560000035679, "prefill_cuda_event_ms": 4.359231948852539, "kv_decode_ms": 29.784599999402417, "kv_decode_cuda_event_ms": 29.725696563720703, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 34.20019999975921, "cuda_event_ms": 34.08492851257324, "tokens_total": 17, "tokens_per_s": 497.07311653498203, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554226.0015655, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.464799998459057, "prefill_cuda_event_ms": 4.417247772216797, "kv_decode_ms": 30.08909999880416, "kv_decode_cuda_event_ms": 30.057279586791992, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 4.464799998459057, "cuda_event_ms": 4.417247772216797, "tokens_total": 9, "tokens_per_s": 2015.7677842470398, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554226.0015655, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.464799998459057, "prefill_cuda_event_ms": 4.417247772216797, "kv_decode_ms": 30.08909999880416, "kv_decode_cuda_event_ms": 30.057279586791992, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 30.08909999880416, "cuda_event_ms": 30.057279586791992, "tokens_total": 8, "tokens_per_s": 265.87701195176817, "gen_tokens": 8}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554226.0015655, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.464799998459057, "prefill_cuda_event_ms": 4.417247772216797, "kv_decode_ms": 30.08909999880416, "kv_decode_cuda_event_ms": 30.057279586791992, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 34.553899997263215, "cuda_event_ms": 34.47452735900879, "tokens_total": 17, "tokens_per_s": 491.9849858148127, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554226.0369897, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.3538, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2028, "kv_decode_ms_equiv": 12.2028, "kv_decode_ms_per_token": 1.52535, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2486.4327999985107, "ollama_total_duration_ms": 2444.0064, "ollama_load_ms": 2406.7135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.3538, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2311.7299899371756, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554226.0369897, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.3538, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2028, "kv_decode_ms_equiv": 12.2028, "kv_decode_ms_per_token": 1.52535, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2486.4327999985107, "ollama_total_duration_ms": 2444.0064, "ollama_load_ms": 2406.7135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.2028, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 655.5872422722654, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554226.0369897, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.3538, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2028, "kv_decode_ms_equiv": 12.2028, "kv_decode_ms_per_token": 1.52535, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2486.4327999985107, "ollama_total_duration_ms": 2444.0064, "ollama_load_ms": 2406.7135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 19.5566, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1278.3408158882423, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554228.5235553, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3173, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.8524, "kv_decode_ms_equiv": 11.8524, "kv_decode_ms_per_token": 1.48155, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 339.4074999996519, "ollama_total_duration_ms": 315.8489, "ollama_load_ms": 291.7218, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3173, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7336.123937340871, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554228.5235553, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3173, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.8524, "kv_decode_ms_equiv": 11.8524, "kv_decode_ms_per_token": 1.48155, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 339.4074999996519, "ollama_total_duration_ms": 315.8489, "ollama_load_ms": 291.7218, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 11.8524, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 674.9687826938004, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554228.5235553, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3173, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.8524, "kv_decode_ms_equiv": 11.8524, "kv_decode_ms_per_token": 1.48155, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 339.4074999996519, "ollama_total_duration_ms": 315.8489, "ollama_load_ms": 291.7218, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.169699999999999, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1764.3281085696947, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554228.8630908, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 1.8527, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8799, "kv_decode_ms_equiv": 12.8799, "kv_decode_ms_per_token": 1.6099875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 295.30349999913597, "ollama_total_duration_ms": 284.6423, "ollama_load_ms": 264.9891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.8527, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 9175.797484751984, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554228.8630908, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 1.8527, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8799, "kv_decode_ms_equiv": 12.8799, "kv_decode_ms_per_token": 1.6099875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 295.30349999913597, "ollama_total_duration_ms": 284.6423, "ollama_load_ms": 264.9891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.8799, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 621.122834804618, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554228.8630908, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 1.8527, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8799, "kv_decode_ms_equiv": 12.8799, "kv_decode_ms_per_token": 1.6099875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 295.30349999913597, "ollama_total_duration_ms": 284.6423, "ollama_load_ms": 264.9891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.7326, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1696.9170411196937, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.1589966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.5509, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.3407, "kv_decode_ms_equiv": 11.3407, "kv_decode_ms_per_token": 1.4175875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 322.8838999984873, "ollama_total_duration_ms": 299.9908, "ollama_load_ms": 277.099, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5509, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6664.314555647026, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.1589966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.5509, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.3407, "kv_decode_ms_equiv": 11.3407, "kv_decode_ms_per_token": 1.4175875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 322.8838999984873, "ollama_total_duration_ms": 299.9908, "ollama_load_ms": 277.099, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 11.3407, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 705.4238274533318, "gen_tokens": 8}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.1589966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.5509, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.3407, "kv_decode_ms_equiv": 11.3407, "kv_decode_ms_per_token": 1.4175875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 322.8838999984873, "ollama_total_duration_ms": 299.9908, "ollama_load_ms": 277.099, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.8916, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1799.6487085720867, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.4820096, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.407699999428587, "prefill_cuda_event_ms": 9.317055702209473, "kv_decode_ms": 60.61940000108734, "kv_decode_cuda_event_ms": 60.58086395263672, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 9.407699999428587, "cuda_event_ms": 9.317055702209473, "tokens_total": 9, "tokens_per_s": 956.6631589598574, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.4820096, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.407699999428587, "prefill_cuda_event_ms": 9.317055702209473, "kv_decode_ms": 60.61940000108734, "kv_decode_cuda_event_ms": 60.58086395263672, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 60.61940000108734, "cuda_event_ms": 60.58086395263672, "tokens_total": 8, "tokens_per_s": 131.970953190835, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.4820096, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.407699999428587, "prefill_cuda_event_ms": 9.317055702209473, "kv_decode_ms": 60.61940000108734, "kv_decode_cuda_event_ms": 60.58086395263672, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 70.02710000051593, "cuda_event_ms": 69.89791965484619, "tokens_total": 17, "tokens_per_s": 242.7631588324342, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.5530927, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.600799999840092, "prefill_cuda_event_ms": 8.553695678710938, "kv_decode_ms": 61.134999999922, "kv_decode_cuda_event_ms": 61.087646484375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 8.600799999840092, "cuda_event_ms": 8.553695678710938, "tokens_total": 9, "tokens_per_s": 1046.414287062521, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.5530927, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.600799999840092, "prefill_cuda_event_ms": 8.553695678710938, "kv_decode_ms": 61.134999999922, "kv_decode_cuda_event_ms": 61.087646484375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 61.134999999922, "cuda_event_ms": 61.087646484375, "tokens_total": 8, "tokens_per_s": 130.85793735192945, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.5530927, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.600799999840092, "prefill_cuda_event_ms": 8.553695678710938, "kv_decode_ms": 61.134999999922, "kv_decode_cuda_event_ms": 61.087646484375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 69.7357999997621, "cuda_event_ms": 69.64134216308594, "tokens_total": 17, "tokens_per_s": 243.7772277661975, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.6236029, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.29650000014226, "prefill_cuda_event_ms": 8.251808166503906, "kv_decode_ms": 61.0153999987233, "kv_decode_cuda_event_ms": 60.96598434448242, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 8.29650000014226, "cuda_event_ms": 8.251808166503906, "tokens_total": 9, "tokens_per_s": 1084.7947929663928, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.6236029, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.29650000014226, "prefill_cuda_event_ms": 8.251808166503906, "kv_decode_ms": 61.0153999987233, "kv_decode_cuda_event_ms": 60.96598434448242, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 61.0153999987233, "cuda_event_ms": 60.96598434448242, "tokens_total": 8, "tokens_per_s": 131.11443996380248, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.6236029, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.29650000014226, "prefill_cuda_event_ms": 8.251808166503906, "kv_decode_ms": 61.0153999987233, "kv_decode_cuda_event_ms": 60.96598434448242, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 69.31189999886556, "cuda_event_ms": 69.21779251098633, "tokens_total": 17, "tokens_per_s": 245.26812856491082, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.693724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.546699998987606, "prefill_cuda_event_ms": 8.500384330749512, "kv_decode_ms": 61.842799999794806, "kv_decode_cuda_event_ms": 61.79635238647461, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 8.546699998987606, "cuda_event_ms": 8.500384330749512, "tokens_total": 9, "tokens_per_s": 1053.0380147970666, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.693724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.546699998987606, "prefill_cuda_event_ms": 8.500384330749512, "kv_decode_ms": 61.842799999794806, "kv_decode_cuda_event_ms": 61.79635238647461, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 61.842799999794806, "cuda_event_ms": 61.79635238647461, "tokens_total": 8, "tokens_per_s": 129.36024888954807, "gen_tokens": 8}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.693724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.546699998987606, "prefill_cuda_event_ms": 8.500384330749512, "kv_decode_ms": 61.842799999794806, "kv_decode_cuda_event_ms": 61.79635238647461, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 70.38949999878241, "cuda_event_ms": 70.29673671722412, "tokens_total": 17, "tokens_per_s": 241.51329389033967, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554229.7648373, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 81.69830000042566, "prefill_cuda_event_ms": null, "kv_decode_ms": 201.37009999962174, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 215.08300000095915, "params_millions_measured": 5.03672, "latency_ms": 81.69830000042566, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 208.08266512169075, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554229.7648373, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 81.69830000042566, "prefill_cuda_event_ms": null, "kv_decode_ms": 201.37009999962174, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 215.08300000095915, "params_millions_measured": 5.03672, "latency_ms": 201.37009999962174, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 39.72784440199924, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554229.7648373, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 81.69830000042566, "prefill_cuda_event_ms": null, "kv_decode_ms": 201.37009999962174, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 215.08300000095915, "params_millions_measured": 5.03672, "latency_ms": 283.0684000000474, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 88.31787652735457, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554230.2646885, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 26.32910000102129, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.7358000012755, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 26.32910000102129, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 645.6734183599357, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554230.2646885, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 26.32910000102129, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.7358000012755, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 130.7358000012755, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 61.192114171649614, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554230.2646885, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 26.32910000102129, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.7358000012755, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 157.06490000229678, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 159.16987181499127, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554230.4223673, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 22.323400000459515, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.01930000021821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 22.323400000459515, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 761.5327414126014, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554230.4223673, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 22.323400000459515, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.01930000021821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 119.01930000021821, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 67.21598933942086, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554230.4223673, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 22.323400000459515, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.01930000021821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 141.34270000067772, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 176.87507030699234, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554230.5646722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 27.058699999543023, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.16270000161603, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 27.058699999543023, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 628.2637377363695, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554230.5646722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 27.058699999543023, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.16270000161603, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 159.16270000161603, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 50.26303273266144, "gen_tokens": 8}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554230.5646722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 27.058699999543023, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.16270000161603, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 186.22140000115905, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 134.24880276834133, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554230.7517471, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.907300001126714, "prefill_cuda_event_ms": 5.844799995422363, "kv_decode_ms": 38.76329999911832, "kv_decode_cuda_event_ms": 38.72022247314453, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 375.027099998988, "params_millions_measured": 74.824704, "latency_ms": 5.907300001126714, "cuda_event_ms": 5.844799995422363, "tokens_total": 9, "tokens_per_s": 1523.5386721993816, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554230.7517471, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.907300001126714, "prefill_cuda_event_ms": 5.844799995422363, "kv_decode_ms": 38.76329999911832, "kv_decode_cuda_event_ms": 38.72022247314453, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 375.027099998988, "params_millions_measured": 74.824704, "latency_ms": 38.76329999911832, "cuda_event_ms": 38.72022247314453, "tokens_total": 8, "tokens_per_s": 206.38077769905973, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554230.7517471, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.907300001126714, "prefill_cuda_event_ms": 5.844799995422363, "kv_decode_ms": 38.76329999911832, "kv_decode_cuda_event_ms": 38.72022247314453, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 375.027099998988, "params_millions_measured": 74.824704, "latency_ms": 44.67060000024503, "cuda_event_ms": 44.565022468566895, "tokens_total": 17, "tokens_per_s": 380.56350261484624, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.1724546, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.425900000773254, "prefill_cuda_event_ms": 5.378911972045898, "kv_decode_ms": 38.31429999991087, "kv_decode_cuda_event_ms": 38.27299118041992, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 5.425900000773254, "cuda_event_ms": 5.378911972045898, "tokens_total": 9, "tokens_per_s": 1658.710997017526, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.1724546, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.425900000773254, "prefill_cuda_event_ms": 5.378911972045898, "kv_decode_ms": 38.31429999991087, "kv_decode_cuda_event_ms": 38.27299118041992, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 38.31429999991087, "cuda_event_ms": 38.27299118041992, "tokens_total": 8, "tokens_per_s": 208.7993255786641, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.1724546, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.425900000773254, "prefill_cuda_event_ms": 5.378911972045898, "kv_decode_ms": 38.31429999991087, "kv_decode_cuda_event_ms": 38.27299118041992, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 43.740200000684126, "cuda_event_ms": 43.65190315246582, "tokens_total": 17, "tokens_per_s": 388.658488066678, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.2168205, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.392699998992612, "prefill_cuda_event_ms": 5.345983982086182, "kv_decode_ms": 38.04669999954058, "kv_decode_cuda_event_ms": 38.005760192871094, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 5.392699998992612, "cuda_event_ms": 5.345983982086182, "tokens_total": 9, "tokens_per_s": 1668.9228033603304, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.2168205, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.392699998992612, "prefill_cuda_event_ms": 5.345983982086182, "kv_decode_ms": 38.04669999954058, "kv_decode_cuda_event_ms": 38.005760192871094, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 38.04669999954058, "cuda_event_ms": 38.005760192871094, "tokens_total": 8, "tokens_per_s": 210.26790760030704, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.2168205, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.392699998992612, "prefill_cuda_event_ms": 5.345983982086182, "kv_decode_ms": 38.04669999954058, "kv_decode_cuda_event_ms": 38.005760192871094, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 43.43939999853319, "cuda_event_ms": 43.351744174957275, "tokens_total": 17, "tokens_per_s": 391.34978845412314, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.260907, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.36889999966661, "prefill_cuda_event_ms": 5.320096015930176, "kv_decode_ms": 38.45770000043558, "kv_decode_cuda_event_ms": 38.416385650634766, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 5.36889999966661, "cuda_event_ms": 5.320096015930176, "tokens_total": 9, "tokens_per_s": 1676.321034207914, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.260907, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.36889999966661, "prefill_cuda_event_ms": 5.320096015930176, "kv_decode_ms": 38.45770000043558, "kv_decode_cuda_event_ms": 38.416385650634766, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 38.45770000043558, "cuda_event_ms": 38.416385650634766, "tokens_total": 8, "tokens_per_s": 208.020760469539, "gen_tokens": 8}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.260907, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 5.36889999966661, "prefill_cuda_event_ms": 5.320096015930176, "kv_decode_ms": 38.45770000043558, "kv_decode_cuda_event_ms": 38.416385650634766, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 43.82660000010219, "cuda_event_ms": 43.73648166656494, "tokens_total": 17, "tokens_per_s": 387.89228459338307, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.305327, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 42.02609999993001, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.97359999935725, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 42.02609999993001, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 404.51053036156844, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.305327, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 42.02609999993001, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.97359999935725, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 179.97359999935725, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 44.450963919311334, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.305327, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 42.02609999993001, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.97359999935725, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 221.99969999928726, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 112.61276479238605, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.5285492, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 39.65699999935168, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.01919999973325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 39.65699999935168, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 428.6758958135491, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.5285492, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 39.65699999935168, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.01919999973325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 290.01919999973325, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.584380620342923, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.5285492, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 39.65699999935168, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.01919999973325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 329.67619999908493, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 75.83198301869953, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554231.858995, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 46.70319999968342, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.27829999908863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 46.70319999968342, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 364.0007536981456, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554231.858995, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 46.70319999968342, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.27829999908863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 192.27829999908863, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 41.60635911612449, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554231.858995, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 46.70319999968342, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.27829999908863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 238.98149999877205, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 104.61060793462447, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554232.0990784, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 33.31059999982244, "prefill_cuda_event_ms": null, "kv_decode_ms": 157.77049999996962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 33.31059999982244, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 510.3480573778502, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554232.0990784, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 33.31059999982244, "prefill_cuda_event_ms": null, "kv_decode_ms": 157.77049999996962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 157.77049999996962, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 50.70656428167205, "gen_tokens": 8}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554232.0990784, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 33.31059999982244, "prefill_cuda_event_ms": null, "kv_decode_ms": 157.77049999996962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 191.08109999979206, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 130.8344990688624, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554232.2909567, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 672.2322, "prefill_cuda_event_ms": null, "kv_decode_ms": 200.1065, "kv_decode_ms_equiv": 200.1065, "kv_decode_ms_per_token": 25.0133125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10742.891199999576, "ollama_total_duration_ms": 10633.9558, "ollama_load_ms": 9722.0925, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 672.2322, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 147.27054133973348, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554232.2909567, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 672.2322, "prefill_cuda_event_ms": null, "kv_decode_ms": 200.1065, "kv_decode_ms_equiv": 200.1065, "kv_decode_ms_per_token": 25.0133125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10742.891199999576, "ollama_total_duration_ms": 10633.9558, "ollama_load_ms": 9722.0925, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 200.1065, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 39.978711336213465, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554232.2909567, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 672.2322, "prefill_cuda_event_ms": null, "kv_decode_ms": 200.1065, "kv_decode_ms_equiv": 200.1065, "kv_decode_ms_per_token": 25.0133125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10742.891199999576, "ollama_total_duration_ms": 10633.9558, "ollama_load_ms": 9722.0925, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 872.3387, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 122.65877921041448, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554243.0345523, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 37.9716, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.7978, "kv_decode_ms_equiv": 203.7978, "kv_decode_ms_per_token": 25.474725, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 709.584399999585, "ollama_total_duration_ms": 693.0466, "ollama_load_ms": 444.5504, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 37.9716, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2607.2117055904937, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554243.0345523, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 37.9716, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.7978, "kv_decode_ms_equiv": 203.7978, "kv_decode_ms_per_token": 25.474725, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 709.584399999585, "ollama_total_duration_ms": 693.0466, "ollama_load_ms": 444.5504, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 203.7978, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 39.25459450494559, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554243.0345523, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 37.9716, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.7978, "kv_decode_ms_equiv": 203.7978, "kv_decode_ms_per_token": 25.474725, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 709.584399999585, "ollama_total_duration_ms": 693.0466, "ollama_load_ms": 444.5504, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 241.7694, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 442.57048245145995, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554243.74441, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 38.072, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.2927, "kv_decode_ms_equiv": 209.2927, "kv_decode_ms_per_token": 26.1615875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 634.7091999996337, "ollama_total_duration_ms": 615.9715, "ollama_load_ms": 362.8886, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 38.072, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2600.336205085102, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554243.74441, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 38.072, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.2927, "kv_decode_ms_equiv": 209.2927, "kv_decode_ms_per_token": 26.1615875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 634.7091999996337, "ollama_total_duration_ms": 615.9715, "ollama_load_ms": 362.8886, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 209.2927, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 38.22398010059596, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554243.74441, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 38.072, "prefill_cuda_event_ms": null, "kv_decode_ms": 209.2927, "kv_decode_ms_equiv": 209.2927, "kv_decode_ms_per_token": 26.1615875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 634.7091999996337, "ollama_total_duration_ms": 615.9715, "ollama_load_ms": 362.8886, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 247.3647, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 432.5596982916318, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554244.3793168, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 40.1947, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.1262, "kv_decode_ms_equiv": 244.1262, "kv_decode_ms_per_token": 30.515775, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 683.8590999996086, "ollama_total_duration_ms": 681.5048, "ollama_load_ms": 395.9637, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 40.1947, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2463.011292533593, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554244.3793168, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 40.1947, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.1262, "kv_decode_ms_equiv": 244.1262, "kv_decode_ms_per_token": 30.515775, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 683.8590999996086, "ollama_total_duration_ms": 681.5048, "ollama_load_ms": 395.9637, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 244.1262, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 32.76993620512669, "gen_tokens": 8}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554244.3793168, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 40.1947, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.1262, "kv_decode_ms_equiv": 244.1262, "kv_decode_ms_per_token": 30.515775, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 683.8590999996086, "ollama_total_duration_ms": 681.5048, "ollama_load_ms": 395.9637, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 284.3209, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 376.33533095878636, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554245.0633314, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 75.55239999965124, "prefill_cuda_event_ms": null, "kv_decode_ms": 299.85800000031304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 75.55239999965124, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 225.009397452344, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554245.0633314, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 75.55239999965124, "prefill_cuda_event_ms": null, "kv_decode_ms": 299.85800000031304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 299.85800000031304, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.67929486620883, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554245.0633314, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 75.55239999965124, "prefill_cuda_event_ms": null, "kv_decode_ms": 299.85800000031304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 375.4103999999643, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 66.59378642680751, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554245.4399874, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 63.171399999191635, "prefill_cuda_event_ms": null, "kv_decode_ms": 310.1600000009057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 63.171399999191635, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 269.10912216948714, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554245.4399874, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 63.171399999191635, "prefill_cuda_event_ms": null, "kv_decode_ms": 310.1600000009057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 310.1600000009057, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.793139024944026, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554245.4399874, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 63.171399999191635, "prefill_cuda_event_ms": null, "kv_decode_ms": 310.1600000009057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 373.33140000009735, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 66.96463249540082, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554245.8141313, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 64.70599999920523, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4890000008454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 64.70599999920523, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 262.7267950454178, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554245.8141313, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 64.70599999920523, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4890000008454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 263.4890000008454, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 30.36179878467159, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554245.8141313, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 64.70599999920523, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4890000008454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 328.19500000005064, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 76.17422568898412, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554246.1440122, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.59759999939706, "prefill_cuda_event_ms": null, "kv_decode_ms": 293.3291999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 54.59759999939706, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 311.3689979081083, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554246.1440122, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.59759999939706, "prefill_cuda_event_ms": null, "kv_decode_ms": 293.3291999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 293.3291999997891, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.27311157568272, "gen_tokens": 8}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554246.1440122, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.59759999939706, "prefill_cuda_event_ms": null, "kv_decode_ms": 293.3291999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 347.9267999991862, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 71.85419461811644, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554246.4931564, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.581300000325427, "prefill_cuda_event_ms": 16.50035285949707, "kv_decode_ms": 86.60640000016429, "kv_decode_cuda_event_ms": 86.53004455566406, "gpu_peak_mb": 579.662109375, "hf_load_ms": 225.22089999984019, "params_millions_measured": 5.03672, "latency_ms": 16.581300000325427, "cuda_event_ms": 16.50035285949707, "tokens_total": 17, "tokens_per_s": 1025.2513373297845, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554246.4931564, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.581300000325427, "prefill_cuda_event_ms": 16.50035285949707, "kv_decode_ms": 86.60640000016429, "kv_decode_cuda_event_ms": 86.53004455566406, "gpu_peak_mb": 579.662109375, "hf_load_ms": 225.22089999984019, "params_millions_measured": 5.03672, "latency_ms": 86.60640000016429, "cuda_event_ms": 86.53004455566406, "tokens_total": 8, "tokens_per_s": 92.37192632397634, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554246.4931564, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.581300000325427, "prefill_cuda_event_ms": 16.50035285949707, "kv_decode_ms": 86.60640000016429, "kv_decode_cuda_event_ms": 86.53004455566406, "gpu_peak_mb": 579.662109375, "hf_load_ms": 225.22089999984019, "params_millions_measured": 5.03672, "latency_ms": 103.18770000048971, "cuda_event_ms": 103.03039741516113, "tokens_total": 25, "tokens_per_s": 242.2769380447607, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554246.8233237, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.655100001007668, "prefill_cuda_event_ms": 11.592576026916504, "kv_decode_ms": 88.50859999984095, "kv_decode_cuda_event_ms": 88.4316177368164, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 11.655100001007668, "cuda_event_ms": 11.592576026916504, "tokens_total": 17, "tokens_per_s": 1458.5889437696997, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554246.8233237, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.655100001007668, "prefill_cuda_event_ms": 11.592576026916504, "kv_decode_ms": 88.50859999984095, "kv_decode_cuda_event_ms": 88.4316177368164, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 88.50859999984095, "cuda_event_ms": 88.4316177368164, "tokens_total": 8, "tokens_per_s": 90.38669688611475, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554246.8233237, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.655100001007668, "prefill_cuda_event_ms": 11.592576026916504, "kv_decode_ms": 88.50859999984095, "kv_decode_cuda_event_ms": 88.4316177368164, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 100.16370000084862, "cuda_event_ms": 100.02419376373291, "tokens_total": 25, "tokens_per_s": 249.59141884523228, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554246.9243524, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.142499999754364, "prefill_cuda_event_ms": 12.08556842803955, "kv_decode_ms": 87.1169000001828, "kv_decode_cuda_event_ms": 87.0492172241211, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 12.142499999754364, "cuda_event_ms": 12.08556842803955, "tokens_total": 17, "tokens_per_s": 1400.0411777100187, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554246.9243524, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.142499999754364, "prefill_cuda_event_ms": 12.08556842803955, "kv_decode_ms": 87.1169000001828, "kv_decode_cuda_event_ms": 87.0492172241211, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 87.1169000001828, "cuda_event_ms": 87.0492172241211, "tokens_total": 8, "tokens_per_s": 91.83063217335803, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554246.9243524, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.142499999754364, "prefill_cuda_event_ms": 12.08556842803955, "kv_decode_ms": 87.1169000001828, "kv_decode_cuda_event_ms": 87.0492172241211, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 99.25939999993716, "cuda_event_ms": 99.13478565216064, "tokens_total": 25, "tokens_per_s": 251.8653145194896, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554247.0244532, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.175799998658476, "prefill_cuda_event_ms": 12.079551696777344, "kv_decode_ms": 87.49189999980445, "kv_decode_cuda_event_ms": 87.42304229736328, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 12.175799998658476, "cuda_event_ms": 12.079551696777344, "tokens_total": 17, "tokens_per_s": 1396.212158697831, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554247.0244532, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.175799998658476, "prefill_cuda_event_ms": 12.079551696777344, "kv_decode_ms": 87.49189999980445, "kv_decode_cuda_event_ms": 87.42304229736328, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 87.49189999980445, "cuda_event_ms": 87.42304229736328, "tokens_total": 8, "tokens_per_s": 91.43703588581207, "gen_tokens": 8}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554247.0244532, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 12.175799998658476, "prefill_cuda_event_ms": 12.079551696777344, "kv_decode_ms": 87.49189999980445, "kv_decode_cuda_event_ms": 87.42304229736328, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 99.66769999846292, "cuda_event_ms": 99.50259399414062, "tokens_total": 25, "tokens_per_s": 250.83351979011806, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554247.1249876, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 13.8554, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.5843, "kv_decode_ms_equiv": 12.5843, "kv_decode_ms_per_token": 1.5730375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2727.830499999982, "ollama_total_duration_ms": 2665.8393, "ollama_load_ms": 2613.5977, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.8554, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1804.3506502879743, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554247.1249876, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 13.8554, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.5843, "kv_decode_ms_equiv": 12.5843, "kv_decode_ms_per_token": 1.5730375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2727.830499999982, "ollama_total_duration_ms": 2665.8393, "ollama_load_ms": 2613.5977, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.5843, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 635.7127531924699, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554247.1249876, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 13.8554, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.5843, "kv_decode_ms_equiv": 12.5843, "kv_decode_ms_per_token": 1.5730375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2727.830499999982, "ollama_total_duration_ms": 2665.8393, "ollama_load_ms": 2613.5977, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 26.439700000000002, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1248.1230876295872, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554249.8530421, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1942, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.7651, "kv_decode_ms_equiv": 12.7651, "kv_decode_ms_per_token": 1.5956375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 283.61889999905543, "ollama_total_duration_ms": 254.8775, "ollama_load_ms": 230.6909, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.1942, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11393.674232066358, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554249.8530421, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1942, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.7651, "kv_decode_ms_equiv": 12.7651, "kv_decode_ms_per_token": 1.5956375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 283.61889999905543, "ollama_total_duration_ms": 254.8775, "ollama_load_ms": 230.6909, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.7651, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 626.7087606050874, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554249.8530421, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1942, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.7651, "kv_decode_ms_equiv": 12.7651, "kv_decode_ms_per_token": 1.5956375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 283.61889999905543, "ollama_total_duration_ms": 254.8775, "ollama_load_ms": 230.6909, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.9593, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 2205.9855741913057, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554250.136803, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3097, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.0911, "kv_decode_ms_equiv": 11.0911, "kv_decode_ms_per_token": 1.3863875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 314.2013999986375, "ollama_total_duration_ms": 290.9979, "ollama_load_ms": 267.6594, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3097, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10823.916525955752, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554250.136803, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3097, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.0911, "kv_decode_ms_equiv": 11.0911, "kv_decode_ms_per_token": 1.3863875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 314.2013999986375, "ollama_total_duration_ms": 290.9979, "ollama_load_ms": 267.6594, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 11.0911, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 721.299059606351, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554250.136803, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3097, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.0911, "kv_decode_ms_equiv": 11.0911, "kv_decode_ms_per_token": 1.3863875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 314.2013999986375, "ollama_total_duration_ms": 290.9979, "ollama_load_ms": 267.6594, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.4008, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 2462.5395498776193, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554250.4511685, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2853, "kv_decode_ms_equiv": 12.2853, "kv_decode_ms_per_token": 1.5356625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 337.13449999959266, "ollama_total_duration_ms": 303.7973, "ollama_load_ms": 282.5982, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6262, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9519.457771685325, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554250.4511685, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2853, "kv_decode_ms_equiv": 12.2853, "kv_decode_ms_per_token": 1.5356625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 337.13449999959266, "ollama_total_duration_ms": 303.7973, "ollama_load_ms": 282.5982, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.2853, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 651.1847492531725, "gen_tokens": 8}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554250.4511685, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.2853, "kv_decode_ms_equiv": 12.2853, "kv_decode_ms_per_token": 1.5356625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 337.13449999959266, "ollama_total_duration_ms": 303.7973, "ollama_load_ms": 282.5982, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.9115, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 2213.0570365154413, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554250.788428, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 28.147399998488254, "prefill_cuda_event_ms": 28.05388832092285, "kv_decode_ms": 96.020299999509, "kv_decode_cuda_event_ms": 95.95484924316406, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 28.147399998488254, "cuda_event_ms": 28.05388832092285, "tokens_total": 9, "tokens_per_s": 319.7453406170152, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554250.788428, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 28.147399998488254, "prefill_cuda_event_ms": 28.05388832092285, "kv_decode_ms": 96.020299999509, "kv_decode_cuda_event_ms": 95.95484924316406, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 96.020299999509, "cuda_event_ms": 95.95484924316406, "tokens_total": 8, "tokens_per_s": 83.3157155314127, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554250.788428, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 28.147399998488254, "prefill_cuda_event_ms": 28.05388832092285, "kv_decode_ms": 96.020299999509, "kv_decode_cuda_event_ms": 95.95484924316406, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 124.16769999799726, "cuda_event_ms": 124.00873756408691, "tokens_total": 17, "tokens_per_s": 136.9116122814081, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554250.9137735, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.304300000323565, "prefill_cuda_event_ms": 12.237792015075684, "kv_decode_ms": 90.54039999864472, "kv_decode_cuda_event_ms": 90.47551727294922, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 12.304300000323565, "cuda_event_ms": 12.237792015075684, "tokens_total": 9, "tokens_per_s": 731.451606329765, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554250.9137735, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.304300000323565, "prefill_cuda_event_ms": 12.237792015075684, "kv_decode_ms": 90.54039999864472, "kv_decode_cuda_event_ms": 90.47551727294922, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 90.54039999864472, "cuda_event_ms": 90.47551727294922, "tokens_total": 8, "tokens_per_s": 88.35834610980017, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554250.9137735, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.304300000323565, "prefill_cuda_event_ms": 12.237792015075684, "kv_decode_ms": 90.54039999864472, "kv_decode_cuda_event_ms": 90.47551727294922, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 102.84469999896828, "cuda_event_ms": 102.7133092880249, "tokens_total": 17, "tokens_per_s": 165.29777421851142, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554251.017461, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 11.90140000107931, "prefill_cuda_event_ms": 11.839072227478027, "kv_decode_ms": 91.45709999938845, "kv_decode_cuda_event_ms": 91.3950424194336, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 11.90140000107931, "cuda_event_ms": 11.839072227478027, "tokens_total": 9, "tokens_per_s": 756.2135546392702, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554251.017461, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 11.90140000107931, "prefill_cuda_event_ms": 11.839072227478027, "kv_decode_ms": 91.45709999938845, "kv_decode_cuda_event_ms": 91.3950424194336, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 91.45709999938845, "cuda_event_ms": 91.3950424194336, "tokens_total": 8, "tokens_per_s": 87.47270578285878, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554251.017461, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 11.90140000107931, "prefill_cuda_event_ms": 11.839072227478027, "kv_decode_ms": 91.45709999938845, "kv_decode_cuda_event_ms": 91.3950424194336, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 103.35850000046776, "cuda_event_ms": 103.23411464691162, "tokens_total": 17, "tokens_per_s": 164.47607114966902, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554251.121595, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.008199999399949, "prefill_cuda_event_ms": 11.943488121032715, "kv_decode_ms": 90.59850000085135, "kv_decode_cuda_event_ms": 90.53388977050781, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 12.008199999399949, "cuda_event_ms": 11.943488121032715, "tokens_total": 9, "tokens_per_s": 749.4878500066397, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554251.121595, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.008199999399949, "prefill_cuda_event_ms": 11.943488121032715, "kv_decode_ms": 90.59850000085135, "kv_decode_cuda_event_ms": 90.53388977050781, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 90.59850000085135, "cuda_event_ms": 90.53388977050781, "tokens_total": 8, "tokens_per_s": 88.30168269811116, "gen_tokens": 8}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554251.121595, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 12.008199999399949, "prefill_cuda_event_ms": 11.943488121032715, "kv_decode_ms": 90.59850000085135, "kv_decode_cuda_event_ms": 90.53388977050781, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 102.6067000002513, "cuda_event_ms": 102.47737789154053, "tokens_total": 17, "tokens_per_s": 165.68118845999692, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554251.2251978, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.5222, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.6792, "kv_decode_ms_equiv": 82.6792, "kv_decode_ms_per_token": 10.3349, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 5918.507000000318, "ollama_total_duration_ms": 5891.493, "ollama_load_ms": 5778.3046, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.5222, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1687.2241610499862, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554251.2251978, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.5222, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.6792, "kv_decode_ms_equiv": 82.6792, "kv_decode_ms_per_token": 10.3349, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 5918.507000000318, "ollama_total_duration_ms": 5891.493, "ollama_load_ms": 5778.3046, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 82.6792, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 96.75952355610602, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554251.2251978, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.5222, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.6792, "kv_decode_ms_equiv": 82.6792, "kv_decode_ms_per_token": 10.3349, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 5918.507000000318, "ollama_total_duration_ms": 5891.493, "ollama_load_ms": 5778.3046, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 105.20139999999999, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 437.25653841108584, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554257.1439512, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.0135, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6736, "kv_decode_ms_equiv": 80.6736, "kv_decode_ms_per_token": 10.0842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.7204999985406, "ollama_total_duration_ms": 341.7369, "ollama_load_ms": 242.7133, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.0135, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3163.1081699754445, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554257.1439512, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.0135, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6736, "kv_decode_ms_equiv": 80.6736, "kv_decode_ms_per_token": 10.0842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.7204999985406, "ollama_total_duration_ms": 341.7369, "ollama_load_ms": 242.7133, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.6736, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.16503044366435, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554257.1439512, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.0135, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6736, "kv_decode_ms_equiv": 80.6736, "kv_decode_ms_per_token": 10.0842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 367.7204999985406, "ollama_total_duration_ms": 341.7369, "ollama_load_ms": 242.7133, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.68709999999999, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 496.2934432083862, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554257.5118134, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.4136, "kv_decode_ms_equiv": 79.4136, "kv_decode_ms_per_token": 9.9267, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 333.3343000012974, "ollama_total_duration_ms": 319.7943, "ollama_load_ms": 220.2761, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.2653, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3373.190239052666, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554257.5118134, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.4136, "kv_decode_ms_equiv": 79.4136, "kv_decode_ms_per_token": 9.9267, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 333.3343000012974, "ollama_total_duration_ms": 319.7943, "ollama_load_ms": 220.2761, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.4136, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.73841256409482, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554257.5118134, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.4136, "kv_decode_ms_equiv": 79.4136, "kv_decode_ms_per_token": 9.9267, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 333.3343000012974, "ollama_total_duration_ms": 319.7943, "ollama_load_ms": 220.2761, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 90.6789, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 507.2844950699667, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554257.8452766, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.7606, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5166, "kv_decode_ms_equiv": 79.5166, "kv_decode_ms_per_token": 9.939575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 357.32029999962833, "ollama_total_duration_ms": 335.5429, "ollama_load_ms": 237.0308, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.7606, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3231.1276635545805, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554257.8452766, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.7606, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5166, "kv_decode_ms_equiv": 79.5166, "kv_decode_ms_per_token": 9.939575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 357.32029999962833, "ollama_total_duration_ms": 335.5429, "ollama_load_ms": 237.0308, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.5166, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.60792337700556, "gen_tokens": 8}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554257.8452766, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.7606, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5166, "kv_decode_ms_equiv": 79.5166, "kv_decode_ms_per_token": 9.939575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 357.32029999962833, "ollama_total_duration_ms": 335.5429, "ollama_load_ms": 237.0308, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.2772, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 503.95936772819505, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.2028482, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 8.166100000380538, "prefill_cuda_event_ms": 8.070560455322266, "kv_decode_ms": 30.102099999567145, "kv_decode_cuda_event_ms": 30.01036834716797, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 250.4289999997127, "params_millions_measured": 25.016064, "latency_ms": 8.166100000380538, "cuda_event_ms": 8.070560455322266, "tokens_total": 17, "tokens_per_s": 2081.7771028040074, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.2028482, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 8.166100000380538, "prefill_cuda_event_ms": 8.070560455322266, "kv_decode_ms": 30.102099999567145, "kv_decode_cuda_event_ms": 30.01036834716797, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 250.4289999997127, "params_millions_measured": 25.016064, "latency_ms": 30.102099999567145, "cuda_event_ms": 30.01036834716797, "tokens_total": 8, "tokens_per_s": 265.7621893527374, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.2028482, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 8.166100000380538, "prefill_cuda_event_ms": 8.070560455322266, "kv_decode_ms": 30.102099999567145, "kv_decode_cuda_event_ms": 30.01036834716797, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 250.4289999997127, "params_millions_measured": 25.016064, "latency_ms": 38.26819999994768, "cuda_event_ms": 38.080928802490234, "tokens_total": 25, "tokens_per_s": 653.2839276483916, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.4927719, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.1959999998653075, "prefill_cuda_event_ms": 4.126143932342529, "kv_decode_ms": 28.95859999989625, "kv_decode_cuda_event_ms": 28.88697624206543, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 4.1959999998653075, "cuda_event_ms": 4.126143932342529, "tokens_total": 17, "tokens_per_s": 4051.4775978421603, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.4927719, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.1959999998653075, "prefill_cuda_event_ms": 4.126143932342529, "kv_decode_ms": 28.95859999989625, "kv_decode_cuda_event_ms": 28.88697624206543, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 28.95859999989625, "cuda_event_ms": 28.88697624206543, "tokens_total": 8, "tokens_per_s": 276.25644886246783, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.4927719, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.1959999998653075, "prefill_cuda_event_ms": 4.126143932342529, "kv_decode_ms": 28.95859999989625, "kv_decode_cuda_event_ms": 28.88697624206543, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 33.15459999976156, "cuda_event_ms": 33.01312017440796, "tokens_total": 25, "tokens_per_s": 754.0431795340555, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.526815, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.067600000780658, "prefill_cuda_event_ms": 3.992288112640381, "kv_decode_ms": 29.153299999961746, "kv_decode_cuda_event_ms": 29.073408126831055, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 4.067600000780658, "cuda_event_ms": 3.992288112640381, "tokens_total": 17, "tokens_per_s": 4179.368668683584, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.526815, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.067600000780658, "prefill_cuda_event_ms": 3.992288112640381, "kv_decode_ms": 29.153299999961746, "kv_decode_cuda_event_ms": 29.073408126831055, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 29.153299999961746, "cuda_event_ms": 29.073408126831055, "tokens_total": 8, "tokens_per_s": 274.4114731440522, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.526815, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.067600000780658, "prefill_cuda_event_ms": 3.992288112640381, "kv_decode_ms": 29.153299999961746, "kv_decode_cuda_event_ms": 29.073408126831055, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 33.220900000742404, "cuda_event_ms": 33.065696239471436, "tokens_total": 25, "tokens_per_s": 752.5383117086326, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.5609636, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.157900000791415, "prefill_cuda_event_ms": 4.062047958374023, "kv_decode_ms": 27.82379999916884, "kv_decode_cuda_event_ms": 27.74527931213379, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 4.157900000791415, "cuda_event_ms": 4.062047958374023, "tokens_total": 17, "tokens_per_s": 4088.6024187123835, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.5609636, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.157900000791415, "prefill_cuda_event_ms": 4.062047958374023, "kv_decode_ms": 27.82379999916884, "kv_decode_cuda_event_ms": 27.74527931213379, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 27.82379999916884, "cuda_event_ms": 27.74527931213379, "tokens_total": 8, "tokens_per_s": 287.5236308569993, "gen_tokens": 8}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.5609636, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 4.157900000791415, "prefill_cuda_event_ms": 4.062047958374023, "kv_decode_ms": 27.82379999916884, "kv_decode_cuda_event_ms": 27.74527931213379, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 31.981699999960256, "cuda_event_ms": 31.807327270507812, "tokens_total": 25, "tokens_per_s": 781.6970329917131, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.593909, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.926299999526236, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.10669999934908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 133.3094000001438, "params_millions_measured": 25.016064, "latency_ms": 37.926299999526236, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 237.30234692317535, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.593909, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.926299999526236, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.10669999934908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 133.3094000001438, "params_millions_measured": 25.016064, "latency_ms": 107.10669999934908, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 74.69187268442235, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.593909, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.926299999526236, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.10669999934908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 133.3094000001438, "params_millions_measured": 25.016064, "latency_ms": 145.0329999988753, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 117.2147028616372, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.8728514, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.56510000086564, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.72429999955057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.56510000086564, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 512.3796619180342, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.8728514, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.56510000086564, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.72429999955057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 96.72429999955057, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 82.70930882970642, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.8728514, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.56510000086564, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.72429999955057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 114.28940000041621, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 148.74520296666262, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554258.9877892, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 15.970300000844873, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.97090000069875, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.970300000844873, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 563.5460823856706, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554258.9877892, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 15.970300000844873, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.97090000069875, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 99.97090000069875, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 80.02328677589263, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554258.9877892, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 15.970300000844873, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.97090000069875, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 115.94120000154362, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 146.62604837429373, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.104231, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.815499999414897, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.198299999538, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.815499999414897, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 505.1780752881245, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.104231, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.815499999414897, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.198299999538, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 95.198299999538, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 84.03511407282299, "gen_tokens": 8}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.104231, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 17.815499999414897, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.198299999538, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 113.01379999895289, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 150.42410749977003, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.217778, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 17.15920000060578, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.01919999923848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.15920000060578, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 990.7221781551495, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.217778, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 17.15920000060578, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.01919999923848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 109.01919999923848, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 73.38156948552073, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.217778, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 17.15920000060578, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.01919999923848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 126.17839999984426, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 198.13216842209806, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.3449607, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 19.981999999799882, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.60919999868202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.981999999799882, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 850.7656891287286, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.3449607, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 19.981999999799882, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.60919999868202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 108.60919999868202, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 73.65858509313281, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.3449607, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 19.981999999799882, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.60919999868202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 128.5911999984819, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 194.41454780961016, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.4744937, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.88800000010815, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.61670000039157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.88800000010815, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1006.6319279897639, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.4744937, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.88800000010815, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.61670000039157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 95.61670000039157, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 83.66739282957096, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.4744937, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.88800000010815, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.61670000039157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 112.50470000049972, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 222.21293865846454, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.5876462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.26850000138802, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.98609999982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.26850000138802, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1044.9641945200583, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.5876462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.26850000138802, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.98609999982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 98.98609999982, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 80.81942818248771, "gen_tokens": 8}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.5876462, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 16.26850000138802, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.98609999982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 115.25460000120802, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 216.9110820716741, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554259.7036514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 47.3231, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.1731, "kv_decode_ms_equiv": 109.1731, "kv_decode_ms_per_token": 13.6466375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 426.4578000002075, "ollama_total_duration_ms": 422.7895, "ollama_load_ms": 255.4432, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 47.3231, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 972.0411384714864, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554259.7036514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 47.3231, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.1731, "kv_decode_ms_equiv": 109.1731, "kv_decode_ms_per_token": 13.6466375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 426.4578000002075, "ollama_total_duration_ms": 422.7895, "ollama_load_ms": 255.4432, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 109.1731, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 73.27812437312853, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554259.7036514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 47.3231, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.1731, "kv_decode_ms_equiv": 109.1731, "kv_decode_ms_per_token": 13.6466375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 426.4578000002075, "ollama_total_duration_ms": 422.7895, "ollama_load_ms": 255.4432, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 156.4962, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 345.0563016865586, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554260.1302474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.0435, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4765, "kv_decode_ms_equiv": 80.4765, "kv_decode_ms_per_token": 10.0595625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.86889999992854, "ollama_total_duration_ms": 330.4753, "ollama_load_ms": 227.2787, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.0435, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3819.487690455432, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554260.1302474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.0435, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4765, "kv_decode_ms_equiv": 80.4765, "kv_decode_ms_per_token": 10.0595625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.86889999992854, "ollama_total_duration_ms": 330.4753, "ollama_load_ms": 227.2787, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.4765, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.40790168558523, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554260.1302474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.0435, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4765, "kv_decode_ms_equiv": 80.4765, "kv_decode_ms_per_token": 10.0595625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 354.86889999992854, "ollama_total_duration_ms": 330.4753, "ollama_load_ms": 227.2787, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.52, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 583.6575875486382, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554260.4852571, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3888, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5358, "kv_decode_ms_equiv": 79.5358, "kv_decode_ms_per_token": 9.941975, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 372.5596999993286, "ollama_total_duration_ms": 359.5596, "ollama_load_ms": 256.5509, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.3888, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4039.0559145827483, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554260.4852571, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3888, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5358, "kv_decode_ms_equiv": 79.5358, "kv_decode_ms_per_token": 9.941975, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 372.5596999993286, "ollama_total_duration_ms": 359.5596, "ollama_load_ms": 256.5509, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.5358, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.5836365510877, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554260.4852571, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.3888, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.5358, "kv_decode_ms_equiv": 79.5358, "kv_decode_ms_per_token": 9.941975, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 372.5596999993286, "ollama_total_duration_ms": 359.5596, "ollama_load_ms": 256.5509, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 90.9246, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 593.898680885041, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554260.8580098, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.1957, "kv_decode_ms_equiv": 83.1957, "kv_decode_ms_per_token": 10.3994625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 365.0833000010607, "ollama_total_duration_ms": 337.8637, "ollama_load_ms": 235.5317, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.513, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3995.483366629028, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554260.8580098, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.1957, "kv_decode_ms_equiv": 83.1957, "kv_decode_ms_per_token": 10.3994625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 365.0833000010607, "ollama_total_duration_ms": 337.8637, "ollama_load_ms": 235.5317, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 83.1957, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 96.15881590034101, "gen_tokens": 8}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554260.8580098, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.513, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.1957, "kv_decode_ms_equiv": 83.1957, "kv_decode_ms_per_token": 10.3994625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 365.0833000010607, "ollama_total_duration_ms": 337.8637, "ollama_load_ms": 235.5317, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 94.70870000000001, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 570.1693719795541, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554261.223322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 292.24840000097174, "prefill_cuda_event_ms": null, "kv_decode_ms": 213.2991000016773, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 292.24840000097174, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 30.795720352857618, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554261.223322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 292.24840000097174, "prefill_cuda_event_ms": null, "kv_decode_ms": 213.2991000016773, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 213.2991000016773, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 37.50601854361829, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554261.223322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 292.24840000097174, "prefill_cuda_event_ms": null, "kv_decode_ms": 213.2991000016773, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 505.54750000264903, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 33.626909439589596, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554261.7359092, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 45.62090000035823, "prefill_cuda_event_ms": null, "kv_decode_ms": 232.5232999992295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 45.62090000035823, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 197.27800196684697, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554261.7359092, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 45.62090000035823, "prefill_cuda_event_ms": null, "kv_decode_ms": 232.5232999992295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 232.5232999992295, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 34.40515423627013, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554261.7359092, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 45.62090000035823, "prefill_cuda_event_ms": null, "kv_decode_ms": 232.5232999992295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 278.1441999995877, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 61.11937620854649, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.0151336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.31679999914195, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.79990000103135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 39.31679999914195, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 228.90977903075571, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.0151336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.31679999914195, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.79990000103135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 233.79990000103135, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 34.21729436139498, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.0151336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 39.31679999914195, "prefill_cuda_event_ms": null, "kv_decode_ms": 233.79990000103135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 273.1167000001733, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 62.244454476746434, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.2892225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 48.828400000274996, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.3989999992482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 48.828400000274996, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 184.31896191456843, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.2892225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 48.828400000274996, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.3989999992482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 244.3989999992482, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 32.73335815623063, "gen_tokens": 8}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.2892225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 48.828400000274996, "prefill_cuda_event_ms": null, "kv_decode_ms": 244.3989999992482, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 293.2273999995232, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 57.97548250957326, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.5834355, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.285599999609985, "prefill_cuda_event_ms": 11.21174430847168, "kv_decode_ms": 41.94189999907394, "kv_decode_cuda_event_ms": 41.89798355102539, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 11.285599999609985, "cuda_event_ms": 11.21174430847168, "tokens_total": 17, "tokens_per_s": 1506.3443680962905, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.5834355, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.285599999609985, "prefill_cuda_event_ms": 11.21174430847168, "kv_decode_ms": 41.94189999907394, "kv_decode_cuda_event_ms": 41.89798355102539, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 41.94189999907394, "cuda_event_ms": 41.89798355102539, "tokens_total": 8, "tokens_per_s": 190.7400475461683, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.5834355, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 11.285599999609985, "prefill_cuda_event_ms": 11.21174430847168, "kv_decode_ms": 41.94189999907394, "kv_decode_cuda_event_ms": 41.89798355102539, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 53.227499998683925, "cuda_event_ms": 53.10972785949707, "tokens_total": 25, "tokens_per_s": 469.6820252805061, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.638491, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.014599999616621, "prefill_cuda_event_ms": 5.962399959564209, "kv_decode_ms": 43.0817999986175, "kv_decode_cuda_event_ms": 43.037696838378906, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 6.014599999616621, "cuda_event_ms": 5.962399959564209, "tokens_total": 17, "tokens_per_s": 2826.4556248268555, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.638491, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.014599999616621, "prefill_cuda_event_ms": 5.962399959564209, "kv_decode_ms": 43.0817999986175, "kv_decode_cuda_event_ms": 43.037696838378906, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 43.0817999986175, "cuda_event_ms": 43.037696838378906, "tokens_total": 8, "tokens_per_s": 185.6932625901592, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.638491, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.014599999616621, "prefill_cuda_event_ms": 5.962399959564209, "kv_decode_ms": 43.0817999986175, "kv_decode_cuda_event_ms": 43.037696838378906, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 49.09639999823412, "cuda_event_ms": 49.000096797943115, "tokens_total": 25, "tokens_per_s": 509.2023040569001, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.6884305, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.229399999938323, "prefill_cuda_event_ms": 6.173344135284424, "kv_decode_ms": 42.141400001128204, "kv_decode_cuda_event_ms": 42.09151840209961, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 6.229399999938323, "cuda_event_ms": 6.173344135284424, "tokens_total": 17, "tokens_per_s": 2728.9947667782317, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.6884305, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.229399999938323, "prefill_cuda_event_ms": 6.173344135284424, "kv_decode_ms": 42.141400001128204, "kv_decode_cuda_event_ms": 42.09151840209961, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 42.141400001128204, "cuda_event_ms": 42.09151840209961, "tokens_total": 8, "tokens_per_s": 189.8370723275882, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.6884305, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 6.229399999938323, "prefill_cuda_event_ms": 6.173344135284424, "kv_decode_ms": 42.141400001128204, "kv_decode_cuda_event_ms": 42.09151840209961, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 48.37080000106653, "cuda_event_ms": 48.26486253738403, "tokens_total": 25, "tokens_per_s": 516.8407386160405, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.737649, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.760500000178581, "prefill_cuda_event_ms": 5.710720062255859, "kv_decode_ms": 40.25559999899997, "kv_decode_cuda_event_ms": 40.208160400390625, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 5.760500000178581, "cuda_event_ms": 5.710720062255859, "tokens_total": 17, "tokens_per_s": 2951.132714082629, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.737649, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.760500000178581, "prefill_cuda_event_ms": 5.710720062255859, "kv_decode_ms": 40.25559999899997, "kv_decode_cuda_event_ms": 40.208160400390625, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 40.25559999899997, "cuda_event_ms": 40.208160400390625, "tokens_total": 8, "tokens_per_s": 198.7301145728479, "gen_tokens": 8}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.737649, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 5.760500000178581, "prefill_cuda_event_ms": 5.710720062255859, "kv_decode_ms": 40.25559999899997, "kv_decode_cuda_event_ms": 40.208160400390625, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 46.01609999917855, "cuda_event_ms": 45.918880462646484, "tokens_total": 25, "tokens_per_s": 543.2881100407528, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.78453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.079299999939394, "prefill_cuda_event_ms": 9.031519889831543, "kv_decode_ms": 62.513000000762986, "kv_decode_cuda_event_ms": 62.44569778442383, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 9.079299999939394, "cuda_event_ms": 9.031519889831543, "tokens_total": 9, "tokens_per_s": 991.2658464925794, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.78453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.079299999939394, "prefill_cuda_event_ms": 9.031519889831543, "kv_decode_ms": 62.513000000762986, "kv_decode_cuda_event_ms": 62.44569778442383, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 62.513000000762986, "cuda_event_ms": 62.44569778442383, "tokens_total": 8, "tokens_per_s": 127.97338153507843, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.78453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 9.079299999939394, "prefill_cuda_event_ms": 9.031519889831543, "kv_decode_ms": 62.513000000762986, "kv_decode_cuda_event_ms": 62.44569778442383, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 71.59230000070238, "cuda_event_ms": 71.47721767425537, "tokens_total": 17, "tokens_per_s": 237.45570403288085, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.8571417, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.404300000620424, "prefill_cuda_event_ms": 8.341119766235352, "kv_decode_ms": 62.03619999905641, "kv_decode_cuda_event_ms": 61.97145462036133, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 8.404300000620424, "cuda_event_ms": 8.341119766235352, "tokens_total": 9, "tokens_per_s": 1070.880382582202, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.8571417, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.404300000620424, "prefill_cuda_event_ms": 8.341119766235352, "kv_decode_ms": 62.03619999905641, "kv_decode_cuda_event_ms": 61.97145462036133, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 62.03619999905641, "cuda_event_ms": 61.97145462036133, "tokens_total": 8, "tokens_per_s": 128.9569638392049, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.8571417, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.404300000620424, "prefill_cuda_event_ms": 8.341119766235352, "kv_decode_ms": 62.03619999905641, "kv_decode_cuda_event_ms": 61.97145462036133, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 70.44049999967683, "cuda_event_ms": 70.31257438659668, "tokens_total": 17, "tokens_per_s": 241.3384345664496, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.9283898, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.405700000366778, "prefill_cuda_event_ms": 8.356415748596191, "kv_decode_ms": 61.789800000042305, "kv_decode_cuda_event_ms": 61.72675323486328, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 8.405700000366778, "cuda_event_ms": 8.356415748596191, "tokens_total": 9, "tokens_per_s": 1070.702023580105, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.9283898, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.405700000366778, "prefill_cuda_event_ms": 8.356415748596191, "kv_decode_ms": 61.789800000042305, "kv_decode_cuda_event_ms": 61.72675323486328, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 61.789800000042305, "cuda_event_ms": 61.72675323486328, "tokens_total": 8, "tokens_per_s": 129.4712072218153, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.9283898, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.405700000366778, "prefill_cuda_event_ms": 8.356415748596191, "kv_decode_ms": 61.789800000042305, "kv_decode_cuda_event_ms": 61.72675323486328, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 70.19550000040908, "cuda_event_ms": 70.08316898345947, "tokens_total": 17, "tokens_per_s": 242.18076657194447, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554262.9994137, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.86769999851822, "prefill_cuda_event_ms": 8.813088417053223, "kv_decode_ms": 61.250299999301205, "kv_decode_cuda_event_ms": 61.202430725097656, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 8.86769999851822, "cuda_event_ms": 8.813088417053223, "tokens_total": 9, "tokens_per_s": 1014.9193140841354, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554262.9994137, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.86769999851822, "prefill_cuda_event_ms": 8.813088417053223, "kv_decode_ms": 61.250299999301205, "kv_decode_cuda_event_ms": 61.202430725097656, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 61.250299999301205, "cuda_event_ms": 61.202430725097656, "tokens_total": 8, "tokens_per_s": 130.61160516913827, "gen_tokens": 8}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554262.9994137, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.86769999851822, "prefill_cuda_event_ms": 8.813088417053223, "kv_decode_ms": 61.250299999301205, "kv_decode_cuda_event_ms": 61.202430725097656, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 70.11799999781942, "cuda_event_ms": 70.01551914215088, "tokens_total": 17, "tokens_per_s": 242.44844405899593, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554263.0703104, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 534.6124000006967, "prefill_cuda_event_ms": null, "kv_decode_ms": 238.48670000006678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 160.72559999884106, "params_millions_measured": 74.824704, "latency_ms": 534.6124000006967, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 16.834626357316576, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554263.0703104, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 534.6124000006967, "prefill_cuda_event_ms": null, "kv_decode_ms": 238.48670000006678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 160.72559999884106, "params_millions_measured": 74.824704, "latency_ms": 238.48670000006678, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 33.54484757429978, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554263.0703104, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 534.6124000006967, "prefill_cuda_event_ms": null, "kv_decode_ms": 238.48670000006678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 160.72559999884106, "params_millions_measured": 74.824704, "latency_ms": 773.0991000007634, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 21.98941895027845, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554264.0067232, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.76810000029218, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.7788000000728, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 34.76810000029218, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 258.85797613111924, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554264.0067232, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.76810000029218, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.7788000000728, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 258.7788000000728, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 30.914433485269075, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554264.0067232, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.76810000029218, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.7788000000728, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 293.546900000365, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 57.912381292321136, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554264.3014793, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 38.10729999895557, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9765999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 38.10729999895557, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 236.17522102711735, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554264.3014793, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 38.10729999895557, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9765999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 273.9765999995143, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 29.199573978267424, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554264.3014793, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 38.10729999895557, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9765999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 312.08389999846986, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 54.472531265096826, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554264.6148007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 46.965600000476115, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.7761000009923, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 46.965600000476115, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 191.62961827185774, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554264.6148007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 46.965600000476115, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.7761000009923, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 292.7761000009923, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.32463476346903, "gen_tokens": 8}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554264.6148007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 46.965600000476115, "prefill_cuda_event_ms": null, "kv_decode_ms": 292.7761000009923, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 339.7417000014684, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 50.038014173492755, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554264.955432, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 57.38709999968705, "prefill_cuda_event_ms": null, "kv_decode_ms": 239.38540000017383, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 57.38709999968705, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 296.23382258543654, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554264.955432, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 57.38709999968705, "prefill_cuda_event_ms": null, "kv_decode_ms": 239.38540000017383, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 239.38540000017383, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 33.418913601222926, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554264.955432, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 57.38709999968705, "prefill_cuda_event_ms": null, "kv_decode_ms": 239.38540000017383, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 296.7724999998609, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 84.23961114999442, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554265.2540247, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.25950000062585, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9750000000495, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 54.25950000062585, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 313.30919009212977, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554265.2540247, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.25950000062585, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9750000000495, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 264.9750000000495, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 30.191527502588947, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554265.2540247, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.25950000062585, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9750000000495, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 319.2345000006753, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 78.31233779540467, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554265.5746722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.168200000276556, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.1058999990055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 54.168200000276556, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 313.8372698356823, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554265.5746722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.168200000276556, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.1058999990055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 287.1058999990055, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.864282830926538, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554265.5746722, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 54.168200000276556, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.1058999990055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 341.27409999928204, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 73.25489980063706, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554265.9178813, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 60.07659999886528, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.5414999986533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 60.07659999886528, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 282.97207232634827, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554265.9178813, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 60.07659999886528, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.5414999986533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 290.5414999986533, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.534792792207245, "gen_tokens": 8}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554265.9178813, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 60.07659999886528, "prefill_cuda_event_ms": null, "kv_decode_ms": 290.5414999986533, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 350.6180999975186, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 71.30265094750365, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554266.2703738, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 44.655399999101064, "prefill_cuda_event_ms": null, "kv_decode_ms": 191.75870000071882, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 44.655399999101064, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 201.54337437759318, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554266.2703738, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 44.655399999101064, "prefill_cuda_event_ms": null, "kv_decode_ms": 191.75870000071882, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 191.75870000071882, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 41.71909801208504, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554266.2703738, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 44.655399999101064, "prefill_cuda_event_ms": null, "kv_decode_ms": 191.75870000071882, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 236.41409999981988, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 71.90772462392451, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554266.5078743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.40429999864136, "prefill_cuda_event_ms": null, "kv_decode_ms": 186.05139999999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 34.40429999864136, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 261.5952075861277, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554266.5078743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.40429999864136, "prefill_cuda_event_ms": null, "kv_decode_ms": 186.05139999999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 186.05139999999665, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 42.99887020468615, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554266.5078743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 34.40429999864136, "prefill_cuda_event_ms": null, "kv_decode_ms": 186.05139999999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 220.455699998638, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 77.11299821281567, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554266.7295833, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 35.998299999846495, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.2249000001175, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 35.998299999846495, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 250.01180611413255, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554266.7295833, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 35.998299999846495, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.2249000001175, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 184.2249000001175, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 43.42518302354838, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554266.7295833, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 35.998299999846495, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.2249000001175, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 220.22319999996398, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 77.19441003492267, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554266.9507048, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.0323000006465, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.99189999884402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 37.0323000006465, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 243.03108367135934, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554266.9507048, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.0323000006465, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.99189999884402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 180.99189999884402, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 44.200873078027776, "gen_tokens": 8}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554266.9507048, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 37.0323000006465, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.99189999884402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 218.02419999949052, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 77.97299565846234, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.1700346, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.841200000460958, "prefill_cuda_event_ms": 8.708895683288574, "kv_decode_ms": 24.438500000542263, "kv_decode_cuda_event_ms": 24.394752502441406, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 8.841200000460958, "cuda_event_ms": 8.708895683288574, "tokens_total": 9, "tokens_per_s": 1017.9613626578703, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.1700346, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.841200000460958, "prefill_cuda_event_ms": 8.708895683288574, "kv_decode_ms": 24.438500000542263, "kv_decode_cuda_event_ms": 24.394752502441406, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 24.438500000542263, "cuda_event_ms": 24.394752502441406, "tokens_total": 8, "tokens_per_s": 327.352333401088, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.1700346, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 8.841200000460958, "prefill_cuda_event_ms": 8.708895683288574, "kv_decode_ms": 24.438500000542263, "kv_decode_cuda_event_ms": 24.394752502441406, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 33.27970000100322, "cuda_event_ms": 33.10364818572998, "tokens_total": 17, "tokens_per_s": 510.82191244174476, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.2059488, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.200499999569729, "prefill_cuda_event_ms": 4.150400161743164, "kv_decode_ms": 24.378000000069733, "kv_decode_cuda_event_ms": 24.33843231201172, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 4.200499999569729, "cuda_event_ms": 4.150400161743164, "tokens_total": 9, "tokens_per_s": 2142.6020714014758, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.2059488, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.200499999569729, "prefill_cuda_event_ms": 4.150400161743164, "kv_decode_ms": 24.378000000069733, "kv_decode_cuda_event_ms": 24.33843231201172, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 24.378000000069733, "cuda_event_ms": 24.33843231201172, "tokens_total": 8, "tokens_per_s": 328.1647386978881, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.2059488, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 4.200499999569729, "prefill_cuda_event_ms": 4.150400161743164, "kv_decode_ms": 24.378000000069733, "kv_decode_cuda_event_ms": 24.33843231201172, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 28.57849999963946, "cuda_event_ms": 28.488832473754883, "tokens_total": 17, "tokens_per_s": 594.8527739459547, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.2357156, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.836300000330084, "prefill_cuda_event_ms": 3.783008098602295, "kv_decode_ms": 24.15380000093137, "kv_decode_cuda_event_ms": 24.111103057861328, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 3.836300000330084, "cuda_event_ms": 3.783008098602295, "tokens_total": 9, "tokens_per_s": 2346.010478644949, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.2357156, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.836300000330084, "prefill_cuda_event_ms": 3.783008098602295, "kv_decode_ms": 24.15380000093137, "kv_decode_cuda_event_ms": 24.111103057861328, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 24.15380000093137, "cuda_event_ms": 24.111103057861328, "tokens_total": 8, "tokens_per_s": 331.21082395695584, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.2357156, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.836300000330084, "prefill_cuda_event_ms": 3.783008098602295, "kv_decode_ms": 24.15380000093137, "kv_decode_cuda_event_ms": 24.111103057861328, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 27.990100001261453, "cuda_event_ms": 27.894111156463623, "tokens_total": 17, "tokens_per_s": 607.3576014102789, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.2643414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.802200000791345, "prefill_cuda_event_ms": 3.7562239170074463, "kv_decode_ms": 24.132800001098076, "kv_decode_cuda_event_ms": 24.09267234802246, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 3.802200000791345, "cuda_event_ms": 3.7562239170074463, "tokens_total": 9, "tokens_per_s": 2367.050654391365, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.2643414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.802200000791345, "prefill_cuda_event_ms": 3.7562239170074463, "kv_decode_ms": 24.132800001098076, "kv_decode_cuda_event_ms": 24.09267234802246, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 24.132800001098076, "cuda_event_ms": 24.09267234802246, "tokens_total": 8, "tokens_per_s": 331.49903863770425, "gen_tokens": 8}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.2643414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 8, "prefill_ms": 3.802200000791345, "prefill_cuda_event_ms": 3.7562239170074463, "kv_decode_ms": 24.132800001098076, "kv_decode_cuda_event_ms": 24.09267234802246, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 27.93500000188942, "cuda_event_ms": 27.848896265029907, "tokens_total": 17, "tokens_per_s": 608.5555754018322, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.292891, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.787100001223735, "prefill_cuda_event_ms": 7.736991882324219, "kv_decode_ms": 44.32079999969574, "kv_decode_cuda_event_ms": 44.27366256713867, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 7.787100001223735, "cuda_event_ms": 7.736991882324219, "tokens_total": 17, "tokens_per_s": 2183.0976868575553, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.292891, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.787100001223735, "prefill_cuda_event_ms": 7.736991882324219, "kv_decode_ms": 44.32079999969574, "kv_decode_cuda_event_ms": 44.27366256713867, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 44.32079999969574, "cuda_event_ms": 44.27366256713867, "tokens_total": 8, "tokens_per_s": 180.5021570020153, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.292891, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.787100001223735, "prefill_cuda_event_ms": 7.736991882324219, "kv_decode_ms": 44.32079999969574, "kv_decode_cuda_event_ms": 44.27366256713867, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 52.10790000091947, "cuda_event_ms": 52.01065444946289, "tokens_total": 25, "tokens_per_s": 479.77370033255727, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.3462958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.70110000121349, "prefill_cuda_event_ms": 7.653120040893555, "kv_decode_ms": 44.071700000131386, "kv_decode_cuda_event_ms": 44.02278518676758, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 7.70110000121349, "cuda_event_ms": 7.653120040893555, "tokens_total": 17, "tokens_per_s": 2207.4768536080883, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.3462958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.70110000121349, "prefill_cuda_event_ms": 7.653120040893555, "kv_decode_ms": 44.071700000131386, "kv_decode_cuda_event_ms": 44.02278518676758, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 44.071700000131386, "cuda_event_ms": 44.02278518676758, "tokens_total": 8, "tokens_per_s": 181.52238284377844, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.3462958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.70110000121349, "prefill_cuda_event_ms": 7.653120040893555, "kv_decode_ms": 44.071700000131386, "kv_decode_cuda_event_ms": 44.02278518676758, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 51.772800001344876, "cuda_event_ms": 51.67590522766113, "tokens_total": 25, "tokens_per_s": 482.8790407192693, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.3987489, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680599999730475, "prefill_cuda_event_ms": 7.630335807800293, "kv_decode_ms": 44.75950000050943, "kv_decode_cuda_event_ms": 44.71500778198242, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 7.680599999730475, "cuda_event_ms": 7.630335807800293, "tokens_total": 17, "tokens_per_s": 2213.36874731096, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.3987489, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680599999730475, "prefill_cuda_event_ms": 7.630335807800293, "kv_decode_ms": 44.75950000050943, "kv_decode_cuda_event_ms": 44.71500778198242, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 44.75950000050943, "cuda_event_ms": 44.71500778198242, "tokens_total": 8, "tokens_per_s": 178.73300639884152, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.3987489, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680599999730475, "prefill_cuda_event_ms": 7.630335807800293, "kv_decode_ms": 44.75950000050943, "kv_decode_cuda_event_ms": 44.71500778198242, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 52.44010000023991, "cuda_event_ms": 52.345343589782715, "tokens_total": 25, "tokens_per_s": 476.7344074455546, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554267.4518046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680999999138294, "prefill_cuda_event_ms": 7.627903938293457, "kv_decode_ms": 45.22440000073402, "kv_decode_cuda_event_ms": 45.18195343017578, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 7.680999999138294, "cuda_event_ms": 7.627903938293457, "tokens_total": 17, "tokens_per_s": 2213.253482867748, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554267.4518046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680999999138294, "prefill_cuda_event_ms": 7.627903938293457, "kv_decode_ms": 45.22440000073402, "kv_decode_cuda_event_ms": 45.18195343017578, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 45.22440000073402, "cuda_event_ms": 45.18195343017578, "tokens_total": 8, "tokens_per_s": 176.8956580932009, "gen_tokens": 8}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "batch_size_config": 1, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554267.4518046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 8, "prefill_ms": 7.680999999138294, "prefill_cuda_event_ms": 7.627903938293457, "kv_decode_ms": 45.22440000073402, "kv_decode_cuda_event_ms": 45.18195343017578, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 52.905399999872316, "cuda_event_ms": 52.80985736846924, "tokens_total": 25, "tokens_per_s": 472.5415553055139, "gen_tokens": 8}
