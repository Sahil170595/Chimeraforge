{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554297.4727173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 153.89040000081877, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.5167000005604, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7682.005299999219, "params_millions_measured": 45.1712, "latency_ms": 153.89040000081877, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 110.46822933665486, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554297.4727173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 153.89040000081877, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.5167000005604, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7682.005299999219, "params_millions_measured": 45.1712, "latency_ms": 887.5167000005604, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 36.055659572354855, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554297.4727173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 153.89040000081877, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.5167000005604, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7682.005299999219, "params_millions_measured": 45.1712, "latency_ms": 1041.4071000013791, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 47.0517245368647, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554306.206902, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 48.6799999998766, "prefill_cuda_event_ms": null, "kv_decode_ms": 837.0512000001327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 48.6799999998766, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 349.2193919482969, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554306.206902, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 48.6799999998766, "prefill_cuda_event_ms": null, "kv_decode_ms": 837.0512000001327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 837.0512000001327, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 38.22944163988407, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554306.206902, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 48.6799999998766, "prefill_cuda_event_ms": null, "kv_decode_ms": 837.0512000001327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 885.7312000000093, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 55.32152418250536, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554307.0937266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.79120000115654, "prefill_cuda_event_ms": null, "kv_decode_ms": 523.0212999995274, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 32.79120000115654, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 518.4317743602068, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554307.0937266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.79120000115654, "prefill_cuda_event_ms": null, "kv_decode_ms": 523.0212999995274, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 523.0212999995274, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 61.18297667806056, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554307.0937266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.79120000115654, "prefill_cuda_event_ms": null, "kv_decode_ms": 523.0212999995274, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 555.8125000006839, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 88.15922635770103, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554307.6505208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 31.930799999827286, "prefill_cuda_event_ms": null, "kv_decode_ms": 900.8964999993623, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 31.930799999827286, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 532.40131785273, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554307.6505208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 31.930799999827286, "prefill_cuda_event_ms": null, "kv_decode_ms": 900.8964999993623, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 900.8964999993623, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 35.52017351607277, "gen_tokens": 32}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554307.6505208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 31.930799999827286, "prefill_cuda_event_ms": null, "kv_decode_ms": 900.8964999993623, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 932.8272999991896, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 52.52847981619167, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554308.5840282, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 758.6186, "prefill_cuda_event_ms": null, "kv_decode_ms": 1191.0076, "kv_decode_ms_equiv": 1191.0076, "kv_decode_ms_per_token": 37.2189875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13118.274700000256, "ollama_total_duration_ms": 12988.537, "ollama_load_ms": 10958.9278, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 758.6186, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 121.2730613248871, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554308.5840282, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 758.6186, "prefill_cuda_event_ms": null, "kv_decode_ms": 1191.0076, "kv_decode_ms_equiv": 1191.0076, "kv_decode_ms_per_token": 37.2189875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13118.274700000256, "ollama_total_duration_ms": 12988.537, "ollama_load_ms": 10958.9278, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1191.0076, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 26.8680065517634, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554308.5840282, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 758.6186, "prefill_cuda_event_ms": null, "kv_decode_ms": 1191.0076, "kv_decode_ms_equiv": 1191.0076, "kv_decode_ms_per_token": 37.2189875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13118.274700000256, "ollama_total_duration_ms": 12988.537, "ollama_load_ms": 10958.9278, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1949.6262, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 63.601935591550834, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554321.70244, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 48.8241, "prefill_cuda_event_ms": null, "kv_decode_ms": 1367.1529, "kv_decode_ms_equiv": 1367.1529, "kv_decode_ms_per_token": 42.723528125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1948.3531999994739, "ollama_total_duration_ms": 1926.1415, "ollama_load_ms": 493.2088, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 48.8241, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1884.3153278811078, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554321.70244, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 48.8241, "prefill_cuda_event_ms": null, "kv_decode_ms": 1367.1529, "kv_decode_ms_equiv": 1367.1529, "kv_decode_ms_per_token": 42.723528125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1948.3531999994739, "ollama_total_duration_ms": 1926.1415, "ollama_load_ms": 493.2088, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1367.1529, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 23.406306639147676, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554321.70244, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 48.8241, "prefill_cuda_event_ms": null, "kv_decode_ms": 1367.1529, "kv_decode_ms_equiv": 1367.1529, "kv_decode_ms_per_token": 42.723528125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1948.3531999994739, "ollama_total_duration_ms": 1926.1415, "ollama_load_ms": 493.2088, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1415.977, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 87.57204389619322, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554323.6514368, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.1194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1540.2998, "kv_decode_ms_equiv": 1540.2998, "kv_decode_ms_per_token": 48.13436875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2053.816600000573, "ollama_total_duration_ms": 2027.8748, "ollama_load_ms": 423.1499, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 49.1194, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1872.9870478873927, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554323.6514368, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.1194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1540.2998, "kv_decode_ms_equiv": 1540.2998, "kv_decode_ms_per_token": 48.13436875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2053.816600000573, "ollama_total_duration_ms": 2027.8748, "ollama_load_ms": 423.1499, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1540.2998, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.77517636501673, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554323.6514368, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 49.1194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1540.2998, "kv_decode_ms_equiv": 1540.2998, "kv_decode_ms_per_token": 48.13436875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2053.816600000573, "ollama_total_duration_ms": 2027.8748, "ollama_load_ms": 423.1499, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1589.4192, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 78.01591927416001, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554325.705375, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 65.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1494.6699, "kv_decode_ms_equiv": 1494.6699, "kv_decode_ms_per_token": 46.708434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2025.1697000003333, "ollama_total_duration_ms": 2004.1022, "ollama_load_ms": 433.0687, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 65.9739, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1394.4908516852877, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554325.705375, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 65.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1494.6699, "kv_decode_ms_equiv": 1494.6699, "kv_decode_ms_per_token": 46.708434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2025.1697000003333, "ollama_total_duration_ms": 2004.1022, "ollama_load_ms": 433.0687, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1494.6699, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.409409529154228, "gen_tokens": 32}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554325.705375, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 65.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1494.6699, "kv_decode_ms_equiv": 1494.6699, "kv_decode_ms_per_token": 46.708434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2025.1697000003333, "ollama_total_duration_ms": 2004.1022, "ollama_load_ms": 433.0687, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1560.6438, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 79.45438927191458, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554327.7307174, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 692.5157000005129, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.6105999998254, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.59199999980046, "params_millions_measured": 96.08832, "latency_ms": 692.5157000005129, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 12.996095250971688, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554327.7307174, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 692.5157000005129, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.6105999998254, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.59199999980046, "params_millions_measured": 96.08832, "latency_ms": 597.6105999998254, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 53.54657363843505, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554327.7307174, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 692.5157000005129, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.6105999998254, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.59199999980046, "params_millions_measured": 96.08832, "latency_ms": 1290.1263000003382, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 31.779834268931072, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554329.323135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.75859999988461, "prefill_cuda_event_ms": null, "kv_decode_ms": 577.3736999999528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 21.75859999988461, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 413.6295533741936, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554329.323135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.75859999988461, "prefill_cuda_event_ms": null, "kv_decode_ms": 577.3736999999528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 577.3736999999528, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 55.42337657569546, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554329.323135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.75859999988461, "prefill_cuda_event_ms": null, "kv_decode_ms": 577.3736999999528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 599.1322999998374, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 68.43229784141353, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554329.9226255, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 23.80079999966256, "prefill_cuda_event_ms": null, "kv_decode_ms": 608.7845000001835, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 23.80079999966256, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 378.1385499700682, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554329.9226255, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 23.80079999966256, "prefill_cuda_event_ms": null, "kv_decode_ms": 608.7845000001835, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 608.7845000001835, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 52.56375614029325, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554329.9226255, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 23.80079999966256, "prefill_cuda_event_ms": null, "kv_decode_ms": 608.7845000001835, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 632.585299999846, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 64.81339354551865, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554330.555648, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 25.001899999551824, "prefill_cuda_event_ms": null, "kv_decode_ms": 600.0282000004518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 25.001899999551824, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 359.97264208565474, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554330.555648, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 25.001899999551824, "prefill_cuda_event_ms": null, "kv_decode_ms": 600.0282000004518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 600.0282000004518, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 53.33082678443431, "gen_tokens": 32}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554330.555648, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 25.001899999551824, "prefill_cuda_event_ms": null, "kv_decode_ms": 600.0282000004518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 625.0301000000036, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 65.596840856144, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554331.181179, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 457.87059999929625, "prefill_cuda_event_ms": 456.2433776855469, "kv_decode_ms": 265.4116999983671, "kv_decode_cuda_event_ms": 265.3682556152344, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 381.12550000005285, "params_millions_measured": 96.08832, "latency_ms": 457.87059999929625, "cuda_event_ms": 456.2433776855469, "tokens_total": 17, "tokens_per_s": 37.128393917465175, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554331.181179, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 457.87059999929625, "prefill_cuda_event_ms": 456.2433776855469, "kv_decode_ms": 265.4116999983671, "kv_decode_cuda_event_ms": 265.3682556152344, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 381.12550000005285, "params_millions_measured": 96.08832, "latency_ms": 265.4116999983671, "cuda_event_ms": 265.3682556152344, "tokens_total": 32, "tokens_per_s": 120.56740528091592, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554331.181179, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 457.87059999929625, "prefill_cuda_event_ms": 456.2433776855469, "kv_decode_ms": 265.4116999983671, "kv_decode_cuda_event_ms": 265.3682556152344, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 381.12550000005285, "params_millions_measured": 96.08832, "latency_ms": 723.2822999976634, "cuda_event_ms": 721.6116333007812, "tokens_total": 49, "tokens_per_s": 67.74671521777638, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554332.4253087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.561999998666579, "prefill_cuda_event_ms": 6.487040042877197, "kv_decode_ms": 147.07069999894884, "kv_decode_cuda_event_ms": 147.03695678710938, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 6.561999998666579, "cuda_event_ms": 6.487040042877197, "tokens_total": 17, "tokens_per_s": 2590.6735756559674, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554332.4253087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.561999998666579, "prefill_cuda_event_ms": 6.487040042877197, "kv_decode_ms": 147.07069999894884, "kv_decode_cuda_event_ms": 147.03695678710938, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 147.07069999894884, "cuda_event_ms": 147.03695678710938, "tokens_total": 32, "tokens_per_s": 217.58242804466636, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554332.4253087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.561999998666579, "prefill_cuda_event_ms": 6.487040042877197, "kv_decode_ms": 147.07069999894884, "kv_decode_cuda_event_ms": 147.03695678710938, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 153.63269999761542, "cuda_event_ms": 153.52399682998657, "tokens_total": 49, "tokens_per_s": 318.9425167998775, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554332.5798223, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.788500000970089, "prefill_cuda_event_ms": 7.738976001739502, "kv_decode_ms": 134.45519999913813, "kv_decode_cuda_event_ms": 134.41944885253906, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 7.788500000970089, "cuda_event_ms": 7.738976001739502, "tokens_total": 17, "tokens_per_s": 2182.7052703193917, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554332.5798223, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.788500000970089, "prefill_cuda_event_ms": 7.738976001739502, "kv_decode_ms": 134.45519999913813, "kv_decode_cuda_event_ms": 134.41944885253906, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 134.45519999913813, "cuda_event_ms": 134.41944885253906, "tokens_total": 32, "tokens_per_s": 237.9974891280153, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554332.5798223, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.788500000970089, "prefill_cuda_event_ms": 7.738976001739502, "kv_decode_ms": 134.45519999913813, "kv_decode_cuda_event_ms": 134.41944885253906, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 142.24370000010822, "cuda_event_ms": 142.15842485427856, "tokens_total": 49, "tokens_per_s": 344.4792282537836, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554332.722875, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.328299999746378, "prefill_cuda_event_ms": 5.2781758308410645, "kv_decode_ms": 153.48979999907897, "kv_decode_cuda_event_ms": 153.4626922607422, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 5.328299999746378, "cuda_event_ms": 5.2781758308410645, "tokens_total": 17, "tokens_per_s": 3190.511044950394, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554332.722875, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.328299999746378, "prefill_cuda_event_ms": 5.2781758308410645, "kv_decode_ms": 153.48979999907897, "kv_decode_cuda_event_ms": 153.4626922607422, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 153.48979999907897, "cuda_event_ms": 153.4626922607422, "tokens_total": 32, "tokens_per_s": 208.48290896327978, "gen_tokens": 32}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554332.722875, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.328299999746378, "prefill_cuda_event_ms": 5.2781758308410645, "kv_decode_ms": 153.48979999907897, "kv_decode_cuda_event_ms": 153.4626922607422, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 158.81809999882535, "cuda_event_ms": 158.74086809158325, "tokens_total": 49, "tokens_per_s": 308.52906564404446, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554332.8824034, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.723999999347143, "prefill_cuda_event_ms": 8.671551704406738, "kv_decode_ms": 184.38729999979842, "kv_decode_cuda_event_ms": 184.34970092773438, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 297.99820000152977, "params_millions_measured": 51.475968, "latency_ms": 8.723999999347143, "cuda_event_ms": 8.671551704406738, "tokens_total": 17, "tokens_per_s": 1948.6474095910348, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554332.8824034, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.723999999347143, "prefill_cuda_event_ms": 8.671551704406738, "kv_decode_ms": 184.38729999979842, "kv_decode_cuda_event_ms": 184.34970092773438, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 297.99820000152977, "params_millions_measured": 51.475968, "latency_ms": 184.38729999979842, "cuda_event_ms": 184.34970092773438, "tokens_total": 32, "tokens_per_s": 173.54774434049952, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554332.8824034, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.723999999347143, "prefill_cuda_event_ms": 8.671551704406738, "kv_decode_ms": 184.38729999979842, "kv_decode_cuda_event_ms": 184.34970092773438, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 297.99820000152977, "params_millions_measured": 51.475968, "latency_ms": 193.11129999914556, "cuda_event_ms": 193.0212526321411, "tokens_total": 49, "tokens_per_s": 253.73968276437893, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554333.3747728, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.362500000046566, "prefill_cuda_event_ms": 5.309599876403809, "kv_decode_ms": 143.97749999989173, "kv_decode_cuda_event_ms": 143.91807556152344, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 5.362500000046566, "cuda_event_ms": 5.309599876403809, "tokens_total": 17, "tokens_per_s": 3170.1631701356414, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554333.3747728, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.362500000046566, "prefill_cuda_event_ms": 5.309599876403809, "kv_decode_ms": 143.97749999989173, "kv_decode_cuda_event_ms": 143.91807556152344, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 143.97749999989173, "cuda_event_ms": 143.91807556152344, "tokens_total": 32, "tokens_per_s": 222.25694987080664, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554333.3747728, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 5.362500000046566, "prefill_cuda_event_ms": 5.309599876403809, "kv_decode_ms": 143.97749999989173, "kv_decode_cuda_event_ms": 143.91807556152344, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 149.3399999999383, "cuda_event_ms": 149.22767543792725, "tokens_total": 49, "tokens_per_s": 328.1103522165545, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554333.5266716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.920799998624716, "prefill_cuda_event_ms": 6.868000030517578, "kv_decode_ms": 142.4458000001323, "kv_decode_cuda_event_ms": 142.40972900390625, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 6.920799998624716, "cuda_event_ms": 6.868000030517578, "tokens_total": 17, "tokens_per_s": 2456.363426681626, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554333.5266716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.920799998624716, "prefill_cuda_event_ms": 6.868000030517578, "kv_decode_ms": 142.4458000001323, "kv_decode_cuda_event_ms": 142.40972900390625, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 142.4458000001323, "cuda_event_ms": 142.40972900390625, "tokens_total": 32, "tokens_per_s": 224.6468481343099, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554333.5266716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.920799998624716, "prefill_cuda_event_ms": 6.868000030517578, "kv_decode_ms": 142.4458000001323, "kv_decode_cuda_event_ms": 142.40972900390625, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 149.366599998757, "cuda_event_ms": 149.27772903442383, "tokens_total": 49, "tokens_per_s": 328.05192057935153, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554333.6770363, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.868900001383736, "prefill_cuda_event_ms": 4.800960063934326, "kv_decode_ms": 137.8681999995024, "kv_decode_cuda_event_ms": 137.83859252929688, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 4.868900001383736, "cuda_event_ms": 4.800960063934326, "tokens_total": 17, "tokens_per_s": 3491.5483980300723, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554333.6770363, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.868900001383736, "prefill_cuda_event_ms": 4.800960063934326, "kv_decode_ms": 137.8681999995024, "kv_decode_cuda_event_ms": 137.83859252929688, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 137.8681999995024, "cuda_event_ms": 137.83859252929688, "tokens_total": 32, "tokens_per_s": 232.1057357687668, "gen_tokens": 32}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554333.6770363, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.868900001383736, "prefill_cuda_event_ms": 4.800960063934326, "kv_decode_ms": 137.8681999995024, "kv_decode_cuda_event_ms": 137.83859252929688, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 142.73710000088613, "cuda_event_ms": 142.6395525932312, "tokens_total": 49, "tokens_per_s": 343.2884652952582, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554333.8204677, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 97.96749999986787, "prefill_cuda_event_ms": null, "kv_decode_ms": 374.2452000005869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 123.5701000005065, "params_millions_measured": 51.475968, "latency_ms": 97.96749999986787, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 91.86720085755111, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554333.8204677, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 97.96749999986787, "prefill_cuda_event_ms": null, "kv_decode_ms": 374.2452000005869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 123.5701000005065, "params_millions_measured": 51.475968, "latency_ms": 374.2452000005869, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 85.50543868017496, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554333.8204677, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 97.96749999986787, "prefill_cuda_event_ms": null, "kv_decode_ms": 374.2452000005869, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 123.5701000005065, "params_millions_measured": 51.475968, "latency_ms": 472.2127000004548, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 86.82528021791984, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554334.4170084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.95459999993909, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.57620000091265, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 17.95459999993909, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 501.26429995825754, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554334.4170084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.95459999993909, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.57620000091265, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 397.57620000091265, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 80.48771531074179, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554334.4170084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.95459999993909, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.57620000091265, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 415.53080000085174, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 98.66897953151958, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554334.8331504, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.618699999526143, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.235300000204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.618699999526143, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 541.5586056825516, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554334.8331504, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.618699999526143, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.235300000204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 385.235300000204, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 83.06611569599944, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554334.8331504, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.618699999526143, "prefill_cuda_event_ms": null, "kv_decode_ms": 385.235300000204, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 401.85399999973015, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 102.02710437130781, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554335.2354522, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 14.613000001190812, "prefill_cuda_event_ms": null, "kv_decode_ms": 375.24800000028335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.613000001190812, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 615.889960943447, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554335.2354522, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 14.613000001190812, "prefill_cuda_event_ms": null, "kv_decode_ms": 375.24800000028335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 375.24800000028335, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 85.27693685236386, "gen_tokens": 32}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554335.2354522, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 14.613000001190812, "prefill_cuda_event_ms": null, "kv_decode_ms": 375.24800000028335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 389.86100000147417, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 105.16568725736857, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554335.6256902, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 42.978100000254926, "prefill_cuda_event_ms": 42.86809539794922, "kv_decode_ms": 128.58690000030037, "kv_decode_cuda_event_ms": 128.5191650390625, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 195.52370000019437, "params_millions_measured": 45.1712, "latency_ms": 42.978100000254926, "cuda_event_ms": 42.86809539794922, "tokens_total": 9, "tokens_per_s": 209.40897805967728, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554335.6256902, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 42.978100000254926, "prefill_cuda_event_ms": 42.86809539794922, "kv_decode_ms": 128.58690000030037, "kv_decode_cuda_event_ms": 128.5191650390625, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 195.52370000019437, "params_millions_measured": 45.1712, "latency_ms": 128.58690000030037, "cuda_event_ms": 128.5191650390625, "tokens_total": 32, "tokens_per_s": 248.85894286218308, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554335.6256902, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 42.978100000254926, "prefill_cuda_event_ms": 42.86809539794922, "kv_decode_ms": 128.58690000030037, "kv_decode_cuda_event_ms": 128.5191650390625, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 195.52370000019437, "params_millions_measured": 45.1712, "latency_ms": 171.5650000005553, "cuda_event_ms": 171.38726043701172, "tokens_total": 41, "tokens_per_s": 238.97648121625795, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554335.9938781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.545699999653152, "prefill_cuda_event_ms": 4.493023872375488, "kv_decode_ms": 129.70189999941795, "kv_decode_cuda_event_ms": 129.6670379638672, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 4.545699999653152, "cuda_event_ms": 4.493023872375488, "tokens_total": 9, "tokens_per_s": 1979.8930859244388, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554335.9938781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.545699999653152, "prefill_cuda_event_ms": 4.493023872375488, "kv_decode_ms": 129.70189999941795, "kv_decode_cuda_event_ms": 129.6670379638672, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 129.70189999941795, "cuda_event_ms": 129.6670379638672, "tokens_total": 32, "tokens_per_s": 246.71959316049808, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554335.9938781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.545699999653152, "prefill_cuda_event_ms": 4.493023872375488, "kv_decode_ms": 129.70189999941795, "kv_decode_cuda_event_ms": 129.6670379638672, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 134.2475999990711, "cuda_event_ms": 134.16006183624268, "tokens_total": 41, "tokens_per_s": 305.4058322106592, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554336.1289558, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.407600001286482, "prefill_cuda_event_ms": 4.358560085296631, "kv_decode_ms": 113.34569999962696, "kv_decode_cuda_event_ms": 113.29808044433594, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 4.407600001286482, "cuda_event_ms": 4.358560085296631, "tokens_total": 9, "tokens_per_s": 2041.9275790391814, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554336.1289558, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.407600001286482, "prefill_cuda_event_ms": 4.358560085296631, "kv_decode_ms": 113.34569999962696, "kv_decode_cuda_event_ms": 113.29808044433594, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 113.34569999962696, "cuda_event_ms": 113.29808044433594, "tokens_total": 32, "tokens_per_s": 282.32213485033236, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554336.1289558, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.407600001286482, "prefill_cuda_event_ms": 4.358560085296631, "kv_decode_ms": 113.34569999962696, "kv_decode_cuda_event_ms": 113.29808044433594, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 117.75330000091344, "cuda_event_ms": 117.65664052963257, "tokens_total": 41, "tokens_per_s": 348.18557101738935, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554336.2474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.582999999911408, "prefill_cuda_event_ms": 4.529600143432617, "kv_decode_ms": 115.16850000043632, "kv_decode_cuda_event_ms": 115.10272216796875, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 4.582999999911408, "cuda_event_ms": 4.529600143432617, "tokens_total": 9, "tokens_per_s": 1963.7791839786112, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554336.2474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.582999999911408, "prefill_cuda_event_ms": 4.529600143432617, "kv_decode_ms": 115.16850000043632, "kv_decode_cuda_event_ms": 115.10272216796875, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 115.16850000043632, "cuda_event_ms": 115.10272216796875, "tokens_total": 32, "tokens_per_s": 277.8537534124241, "gen_tokens": 32}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554336.2474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.582999999911408, "prefill_cuda_event_ms": 4.529600143432617, "kv_decode_ms": 115.16850000043632, "kv_decode_cuda_event_ms": 115.10272216796875, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 119.75150000034773, "cuda_event_ms": 119.63232231140137, "tokens_total": 41, "tokens_per_s": 342.37566961483526, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554336.3681788, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.7781, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0717, "kv_decode_ms_equiv": 67.0717, "kv_decode_ms_per_token": 2.095990625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 3252.8633999991143, "ollama_total_duration_ms": 3171.862, "ollama_load_ms": 3048.4119, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 7.7781, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2185.6237384451215, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554336.3681788, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.7781, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0717, "kv_decode_ms_equiv": 67.0717, "kv_decode_ms_per_token": 2.095990625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 3252.8633999991143, "ollama_total_duration_ms": 3171.862, "ollama_load_ms": 3048.4119, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 67.0717, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 477.1013706227812, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554336.3681788, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 7.7781, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0717, "kv_decode_ms_equiv": 67.0717, "kv_decode_ms_per_token": 2.095990625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 3252.8633999991143, "ollama_total_duration_ms": 3171.862, "ollama_load_ms": 3048.4119, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 74.8498, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 654.6443677872218, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554339.6211696, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8052, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.1038, "kv_decode_ms_equiv": 63.1038, "kv_decode_ms_per_token": 1.97199375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 414.43900000012945, "ollama_total_duration_ms": 387.7938, "ollama_load_ms": 294.8558, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.8052, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6060.173962640809, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554339.6211696, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8052, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.1038, "kv_decode_ms_equiv": 63.1038, "kv_decode_ms_per_token": 1.97199375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 414.43900000012945, "ollama_total_duration_ms": 387.7938, "ollama_load_ms": 294.8558, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.1038, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 507.1009986720292, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554339.6211696, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8052, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.1038, "kv_decode_ms_equiv": 63.1038, "kv_decode_ms_per_token": 1.97199375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 414.43900000012945, "ollama_total_duration_ms": 387.7938, "ollama_load_ms": 294.8558, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 65.909, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 743.449301309381, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554340.0356972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9264, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.9724, "kv_decode_ms_equiv": 63.9724, "kv_decode_ms_per_token": 1.9991375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 390.55759999973816, "ollama_total_duration_ms": 379.4003, "ollama_load_ms": 293.5041, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.9264, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5809.185347184253, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554340.0356972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9264, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.9724, "kv_decode_ms_equiv": 63.9724, "kv_decode_ms_per_token": 1.9991375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 390.55759999973816, "ollama_total_duration_ms": 379.4003, "ollama_load_ms": 293.5041, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.9724, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 500.21571802839975, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554340.0356972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9264, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.9724, "kv_decode_ms_equiv": 63.9724, "kv_decode_ms_per_token": 1.9991375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 390.55759999973816, "ollama_total_duration_ms": 379.4003, "ollama_load_ms": 293.5041, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 66.8988, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 732.4496104563909, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554340.4264302, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9274, "prefill_cuda_event_ms": null, "kv_decode_ms": 58.2434, "kv_decode_ms_equiv": 58.2434, "kv_decode_ms_per_token": 1.82010625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 402.0455000008951, "ollama_total_duration_ms": 384.4076, "ollama_load_ms": 292.637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.9274, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5807.200929152148, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554340.4264302, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9274, "prefill_cuda_event_ms": null, "kv_decode_ms": 58.2434, "kv_decode_ms_equiv": 58.2434, "kv_decode_ms_per_token": 1.82010625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 402.0455000008951, "ollama_total_duration_ms": 384.4076, "ollama_load_ms": 292.637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 58.2434, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 549.4184748829911, "gen_tokens": 32}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554340.4264302, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.9274, "prefill_cuda_event_ms": null, "kv_decode_ms": 58.2434, "kv_decode_ms_equiv": 58.2434, "kv_decode_ms_per_token": 1.82010625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 402.0455000008951, "ollama_total_duration_ms": 384.4076, "ollama_load_ms": 292.637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 61.1708, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 801.0357883173018, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554340.8286083, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.704800000079558, "prefill_cuda_event_ms": 6.610527992248535, "kv_decode_ms": 146.69770000000426, "kv_decode_cuda_event_ms": 146.66342163085938, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 6.704800000079558, "cuda_event_ms": 6.610527992248535, "tokens_total": 9, "tokens_per_s": 1342.3219186095346, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554340.8286083, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.704800000079558, "prefill_cuda_event_ms": 6.610527992248535, "kv_decode_ms": 146.69770000000426, "kv_decode_cuda_event_ms": 146.66342163085938, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 146.69770000000426, "cuda_event_ms": 146.66342163085938, "tokens_total": 32, "tokens_per_s": 218.13566265864475, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554340.8286083, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.704800000079558, "prefill_cuda_event_ms": 6.610527992248535, "kv_decode_ms": 146.69770000000426, "kv_decode_cuda_event_ms": 146.66342163085938, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 153.40250000008382, "cuda_event_ms": 153.2739496231079, "tokens_total": 41, "tokens_per_s": 267.27074200210296, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554340.9835768, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.470099999001832, "prefill_cuda_event_ms": 5.423935890197754, "kv_decode_ms": 142.77489999949466, "kv_decode_cuda_event_ms": 142.74627685546875, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 5.470099999001832, "cuda_event_ms": 5.423935890197754, "tokens_total": 9, "tokens_per_s": 1645.3081299505118, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554340.9835768, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.470099999001832, "prefill_cuda_event_ms": 5.423935890197754, "kv_decode_ms": 142.77489999949466, "kv_decode_cuda_event_ms": 142.74627685546875, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 142.77489999949466, "cuda_event_ms": 142.74627685546875, "tokens_total": 32, "tokens_per_s": 224.12903108398788, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554340.9835768, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.470099999001832, "prefill_cuda_event_ms": 5.423935890197754, "kv_decode_ms": 142.77489999949466, "kv_decode_cuda_event_ms": 142.74627685546875, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 148.2449999984965, "cuda_event_ms": 148.1702127456665, "tokens_total": 41, "tokens_per_s": 276.569192892953, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554341.1326323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.5881000007502735, "prefill_cuda_event_ms": 5.543263912200928, "kv_decode_ms": 145.3923000008217, "kv_decode_cuda_event_ms": 145.36614990234375, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 5.5881000007502735, "cuda_event_ms": 5.543263912200928, "tokens_total": 9, "tokens_per_s": 1610.5653082070178, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554341.1326323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.5881000007502735, "prefill_cuda_event_ms": 5.543263912200928, "kv_decode_ms": 145.3923000008217, "kv_decode_cuda_event_ms": 145.36614990234375, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 145.3923000008217, "cuda_event_ms": 145.36614990234375, "tokens_total": 32, "tokens_per_s": 220.0941865547154, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554341.1326323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.5881000007502735, "prefill_cuda_event_ms": 5.543263912200928, "kv_decode_ms": 145.3923000008217, "kv_decode_cuda_event_ms": 145.36614990234375, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 150.98040000157198, "cuda_event_ms": 150.90941381454468, "tokens_total": 41, "tokens_per_s": 271.55842744868283, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554341.284323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.481399999553105, "prefill_cuda_event_ms": 5.434239864349365, "kv_decode_ms": 145.95190000000002, "kv_decode_cuda_event_ms": 145.9220428466797, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 5.481399999553105, "cuda_event_ms": 5.434239864349365, "tokens_total": 9, "tokens_per_s": 1641.9162988896567, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554341.284323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.481399999553105, "prefill_cuda_event_ms": 5.434239864349365, "kv_decode_ms": 145.95190000000002, "kv_decode_cuda_event_ms": 145.9220428466797, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 145.95190000000002, "cuda_event_ms": 145.9220428466797, "tokens_total": 32, "tokens_per_s": 219.25031465845936, "gen_tokens": 32}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554341.284323, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.481399999553105, "prefill_cuda_event_ms": 5.434239864349365, "kv_decode_ms": 145.95190000000002, "kv_decode_cuda_event_ms": 145.9220428466797, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 151.43329999955313, "cuda_event_ms": 151.35628271102905, "tokens_total": 41, "tokens_per_s": 270.74626254675155, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554341.4364328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 39.51770000094257, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.10359999995853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.5041999999812, "params_millions_measured": 5.03672, "latency_ms": 39.51770000094257, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 430.18697949512546, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554341.4364328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 39.51770000094257, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.10359999995853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.5041999999812, "params_millions_measured": 5.03672, "latency_ms": 333.10359999995853, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 96.06620883113837, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554341.4364328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 39.51770000094257, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.10359999995853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.5041999999812, "params_millions_measured": 5.03672, "latency_ms": 372.6213000009011, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 131.50080255713107, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554341.957318, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 17.868400000224938, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5539999984903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 17.868400000224938, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 951.4002372784354, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554341.957318, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 17.868400000224938, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5539999984903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 322.5539999984903, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 99.20819459733804, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554341.957318, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 17.868400000224938, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5539999984903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 340.42239999871526, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 143.9388242377262, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554342.2981684, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.594900001611677, "prefill_cuda_event_ms": null, "kv_decode_ms": 308.57380000088597, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.594900001611677, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1164.790440367713, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554342.2981684, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.594900001611677, "prefill_cuda_event_ms": null, "kv_decode_ms": 308.57380000088597, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 308.57380000088597, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 103.70290672736351, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554342.2981684, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.594900001611677, "prefill_cuda_event_ms": null, "kv_decode_ms": 308.57380000088597, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 323.16870000249764, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 151.62359473433318, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554342.621731, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.339399998789304, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.03519999851414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.339399998789304, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1185.5447230313216, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554342.621731, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.339399998789304, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.03519999851414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 311.03519999851414, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 102.88224612568888, "gen_tokens": 32}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554342.621731, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 14.339399998789304, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.03519999851414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 325.37459999730345, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 150.5956519052381, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554342.9477406, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.422399999384652, "prefill_cuda_event_ms": 4.362688064575195, "kv_decode_ms": 115.55229999976291, "kv_decode_cuda_event_ms": 115.51744079589844, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 334.6292000005633, "params_millions_measured": 74.824704, "latency_ms": 4.422399999384652, "cuda_event_ms": 4.362688064575195, "tokens_total": 9, "tokens_per_s": 2035.0940668533583, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554342.9477406, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.422399999384652, "prefill_cuda_event_ms": 4.362688064575195, "kv_decode_ms": 115.55229999976291, "kv_decode_cuda_event_ms": 115.51744079589844, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 334.6292000005633, "params_millions_measured": 74.824704, "latency_ms": 115.55229999976291, "cuda_event_ms": 115.51744079589844, "tokens_total": 32, "tokens_per_s": 276.93087891859926, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554342.9477406, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.422399999384652, "prefill_cuda_event_ms": 4.362688064575195, "kv_decode_ms": 115.55229999976291, "kv_decode_cuda_event_ms": 115.51744079589844, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 334.6292000005633, "params_millions_measured": 74.824704, "latency_ms": 119.97469999914756, "cuda_event_ms": 119.88012886047363, "tokens_total": 41, "tokens_per_s": 341.73871658184027, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554343.4033065, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.189100000075996, "prefill_cuda_event_ms": 4.134079933166504, "kv_decode_ms": 119.07149999933608, "kv_decode_cuda_event_ms": 119.03180694580078, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 4.189100000075996, "cuda_event_ms": 4.134079933166504, "tokens_total": 9, "tokens_per_s": 2148.4328375633736, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554343.4033065, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.189100000075996, "prefill_cuda_event_ms": 4.134079933166504, "kv_decode_ms": 119.07149999933608, "kv_decode_cuda_event_ms": 119.03180694580078, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 119.07149999933608, "cuda_event_ms": 119.03180694580078, "tokens_total": 32, "tokens_per_s": 268.74608953593787, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554343.4033065, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.189100000075996, "prefill_cuda_event_ms": 4.134079933166504, "kv_decode_ms": 119.07149999933608, "kv_decode_cuda_event_ms": 119.03180694580078, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 123.26059999941208, "cuda_event_ms": 123.16588687896729, "tokens_total": 41, "tokens_per_s": 332.6285934045069, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554343.5273015, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.70599999971455, "prefill_cuda_event_ms": 4.6493120193481445, "kv_decode_ms": 119.77409999963129, "kv_decode_cuda_event_ms": 119.74224090576172, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 4.70599999971455, "cuda_event_ms": 4.6493120193481445, "tokens_total": 9, "tokens_per_s": 1912.4521888112856, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554343.5273015, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.70599999971455, "prefill_cuda_event_ms": 4.6493120193481445, "kv_decode_ms": 119.77409999963129, "kv_decode_cuda_event_ms": 119.74224090576172, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 119.77409999963129, "cuda_event_ms": 119.74224090576172, "tokens_total": 32, "tokens_per_s": 267.1696134648351, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554343.5273015, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.70599999971455, "prefill_cuda_event_ms": 4.6493120193481445, "kv_decode_ms": 119.77409999963129, "kv_decode_cuda_event_ms": 119.74224090576172, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 124.48009999934584, "cuda_event_ms": 124.39155292510986, "tokens_total": 41, "tokens_per_s": 329.36991535366263, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554343.652602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.505500000959728, "prefill_cuda_event_ms": 4.455840110778809, "kv_decode_ms": 108.4883000003174, "kv_decode_cuda_event_ms": 108.44774627685547, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 4.505500000959728, "cuda_event_ms": 4.455840110778809, "tokens_total": 9, "tokens_per_s": 1997.5585391372517, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554343.652602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.505500000959728, "prefill_cuda_event_ms": 4.455840110778809, "kv_decode_ms": 108.4883000003174, "kv_decode_cuda_event_ms": 108.44774627685547, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 108.4883000003174, "cuda_event_ms": 108.44774627685547, "tokens_total": 32, "tokens_per_s": 294.96268261099476, "gen_tokens": 32}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554343.652602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.505500000959728, "prefill_cuda_event_ms": 4.455840110778809, "kv_decode_ms": 108.4883000003174, "kv_decode_cuda_event_ms": 108.44774627685547, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 112.99380000127712, "cuda_event_ms": 112.90358638763428, "tokens_total": 41, "tokens_per_s": 362.85176708400456, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554343.7664056, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 43.85259999980917, "prefill_cuda_event_ms": null, "kv_decode_ms": 700.310599999284, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 43.85259999980917, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 387.6623050873603, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554343.7664056, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 43.85259999980917, "prefill_cuda_event_ms": null, "kv_decode_ms": 700.310599999284, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 700.310599999284, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 45.69401062904476, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554343.7664056, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 43.85259999980917, "prefill_cuda_event_ms": null, "kv_decode_ms": 700.310599999284, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 744.1631999990932, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 65.84577146526422, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554344.511424, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.773300001281314, "prefill_cuda_event_ms": null, "kv_decode_ms": 675.4041999993206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 32.773300001281314, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 518.7149295107714, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554344.511424, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.773300001281314, "prefill_cuda_event_ms": null, "kv_decode_ms": 675.4041999993206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 675.4041999993206, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 47.3790361387036, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554344.511424, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 32.773300001281314, "prefill_cuda_event_ms": null, "kv_decode_ms": 675.4041999993206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 708.1775000006019, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 69.19169276058383, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554345.2202013, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 35.92180000123335, "prefill_cuda_event_ms": null, "kv_decode_ms": 681.36609999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 35.92180000123335, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 473.2502268654777, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554345.2202013, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 35.92180000123335, "prefill_cuda_event_ms": null, "kv_decode_ms": 681.36609999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 681.36609999965, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 46.96447328391658, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554345.2202013, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 35.92180000123335, "prefill_cuda_event_ms": null, "kv_decode_ms": 681.36609999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 717.2879000008834, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 68.31287687961787, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554345.9380517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 42.963699999745586, "prefill_cuda_event_ms": null, "kv_decode_ms": 689.1920999987633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 42.963699999745586, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 395.68286716694945, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554345.9380517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 42.963699999745586, "prefill_cuda_event_ms": null, "kv_decode_ms": 689.1920999987633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 689.1920999987633, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 46.43117644566358, "gen_tokens": 32}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554345.9380517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 42.963699999745586, "prefill_cuda_event_ms": null, "kv_decode_ms": 689.1920999987633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 732.1557999985089, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 66.92564615359161, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554346.670751, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 984.1986, "prefill_cuda_event_ms": null, "kv_decode_ms": 1273.5213, "kv_decode_ms_equiv": 1273.5213, "kv_decode_ms_per_token": 39.797540625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13834.999499998958, "ollama_total_duration_ms": 13675.2359, "ollama_load_ms": 11344.4446, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 984.1986, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 100.58945420162149, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554346.670751, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 984.1986, "prefill_cuda_event_ms": null, "kv_decode_ms": 1273.5213, "kv_decode_ms_equiv": 1273.5213, "kv_decode_ms_per_token": 39.797540625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13834.999499998958, "ollama_total_duration_ms": 13675.2359, "ollama_load_ms": 11344.4446, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1273.5213, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 25.12718083317491, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554346.670751, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 984.1986, "prefill_cuda_event_ms": null, "kv_decode_ms": 1273.5213, "kv_decode_ms_equiv": 1273.5213, "kv_decode_ms_per_token": 39.797540625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 13834.999499998958, "ollama_total_duration_ms": 13675.2359, "ollama_load_ms": 11344.4446, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2257.7199, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 58.02314095738802, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554360.5062249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 46.3073, "prefill_cuda_event_ms": null, "kv_decode_ms": 1370.3845, "kv_decode_ms_equiv": 1370.3845, "kv_decode_ms_per_token": 42.824515625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1897.2785000005388, "ollama_total_duration_ms": 1876.6681, "ollama_load_ms": 452.8917, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 46.3073, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2137.8918658613225, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554360.5062249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 46.3073, "prefill_cuda_event_ms": null, "kv_decode_ms": 1370.3845, "kv_decode_ms_equiv": 1370.3845, "kv_decode_ms_per_token": 42.824515625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1897.2785000005388, "ollama_total_duration_ms": 1876.6681, "ollama_load_ms": 452.8917, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1370.3845, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 23.351110582467914, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554360.5062249, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 46.3073, "prefill_cuda_event_ms": null, "kv_decode_ms": 1370.3845, "kv_decode_ms_equiv": 1370.3845, "kv_decode_ms_per_token": 42.824515625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1897.2785000005388, "ollama_total_duration_ms": 1876.6681, "ollama_load_ms": 452.8917, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1416.6917999999998, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 92.46894772737446, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554362.403705, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1421.2402, "kv_decode_ms_equiv": 1421.2402, "kv_decode_ms_per_token": 44.41375625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1907.5009000007412, "ollama_total_duration_ms": 1903.4503, "ollama_load_ms": 427.0935, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 48.494, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2041.4896688250092, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554362.403705, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1421.2402, "kv_decode_ms_equiv": 1421.2402, "kv_decode_ms_per_token": 44.41375625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1907.5009000007412, "ollama_total_duration_ms": 1903.4503, "ollama_load_ms": 427.0935, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1421.2402, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 22.515546633144773, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554362.403705, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1421.2402, "kv_decode_ms_equiv": 1421.2402, "kv_decode_ms_per_token": 44.41375625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1907.5009000007412, "ollama_total_duration_ms": 1903.4503, "ollama_load_ms": 427.0935, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1469.7341999999999, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 89.13176273641861, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554364.3114755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.8198, "prefill_cuda_event_ms": null, "kv_decode_ms": 1572.8304, "kv_decode_ms_equiv": 1572.8304, "kv_decode_ms_per_token": 49.15095, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2054.664399998728, "ollama_total_duration_ms": 2051.3433, "ollama_load_ms": 416.7051, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.8198, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1839.4717185868396, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554364.3114755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.8198, "prefill_cuda_event_ms": null, "kv_decode_ms": 1572.8304, "kv_decode_ms_equiv": 1572.8304, "kv_decode_ms_per_token": 49.15095, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2054.664399998728, "ollama_total_duration_ms": 2051.3433, "ollama_load_ms": 416.7051, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1572.8304, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.345486709819443, "gen_tokens": 32}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554364.3114755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.8198, "prefill_cuda_event_ms": null, "kv_decode_ms": 1572.8304, "kv_decode_ms_equiv": 1572.8304, "kv_decode_ms_per_token": 49.15095, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2054.664399998728, "ollama_total_duration_ms": 2051.3433, "ollama_load_ms": 416.7051, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1626.6502, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 80.53360212293953, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554366.3663244, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 62.70519999998214, "prefill_cuda_event_ms": null, "kv_decode_ms": 444.1867000005004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 62.70519999998214, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 271.10989200265436, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554366.3663244, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 62.70519999998214, "prefill_cuda_event_ms": null, "kv_decode_ms": 444.1867000005004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 444.1867000005004, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 72.04177882850601, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554366.3663244, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 62.70519999998214, "prefill_cuda_event_ms": null, "kv_decode_ms": 444.1867000005004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 506.89190000048256, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 96.66755377222115, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554366.8745797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 21.541299998716568, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.32269999921846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 21.541299998716568, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 789.1817114571944, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554366.8745797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 21.541299998716568, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.32269999921846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 426.32269999921846, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 75.06051167357184, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554366.8745797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 21.541299998716568, "prefill_cuda_event_ms": null, "kv_decode_ms": 426.32269999921846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 447.863999997935, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 109.40821320808533, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554367.3229623, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.576800000228104, "prefill_cuda_event_ms": null, "kv_decode_ms": 398.18170000035025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 23.576800000228104, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 721.0478097042654, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554367.3229623, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.576800000228104, "prefill_cuda_event_ms": null, "kv_decode_ms": 398.18170000035025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 398.18170000035025, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 80.36532065630303, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554367.3229623, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.576800000228104, "prefill_cuda_event_ms": null, "kv_decode_ms": 398.18170000035025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 421.75850000057835, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 116.18023110365958, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554367.745208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.06350000051316, "prefill_cuda_event_ms": null, "kv_decode_ms": 402.0944999992935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 22.06350000051316, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 770.503319944914, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554367.745208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.06350000051316, "prefill_cuda_event_ms": null, "kv_decode_ms": 402.0944999992935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 402.0944999992935, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 79.58328203956091, "gen_tokens": 32}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554367.745208, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.06350000051316, "prefill_cuda_event_ms": null, "kv_decode_ms": 402.0944999992935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 424.15799999980663, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 115.52298907487855, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554368.1698604, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 10.695599999962724, "prefill_cuda_event_ms": 10.576512336730957, "kv_decode_ms": 209.78869999999006, "kv_decode_cuda_event_ms": 209.75616455078125, "gpu_peak_mb": 579.755859375, "hf_load_ms": 165.22009999971488, "params_millions_measured": 5.03672, "latency_ms": 10.695599999962724, "cuda_event_ms": 10.576512336730957, "tokens_total": 17, "tokens_per_s": 1589.4386476737395, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554368.1698604, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 10.695599999962724, "prefill_cuda_event_ms": 10.576512336730957, "kv_decode_ms": 209.78869999999006, "kv_decode_cuda_event_ms": 209.75616455078125, "gpu_peak_mb": 579.755859375, "hf_load_ms": 165.22009999971488, "params_millions_measured": 5.03672, "latency_ms": 209.78869999999006, "cuda_event_ms": 209.75616455078125, "tokens_total": 32, "tokens_per_s": 152.5344310727962, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554368.1698604, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 10.695599999962724, "prefill_cuda_event_ms": 10.576512336730957, "kv_decode_ms": 209.78869999999006, "kv_decode_cuda_event_ms": 209.75616455078125, "gpu_peak_mb": 579.755859375, "hf_load_ms": 165.22009999971488, "params_millions_measured": 5.03672, "latency_ms": 220.48429999995278, "cuda_event_ms": 220.3326768875122, "tokens_total": 49, "tokens_per_s": 222.2380459742961, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554368.5572422, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.568400000309339, "prefill_cuda_event_ms": 7.502719879150391, "kv_decode_ms": 210.89740000024904, "kv_decode_cuda_event_ms": 210.8651580810547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 7.568400000309339, "cuda_event_ms": 7.502719879150391, "tokens_total": 17, "tokens_per_s": 2246.1814913727035, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554368.5572422, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.568400000309339, "prefill_cuda_event_ms": 7.502719879150391, "kv_decode_ms": 210.89740000024904, "kv_decode_cuda_event_ms": 210.8651580810547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 210.89740000024904, "cuda_event_ms": 210.8651580810547, "tokens_total": 32, "tokens_per_s": 151.7325486229902, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554368.5572422, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.568400000309339, "prefill_cuda_event_ms": 7.502719879150391, "kv_decode_ms": 210.89740000024904, "kv_decode_cuda_event_ms": 210.8651580810547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 218.46580000055837, "cuda_event_ms": 218.36787796020508, "tokens_total": 49, "tokens_per_s": 224.29139938550912, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554368.7770865, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.232800000565476, "prefill_cuda_event_ms": 7.176799774169922, "kv_decode_ms": 212.36599999974715, "kv_decode_cuda_event_ms": 212.3284454345703, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 7.232800000565476, "cuda_event_ms": 7.176799774169922, "tokens_total": 17, "tokens_per_s": 2350.403716219293, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554368.7770865, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.232800000565476, "prefill_cuda_event_ms": 7.176799774169922, "kv_decode_ms": 212.36599999974715, "kv_decode_cuda_event_ms": 212.3284454345703, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 212.36599999974715, "cuda_event_ms": 212.3284454345703, "tokens_total": 32, "tokens_per_s": 150.68325438176592, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554368.7770865, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.232800000565476, "prefill_cuda_event_ms": 7.176799774169922, "kv_decode_ms": 212.36599999974715, "kv_decode_cuda_event_ms": 212.3284454345703, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 219.59880000031262, "cuda_event_ms": 219.50524520874023, "tokens_total": 49, "tokens_per_s": 223.13418834679536, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554368.9974756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.42790000003879, "prefill_cuda_event_ms": 7.378592014312744, "kv_decode_ms": 212.96179999990272, "kv_decode_cuda_event_ms": 212.89471435546875, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 7.42790000003879, "cuda_event_ms": 7.378592014312744, "tokens_total": 17, "tokens_per_s": 2288.668398862562, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554368.9974756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.42790000003879, "prefill_cuda_event_ms": 7.378592014312744, "kv_decode_ms": 212.96179999990272, "kv_decode_cuda_event_ms": 212.89471435546875, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 212.96179999990272, "cuda_event_ms": 212.89471435546875, "tokens_total": 32, "tokens_per_s": 150.26169012477644, "gen_tokens": 32}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554368.9974756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.42790000003879, "prefill_cuda_event_ms": 7.378592014312744, "kv_decode_ms": 212.96179999990272, "kv_decode_cuda_event_ms": 212.89471435546875, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 220.3896999999415, "cuda_event_ms": 220.2733063697815, "tokens_total": 49, "tokens_per_s": 222.33343935770594, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554369.2187707, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.6964, "kv_decode_ms_equiv": 72.6964, "kv_decode_ms_per_token": 2.2717625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1933.5797000003367, "ollama_total_duration_ms": 1868.4966, "ollama_load_ms": 1741.072, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.86, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2821.6704288939054, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554369.2187707, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.6964, "kv_decode_ms_equiv": 72.6964, "kv_decode_ms_per_token": 2.2717625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1933.5797000003367, "ollama_total_duration_ms": 1868.4966, "ollama_load_ms": 1741.072, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 72.6964, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 440.18685932178215, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554369.2187707, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 72.6964, "kv_decode_ms_equiv": 72.6964, "kv_decode_ms_per_token": 2.2717625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1933.5797000003367, "ollama_total_duration_ms": 1868.4966, "ollama_load_ms": 1741.072, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 81.5564, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 698.9028451476524, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554371.1525729, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2351, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0702, "kv_decode_ms_equiv": 67.0702, "kv_decode_ms_per_token": 2.09594375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 417.4070000008214, "ollama_total_duration_ms": 388.5333, "ollama_load_ms": 287.9604, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2351, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11185.181871057222, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554371.1525729, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2351, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0702, "kv_decode_ms_equiv": 67.0702, "kv_decode_ms_per_token": 2.09594375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 417.4070000008214, "ollama_total_duration_ms": 388.5333, "ollama_load_ms": 287.9604, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 67.0702, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 477.1120408169351, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554371.1525729, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2351, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0702, "kv_decode_ms_equiv": 67.0702, "kv_decode_ms_per_token": 2.09594375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 417.4070000008214, "ollama_total_duration_ms": 388.5333, "ollama_load_ms": 287.9604, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 69.3053, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 822.4479224532612, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554371.570103, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.5255, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.0701, "kv_decode_ms_equiv": 63.0701, "kv_decode_ms_per_token": 1.970940625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 396.8416999996407, "ollama_total_duration_ms": 367.8285, "ollama_load_ms": 266.589, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5255, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9899.029895070284, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554371.570103, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.5255, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.0701, "kv_decode_ms_equiv": 63.0701, "kv_decode_ms_per_token": 1.970940625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 396.8416999996407, "ollama_total_duration_ms": 367.8285, "ollama_load_ms": 266.589, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.0701, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 507.3719559664564, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554371.570103, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.5255, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.0701, "kv_decode_ms_equiv": 63.0701, "kv_decode_ms_per_token": 1.970940625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 396.8416999996407, "ollama_total_duration_ms": 367.8285, "ollama_load_ms": 266.589, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 65.59559999999999, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 868.9607229753217, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554371.9671402, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2018, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.6974, "kv_decode_ms_equiv": 63.6974, "kv_decode_ms_per_token": 1.99054375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 329.246699999203, "ollama_total_duration_ms": 315.7019, "ollama_load_ms": 228.8261, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2018, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11354.346443818695, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554371.9671402, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2018, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.6974, "kv_decode_ms_equiv": 63.6974, "kv_decode_ms_per_token": 1.99054375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 329.246699999203, "ollama_total_duration_ms": 315.7019, "ollama_load_ms": 228.8261, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.6974, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 502.3752931830812, "gen_tokens": 32}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554371.9671402, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2018, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.6974, "kv_decode_ms_equiv": 63.6974, "kv_decode_ms_per_token": 1.99054375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 329.246699999203, "ollama_total_duration_ms": 315.7019, "ollama_load_ms": 228.8261, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 65.89920000000001, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 864.9573894675503, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554372.2965384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.87960000061139, "prefill_cuda_event_ms": 11.772255897521973, "kv_decode_ms": 247.657200001413, "kv_decode_cuda_event_ms": 247.6136016845703, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 11.87960000061139, "cuda_event_ms": 11.772255897521973, "tokens_total": 9, "tokens_per_s": 757.6012659969031, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554372.2965384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.87960000061139, "prefill_cuda_event_ms": 11.772255897521973, "kv_decode_ms": 247.657200001413, "kv_decode_cuda_event_ms": 247.6136016845703, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 247.657200001413, "cuda_event_ms": 247.6136016845703, "tokens_total": 32, "tokens_per_s": 129.21086081816892, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554372.2965384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.87960000061139, "prefill_cuda_event_ms": 11.772255897521973, "kv_decode_ms": 247.657200001413, "kv_decode_cuda_event_ms": 247.6136016845703, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 259.5368000020244, "cuda_event_ms": 259.3858575820923, "tokens_total": 41, "tokens_per_s": 157.9737439919125, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554372.5574129, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 8.457600000838283, "prefill_cuda_event_ms": 8.388511657714844, "kv_decode_ms": 231.03079999964393, "kv_decode_cuda_event_ms": 230.98265075683594, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 8.457600000838283, "cuda_event_ms": 8.388511657714844, "tokens_total": 9, "tokens_per_s": 1064.1316684529838, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554372.5574129, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 8.457600000838283, "prefill_cuda_event_ms": 8.388511657714844, "kv_decode_ms": 231.03079999964393, "kv_decode_cuda_event_ms": 230.98265075683594, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 231.03079999964393, "cuda_event_ms": 230.98265075683594, "tokens_total": 32, "tokens_per_s": 138.50967057227572, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554372.5574129, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 8.457600000838283, "prefill_cuda_event_ms": 8.388511657714844, "kv_decode_ms": 231.03079999964393, "kv_decode_cuda_event_ms": 230.98265075683594, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 239.48840000048222, "cuda_event_ms": 239.37116241455078, "tokens_total": 41, "tokens_per_s": 171.19827098062973, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554372.79771, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.586400000946014, "prefill_cuda_event_ms": 7.52620792388916, "kv_decode_ms": 228.45439999946393, "kv_decode_cuda_event_ms": 228.40911865234375, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 7.586400000946014, "cuda_event_ms": 7.52620792388916, "tokens_total": 9, "tokens_per_s": 1186.3334386372603, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554372.79771, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.586400000946014, "prefill_cuda_event_ms": 7.52620792388916, "kv_decode_ms": 228.45439999946393, "kv_decode_cuda_event_ms": 228.40911865234375, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 228.45439999946393, "cuda_event_ms": 228.40911865234375, "tokens_total": 32, "tokens_per_s": 140.0717167192888, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554372.79771, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.586400000946014, "prefill_cuda_event_ms": 7.52620792388916, "kv_decode_ms": 228.45439999946393, "kv_decode_cuda_event_ms": 228.40911865234375, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 236.04080000040994, "cuda_event_ms": 235.9353265762329, "tokens_total": 41, "tokens_per_s": 173.6987842776706, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554373.0345383, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.682299999942188, "prefill_cuda_event_ms": 7.6229119300842285, "kv_decode_ms": 222.23289999965345, "kv_decode_cuda_event_ms": 222.16908264160156, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 7.682299999942188, "cuda_event_ms": 7.6229119300842285, "tokens_total": 9, "tokens_per_s": 1171.5241529317689, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554373.0345383, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.682299999942188, "prefill_cuda_event_ms": 7.6229119300842285, "kv_decode_ms": 222.23289999965345, "kv_decode_cuda_event_ms": 222.16908264160156, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 222.23289999965345, "cuda_event_ms": 222.16908264160156, "tokens_total": 32, "tokens_per_s": 143.99308113267614, "gen_tokens": 32}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554373.0345383, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 7.682299999942188, "prefill_cuda_event_ms": 7.6229119300842285, "kv_decode_ms": 222.23289999965345, "kv_decode_cuda_event_ms": 222.16908264160156, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 229.91519999959564, "cuda_event_ms": 229.7919945716858, "tokens_total": 41, "tokens_per_s": 178.32661781418588, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554373.265481, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.9645, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.8551, "kv_decode_ms_equiv": 362.8551, "kv_decode_ms_per_token": 11.339221875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 7347.415399999591, "ollama_total_duration_ms": 7344.1078, "ollama_load_ms": 6926.3503, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 21.9645, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1730.0644221357188, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554373.265481, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.9645, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.8551, "kv_decode_ms_equiv": 362.8551, "kv_decode_ms_per_token": 11.339221875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 7347.415399999591, "ollama_total_duration_ms": 7344.1078, "ollama_load_ms": 6926.3503, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 362.8551, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 88.18947287774101, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554373.265481, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 21.9645, "prefill_cuda_event_ms": null, "kv_decode_ms": 362.8551, "kv_decode_ms_equiv": 362.8551, "kv_decode_ms_per_token": 11.339221875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 7347.415399999591, "ollama_total_duration_ms": 7344.1078, "ollama_load_ms": 6926.3503, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 384.8196, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 181.90341656194227, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554380.613276, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.2991, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.2878, "kv_decode_ms_equiv": 356.2878, "kv_decode_ms_per_token": 11.13399375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 659.4655999997485, "ollama_total_duration_ms": 638.9635, "ollama_load_ms": 239.6311, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.2991, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3089.656966769927, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554380.613276, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.2991, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.2878, "kv_decode_ms_equiv": 356.2878, "kv_decode_ms_per_token": 11.13399375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 659.4655999997485, "ollama_total_duration_ms": 638.9635, "ollama_load_ms": 239.6311, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 356.2878, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.81503155595, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554380.613276, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.2991, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.2878, "kv_decode_ms_equiv": 356.2878, "kv_decode_ms_per_token": 11.13399375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 659.4655999997485, "ollama_total_duration_ms": 638.9635, "ollama_load_ms": 239.6311, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 368.5869, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 189.91450862740916, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554381.2728605, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5869, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.0652, "kv_decode_ms_equiv": 353.0652, "kv_decode_ms_per_token": 11.0332875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 653.7646999986464, "ollama_total_duration_ms": 650.9288, "ollama_load_ms": 253.2058, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5869, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3279.5657164556524, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554381.2728605, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5869, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.0652, "kv_decode_ms_equiv": 353.0652, "kv_decode_ms_per_token": 11.0332875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 653.7646999986464, "ollama_total_duration_ms": 650.9288, "ollama_load_ms": 253.2058, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 353.0652, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.63481759176491, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554381.2728605, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5869, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.0652, "kv_decode_ms_equiv": 353.0652, "kv_decode_ms_per_token": 11.0332875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 653.7646999986464, "ollama_total_duration_ms": 650.9288, "ollama_load_ms": 253.2058, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 364.6521, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 191.9637923379572, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554381.926744, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2892, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.0641, "kv_decode_ms_equiv": 356.0641, "kv_decode_ms_per_token": 11.127003125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 650.3730999993422, "ollama_total_duration_ms": 646.9583, "ollama_load_ms": 247.8287, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.2892, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3366.0489671544487, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554381.926744, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2892, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.0641, "kv_decode_ms_equiv": 356.0641, "kv_decode_ms_per_token": 11.127003125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 650.3730999993422, "ollama_total_duration_ms": 646.9583, "ollama_load_ms": 247.8287, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 356.0641, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.87145853794303, "gen_tokens": 32}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554381.926744, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.2892, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.0641, "kv_decode_ms_equiv": 356.0641, "kv_decode_ms_per_token": 11.127003125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 650.3730999993422, "ollama_total_duration_ms": 646.9583, "ollama_load_ms": 247.8287, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 367.3533, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 190.55225582565885, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554382.598331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.142799998997361, "prefill_cuda_event_ms": 6.071807861328125, "kv_decode_ms": 70.23349999872153, "kv_decode_cuda_event_ms": 70.17366027832031, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 189.90069999927073, "params_millions_measured": 25.016064, "latency_ms": 6.142799998997361, "cuda_event_ms": 6.071807861328125, "tokens_total": 17, "tokens_per_s": 2767.467604801519, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554382.598331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.142799998997361, "prefill_cuda_event_ms": 6.071807861328125, "kv_decode_ms": 70.23349999872153, "kv_decode_cuda_event_ms": 70.17366027832031, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 189.90069999927073, "params_millions_measured": 25.016064, "latency_ms": 70.23349999872153, "cuda_event_ms": 70.17366027832031, "tokens_total": 32, "tokens_per_s": 455.6230289047606, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554382.598331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 6.142799998997361, "prefill_cuda_event_ms": 6.071807861328125, "kv_decode_ms": 70.23349999872153, "kv_decode_cuda_event_ms": 70.17366027832031, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 189.90069999927073, "params_millions_measured": 25.016064, "latency_ms": 76.37629999771889, "cuda_event_ms": 76.24546813964844, "tokens_total": 49, "tokens_per_s": 641.5602746069588, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554382.865507, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.8223999997862848, "prefill_cuda_event_ms": 2.7486720085144043, "kv_decode_ms": 72.7351000005001, "kv_decode_cuda_event_ms": 72.6588134765625, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.8223999997862848, "cuda_event_ms": 2.7486720085144043, "tokens_total": 17, "tokens_per_s": 6023.242630841574, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554382.865507, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.8223999997862848, "prefill_cuda_event_ms": 2.7486720085144043, "kv_decode_ms": 72.7351000005001, "kv_decode_cuda_event_ms": 72.6588134765625, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 72.7351000005001, "cuda_event_ms": 72.6588134765625, "tokens_total": 32, "tokens_per_s": 439.95265009300846, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554382.865507, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.8223999997862848, "prefill_cuda_event_ms": 2.7486720085144043, "kv_decode_ms": 72.7351000005001, "kv_decode_cuda_event_ms": 72.6588134765625, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 75.55750000028638, "cuda_event_ms": 75.4074854850769, "tokens_total": 49, "tokens_per_s": 648.5127220966056, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554382.941876, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7602999998634914, "prefill_cuda_event_ms": 2.6904640197753906, "kv_decode_ms": 70.39550000081363, "kv_decode_cuda_event_ms": 70.32841491699219, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.7602999998634914, "cuda_event_ms": 2.6904640197753906, "tokens_total": 17, "tokens_per_s": 6158.7508607183, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554382.941876, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7602999998634914, "prefill_cuda_event_ms": 2.6904640197753906, "kv_decode_ms": 70.39550000081363, "kv_decode_cuda_event_ms": 70.32841491699219, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 70.39550000081363, "cuda_event_ms": 70.32841491699219, "tokens_total": 32, "tokens_per_s": 454.57451114957837, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554382.941876, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7602999998634914, "prefill_cuda_event_ms": 2.6904640197753906, "kv_decode_ms": 70.39550000081363, "kv_decode_cuda_event_ms": 70.32841491699219, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 73.15580000067712, "cuda_event_ms": 73.01887893676758, "tokens_total": 49, "tokens_per_s": 669.8033511976694, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554383.0161703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7883000002475455, "prefill_cuda_event_ms": 2.7286720275878906, "kv_decode_ms": 68.98230000115291, "kv_decode_cuda_event_ms": 68.93536376953125, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.7883000002475455, "cuda_event_ms": 2.7286720275878906, "tokens_total": 17, "tokens_per_s": 6096.904923606046, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554383.0161703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7883000002475455, "prefill_cuda_event_ms": 2.7286720275878906, "kv_decode_ms": 68.98230000115291, "kv_decode_cuda_event_ms": 68.93536376953125, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 68.98230000115291, "cuda_event_ms": 68.93536376953125, "tokens_total": 32, "tokens_per_s": 463.88711306328116, "gen_tokens": 32}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554383.0161703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 2.7883000002475455, "prefill_cuda_event_ms": 2.7286720275878906, "kv_decode_ms": 68.98230000115291, "kv_decode_cuda_event_ms": 68.93536376953125, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 71.77060000140045, "cuda_event_ms": 71.66403579711914, "tokens_total": 49, "tokens_per_s": 682.7308117675465, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554383.0888722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 35.29530000014347, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.35910000052536, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 114.37440000008792, "params_millions_measured": 25.016064, "latency_ms": 35.29530000014347, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 254.99145778512766, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554383.0888722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 35.29530000014347, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.35910000052536, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 114.37440000008792, "params_millions_measured": 25.016064, "latency_ms": 313.35910000052536, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 102.11926189456872, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554383.0888722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 35.29530000014347, "prefill_cuda_event_ms": null, "kv_decode_ms": 313.35910000052536, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 114.37440000008792, "params_millions_measured": 25.016064, "latency_ms": 348.65440000066883, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 117.59495936354553, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554383.552384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.779599999499624, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.3320999986172, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.779599999499624, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 764.0327345905042, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554383.552384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.779599999499624, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.3320999986172, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 286.3320999986172, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 111.75833935543567, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554383.552384, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 11.779599999499624, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.3320999986172, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 298.1116999981168, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 137.53234106631507, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554383.8510168, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 12.271399999008281, "prefill_cuda_event_ms": null, "kv_decode_ms": 204.92109999941022, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.271399999008281, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 733.4126506125901, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554383.8510168, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 12.271399999008281, "prefill_cuda_event_ms": null, "kv_decode_ms": 204.92109999941022, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 204.92109999941022, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 156.15766263255517, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554383.8510168, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 12.271399999008281, "prefill_cuda_event_ms": null, "kv_decode_ms": 204.92109999941022, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 217.1924999984185, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 188.7726325738621, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554384.0685856, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.095599999753176, "prefill_cuda_event_ms": null, "kv_decode_ms": 219.64759999900707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 6.095599999753176, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1476.4748343664987, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554384.0685856, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.095599999753176, "prefill_cuda_event_ms": null, "kv_decode_ms": 219.64759999900707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 219.64759999900707, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 145.68791099991375, "gen_tokens": 32}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554384.0685856, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.095599999753176, "prefill_cuda_event_ms": null, "kv_decode_ms": 219.64759999900707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 225.74319999876025, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 181.62230357426122, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554384.294733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 9.803200000533252, "prefill_cuda_event_ms": null, "kv_decode_ms": 188.85260000024573, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.803200000533252, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1734.127631699371, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554384.294733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 9.803200000533252, "prefill_cuda_event_ms": null, "kv_decode_ms": 188.85260000024573, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 188.85260000024573, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 169.44431794933382, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554384.294733, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 9.803200000533252, "prefill_cuda_event_ms": null, "kv_decode_ms": 188.85260000024573, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 198.65580000077898, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 246.65778698536795, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554384.493958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.338100000401028, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.5851999985607, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.338100000401028, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2038.8337869757345, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554384.493958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.338100000401028, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.5851999985607, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 183.5851999985607, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 174.3059898088238, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554384.493958, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 8.338100000401028, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.5851999985607, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 191.92329999896174, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 255.31032449038275, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554384.6862576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.186299999375478, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.61619999996037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 7.186299999375478, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2365.6123459189535, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554384.6862576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.186299999375478, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.61619999996037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 183.61619999996037, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 174.2765616541836, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554384.6862576, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.186299999375478, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.61619999996037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 190.80249999933585, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 256.81005228008314, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554384.8774464, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.365400000708178, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.6022000000812, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 7.365400000708178, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2308.089173482155, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554384.8774464, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.365400000708178, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.6022000000812, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 189.6022000000812, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 168.77441295505164, "gen_tokens": 32}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554384.8774464, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.365400000708178, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.6022000000812, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 196.9676000007894, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 248.77187923193267, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554385.074859, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 224.7119, "prefill_cuda_event_ms": null, "kv_decode_ms": 555.5603, "kv_decode_ms_equiv": 555.5603, "kv_decode_ms_per_token": 17.361259375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1100.969699999041, "ollama_total_duration_ms": 1076.7546, "ollama_load_ms": 264.6273, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 224.7119, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 204.70655982170948, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554385.074859, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 224.7119, "prefill_cuda_event_ms": null, "kv_decode_ms": 555.5603, "kv_decode_ms_equiv": 555.5603, "kv_decode_ms_per_token": 17.361259375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1100.969699999041, "ollama_total_duration_ms": 1076.7546, "ollama_load_ms": 264.6273, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 555.5603, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 57.59950810020082, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554385.074859, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 224.7119, "prefill_cuda_event_ms": null, "kv_decode_ms": 555.5603, "kv_decode_ms_equiv": 555.5603, "kv_decode_ms_per_token": 17.361259375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1100.969699999041, "ollama_total_duration_ms": 1076.7546, "ollama_load_ms": 264.6273, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 780.2722, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 99.96511473816446, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554386.175924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 14.4681, "prefill_cuda_event_ms": null, "kv_decode_ms": 443.9978, "kv_decode_ms_equiv": 443.9978, "kv_decode_ms_per_token": 13.87493125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 726.2408999995387, "ollama_total_duration_ms": 722.5268, "ollama_load_ms": 230.9398, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 14.4681, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3179.408491785376, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554386.175924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 14.4681, "prefill_cuda_event_ms": null, "kv_decode_ms": 443.9978, "kv_decode_ms_equiv": 443.9978, "kv_decode_ms_per_token": 13.87493125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 726.2408999995387, "ollama_total_duration_ms": 722.5268, "ollama_load_ms": 230.9398, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 443.9978, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 72.07242918771219, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554386.175924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 14.4681, "prefill_cuda_event_ms": null, "kv_decode_ms": 443.9978, "kv_decode_ms_equiv": 443.9978, "kv_decode_ms_per_token": 13.87493125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 726.2408999995387, "ollama_total_duration_ms": 722.5268, "ollama_load_ms": 230.9398, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 458.4659, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 170.13260964446866, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554386.9022775, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.9644, "prefill_cuda_event_ms": null, "kv_decode_ms": 376.8863, "kv_decode_ms_equiv": 376.8863, "kv_decode_ms_per_token": 11.777696875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 679.8847000009118, "ollama_total_duration_ms": 653.805, "ollama_load_ms": 233.7327, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.9644, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3548.1780876862795, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554386.9022775, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.9644, "prefill_cuda_event_ms": null, "kv_decode_ms": 376.8863, "kv_decode_ms_equiv": 376.8863, "kv_decode_ms_per_token": 11.777696875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 679.8847000009118, "ollama_total_duration_ms": 653.805, "ollama_load_ms": 233.7327, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 376.8863, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 84.90624360715685, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554386.9022775, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.9644, "prefill_cuda_event_ms": null, "kv_decode_ms": 376.8863, "kv_decode_ms_equiv": 376.8863, "kv_decode_ms_per_token": 11.777696875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 679.8847000009118, "ollama_total_duration_ms": 653.805, "ollama_load_ms": 233.7327, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 389.8507, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 200.076593424098, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554387.5823488, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9255, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.9003, "kv_decode_ms_equiv": 354.9003, "kv_decode_ms_per_token": 11.090634375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 646.6338999998698, "ollama_total_duration_ms": 632.9276, "ollama_load_ms": 236.5784, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9255, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3857.2806171648986, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554387.5823488, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9255, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.9003, "kv_decode_ms_equiv": 354.9003, "kv_decode_ms_per_token": 11.090634375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 646.6338999998698, "ollama_total_duration_ms": 632.9276, "ollama_load_ms": 236.5784, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 354.9003, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.16616779416641, "gen_tokens": 32}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554387.5823488, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9255, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.9003, "kv_decode_ms_equiv": 354.9003, "kv_decode_ms_per_token": 11.090634375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 646.6338999998698, "ollama_total_duration_ms": 632.9276, "ollama_load_ms": 236.5784, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 366.8258, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 212.63498914198507, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554388.2291322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 37.45369999887771, "prefill_cuda_event_ms": null, "kv_decode_ms": 335.962400000426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 37.45369999887771, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 240.29668631589624, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554388.2291322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 37.45369999887771, "prefill_cuda_event_ms": null, "kv_decode_ms": 335.962400000426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 335.962400000426, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 95.24875402711561, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554388.2291322, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 37.45369999887771, "prefill_cuda_event_ms": null, "kv_decode_ms": 335.962400000426, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 373.4160999993037, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 109.79708694958909, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554388.6080441, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.9459000001807, "prefill_cuda_event_ms": null, "kv_decode_ms": 363.2000000015978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 13.9459000001807, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 645.350963357215, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554388.6080441, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.9459000001807, "prefill_cuda_event_ms": null, "kv_decode_ms": 363.2000000015978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 363.2000000015978, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 88.1057268718591, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554388.6080441, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.9459000001807, "prefill_cuda_event_ms": null, "kv_decode_ms": 363.2000000015978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 377.1459000017785, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 108.71124410952541, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554388.985608, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.497700000400073, "prefill_cuda_event_ms": null, "kv_decode_ms": 384.7433999999339, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.497700000400073, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 545.5305890991925, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554388.985608, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.497700000400073, "prefill_cuda_event_ms": null, "kv_decode_ms": 384.7433999999339, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 384.7433999999339, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 83.17231692604862, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554388.985608, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 16.497700000400073, "prefill_cuda_event_ms": null, "kv_decode_ms": 384.7433999999339, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 401.241100000334, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 102.18295184607427, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554389.3872225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.780400000541704, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3157000005158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 13.780400000541704, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 653.1015064618017, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554389.3872225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.780400000541704, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3157000005158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 319.3157000005158, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 100.21430202131718, "gen_tokens": 32}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554389.3872225, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 13.780400000541704, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3157000005158, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 333.0961000010575, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 123.0876014455583, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554389.7206855, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.168399999500252, "prefill_cuda_event_ms": 7.119135856628418, "kv_decode_ms": 93.40230000088923, "kv_decode_cuda_event_ms": 93.37334442138672, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 7.168399999500252, "cuda_event_ms": 7.119135856628418, "tokens_total": 17, "tokens_per_s": 2371.519446624792, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554389.7206855, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.168399999500252, "prefill_cuda_event_ms": 7.119135856628418, "kv_decode_ms": 93.40230000088923, "kv_decode_cuda_event_ms": 93.37334442138672, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 93.40230000088923, "cuda_event_ms": 93.37334442138672, "tokens_total": 32, "tokens_per_s": 342.60398298216796, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554389.7206855, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 7.168399999500252, "prefill_cuda_event_ms": 7.119135856628418, "kv_decode_ms": 93.40230000088923, "kv_decode_cuda_event_ms": 93.37334442138672, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 100.57070000038948, "cuda_event_ms": 100.49248027801514, "tokens_total": 49, "tokens_per_s": 487.21943866166026, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554389.8299062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.9902999997138977, "prefill_cuda_event_ms": 3.9290239810943604, "kv_decode_ms": 94.56959999988612, "kv_decode_cuda_event_ms": 94.53874969482422, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 3.9902999997138977, "cuda_event_ms": 3.9290239810943604, "tokens_total": 17, "tokens_per_s": 4260.331303716234, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554389.8299062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.9902999997138977, "prefill_cuda_event_ms": 3.9290239810943604, "kv_decode_ms": 94.56959999988612, "kv_decode_cuda_event_ms": 94.53874969482422, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 94.56959999988612, "cuda_event_ms": 94.53874969482422, "tokens_total": 32, "tokens_per_s": 338.3751226613894, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554389.8299062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.9902999997138977, "prefill_cuda_event_ms": 3.9290239810943604, "kv_decode_ms": 94.56959999988612, "kv_decode_cuda_event_ms": 94.53874969482422, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 98.55989999960002, "cuda_event_ms": 98.46777367591858, "tokens_total": 49, "tokens_per_s": 497.1595953343992, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554389.929391, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6337999990792014, "prefill_cuda_event_ms": 3.5906879901885986, "kv_decode_ms": 95.16439999970316, "kv_decode_cuda_event_ms": 95.1365737915039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 3.6337999990792014, "cuda_event_ms": 3.5906879901885986, "tokens_total": 17, "tokens_per_s": 4678.2982014166355, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554389.929391, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6337999990792014, "prefill_cuda_event_ms": 3.5906879901885986, "kv_decode_ms": 95.16439999970316, "kv_decode_cuda_event_ms": 95.1365737915039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 95.16439999970316, "cuda_event_ms": 95.1365737915039, "tokens_total": 32, "tokens_per_s": 336.2601981423706, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554389.929391, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6337999990792014, "prefill_cuda_event_ms": 3.5906879901885986, "kv_decode_ms": 95.16439999970316, "kv_decode_cuda_event_ms": 95.1365737915039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 98.79819999878237, "cuda_event_ms": 98.7272617816925, "tokens_total": 49, "tokens_per_s": 495.960452726911, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.028863, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6069999987375923, "prefill_cuda_event_ms": 3.5582399368286133, "kv_decode_ms": 93.74840000054974, "kv_decode_cuda_event_ms": 93.70931243896484, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 3.6069999987375923, "cuda_event_ms": 3.5582399368286133, "tokens_total": 17, "tokens_per_s": 4713.057944538343, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.028863, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6069999987375923, "prefill_cuda_event_ms": 3.5582399368286133, "kv_decode_ms": 93.74840000054974, "kv_decode_cuda_event_ms": 93.70931243896484, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 93.74840000054974, "cuda_event_ms": 93.70931243896484, "tokens_total": 32, "tokens_per_s": 341.3391588529762, "gen_tokens": 32}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.028863, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 3.6069999987375923, "prefill_cuda_event_ms": 3.5582399368286133, "kv_decode_ms": 93.74840000054974, "kv_decode_cuda_event_ms": 93.70931243896484, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 97.35539999928733, "cuda_event_ms": 97.26755237579346, "tokens_total": 49, "tokens_per_s": 503.3105508308599, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.1269724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.766300000890624, "prefill_cuda_event_ms": 6.703872203826904, "kv_decode_ms": 154.5447000007698, "kv_decode_cuda_event_ms": 154.51443481445312, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 6.766300000890624, "cuda_event_ms": 6.703872203826904, "tokens_total": 9, "tokens_per_s": 1330.121336449073, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.1269724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.766300000890624, "prefill_cuda_event_ms": 6.703872203826904, "kv_decode_ms": 154.5447000007698, "kv_decode_cuda_event_ms": 154.51443481445312, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 154.5447000007698, "cuda_event_ms": 154.51443481445312, "tokens_total": 32, "tokens_per_s": 207.0598344675722, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.1269724, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 6.766300000890624, "prefill_cuda_event_ms": 6.703872203826904, "kv_decode_ms": 154.5447000007698, "kv_decode_cuda_event_ms": 154.51443481445312, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 161.31100000166043, "cuda_event_ms": 161.21830701828003, "tokens_total": 41, "tokens_per_s": 254.16741573468624, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.2902446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.361399998946581, "prefill_cuda_event_ms": 5.3144001960754395, "kv_decode_ms": 153.76019999894197, "kv_decode_cuda_event_ms": 153.71353149414062, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 5.361399998946581, "cuda_event_ms": 5.3144001960754395, "tokens_total": 9, "tokens_per_s": 1678.6660203992126, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.2902446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.361399998946581, "prefill_cuda_event_ms": 5.3144001960754395, "kv_decode_ms": 153.76019999894197, "kv_decode_cuda_event_ms": 153.71353149414062, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 153.76019999894197, "cuda_event_ms": 153.71353149414062, "tokens_total": 32, "tokens_per_s": 208.1162745640302, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.2902446, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.361399998946581, "prefill_cuda_event_ms": 5.3144001960754395, "kv_decode_ms": 153.76019999894197, "kv_decode_cuda_event_ms": 153.71353149414062, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 159.12159999788855, "cuda_event_ms": 159.02793169021606, "tokens_total": 41, "tokens_per_s": 257.6645785395826, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.4500465, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.333400000381516, "prefill_cuda_event_ms": 5.2867841720581055, "kv_decode_ms": 147.60829999977432, "kv_decode_cuda_event_ms": 147.54202270507812, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 5.333400000381516, "cuda_event_ms": 5.2867841720581055, "tokens_total": 9, "tokens_per_s": 1687.4789063929575, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.4500465, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.333400000381516, "prefill_cuda_event_ms": 5.2867841720581055, "kv_decode_ms": 147.60829999977432, "kv_decode_cuda_event_ms": 147.54202270507812, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 147.60829999977432, "cuda_event_ms": 147.54202270507812, "tokens_total": 32, "tokens_per_s": 216.7899772577079, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.4500465, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.333400000381516, "prefill_cuda_event_ms": 5.2867841720581055, "kv_decode_ms": 147.60829999977432, "kv_decode_cuda_event_ms": 147.54202270507812, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 152.94170000015583, "cuda_event_ms": 152.82880687713623, "tokens_total": 41, "tokens_per_s": 268.0760054318621, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.6039386, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.178200000955258, "prefill_cuda_event_ms": 5.119487762451172, "kv_decode_ms": 150.30730000034964, "kv_decode_cuda_event_ms": 150.27711486816406, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 5.178200000955258, "cuda_event_ms": 5.119487762451172, "tokens_total": 9, "tokens_per_s": 1738.05569470853, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.6039386, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.178200000955258, "prefill_cuda_event_ms": 5.119487762451172, "kv_decode_ms": 150.30730000034964, "kv_decode_cuda_event_ms": 150.27711486816406, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 150.30730000034964, "cuda_event_ms": 150.27711486816406, "tokens_total": 32, "tokens_per_s": 212.89717798088026, "gen_tokens": 32}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.6039386, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 5.178200000955258, "prefill_cuda_event_ms": 5.119487762451172, "kv_decode_ms": 150.30730000034964, "kv_decode_cuda_event_ms": 150.27711486816406, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 155.4855000013049, "cuda_event_ms": 155.39660263061523, "tokens_total": 41, "tokens_per_s": 263.6901833267791, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554390.7602918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 455.6255000006786, "prefill_cuda_event_ms": null, "kv_decode_ms": 489.06690000148956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 120.19630000031611, "params_millions_measured": 74.824704, "latency_ms": 455.6255000006786, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 19.75306474283506, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554390.7602918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 455.6255000006786, "prefill_cuda_event_ms": null, "kv_decode_ms": 489.06690000148956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 120.19630000031611, "params_millions_measured": 74.824704, "latency_ms": 489.06690000148956, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 65.43072123650678, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554390.7602918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 455.6255000006786, "prefill_cuda_event_ms": null, "kv_decode_ms": 489.06690000148956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 120.19630000031611, "params_millions_measured": 74.824704, "latency_ms": 944.6924000021681, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 43.40037032149925, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554391.8259287, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 20.569199999954435, "prefill_cuda_event_ms": null, "kv_decode_ms": 469.67810000023746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 20.569199999954435, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 437.5474009694075, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554391.8259287, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 20.569199999954435, "prefill_cuda_event_ms": null, "kv_decode_ms": 469.67810000023746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 469.67810000023746, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 68.13176939692062, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554391.8259287, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 20.569199999954435, "prefill_cuda_event_ms": null, "kv_decode_ms": 469.67810000023746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 490.2473000001919, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 83.63126120222172, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554392.3166044, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.430599998988328, "prefill_cuda_event_ms": null, "kv_decode_ms": 456.079999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 17.430599998988328, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 516.3333448373756, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554392.3166044, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.430599998988328, "prefill_cuda_event_ms": null, "kv_decode_ms": 456.079999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 456.079999999929, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 70.16312927557661, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554392.3166044, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.430599998988328, "prefill_cuda_event_ms": null, "kv_decode_ms": 456.079999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 473.5105999989173, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 86.58729075989798, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554392.7907012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 19.398899999941932, "prefill_cuda_event_ms": null, "kv_decode_ms": 470.88260000055016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.398899999941932, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 463.94383186814406, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554392.7907012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 19.398899999941932, "prefill_cuda_event_ms": null, "kv_decode_ms": 470.88260000055016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 470.88260000055016, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 67.95749089043132, "gen_tokens": 32}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554392.7907012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 19.398899999941932, "prefill_cuda_event_ms": null, "kv_decode_ms": 470.88260000055016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 490.2815000004921, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 83.62542743293159, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554393.2813928, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 25.981999999203254, "prefill_cuda_event_ms": null, "kv_decode_ms": 581.9511999998213, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 25.981999999203254, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 654.2991301871031, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554393.2813928, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 25.981999999203254, "prefill_cuda_event_ms": null, "kv_decode_ms": 581.9511999998213, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 581.9511999998213, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 54.98742849917627, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554393.2813928, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 25.981999999203254, "prefill_cuda_event_ms": null, "kv_decode_ms": 581.9511999998213, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 607.9331999990245, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 80.60096076358164, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554393.8900824, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.745499998767627, "prefill_cuda_event_ms": null, "kv_decode_ms": 505.89249999939057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 23.745499998767627, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 715.9251226919747, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554393.8900824, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.745499998767627, "prefill_cuda_event_ms": null, "kv_decode_ms": 505.89249999939057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 505.89249999939057, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 63.25454518507104, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554393.8900824, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 23.745499998767627, "prefill_cuda_event_ms": null, "kv_decode_ms": 505.89249999939057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 529.6379999981582, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 92.51602037650319, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554394.4201052, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.744800000509713, "prefill_cuda_event_ms": null, "kv_decode_ms": 475.6529000005685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.744800000509713, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 747.4235869130099, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554394.4201052, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.744800000509713, "prefill_cuda_event_ms": null, "kv_decode_ms": 475.6529000005685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 475.6529000005685, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 67.27594849093059, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554394.4201052, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 22.744800000509713, "prefill_cuda_event_ms": null, "kv_decode_ms": 475.6529000005685, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 498.3977000010782, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 98.31506044248198, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554394.9191809, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 20.855299999311683, "prefill_cuda_event_ms": null, "kv_decode_ms": 485.3117000002385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 20.855299999311683, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 815.1405158670015, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554394.9191809, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 20.855299999311683, "prefill_cuda_event_ms": null, "kv_decode_ms": 485.3117000002385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 485.3117000002385, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 65.93700502168869, "gen_tokens": 32}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554394.9191809, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 20.855299999311683, "prefill_cuda_event_ms": null, "kv_decode_ms": 485.3117000002385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 506.16699999955017, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 96.80599485949014, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554395.425883, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 15.923200000543147, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.47200000018347, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 15.923200000543147, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 565.213022488759, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554395.425883, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 15.923200000543147, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.47200000018347, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 413.47200000018347, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 77.39339060440804, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554395.425883, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 15.923200000543147, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.47200000018347, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 429.3952000007266, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 95.48313534927875, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554395.8562586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 18.045599999823025, "prefill_cuda_event_ms": null, "kv_decode_ms": 427.72799999875133, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.045599999823025, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 498.7365341184701, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554395.8562586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 18.045599999823025, "prefill_cuda_event_ms": null, "kv_decode_ms": 427.72799999875133, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 427.72799999875133, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 74.81390042291694, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554395.8562586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 18.045599999823025, "prefill_cuda_event_ms": null, "kv_decode_ms": 427.72799999875133, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 445.77359999857435, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 91.97493974549216, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554396.3023562, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.220000000539585, "prefill_cuda_event_ms": null, "kv_decode_ms": 406.5759999994043, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.220000000539585, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 424.1281809505724, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554396.3023562, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.220000000539585, "prefill_cuda_event_ms": null, "kv_decode_ms": 406.5759999994043, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 406.5759999994043, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 78.7060721735835, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554396.3023562, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 21.220000000539585, "prefill_cuda_event_ms": null, "kv_decode_ms": 406.5759999994043, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 427.7959999999439, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 95.84007330598084, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554396.730699, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.583799999556504, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.62500000101863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 17.583799999556504, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 511.8347570051409, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554396.730699, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.583799999556504, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.62500000101863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 395.62500000101863, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 80.8846761451314, "gen_tokens": 32}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554396.730699, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 17.583799999556504, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.62500000101863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 413.20880000057514, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 99.2234434502434, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.1443536, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 9.016300000439514, "prefill_cuda_event_ms": 8.939583778381348, "kv_decode_ms": 105.78219999842986, "kv_decode_cuda_event_ms": 105.70137786865234, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 9.016300000439514, "cuda_event_ms": 8.939583778381348, "tokens_total": 9, "tokens_per_s": 998.1921630337589, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.1443536, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 9.016300000439514, "prefill_cuda_event_ms": 8.939583778381348, "kv_decode_ms": 105.78219999842986, "kv_decode_cuda_event_ms": 105.70137786865234, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 105.78219999842986, "cuda_event_ms": 105.70137786865234, "tokens_total": 32, "tokens_per_s": 302.5083615246703, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.1443536, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 9.016300000439514, "prefill_cuda_event_ms": 8.939583778381348, "kv_decode_ms": 105.78219999842986, "kv_decode_cuda_event_ms": 105.70137786865234, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 114.79849999886937, "cuda_event_ms": 114.64096164703369, "tokens_total": 41, "tokens_per_s": 357.1475237080955, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.2612343, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.114899998967303, "prefill_cuda_event_ms": 4.063968181610107, "kv_decode_ms": 100.37440000087372, "kv_decode_cuda_event_ms": 100.31513977050781, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 4.114899998967303, "cuda_event_ms": 4.063968181610107, "tokens_total": 9, "tokens_per_s": 2187.173443402923, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.2612343, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.114899998967303, "prefill_cuda_event_ms": 4.063968181610107, "kv_decode_ms": 100.37440000087372, "kv_decode_cuda_event_ms": 100.31513977050781, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 100.37440000087372, "cuda_event_ms": 100.31513977050781, "tokens_total": 32, "tokens_per_s": 318.8063888772581, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.2612343, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 4.114899998967303, "prefill_cuda_event_ms": 4.063968181610107, "kv_decode_ms": 100.37440000087372, "kv_decode_cuda_event_ms": 100.31513977050781, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 104.48929999984102, "cuda_event_ms": 104.37910795211792, "tokens_total": 41, "tokens_per_s": 392.38467479504965, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.3670118, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 3.9032000004226575, "prefill_cuda_event_ms": 3.8406078815460205, "kv_decode_ms": 60.488099999929545, "kv_decode_cuda_event_ms": 60.42726516723633, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 3.9032000004226575, "cuda_event_ms": 3.8406078815460205, "tokens_total": 9, "tokens_per_s": 2305.800368678376, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.3670118, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 3.9032000004226575, "prefill_cuda_event_ms": 3.8406078815460205, "kv_decode_ms": 60.488099999929545, "kv_decode_cuda_event_ms": 60.42726516723633, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 60.488099999929545, "cuda_event_ms": 60.42726516723633, "tokens_total": 32, "tokens_per_s": 529.0296769122732, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.3670118, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 3.9032000004226575, "prefill_cuda_event_ms": 3.8406078815460205, "kv_decode_ms": 60.488099999929545, "kv_decode_cuda_event_ms": 60.42726516723633, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 64.3913000003522, "cuda_event_ms": 64.26787304878235, "tokens_total": 41, "tokens_per_s": 636.7319808697098, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.432602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 2.297299999554525, "prefill_cuda_event_ms": 2.251391887664795, "kv_decode_ms": 62.85720000050787, "kv_decode_cuda_event_ms": 62.80064010620117, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 2.297299999554525, "cuda_event_ms": 2.251391887664795, "tokens_total": 9, "tokens_per_s": 3917.6424505920913, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.432602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 2.297299999554525, "prefill_cuda_event_ms": 2.251391887664795, "kv_decode_ms": 62.85720000050787, "kv_decode_cuda_event_ms": 62.80064010620117, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 62.85720000050787, "cuda_event_ms": 62.80064010620117, "tokens_total": 32, "tokens_per_s": 509.0904462772991, "gen_tokens": 32}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.432602, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 32, "prefill_ms": 2.297299999554525, "prefill_cuda_event_ms": 2.251391887664795, "kv_decode_ms": 62.85720000050787, "kv_decode_cuda_event_ms": 62.80064010620117, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 65.1545000000624, "cuda_event_ms": 65.05203199386597, "tokens_total": 41, "tokens_per_s": 629.273496074112, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.4984598, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.6445000007224735, "prefill_cuda_event_ms": 4.594016075134277, "kv_decode_ms": 113.48320000070089, "kv_decode_cuda_event_ms": 113.42556762695312, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 4.6445000007224735, "cuda_event_ms": 4.594016075134277, "tokens_total": 17, "tokens_per_s": 3660.2432979557693, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.4984598, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.6445000007224735, "prefill_cuda_event_ms": 4.594016075134277, "kv_decode_ms": 113.48320000070089, "kv_decode_cuda_event_ms": 113.42556762695312, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 113.48320000070089, "cuda_event_ms": 113.42556762695312, "tokens_total": 32, "tokens_per_s": 281.98006400773295, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.4984598, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.6445000007224735, "prefill_cuda_event_ms": 4.594016075134277, "kv_decode_ms": 113.48320000070089, "kv_decode_cuda_event_ms": 113.42556762695312, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 118.12770000142336, "cuda_event_ms": 118.0195837020874, "tokens_total": 49, "tokens_per_s": 414.8053335450498, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.6192698, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.502900001170929, "prefill_cuda_event_ms": 4.44755220413208, "kv_decode_ms": 122.23369999992428, "kv_decode_cuda_event_ms": 122.20416259765625, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 4.502900001170929, "cuda_event_ms": 4.44755220413208, "tokens_total": 17, "tokens_per_s": 3775.34477682812, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.6192698, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.502900001170929, "prefill_cuda_event_ms": 4.44755220413208, "kv_decode_ms": 122.23369999992428, "kv_decode_cuda_event_ms": 122.20416259765625, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 122.23369999992428, "cuda_event_ms": 122.20416259765625, "tokens_total": 32, "tokens_per_s": 261.79359701964205, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.6192698, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.502900001170929, "prefill_cuda_event_ms": 4.44755220413208, "kv_decode_ms": 122.23369999992428, "kv_decode_cuda_event_ms": 122.20416259765625, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 126.73660000109521, "cuda_event_ms": 126.65171480178833, "tokens_total": 49, "tokens_per_s": 386.62864554971935, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.7469265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.237399998601177, "prefill_cuda_event_ms": 4.184959888458252, "kv_decode_ms": 116.05339999914577, "kv_decode_cuda_event_ms": 116.00588989257812, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 4.237399998601177, "cuda_event_ms": 4.184959888458252, "tokens_total": 17, "tokens_per_s": 4011.89408732051, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.7469265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.237399998601177, "prefill_cuda_event_ms": 4.184959888458252, "kv_decode_ms": 116.05339999914577, "kv_decode_cuda_event_ms": 116.00588989257812, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 116.05339999914577, "cuda_event_ms": 116.00588989257812, "tokens_total": 32, "tokens_per_s": 275.73513572403346, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.7469265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.237399998601177, "prefill_cuda_event_ms": 4.184959888458252, "kv_decode_ms": 116.05339999914577, "kv_decode_cuda_event_ms": 116.00588989257812, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 120.29079999774694, "cuda_event_ms": 120.19084978103638, "tokens_total": 49, "tokens_per_s": 407.34619772183555, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554397.8678868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.0728000003582565, "prefill_cuda_event_ms": 4.029791831970215, "kv_decode_ms": 112.80830000032438, "kv_decode_cuda_event_ms": 112.75263977050781, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 4.0728000003582565, "cuda_event_ms": 4.029791831970215, "tokens_total": 17, "tokens_per_s": 4174.032606193436, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554397.8678868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.0728000003582565, "prefill_cuda_event_ms": 4.029791831970215, "kv_decode_ms": 112.80830000032438, "kv_decode_cuda_event_ms": 112.75263977050781, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 112.80830000032438, "cuda_event_ms": 112.75263977050781, "tokens_total": 32, "tokens_per_s": 283.66707059594006, "gen_tokens": 32}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "batch_size_config": 1, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554397.8678868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 32, "prefill_ms": 4.0728000003582565, "prefill_cuda_event_ms": 4.029791831970215, "kv_decode_ms": 112.80830000032438, "kv_decode_cuda_event_ms": 112.75263977050781, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 116.88110000068264, "cuda_event_ms": 116.78243160247803, "tokens_total": 49, "tokens_per_s": 419.2294562569468, "gen_tokens": 32}
