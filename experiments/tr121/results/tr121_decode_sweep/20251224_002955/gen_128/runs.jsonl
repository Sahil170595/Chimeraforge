{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554569.6825147, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 88.95250000023225, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0935999998328, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5708.487199999581, "params_millions_measured": 45.1712, "latency_ms": 88.95250000023225, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 191.11323459099648, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554569.6825147, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 88.95250000023225, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0935999998328, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5708.487199999581, "params_millions_measured": 45.1712, "latency_ms": 1349.0935999998328, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 94.87851695391325, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554569.6825147, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 88.95250000023225, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0935999998328, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5708.487199999581, "params_millions_measured": 45.1712, "latency_ms": 1438.046100000065, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 100.83125986016265, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554576.8319554, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.84920000116108, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.4131999999663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.84920000116108, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1008.9499797514736, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554576.8319554, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.84920000116108, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.4131999999663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1315.4131999999663, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 97.30782692465247, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554576.8319554, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.84920000116108, "prefill_cuda_event_ms": null, "kv_decode_ms": 1315.4131999999663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1332.2624000011274, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 108.83741821421764, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554578.1648371, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.369700000723242, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.0228999990504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.369700000723242, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1038.504065392091, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554578.1648371, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.369700000723242, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.0228999990504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1318.0228999990504, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 97.11515634522907, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554578.1648371, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 16.369700000723242, "prefill_cuda_event_ms": null, "kv_decode_ms": 1318.0228999990504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1334.3925999997737, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 108.66367214568231, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554579.4998696, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 14.993200000390061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1300.6709000001138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.993200000390061, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1133.8473440998407, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554579.4998696, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 14.993200000390061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1300.6709000001138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1300.6709000001138, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 98.410750943985, "gen_tokens": 128}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554579.4998696, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 14.993200000390061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1300.6709000001138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1315.6641000005038, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 110.21050129736342, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554580.8159616, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 768.6903, "prefill_cuda_event_ms": null, "kv_decode_ms": 4945.5059, "kv_decode_ms_equiv": 4945.5059, "kv_decode_ms_per_token": 38.63676484375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 16710.453400000915, "ollama_total_duration_ms": 16592.3112, "ollama_load_ms": 10790.7659, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 768.6903, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 119.68409123934569, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554580.8159616, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 768.6903, "prefill_cuda_event_ms": null, "kv_decode_ms": 4945.5059, "kv_decode_ms_equiv": 4945.5059, "kv_decode_ms_per_token": 38.63676484375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 16710.453400000915, "ollama_total_duration_ms": 16592.3112, "ollama_load_ms": 10790.7659, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 4945.5059, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 25.882084176666336, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554580.8159616, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 768.6903, "prefill_cuda_event_ms": null, "kv_decode_ms": 4945.5059, "kv_decode_ms_equiv": 4945.5059, "kv_decode_ms_per_token": 38.63676484375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 16710.453400000915, "ollama_total_duration_ms": 16592.3112, "ollama_load_ms": 10790.7659, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 5714.1962, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 38.50060311194775, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554597.526575, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 45.6195, "prefill_cuda_event_ms": null, "kv_decode_ms": 6058.772, "kv_decode_ms_equiv": 6058.772, "kv_decode_ms_per_token": 47.33415625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6660.783200000878, "ollama_total_duration_ms": 6640.0032, "ollama_load_ms": 501.0928, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 45.6195, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2016.6814629708786, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554597.526575, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 45.6195, "prefill_cuda_event_ms": null, "kv_decode_ms": 6058.772, "kv_decode_ms_equiv": 6058.772, "kv_decode_ms_per_token": 47.33415625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6660.783200000878, "ollama_total_duration_ms": 6640.0032, "ollama_load_ms": 501.0928, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6058.772, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 21.126393269131103, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554597.526575, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 45.6195, "prefill_cuda_event_ms": null, "kv_decode_ms": 6058.772, "kv_decode_ms_equiv": 6058.772, "kv_decode_ms_per_token": 47.33415625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6660.783200000878, "ollama_total_duration_ms": 6640.0032, "ollama_load_ms": 501.0928, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6104.3915, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 36.03962819226126, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554604.1882985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 56.3622, "prefill_cuda_event_ms": null, "kv_decode_ms": 6449.9039, "kv_decode_ms_equiv": 6449.9039, "kv_decode_ms_per_token": 50.38987421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6980.476499998986, "ollama_total_duration_ms": 6976.801, "ollama_load_ms": 442.934, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.3622, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1632.2996618300917, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554604.1882985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 56.3622, "prefill_cuda_event_ms": null, "kv_decode_ms": 6449.9039, "kv_decode_ms_equiv": 6449.9039, "kv_decode_ms_per_token": 50.38987421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6980.476499998986, "ollama_total_duration_ms": 6976.801, "ollama_load_ms": 442.934, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6449.9039, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 19.84525691925425, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554604.1882985, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 56.3622, "prefill_cuda_event_ms": null, "kv_decode_ms": 6449.9039, "kv_decode_ms_equiv": 6449.9039, "kv_decode_ms_per_token": 50.38987421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6980.476499998986, "ollama_total_duration_ms": 6976.801, "ollama_load_ms": 442.934, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6506.2661, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 33.813557056942386, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554611.1689231, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 59.5007, "prefill_cuda_event_ms": null, "kv_decode_ms": 6409.7785, "kv_decode_ms_equiv": 6409.7785, "kv_decode_ms_per_token": 50.07639453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6922.690299999886, "ollama_total_duration_ms": 6918.6743, "ollama_load_ms": 420.2634, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 59.5007, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1546.2002968032307, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554611.1689231, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 59.5007, "prefill_cuda_event_ms": null, "kv_decode_ms": 6409.7785, "kv_decode_ms_equiv": 6409.7785, "kv_decode_ms_per_token": 50.07639453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6922.690299999886, "ollama_total_duration_ms": 6918.6743, "ollama_load_ms": 420.2634, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6409.7785, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 19.969488805268387, "gen_tokens": 128}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554611.1689231, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 59.5007, "prefill_cuda_event_ms": null, "kv_decode_ms": 6409.7785, "kv_decode_ms_equiv": 6409.7785, "kv_decode_ms_per_token": 50.07639453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6922.690299999886, "ollama_total_duration_ms": 6918.6743, "ollama_load_ms": 420.2634, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6469.2792, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 34.00687977727102, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554618.0917385, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 667.2029000001203, "prefill_cuda_event_ms": null, "kv_decode_ms": 2422.8977000002487, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.97069999968517, "params_millions_measured": 96.08832, "latency_ms": 667.2029000001203, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 13.489150002193302, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554618.0917385, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 667.2029000001203, "prefill_cuda_event_ms": null, "kv_decode_ms": 2422.8977000002487, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.97069999968517, "params_millions_measured": 96.08832, "latency_ms": 2422.8977000002487, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 52.82930434908038, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554618.0917385, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 667.2029000001203, "prefill_cuda_event_ms": null, "kv_decode_ms": 2422.8977000002487, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.97069999968517, "params_millions_measured": 96.08832, "latency_ms": 3090.100600000369, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 44.33512617679296, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554621.472772, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.454699999594595, "prefill_cuda_event_ms": null, "kv_decode_ms": 2501.803299999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 21.454699999594595, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 419.48850369243394, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554621.472772, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.454699999594595, "prefill_cuda_event_ms": null, "kv_decode_ms": 2501.803299999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2501.803299999665, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 51.163095036295275, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554621.472772, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.454699999594595, "prefill_cuda_event_ms": null, "kv_decode_ms": 2501.803299999665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2523.2579999992595, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 54.29488383670643, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554623.9964101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 23.92409999993106, "prefill_cuda_event_ms": null, "kv_decode_ms": 2454.146000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 23.92409999993106, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 376.1896999271001, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554623.9964101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 23.92409999993106, "prefill_cuda_event_ms": null, "kv_decode_ms": 2454.146000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2454.146000000037, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 52.15663615775022, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554623.9964101, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 23.92409999993106, "prefill_cuda_event_ms": null, "kv_decode_ms": 2454.146000000037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2478.070099999968, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 55.28495743522419, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554626.4748702, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.885599999222904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2411.4260000005743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 21.885599999222904, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 411.22930147309484, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554626.4748702, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.885599999222904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2411.4260000005743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2411.4260000005743, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 53.08062532292905, "gen_tokens": 128}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554626.4748702, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 21.885599999222904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2411.4260000005743, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2433.311599999797, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 56.30187272358025, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554628.9087188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 457.9209000003175, "prefill_cuda_event_ms": 456.3148193359375, "kv_decode_ms": 667.8165000012086, "kv_decode_cuda_event_ms": 667.7760009765625, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 358.14940000091156, "params_millions_measured": 96.08832, "latency_ms": 457.9209000003175, "cuda_event_ms": 456.3148193359375, "tokens_total": 17, "tokens_per_s": 37.12431557500043, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554628.9087188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 457.9209000003175, "prefill_cuda_event_ms": 456.3148193359375, "kv_decode_ms": 667.8165000012086, "kv_decode_cuda_event_ms": 667.7760009765625, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 358.14940000091156, "params_millions_measured": 96.08832, "latency_ms": 667.8165000012086, "cuda_event_ms": 667.7760009765625, "tokens_total": 128, "tokens_per_s": 191.66941817066265, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554628.9087188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 457.9209000003175, "prefill_cuda_event_ms": 456.3148193359375, "kv_decode_ms": 667.8165000012086, "kv_decode_cuda_event_ms": 667.7760009765625, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 358.14940000091156, "params_millions_measured": 96.08832, "latency_ms": 1125.737400001526, "cuda_event_ms": 1124.0908203125, "tokens_total": 145, "tokens_per_s": 128.80446185744867, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554630.5198803, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.480999998530024, "prefill_cuda_event_ms": 6.430784225463867, "kv_decode_ms": 556.7661999994016, "kv_decode_cuda_event_ms": 556.7354736328125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 6.480999998530024, "cuda_event_ms": 6.430784225463867, "tokens_total": 17, "tokens_per_s": 2623.0519987433768, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554630.5198803, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.480999998530024, "prefill_cuda_event_ms": 6.430784225463867, "kv_decode_ms": 556.7661999994016, "kv_decode_cuda_event_ms": 556.7354736328125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 556.7661999994016, "cuda_event_ms": 556.7354736328125, "tokens_total": 128, "tokens_per_s": 229.8990132664978, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554630.5198803, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.480999998530024, "prefill_cuda_event_ms": 6.430784225463867, "kv_decode_ms": 556.7661999994016, "kv_decode_cuda_event_ms": 556.7354736328125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 563.2471999979316, "cuda_event_ms": 563.1662578582764, "tokens_total": 145, "tokens_per_s": 257.43581148833493, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554631.0838716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.092800000056741, "prefill_cuda_event_ms": 5.050271987915039, "kv_decode_ms": 550.1039000009769, "kv_decode_cuda_event_ms": 550.0805053710938, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 5.092800000056741, "cuda_event_ms": 5.050271987915039, "tokens_total": 17, "tokens_per_s": 3338.045868640158, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554631.0838716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.092800000056741, "prefill_cuda_event_ms": 5.050271987915039, "kv_decode_ms": 550.1039000009769, "kv_decode_cuda_event_ms": 550.0805053710938, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 550.1039000009769, "cuda_event_ms": 550.0805053710938, "tokens_total": 128, "tokens_per_s": 232.68331673302572, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554631.0838716, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.092800000056741, "prefill_cuda_event_ms": 5.050271987915039, "kv_decode_ms": 550.1039000009769, "kv_decode_cuda_event_ms": 550.0805053710938, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 555.1967000010336, "cuda_event_ms": 555.1307773590088, "tokens_total": 145, "tokens_per_s": 261.16869930914584, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554631.6398854, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.259600000703358, "prefill_cuda_event_ms": 6.199647903442383, "kv_decode_ms": 559.7426000003907, "kv_decode_cuda_event_ms": 559.7234497070312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 6.259600000703358, "cuda_event_ms": 6.199647903442383, "tokens_total": 17, "tokens_per_s": 2715.828487138124, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554631.6398854, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.259600000703358, "prefill_cuda_event_ms": 6.199647903442383, "kv_decode_ms": 559.7426000003907, "kv_decode_cuda_event_ms": 559.7234497070312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 559.7426000003907, "cuda_event_ms": 559.7234497070312, "tokens_total": 128, "tokens_per_s": 228.67653810860682, "gen_tokens": 128}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554631.6398854, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.259600000703358, "prefill_cuda_event_ms": 6.199647903442383, "kv_decode_ms": 559.7426000003907, "kv_decode_cuda_event_ms": 559.7234497070312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 566.002200001094, "cuda_event_ms": 565.9230976104736, "tokens_total": 145, "tokens_per_s": 256.1827498192052, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554632.2067873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 9.4745000005787, "prefill_cuda_event_ms": 9.376640319824219, "kv_decode_ms": 580.3710000000137, "kv_decode_cuda_event_ms": 580.341552734375, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 298.09909999858064, "params_millions_measured": 51.475968, "latency_ms": 9.4745000005787, "cuda_event_ms": 9.376640319824219, "tokens_total": 17, "tokens_per_s": 1794.2899360347926, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554632.2067873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 9.4745000005787, "prefill_cuda_event_ms": 9.376640319824219, "kv_decode_ms": 580.3710000000137, "kv_decode_cuda_event_ms": 580.341552734375, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 298.09909999858064, "params_millions_measured": 51.475968, "latency_ms": 580.3710000000137, "cuda_event_ms": 580.341552734375, "tokens_total": 128, "tokens_per_s": 220.54858013235838, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554632.2067873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 9.4745000005787, "prefill_cuda_event_ms": 9.376640319824219, "kv_decode_ms": 580.3710000000137, "kv_decode_cuda_event_ms": 580.341552734375, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 298.09909999858064, "params_millions_measured": 51.475968, "latency_ms": 589.8455000005924, "cuda_event_ms": 589.7181930541992, "tokens_total": 145, "tokens_per_s": 245.82708522800354, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554633.096458, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.47690000005241, "prefill_cuda_event_ms": 6.42790412902832, "kv_decode_ms": 574.188900000081, "kv_decode_cuda_event_ms": 574.1362915039062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 6.47690000005241, "cuda_event_ms": 6.42790412902832, "tokens_total": 17, "tokens_per_s": 2624.7124395717765, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554633.096458, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.47690000005241, "prefill_cuda_event_ms": 6.42790412902832, "kv_decode_ms": 574.188900000081, "kv_decode_cuda_event_ms": 574.1362915039062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 574.188900000081, "cuda_event_ms": 574.1362915039062, "tokens_total": 128, "tokens_per_s": 222.92315299021269, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554633.096458, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.47690000005241, "prefill_cuda_event_ms": 6.42790412902832, "kv_decode_ms": 574.188900000081, "kv_decode_cuda_event_ms": 574.1362915039062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 580.6658000001335, "cuda_event_ms": 580.5641956329346, "tokens_total": 145, "tokens_per_s": 249.71334630000024, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554633.6799138, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.011799999920186, "prefill_cuda_event_ms": 4.966495990753174, "kv_decode_ms": 577.9693999993469, "kv_decode_cuda_event_ms": 577.9158935546875, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.011799999920186, "cuda_event_ms": 4.966495990753174, "tokens_total": 17, "tokens_per_s": 3391.9948921087694, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554633.6799138, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.011799999920186, "prefill_cuda_event_ms": 4.966495990753174, "kv_decode_ms": 577.9693999993469, "kv_decode_cuda_event_ms": 577.9158935546875, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 577.9693999993469, "cuda_event_ms": 577.9158935546875, "tokens_total": 128, "tokens_per_s": 221.46501181575465, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554633.6799138, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.011799999920186, "prefill_cuda_event_ms": 4.966495990753174, "kv_decode_ms": 577.9693999993469, "kv_decode_cuda_event_ms": 577.9158935546875, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 582.9811999992671, "cuda_event_ms": 582.8823895454407, "tokens_total": 145, "tokens_per_s": 248.72157112473315, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554634.2638175, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.473299999721348, "prefill_cuda_event_ms": 5.43017578125, "kv_decode_ms": 570.2934999990248, "kv_decode_cuda_event_ms": 570.2625122070312, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.473299999721348, "cuda_event_ms": 5.43017578125, "tokens_total": 17, "tokens_per_s": 3105.9872473399028, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554634.2638175, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.473299999721348, "prefill_cuda_event_ms": 5.43017578125, "kv_decode_ms": 570.2934999990248, "kv_decode_cuda_event_ms": 570.2625122070312, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 570.2934999990248, "cuda_event_ms": 570.2625122070312, "tokens_total": 128, "tokens_per_s": 224.44583359308652, "gen_tokens": 128}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554634.2638175, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 5.473299999721348, "prefill_cuda_event_ms": 5.43017578125, "kv_decode_ms": 570.2934999990248, "kv_decode_cuda_event_ms": 570.2625122070312, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 575.7667999987461, "cuda_event_ms": 575.6926879882812, "tokens_total": 145, "tokens_per_s": 251.83807055272337, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554634.8403695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 98.00209999957588, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.005799999548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.28439999955299, "params_millions_measured": 51.475968, "latency_ms": 98.00209999957588, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 91.83476680641486, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554634.8403695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 98.00209999957588, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.005799999548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.28439999955299, "params_millions_measured": 51.475968, "latency_ms": 1656.005799999548, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 77.29441527320432, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554634.8403695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 98.00209999957588, "prefill_cuda_event_ms": null, "kv_decode_ms": 1656.005799999548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.28439999955299, "params_millions_measured": 51.475968, "latency_ms": 1754.0078999991238, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 78.10683178796883, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554636.722414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.039700001376332, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.0275999993464, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.039700001376332, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 641.0393383845608, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554636.722414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.039700001376332, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.0275999993464, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1617.0275999993464, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 79.15758518905412, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554636.722414, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.039700001376332, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.0275999993464, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1631.0673000007228, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 83.99408166661136, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554638.3539329, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.253999999709777, "prefill_cuda_event_ms": null, "kv_decode_ms": 1608.0746000006911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.253999999709777, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 631.4017118130523, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554638.3539329, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.253999999709777, "prefill_cuda_event_ms": null, "kv_decode_ms": 1608.0746000006911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1608.0746000006911, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 79.59829724314095, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554638.3539329, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.253999999709777, "prefill_cuda_event_ms": null, "kv_decode_ms": 1608.0746000006911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1622.328600000401, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 84.44651718521521, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554639.9766784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.398699999219389, "prefill_cuda_event_ms": null, "kv_decode_ms": 1595.0235999989673, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 14.398699999219389, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 625.056428739256, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554639.9766784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.398699999219389, "prefill_cuda_event_ms": null, "kv_decode_ms": 1595.0235999989673, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1595.0235999989673, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 80.24959630696554, "gen_tokens": 128}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554639.9766784, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.398699999219389, "prefill_cuda_event_ms": null, "kv_decode_ms": 1595.0235999989673, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1609.4222999981866, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 85.12371178164635, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554641.5866072, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 48.501900000701426, "prefill_cuda_event_ms": 48.43929672241211, "kv_decode_ms": 509.46509999994305, "kv_decode_cuda_event_ms": 509.4051818847656, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 298.63189999923634, "params_millions_measured": 45.1712, "latency_ms": 48.501900000701426, "cuda_event_ms": 48.43929672241211, "tokens_total": 9, "tokens_per_s": 185.5597409559181, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554641.5866072, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 48.501900000701426, "prefill_cuda_event_ms": 48.43929672241211, "kv_decode_ms": 509.46509999994305, "kv_decode_cuda_event_ms": 509.4051818847656, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 298.63189999923634, "params_millions_measured": 45.1712, "latency_ms": 509.46509999994305, "cuda_event_ms": 509.4051818847656, "tokens_total": 128, "tokens_per_s": 251.24390267363614, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554641.5866072, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 48.501900000701426, "prefill_cuda_event_ms": 48.43929672241211, "kv_decode_ms": 509.46509999994305, "kv_decode_cuda_event_ms": 509.4051818847656, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 298.63189999923634, "params_millions_measured": 45.1712, "latency_ms": 557.9670000006445, "cuda_event_ms": 557.8444786071777, "tokens_total": 137, "tokens_per_s": 245.53423410316697, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554642.4447453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.04280000111612, "prefill_cuda_event_ms": 3.9989120960235596, "kv_decode_ms": 436.8993000007322, "kv_decode_cuda_event_ms": 436.8793640136719, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.04280000111612, "cuda_event_ms": 3.9989120960235596, "tokens_total": 9, "tokens_per_s": 2226.179874719332, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554642.4447453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.04280000111612, "prefill_cuda_event_ms": 3.9989120960235596, "kv_decode_ms": 436.8993000007322, "kv_decode_cuda_event_ms": 436.8793640136719, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 436.8993000007322, "cuda_event_ms": 436.8793640136719, "tokens_total": 128, "tokens_per_s": 292.97368981773485, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554642.4447453, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.04280000111612, "prefill_cuda_event_ms": 3.9989120960235596, "kv_decode_ms": 436.8993000007322, "kv_decode_cuda_event_ms": 436.8793640136719, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 440.9421000018483, "cuda_event_ms": 440.87827610969543, "tokens_total": 137, "tokens_per_s": 310.69838874406804, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554642.8864722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.141699999308912, "prefill_cuda_event_ms": 4.092031955718994, "kv_decode_ms": 424.9632000010024, "kv_decode_cuda_event_ms": 424.9374694824219, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.141699999308912, "cuda_event_ms": 4.092031955718994, "tokens_total": 9, "tokens_per_s": 2173.0207406383242, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554642.8864722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.141699999308912, "prefill_cuda_event_ms": 4.092031955718994, "kv_decode_ms": 424.9632000010024, "kv_decode_cuda_event_ms": 424.9374694824219, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 424.9632000010024, "cuda_event_ms": 424.9374694824219, "tokens_total": 128, "tokens_per_s": 301.2025511848981, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554642.8864722, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.141699999308912, "prefill_cuda_event_ms": 4.092031955718994, "kv_decode_ms": 424.9632000010024, "kv_decode_cuda_event_ms": 424.9374694824219, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 429.1049000003113, "cuda_event_ms": 429.02950143814087, "tokens_total": 137, "tokens_per_s": 319.26925094516656, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554643.3162746, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.150400000071386, "prefill_cuda_event_ms": 4.0905280113220215, "kv_decode_ms": 449.7042999992118, "kv_decode_cuda_event_ms": 449.65887451171875, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.150400000071386, "cuda_event_ms": 4.0905280113220215, "tokens_total": 9, "tokens_per_s": 2168.4656900166738, "gen_tokens": 0}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554643.3162746, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.150400000071386, "prefill_cuda_event_ms": 4.0905280113220215, "kv_decode_ms": 449.7042999992118, "kv_decode_cuda_event_ms": 449.65887451171875, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 449.7042999992118, "cuda_event_ms": 449.65887451171875, "tokens_total": 128, "tokens_per_s": 284.63147895233453, "gen_tokens": 128}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554643.3162746, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.150400000071386, "prefill_cuda_event_ms": 4.0905280113220215, "kv_decode_ms": 449.7042999992118, "kv_decode_cuda_event_ms": 449.65887451171875, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 453.8546999992832, "cuda_event_ms": 453.74940252304077, "tokens_total": 137, "tokens_per_s": 301.8587226269032, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554643.7708108, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.6057, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.724, "kv_decode_ms_equiv": 282.7208648648649, "kv_decode_ms_per_token": 2.208756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 128, "ollama_wall_ms": 3205.8066000008694, "ollama_total_duration_ms": 3150.7851, "ollama_load_ms": 3016.9904, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.6057, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1975.4348861800897, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554643.7708108, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.6057, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.724, "kv_decode_ms_equiv": 282.7208648648649, "kv_decode_ms_per_token": 2.208756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 128, "ollama_wall_ms": 3205.8066000008694, "ollama_total_duration_ms": 3150.7851, "ollama_load_ms": 3016.9904, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 282.7208648648649, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 452.74338015760355, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554643.7708108, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.6057, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.724, "kv_decode_ms_equiv": 282.7208648648649, "kv_decode_ms_per_token": 2.208756756756757, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 128, "ollama_wall_ms": 3205.8066000008694, "ollama_total_duration_ms": 3150.7851, "ollama_load_ms": 3016.9904, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 291.3265648648649, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 497.7232339497082, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554646.9767296, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2762, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.9784, "kv_decode_ms_equiv": 263.94239999999996, "kv_decode_ms_per_token": 2.0620499999999997, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 453.43339999999444, "ollama_total_duration_ms": 425.0176, "ollama_load_ms": 292.7976, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2762, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7468.587997539759, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554646.9767296, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2762, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.9784, "kv_decode_ms_equiv": 263.94239999999996, "kv_decode_ms_per_token": 2.0620499999999997, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 453.43339999999444, "ollama_total_duration_ms": 425.0176, "ollama_load_ms": 292.7976, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 263.94239999999996, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 484.95429305787934, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554646.9767296, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2762, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.9784, "kv_decode_ms_equiv": 263.94239999999996, "kv_decode_ms_per_token": 2.0620499999999997, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 453.43339999999444, "ollama_total_duration_ms": 425.0176, "ollama_load_ms": 292.7976, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 266.2186, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 544.665173658039, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554647.4303062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.1125, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.6245, "kv_decode_ms_equiv": 265.6653333333333, "kv_decode_ms_per_token": 2.0755104166666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 455.0686000002315, "ollama_total_duration_ms": 424.8007, "ollama_load_ms": 293.3617, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.1125, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5461.847389558233, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554647.4303062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.1125, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.6245, "kv_decode_ms_equiv": 265.6653333333333, "kv_decode_ms_per_token": 2.0755104166666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 455.0686000002315, "ollama_total_duration_ms": 424.8007, "ollama_load_ms": 293.3617, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 265.6653333333333, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 481.80919352167393, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554647.4303062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.1125, "prefill_cuda_event_ms": null, "kv_decode_ms": 99.6245, "kv_decode_ms_equiv": 265.6653333333333, "kv_decode_ms_per_token": 2.0755104166666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 455.0686000002315, "ollama_total_duration_ms": 424.8007, "ollama_load_ms": 293.3617, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 268.7778333333333, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 539.4790120961117, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554647.8854623, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4952, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.4403, "kv_decode_ms_equiv": 262.50746666666663, "kv_decode_ms_per_token": 2.050839583333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 447.3647999984678, "ollama_total_duration_ms": 431.0085, "ollama_load_ms": 290.7788, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4952, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6813.081115742225, "gen_tokens": 0}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554647.8854623, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4952, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.4403, "kv_decode_ms_equiv": 262.50746666666663, "kv_decode_ms_per_token": 2.050839583333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 447.3647999984678, "ollama_total_duration_ms": 431.0085, "ollama_load_ms": 290.7788, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 262.50746666666663, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 487.6051779606524, "gen_tokens": 128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554647.8854623, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.4952, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.4403, "kv_decode_ms_equiv": 262.50746666666663, "kv_decode_ms_per_token": 2.050839583333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 128, "ollama_wall_ms": 447.3647999984678, "ollama_total_duration_ms": 431.0085, "ollama_load_ms": 290.7788, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 265.00266666666664, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 547.1643052648527, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554648.3329139, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.689399999027955, "prefill_cuda_event_ms": 5.6359357833862305, "kv_decode_ms": 539.3041000006633, "kv_decode_cuda_event_ms": 539.2824096679688, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 5.689399999027955, "cuda_event_ms": 5.6359357833862305, "tokens_total": 9, "tokens_per_s": 1581.8891274190014, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554648.3329139, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.689399999027955, "prefill_cuda_event_ms": 5.6359357833862305, "kv_decode_ms": 539.3041000006633, "kv_decode_cuda_event_ms": 539.2824096679688, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 539.3041000006633, "cuda_event_ms": 539.2824096679688, "tokens_total": 128, "tokens_per_s": 237.34290171323113, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554648.3329139, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.689399999027955, "prefill_cuda_event_ms": 5.6359357833862305, "kv_decode_ms": 539.3041000006633, "kv_decode_cuda_event_ms": 539.2824096679688, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 544.9934999996913, "cuda_event_ms": 544.918345451355, "tokens_total": 137, "tokens_per_s": 251.3791448890264, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554648.878865, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.349799999748939, "prefill_cuda_event_ms": 5.303199768066406, "kv_decode_ms": 543.864900000699, "kv_decode_cuda_event_ms": 543.8369750976562, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 5.349799999748939, "cuda_event_ms": 5.303199768066406, "tokens_total": 9, "tokens_per_s": 1682.3058806726162, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554648.878865, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.349799999748939, "prefill_cuda_event_ms": 5.303199768066406, "kv_decode_ms": 543.864900000699, "kv_decode_cuda_event_ms": 543.8369750976562, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 543.864900000699, "cuda_event_ms": 543.8369750976562, "tokens_total": 128, "tokens_per_s": 235.35256641830625, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554648.878865, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.349799999748939, "prefill_cuda_event_ms": 5.303199768066406, "kv_decode_ms": 543.864900000699, "kv_decode_cuda_event_ms": 543.8369750976562, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 549.2147000004479, "cuda_event_ms": 549.1401748657227, "tokens_total": 137, "tokens_per_s": 249.44707415859094, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554649.4288688, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.424799999673269, "prefill_cuda_event_ms": 5.377471923828125, "kv_decode_ms": 540.3690000002825, "kv_decode_cuda_event_ms": 540.337158203125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 5.424799999673269, "cuda_event_ms": 5.377471923828125, "tokens_total": 9, "tokens_per_s": 1659.0473382506382, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554649.4288688, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.424799999673269, "prefill_cuda_event_ms": 5.377471923828125, "kv_decode_ms": 540.3690000002825, "kv_decode_cuda_event_ms": 540.337158203125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 540.3690000002825, "cuda_event_ms": 540.337158203125, "tokens_total": 128, "tokens_per_s": 236.87517233581696, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554649.4288688, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.424799999673269, "prefill_cuda_event_ms": 5.377471923828125, "kv_decode_ms": 540.3690000002825, "kv_decode_cuda_event_ms": 540.337158203125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 545.7937999999558, "cuda_event_ms": 545.7146301269531, "tokens_total": 137, "tokens_per_s": 251.0105464737985, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554649.975591, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.862800000410061, "prefill_cuda_event_ms": 4.818016052246094, "kv_decode_ms": 567.6418999992165, "kv_decode_cuda_event_ms": 567.6215209960938, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 4.862800000410061, "cuda_event_ms": 4.818016052246094, "tokens_total": 9, "tokens_per_s": 1850.7855554908826, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554649.975591, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.862800000410061, "prefill_cuda_event_ms": 4.818016052246094, "kv_decode_ms": 567.6418999992165, "kv_decode_cuda_event_ms": 567.6215209960938, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 567.6418999992165, "cuda_event_ms": 567.6215209960938, "tokens_total": 128, "tokens_per_s": 225.49427729027167, "gen_tokens": 128}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554649.975591, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.862800000410061, "prefill_cuda_event_ms": 4.818016052246094, "kv_decode_ms": 567.6418999992165, "kv_decode_cuda_event_ms": 567.6215209960938, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 572.5046999996266, "cuda_event_ms": 572.4395370483398, "tokens_total": 137, "tokens_per_s": 239.29934549024549, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554650.5504694, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.625199998525204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1309.820699998454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.26660000106494, "params_millions_measured": 5.03672, "latency_ms": 36.625199998525204, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 464.16128787513907, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554650.5504694, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.625199998525204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1309.820699998454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.26660000106494, "params_millions_measured": 5.03672, "latency_ms": 1309.820699998454, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 97.72329907456118, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554650.5504694, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.625199998525204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1309.820699998454, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.26660000106494, "params_millions_measured": 5.03672, "latency_ms": 1346.4458999969793, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 107.69092170753039, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554652.037166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 13.186899999709567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.5527999995684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.186899999709567, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1289.1581797370432, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554652.037166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 13.186899999709567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.5527999995684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1265.5527999995684, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 101.14157228370374, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554652.037166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 13.186899999709567, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.5527999995684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1278.739699999278, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 113.39289771020785, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554653.316374, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.999199998987024, "prefill_cuda_event_ms": null, "kv_decode_ms": 1402.7686999997968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 15.999199998987024, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1062.5531277236576, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554653.316374, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.999199998987024, "prefill_cuda_event_ms": null, "kv_decode_ms": 1402.7686999997968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1402.7686999997968, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 91.24811524524218, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554653.316374, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.999199998987024, "prefill_cuda_event_ms": null, "kv_decode_ms": 1402.7686999997968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1418.7678999987838, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 102.20135372397719, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554654.7357469, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.565799999720184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.8245999998617, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.565799999720184, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1352.8784478806408, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554654.7357469, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.565799999720184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.8245999998617, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1313.8245999998617, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 97.42548586775851, "gen_tokens": 128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554654.7357469, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.565799999720184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.8245999998617, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1326.3903999995819, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 109.31924718397066, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554656.0625844, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.106599999649916, "prefill_cuda_event_ms": 14.02188777923584, "kv_decode_ms": 541.3339999995515, "kv_decode_cuda_event_ms": 541.2260131835938, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 443.00599999951373, "params_millions_measured": 74.824704, "latency_ms": 14.106599999649916, "cuda_event_ms": 14.02188777923584, "tokens_total": 9, "tokens_per_s": 637.999234416752, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554656.0625844, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.106599999649916, "prefill_cuda_event_ms": 14.02188777923584, "kv_decode_ms": 541.3339999995515, "kv_decode_cuda_event_ms": 541.2260131835938, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 443.00599999951373, "params_millions_measured": 74.824704, "latency_ms": 541.3339999995515, "cuda_event_ms": 541.2260131835938, "tokens_total": 128, "tokens_per_s": 236.4529107724733, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554656.0625844, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.106599999649916, "prefill_cuda_event_ms": 14.02188777923584, "kv_decode_ms": 541.3339999995515, "kv_decode_cuda_event_ms": 541.2260131835938, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 443.00599999951373, "params_millions_measured": 74.824704, "latency_ms": 555.4405999992014, "cuda_event_ms": 555.2479009628296, "tokens_total": 137, "tokens_per_s": 246.65103703293744, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554657.062628, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.2221000003337394, "prefill_cuda_event_ms": 4.161215782165527, "kv_decode_ms": 428.8517999993928, "kv_decode_cuda_event_ms": 428.8163757324219, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 4.2221000003337394, "cuda_event_ms": 4.161215782165527, "tokens_total": 9, "tokens_per_s": 2131.6406525872403, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554657.062628, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.2221000003337394, "prefill_cuda_event_ms": 4.161215782165527, "kv_decode_ms": 428.8517999993928, "kv_decode_cuda_event_ms": 428.8163757324219, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 428.8517999993928, "cuda_event_ms": 428.8163757324219, "tokens_total": 128, "tokens_per_s": 298.4714066728441, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554657.062628, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.2221000003337394, "prefill_cuda_event_ms": 4.161215782165527, "kv_decode_ms": 428.8517999993928, "kv_decode_cuda_event_ms": 428.8163757324219, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 433.07389999972656, "cuda_event_ms": 432.9775915145874, "tokens_total": 137, "tokens_per_s": 316.3432384174768, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554657.4966607, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 3.9252000005944865, "prefill_cuda_event_ms": 3.87990403175354, "kv_decode_ms": 433.466700000281, "kv_decode_cuda_event_ms": 433.4118957519531, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 3.9252000005944865, "cuda_event_ms": 3.87990403175354, "tokens_total": 9, "tokens_per_s": 2292.8767957395585, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554657.4966607, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 3.9252000005944865, "prefill_cuda_event_ms": 3.87990403175354, "kv_decode_ms": 433.466700000281, "kv_decode_cuda_event_ms": 433.4118957519531, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 433.466700000281, "cuda_event_ms": 433.4118957519531, "tokens_total": 128, "tokens_per_s": 295.2937330593493, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554657.4966607, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 3.9252000005944865, "prefill_cuda_event_ms": 3.87990403175354, "kv_decode_ms": 433.466700000281, "kv_decode_cuda_event_ms": 433.4118957519531, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 437.39190000087547, "cuda_event_ms": 437.29179978370667, "tokens_total": 137, "tokens_per_s": 313.22024939127994, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554657.9348586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.298699999708333, "prefill_cuda_event_ms": 4.240287780761719, "kv_decode_ms": 436.31439999990107, "kv_decode_cuda_event_ms": 436.28851318359375, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 4.298699999708333, "cuda_event_ms": 4.240287780761719, "tokens_total": 9, "tokens_per_s": 2093.6562217904598, "gen_tokens": 0}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554657.9348586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.298699999708333, "prefill_cuda_event_ms": 4.240287780761719, "kv_decode_ms": 436.31439999990107, "kv_decode_cuda_event_ms": 436.28851318359375, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 436.31439999990107, "cuda_event_ms": 436.28851318359375, "tokens_total": 128, "tokens_per_s": 293.36643484613165, "gen_tokens": 128}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554657.9348586, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 4.298699999708333, "prefill_cuda_event_ms": 4.240287780761719, "kv_decode_ms": 436.31439999990107, "kv_decode_cuda_event_ms": 436.28851318359375, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 440.6130999996094, "cuda_event_ms": 440.52880096435547, "tokens_total": 137, "tokens_per_s": 310.9303831414033, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554658.37637, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 53.18249999982072, "prefill_cuda_event_ms": null, "kv_decode_ms": 2773.6879999993107, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 53.18249999982072, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 319.6540215307161, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554658.37637, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 53.18249999982072, "prefill_cuda_event_ms": null, "kv_decode_ms": 2773.6879999993107, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2773.6879999993107, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 46.14794454171911, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554658.37637, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 53.18249999982072, "prefill_cuda_event_ms": null, "kv_decode_ms": 2773.6879999993107, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2826.8704999991314, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 51.29347099559197, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554661.203955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 29.72070000032545, "prefill_cuda_event_ms": null, "kv_decode_ms": 2682.3822999995173, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 29.72070000032545, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 571.9919113551782, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554661.203955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 29.72070000032545, "prefill_cuda_event_ms": null, "kv_decode_ms": 2682.3822999995173, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2682.3822999995173, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 47.71877595524808, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554661.203955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 29.72070000032545, "prefill_cuda_event_ms": null, "kv_decode_ms": 2682.3822999995173, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2712.1029999998427, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 53.46404616639133, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554663.91653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.196700000800774, "prefill_cuda_event_ms": null, "kv_decode_ms": 2673.913200000243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 36.196700000800774, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 469.65607360958074, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554663.91653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.196700000800774, "prefill_cuda_event_ms": null, "kv_decode_ms": 2673.913200000243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2673.913200000243, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 47.869915897041224, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554663.91653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 36.196700000800774, "prefill_cuda_event_ms": null, "kv_decode_ms": 2673.913200000243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2710.1099000010436, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 53.50336530630886, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554666.6271336, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 28.965100000277744, "prefill_cuda_event_ms": null, "kv_decode_ms": 2511.3058000006276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 28.965100000277744, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 586.9132162442729, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554666.6271336, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 28.965100000277744, "prefill_cuda_event_ms": null, "kv_decode_ms": 2511.3058000006276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2511.3058000006276, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 50.969499612499604, "gen_tokens": 128}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554666.6271336, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 28.965100000277744, "prefill_cuda_event_ms": null, "kv_decode_ms": 2511.3058000006276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2540.2709000009054, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 57.080526332820774, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554669.1679363, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 966.3877, "prefill_cuda_event_ms": null, "kv_decode_ms": 5662.6361, "kv_decode_ms_equiv": 5662.6361, "kv_decode_ms_per_token": 44.23934453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 17602.840899999137, "ollama_total_duration_ms": 17477.2013, "ollama_load_ms": 10736.0078, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 966.3877, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 102.44335684322141, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554669.1679363, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 966.3877, "prefill_cuda_event_ms": null, "kv_decode_ms": 5662.6361, "kv_decode_ms_equiv": 5662.6361, "kv_decode_ms_per_token": 44.23934453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 17602.840899999137, "ollama_total_duration_ms": 17477.2013, "ollama_load_ms": 10736.0078, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 5662.6361, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 22.604313210237898, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554669.1679363, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 966.3877, "prefill_cuda_event_ms": null, "kv_decode_ms": 5662.6361, "kv_decode_ms_equiv": 5662.6361, "kv_decode_ms_per_token": 44.23934453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 17602.840899999137, "ollama_total_duration_ms": 17477.2013, "ollama_load_ms": 10736.0078, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6629.0238, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 34.243352693951714, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554686.7715242, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 61.8738, "prefill_cuda_event_ms": null, "kv_decode_ms": 6851.9112, "kv_decode_ms_equiv": 6851.9112, "kv_decode_ms_per_token": 53.53055625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7455.743300000904, "ollama_total_duration_ms": 7434.5109, "ollama_load_ms": 482.371, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.8738, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1600.0310309048418, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554686.7715242, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 61.8738, "prefill_cuda_event_ms": null, "kv_decode_ms": 6851.9112, "kv_decode_ms_equiv": 6851.9112, "kv_decode_ms_per_token": 53.53055625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7455.743300000904, "ollama_total_duration_ms": 7434.5109, "ollama_load_ms": 482.371, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6851.9112, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 18.680919274026785, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554686.7715242, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 61.8738, "prefill_cuda_event_ms": null, "kv_decode_ms": 6851.9112, "kv_decode_ms_equiv": 6851.9112, "kv_decode_ms_per_token": 53.53055625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7455.743300000904, "ollama_total_duration_ms": 7434.5109, "ollama_load_ms": 482.371, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6913.785, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 32.832956188252886, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554694.2275305, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 59.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 6768.0658, "kv_decode_ms_equiv": 6768.0658, "kv_decode_ms_per_token": 52.8755140625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7325.540300000284, "ollama_total_duration_ms": 7306.1434, "ollama_load_ms": 455.3465, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 59.2794, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1670.0573892448303, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554694.2275305, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 59.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 6768.0658, "kv_decode_ms_equiv": 6768.0658, "kv_decode_ms_per_token": 52.8755140625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7325.540300000284, "ollama_total_duration_ms": 7306.1434, "ollama_load_ms": 455.3465, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6768.0658, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 18.91234568079997, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554694.2275305, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 59.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 6768.0658, "kv_decode_ms_equiv": 6768.0658, "kv_decode_ms_per_token": 52.8755140625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7325.540300000284, "ollama_total_duration_ms": 7306.1434, "ollama_load_ms": 455.3465, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6827.345200000001, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 33.248648391178456, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554701.5542226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.9072, "prefill_cuda_event_ms": null, "kv_decode_ms": 6774.4636, "kv_decode_ms_equiv": 6774.4636, "kv_decode_ms_per_token": 52.925496875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7326.909100000194, "ollama_total_duration_ms": 7311.4429, "ollama_load_ms": 425.5397, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.9072, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1770.7915975044357, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554701.5542226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.9072, "prefill_cuda_event_ms": null, "kv_decode_ms": 6774.4636, "kv_decode_ms_equiv": 6774.4636, "kv_decode_ms_per_token": 52.925496875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7326.909100000194, "ollama_total_duration_ms": 7311.4429, "ollama_load_ms": 425.5397, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6774.4636, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 18.894484871097397, "gen_tokens": 128}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554701.5542226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.9072, "prefill_cuda_event_ms": null, "kv_decode_ms": 6774.4636, "kv_decode_ms_equiv": 6774.4636, "kv_decode_ms_per_token": 52.925496875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7326.909100000194, "ollama_total_duration_ms": 7311.4429, "ollama_load_ms": 425.5397, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6830.3708, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 33.23392047764084, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554708.8815272, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 47.84269999981916, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4552999988955, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 47.84269999981916, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 355.3311163472015, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554708.8815272, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 47.84269999981916, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4552999988955, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1638.4552999988955, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 78.12236317956693, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554708.8815272, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 47.84269999981916, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.4552999988955, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1686.2979999987147, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 85.98717427175417, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554710.5756335, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 20.25800000046729, "prefill_cuda_event_ms": null, "kv_decode_ms": 1654.5624000009411, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.25800000046729, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 839.1746470336589, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554710.5756335, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 20.25800000046729, "prefill_cuda_event_ms": null, "kv_decode_ms": 1654.5624000009411, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1654.5624000009411, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 77.36184504127931, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554710.5756335, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 20.25800000046729, "prefill_cuda_event_ms": null, "kv_decode_ms": 1654.5624000009411, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1674.8204000014084, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 86.57644724167324, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554712.25096, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.773800001028576, "prefill_cuda_event_ms": null, "kv_decode_ms": 1739.112600000226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 21.773800001028576, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 780.7548521248902, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554712.25096, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.773800001028576, "prefill_cuda_event_ms": null, "kv_decode_ms": 1739.112600000226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1739.112600000226, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.60075477573066, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554712.25096, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.773800001028576, "prefill_cuda_event_ms": null, "kv_decode_ms": 1739.112600000226, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1760.8864000012545, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 82.34489175445769, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554714.0123265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 22.166000000652275, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.4055999999546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 22.166000000652275, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 766.940359085976, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554714.0123265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 22.166000000652275, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.4055999999546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1724.4055999999546, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 74.22847617753234, "gen_tokens": 128}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554714.0123265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 22.166000000652275, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.4055999999546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1746.5716000006068, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 83.01978573334733, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554715.7593925, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.952700000823825, "prefill_cuda_event_ms": 12.693535804748535, "kv_decode_ms": 766.2421999993967, "kv_decode_cuda_event_ms": 766.2202758789062, "gpu_peak_mb": 580.107421875, "hf_load_ms": 198.89420000072278, "params_millions_measured": 5.03672, "latency_ms": 12.952700000823825, "cuda_event_ms": 12.693535804748535, "tokens_total": 17, "tokens_per_s": 1312.467670749632, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554715.7593925, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.952700000823825, "prefill_cuda_event_ms": 12.693535804748535, "kv_decode_ms": 766.2421999993967, "kv_decode_cuda_event_ms": 766.2202758789062, "gpu_peak_mb": 580.107421875, "hf_load_ms": 198.89420000072278, "params_millions_measured": 5.03672, "latency_ms": 766.2421999993967, "cuda_event_ms": 766.2202758789062, "tokens_total": 128, "tokens_per_s": 167.04900878612634, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554715.7593925, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 12.952700000823825, "prefill_cuda_event_ms": 12.693535804748535, "kv_decode_ms": 766.2421999993967, "kv_decode_cuda_event_ms": 766.2202758789062, "gpu_peak_mb": 580.107421875, "hf_load_ms": 198.89420000072278, "params_millions_measured": 5.03672, "latency_ms": 779.1949000002205, "cuda_event_ms": 778.9138116836548, "tokens_total": 145, "tokens_per_s": 186.08951367617905, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554716.7408407, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.749999998646672, "prefill_cuda_event_ms": 6.6862077713012695, "kv_decode_ms": 789.2990000000282, "kv_decode_cuda_event_ms": 789.2469482421875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.749999998646672, "cuda_event_ms": 6.6862077713012695, "tokens_total": 17, "tokens_per_s": 2518.518519023464, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554716.7408407, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.749999998646672, "prefill_cuda_event_ms": 6.6862077713012695, "kv_decode_ms": 789.2990000000282, "kv_decode_cuda_event_ms": 789.2469482421875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 789.2990000000282, "cuda_event_ms": 789.2469482421875, "tokens_total": 128, "tokens_per_s": 162.1692159751823, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554716.7408407, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.749999998646672, "prefill_cuda_event_ms": 6.6862077713012695, "kv_decode_ms": 789.2990000000282, "kv_decode_cuda_event_ms": 789.2469482421875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 796.0489999986748, "cuda_event_ms": 795.9331560134888, "tokens_total": 145, "tokens_per_s": 182.14959129430648, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554717.5376782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.869499999083928, "prefill_cuda_event_ms": 6.785056114196777, "kv_decode_ms": 776.899799999228, "kv_decode_cuda_event_ms": 776.8576049804688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.869499999083928, "cuda_event_ms": 6.785056114196777, "tokens_total": 17, "tokens_per_s": 2474.70703868797, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554717.5376782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.869499999083928, "prefill_cuda_event_ms": 6.785056114196777, "kv_decode_ms": 776.899799999228, "kv_decode_cuda_event_ms": 776.8576049804688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 776.899799999228, "cuda_event_ms": 776.8576049804688, "tokens_total": 128, "tokens_per_s": 164.75741144498582, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554717.5376782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.869499999083928, "prefill_cuda_event_ms": 6.785056114196777, "kv_decode_ms": 776.899799999228, "kv_decode_cuda_event_ms": 776.8576049804688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 783.7692999983119, "cuda_event_ms": 783.6426610946655, "tokens_total": 145, "tokens_per_s": 185.00341873598813, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554718.3223217, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.282100001044455, "prefill_cuda_event_ms": 6.237823963165283, "kv_decode_ms": 764.525999999023, "kv_decode_cuda_event_ms": 764.4989624023438, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.282100001044455, "cuda_event_ms": 6.237823963165283, "tokens_total": 17, "tokens_per_s": 2706.101462436701, "gen_tokens": 0}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554718.3223217, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.282100001044455, "prefill_cuda_event_ms": 6.237823963165283, "kv_decode_ms": 764.525999999023, "kv_decode_cuda_event_ms": 764.4989624023438, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 764.525999999023, "cuda_event_ms": 764.4989624023438, "tokens_total": 128, "tokens_per_s": 167.42399866082195, "gen_tokens": 128}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554718.3223217, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 6.282100001044455, "prefill_cuda_event_ms": 6.237823963165283, "kv_decode_ms": 764.525999999023, "kv_decode_cuda_event_ms": 764.4989624023438, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 770.8081000000675, "cuda_event_ms": 770.736786365509, "tokens_total": 145, "tokens_per_s": 188.11426605401175, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554719.0940156, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.8601, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.4488, "kv_decode_ms_equiv": 259.4488, "kv_decode_ms_per_token": 2.02694375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2251.477600000726, "ollama_total_duration_ms": 2166.5557, "ollama_load_ms": 1795.1811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.8601, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2821.6385819573143, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554719.0940156, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.8601, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.4488, "kv_decode_ms_equiv": 259.4488, "kv_decode_ms_per_token": 2.02694375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2251.477600000726, "ollama_total_duration_ms": 2166.5557, "ollama_load_ms": 1795.1811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 259.4488, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 493.3536019438132, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554719.0940156, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.8601, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.4488, "kv_decode_ms_equiv": 259.4488, "kv_decode_ms_per_token": 2.02694375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2251.477600000726, "ollama_total_duration_ms": 2166.5557, "ollama_load_ms": 1795.1811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 268.3089, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 570.238258961965, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554721.3457036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3827, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.1892, "kv_decode_ms_equiv": 250.1892, "kv_decode_ms_per_token": 1.954603125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 696.8170999989525, "ollama_total_duration_ms": 621.6688, "ollama_load_ms": 293.0232, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3827, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10492.298652788853, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554721.3457036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3827, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.1892, "kv_decode_ms_equiv": 250.1892, "kv_decode_ms_per_token": 1.954603125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 696.8170999989525, "ollama_total_duration_ms": 621.6688, "ollama_load_ms": 293.0232, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 250.1892, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 511.6128114243141, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554721.3457036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3827, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.1892, "kv_decode_ms_equiv": 250.1892, "kv_decode_ms_per_token": 1.954603125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 696.8170999989525, "ollama_total_duration_ms": 621.6688, "ollama_load_ms": 293.0232, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 252.5719, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 605.7681000934783, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554722.04267, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3446, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.9851, "kv_decode_ms_equiv": 250.9851, "kv_decode_ms_per_token": 1.96082109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 664.3999999996595, "ollama_total_duration_ms": 609.2043, "ollama_load_ms": 290.8886, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3446, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10662.799624669455, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554722.04267, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3446, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.9851, "kv_decode_ms_equiv": 250.9851, "kv_decode_ms_per_token": 1.96082109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 664.3999999996595, "ollama_total_duration_ms": 609.2043, "ollama_load_ms": 290.8886, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 250.9851, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 509.990433695068, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554722.04267, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3446, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.9851, "kv_decode_ms_equiv": 250.9851, "kv_decode_ms_per_token": 1.96082109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 664.3999999996595, "ollama_total_duration_ms": 609.2043, "ollama_load_ms": 290.8886, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 253.3297, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 603.9560304220153, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554722.7071924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9897, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.7599, "kv_decode_ms_equiv": 252.7599, "kv_decode_ms_per_token": 1.97468671875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.012900000147, "ollama_total_duration_ms": 642.2217, "ollama_load_ms": 296.0929, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.9897, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 12564.708247474493, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554722.7071924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9897, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.7599, "kv_decode_ms_equiv": 252.7599, "kv_decode_ms_per_token": 1.97468671875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.012900000147, "ollama_total_duration_ms": 642.2217, "ollama_load_ms": 296.0929, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 252.7599, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 506.409442320558, "gen_tokens": 128}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554722.7071924, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9897, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.7599, "kv_decode_ms_equiv": 252.7599, "kv_decode_ms_per_token": 1.97468671875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.012900000147, "ollama_total_duration_ms": 642.2217, "ollama_load_ms": 296.0929, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 254.7496, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 600.5897555874475, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554723.4222984, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.818500001027132, "prefill_cuda_event_ms": 11.708479881286621, "kv_decode_ms": 822.4093999997422, "kv_decode_cuda_event_ms": 822.3805541992188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 11.818500001027132, "cuda_event_ms": 11.708479881286621, "tokens_total": 9, "tokens_per_s": 761.5179590656869, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554723.4222984, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.818500001027132, "prefill_cuda_event_ms": 11.708479881286621, "kv_decode_ms": 822.4093999997422, "kv_decode_cuda_event_ms": 822.3805541992188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 822.4093999997422, "cuda_event_ms": 822.3805541992188, "tokens_total": 128, "tokens_per_s": 155.640244384415, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554723.4222984, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.818500001027132, "prefill_cuda_event_ms": 11.708479881286621, "kv_decode_ms": 822.4093999997422, "kv_decode_cuda_event_ms": 822.3805541992188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 834.2279000007693, "cuda_event_ms": 834.0890340805054, "tokens_total": 137, "tokens_per_s": 164.22370913256876, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554724.2575016, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.8864999993820675, "prefill_cuda_event_ms": 6.830272197723389, "kv_decode_ms": 829.5402999992803, "kv_decode_cuda_event_ms": 829.5086669921875, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 6.8864999993820675, "cuda_event_ms": 6.830272197723389, "tokens_total": 9, "tokens_per_s": 1306.9048138833339, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554724.2575016, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.8864999993820675, "prefill_cuda_event_ms": 6.830272197723389, "kv_decode_ms": 829.5402999992803, "kv_decode_cuda_event_ms": 829.5086669921875, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 829.5402999992803, "cuda_event_ms": 829.5086669921875, "tokens_total": 128, "tokens_per_s": 154.30232865131575, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554724.2575016, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.8864999993820675, "prefill_cuda_event_ms": 6.830272197723389, "kv_decode_ms": 829.5402999992803, "kv_decode_cuda_event_ms": 829.5086669921875, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 836.4267999986623, "cuda_event_ms": 836.3389391899109, "tokens_total": 137, "tokens_per_s": 163.79197797131692, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554725.0946221, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.341200000155368, "prefill_cuda_event_ms": 7.281184196472168, "kv_decode_ms": 841.356499999165, "kv_decode_cuda_event_ms": 841.3101806640625, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 7.341200000155368, "cuda_event_ms": 7.281184196472168, "tokens_total": 9, "tokens_per_s": 1225.9576090842813, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554725.0946221, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.341200000155368, "prefill_cuda_event_ms": 7.281184196472168, "kv_decode_ms": 841.356499999165, "kv_decode_cuda_event_ms": 841.3101806640625, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 841.356499999165, "cuda_event_ms": 841.3101806640625, "tokens_total": 128, "tokens_per_s": 152.1352720281201, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554725.0946221, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.341200000155368, "prefill_cuda_event_ms": 7.281184196472168, "kv_decode_ms": 841.356499999165, "kv_decode_cuda_event_ms": 841.3101806640625, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 848.6976999993203, "cuda_event_ms": 848.5913648605347, "tokens_total": 137, "tokens_per_s": 161.42379082694546, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554725.9442606, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.331399998292909, "prefill_cuda_event_ms": 7.2309441566467285, "kv_decode_ms": 834.6912999986671, "kv_decode_cuda_event_ms": 834.6500854492188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 7.331399998292909, "cuda_event_ms": 7.2309441566467285, "tokens_total": 9, "tokens_per_s": 1227.5963666005973, "gen_tokens": 0}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554725.9442606, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.331399998292909, "prefill_cuda_event_ms": 7.2309441566467285, "kv_decode_ms": 834.6912999986671, "kv_decode_cuda_event_ms": 834.6500854492188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 834.6912999986671, "cuda_event_ms": 834.6500854492188, "tokens_total": 128, "tokens_per_s": 153.35010680020793, "gen_tokens": 128}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554725.9442606, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 7.331399998292909, "prefill_cuda_event_ms": 7.2309441566467285, "kv_decode_ms": 834.6912999986671, "kv_decode_cuda_event_ms": 834.6500854492188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 842.02269999696, "cuda_event_ms": 841.8810296058655, "tokens_total": 137, "tokens_per_s": 162.70345205716498, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554726.7873216, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.0969, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.6834, "kv_decode_ms_equiv": 1461.6992914285715, "kv_decode_ms_per_token": 11.419525714285715, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 7112.083600000915, "ollama_total_duration_ms": 7108.92, "ollama_load_ms": 6653.829, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 22.0969, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1719.69823821441, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554726.7873216, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.0969, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.6834, "kv_decode_ms_equiv": 1461.6992914285715, "kv_decode_ms_per_token": 11.419525714285715, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 7112.083600000915, "ollama_total_duration_ms": 7108.92, "ollama_load_ms": 6653.829, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1461.6992914285715, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.56931110974335, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554726.7873216, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 22.0969, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.6834, "kv_decode_ms_equiv": 1461.6992914285715, "kv_decode_ms_per_token": 11.419525714285715, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 7112.083600000915, "ollama_total_duration_ms": 7108.92, "ollama_load_ms": 6653.829, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1483.7961914285715, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 111.87520291461206, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554733.8996854, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.3274, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1708, "kv_decode_ms_equiv": 1430.567497142857, "kv_decode_ms_per_token": 11.17630857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 662.9061000003276, "ollama_total_duration_ms": 659.5931, "ollama_load_ms": 224.0777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.3274, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3354.697459258082, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554733.8996854, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.3274, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1708, "kv_decode_ms_equiv": 1430.567497142857, "kv_decode_ms_per_token": 11.17630857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 662.9061000003276, "ollama_total_duration_ms": 659.5931, "ollama_load_ms": 224.0777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1430.567497142857, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.47498126138251, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554733.8996854, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.3274, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.1708, "kv_decode_ms_equiv": 1430.567497142857, "kv_decode_ms_per_token": 11.17630857142857, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 662.9061000003276, "ollama_total_duration_ms": 659.5931, "ollama_load_ms": 224.0777, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1441.894897142857, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 115.1262830105941, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554734.5627556, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4062, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.1895, "kv_decode_ms_equiv": 1426.9787428571428, "kv_decode_ms_per_token": 11.148271428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 677.5443000005907, "ollama_total_duration_ms": 663.3501, "ollama_load_ms": 229.4788, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4062, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3331.5214532447267, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554734.5627556, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4062, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.1895, "kv_decode_ms_equiv": 1426.9787428571428, "kv_decode_ms_per_token": 11.148271428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 677.5443000005907, "ollama_total_duration_ms": 663.3501, "ollama_load_ms": 229.4788, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1426.9787428571428, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.70000474128597, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554734.5627556, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4062, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.1895, "kv_decode_ms_equiv": 1426.9787428571428, "kv_decode_ms_per_token": 11.148271428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 677.5443000005907, "ollama_total_duration_ms": 663.3501, "ollama_load_ms": 229.4788, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1438.3849428571427, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 115.40721475453235, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554735.2404287, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.5631, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.2589, "kv_decode_ms_equiv": 1460.1468342857142, "kv_decode_ms_per_token": 11.407397142857143, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 682.8891999994084, "ollama_total_duration_ms": 679.9315, "ollama_load_ms": 242.7485, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.5631, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3024.731157118864, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554735.2404287, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.5631, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.2589, "kv_decode_ms_equiv": 1460.1468342857142, "kv_decode_ms_per_token": 11.407397142857143, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 682.8891999994084, "ollama_total_duration_ms": 679.9315, "ollama_load_ms": 242.7485, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1460.1468342857142, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.66241654224865, "gen_tokens": 128}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554735.2404287, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 12.5631, "prefill_cuda_event_ms": null, "kv_decode_ms": 399.2589, "kv_decode_ms_equiv": 1460.1468342857142, "kv_decode_ms_per_token": 11.407397142857143, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 128, "ollama_wall_ms": 682.8891999994084, "ollama_total_duration_ms": 679.9315, "ollama_load_ms": 242.7485, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1472.7099342857143, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 112.71737640617764, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554735.9234653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.3696000011550495, "prefill_cuda_event_ms": 4.284192085266113, "kv_decode_ms": 245.53089999972144, "kv_decode_cuda_event_ms": 245.48439025878906, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 190.0080000013986, "params_millions_measured": 25.016064, "latency_ms": 4.3696000011550495, "cuda_event_ms": 4.284192085266113, "tokens_total": 17, "tokens_per_s": 3890.516293369247, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554735.9234653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.3696000011550495, "prefill_cuda_event_ms": 4.284192085266113, "kv_decode_ms": 245.53089999972144, "kv_decode_cuda_event_ms": 245.48439025878906, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 190.0080000013986, "params_millions_measured": 25.016064, "latency_ms": 245.53089999972144, "cuda_event_ms": 245.48439025878906, "tokens_total": 128, "tokens_per_s": 521.3193125596217, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554735.9234653, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.3696000011550495, "prefill_cuda_event_ms": 4.284192085266113, "kv_decode_ms": 245.53089999972144, "kv_decode_cuda_event_ms": 245.48439025878906, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 190.0080000013986, "params_millions_measured": 25.016064, "latency_ms": 249.9005000008765, "cuda_event_ms": 249.76858234405518, "tokens_total": 145, "tokens_per_s": 580.2309319088655, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554736.3649218, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.7037999989261152, "prefill_cuda_event_ms": 2.631103992462158, "kv_decode_ms": 240.89399999866146, "kv_decode_cuda_event_ms": 240.82943725585938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 2.7037999989261152, "cuda_event_ms": 2.631103992462158, "tokens_total": 17, "tokens_per_s": 6287.447298894886, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554736.3649218, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.7037999989261152, "prefill_cuda_event_ms": 2.631103992462158, "kv_decode_ms": 240.89399999866146, "kv_decode_cuda_event_ms": 240.82943725585938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 240.89399999866146, "cuda_event_ms": 240.82943725585938, "tokens_total": 128, "tokens_per_s": 531.354039539014, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554736.3649218, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.7037999989261152, "prefill_cuda_event_ms": 2.631103992462158, "kv_decode_ms": 240.89399999866146, "kv_decode_cuda_event_ms": 240.82943725585938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 243.59779999758757, "cuda_event_ms": 243.46054124832153, "tokens_total": 145, "tokens_per_s": 595.2434710060435, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554736.6093087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.4039999989327043, "prefill_cuda_event_ms": 2.3380799293518066, "kv_decode_ms": 238.95189999893773, "kv_decode_cuda_event_ms": 238.9022674560547, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 2.4039999989327043, "cuda_event_ms": 2.3380799293518066, "tokens_total": 17, "tokens_per_s": 7071.547424104589, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554736.6093087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.4039999989327043, "prefill_cuda_event_ms": 2.3380799293518066, "kv_decode_ms": 238.95189999893773, "kv_decode_cuda_event_ms": 238.9022674560547, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 238.95189999893773, "cuda_event_ms": 238.9022674560547, "tokens_total": 128, "tokens_per_s": 535.6726604834238, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554736.6093087, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.4039999989327043, "prefill_cuda_event_ms": 2.3380799293518066, "kv_decode_ms": 238.95189999893773, "kv_decode_cuda_event_ms": 238.9022674560547, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 241.35589999787044, "cuda_event_ms": 241.2403473854065, "tokens_total": 145, "tokens_per_s": 600.7725520746723, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554736.8514023, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.6564999989204807, "prefill_cuda_event_ms": 2.5911359786987305, "kv_decode_ms": 244.62680000033288, "kv_decode_cuda_event_ms": 244.5813751220703, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 2.6564999989204807, "cuda_event_ms": 2.5911359786987305, "tokens_total": 17, "tokens_per_s": 6399.397706346047, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554736.8514023, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.6564999989204807, "prefill_cuda_event_ms": 2.5911359786987305, "kv_decode_ms": 244.62680000033288, "kv_decode_cuda_event_ms": 244.5813751220703, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 244.62680000033288, "cuda_event_ms": 244.5813751220703, "tokens_total": 128, "tokens_per_s": 523.2460221031621, "gen_tokens": 128}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554736.8514023, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 2.6564999989204807, "prefill_cuda_event_ms": 2.5911359786987305, "kv_decode_ms": 244.62680000033288, "kv_decode_cuda_event_ms": 244.5813751220703, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 247.28329999925336, "cuda_event_ms": 247.17251110076904, "tokens_total": 145, "tokens_per_s": 586.3719871112922, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554737.0994835, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 31.38589999980468, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.323000000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 93.94259999862697, "params_millions_measured": 25.016064, "latency_ms": 31.38589999980468, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 286.75296869154647, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554737.0994835, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 31.38589999980468, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.323000000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 93.94259999862697, "params_millions_measured": 25.016064, "latency_ms": 929.323000000295, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 137.7346735203577, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554737.0994835, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 31.38589999980468, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.323000000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 93.94259999862697, "params_millions_measured": 25.016064, "latency_ms": 960.7089000000997, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 142.60302990842052, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554738.155012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.09469999896828, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.4814999993687, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.09469999896828, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 989.5873421906139, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554738.155012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.09469999896828, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.4814999993687, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 920.4814999993687, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 139.0576562376189, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554738.155012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.09469999896828, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.4814999993687, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 929.576199998337, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 147.37898840379637, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554739.085615, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 10.23940000050061, "prefill_cuda_event_ms": null, "kv_decode_ms": 954.9521999997523, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.23940000050061, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 878.9577513877753, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554739.085615, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 10.23940000050061, "prefill_cuda_event_ms": null, "kv_decode_ms": 954.9521999997523, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 954.9521999997523, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 134.03812253642977, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554739.085615, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 10.23940000050061, "prefill_cuda_event_ms": null, "kv_decode_ms": 954.9521999997523, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 965.191600000253, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 141.94072969549683, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554740.051375, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.108399999604444, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.6149999996996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.108399999604444, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 988.098897763696, "gen_tokens": 0}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554740.051375, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.108399999604444, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.6149999996996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 923.6149999996996, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 138.58588264595272, "gen_tokens": 128}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554740.051375, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 9.108399999604444, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.6149999996996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 932.7233999993041, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 146.8817014777395, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554740.9844823, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.369299999685609, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.546399998595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.369299999685609, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2031.2331975958086, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554740.9844823, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.369299999685609, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.546399998595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 803.546399998595, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 159.29385036162668, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554740.9844823, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.369299999685609, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.546399998595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 811.9156999982806, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 178.58996937774089, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554741.7968526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 7.911499998954241, "prefill_cuda_event_ms": null, "kv_decode_ms": 799.5090000003984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 7.911499998954241, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2148.770777001466, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554741.7968526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 7.911499998954241, "prefill_cuda_event_ms": null, "kv_decode_ms": 799.5090000003984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 799.5090000003984, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 160.0982603071838, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554741.7968526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 7.911499998954241, "prefill_cuda_event_ms": null, "kv_decode_ms": 799.5090000003984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 807.4204999993526, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 179.58424389784042, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554742.6048214, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.38839999960328, "prefill_cuda_event_ms": null, "kv_decode_ms": 875.6885999991937, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.38839999960328, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2026.6081732873965, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554742.6048214, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.38839999960328, "prefill_cuda_event_ms": null, "kv_decode_ms": 875.6885999991937, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 875.6885999991937, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 146.17068213531368, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554742.6048214, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.38839999960328, "prefill_cuda_event_ms": null, "kv_decode_ms": 875.6885999991937, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 884.076999998797, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 164.0128631331856, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554743.489415, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.966100000179722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1038.1855000014184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.966100000179722, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1896.0306041265703, "gen_tokens": 0}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554743.489415, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.966100000179722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1038.1855000014184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1038.1855000014184, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 123.29203210777372, "gen_tokens": 128}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554743.489415, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 8.966100000179722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1038.1855000014184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1047.1516000015981, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 138.47087661402486, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554744.5370722, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 323.1766, "prefill_cuda_event_ms": null, "kv_decode_ms": 1521.2092, "kv_decode_ms_equiv": 1521.2092, "kv_decode_ms_per_token": 11.884446875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2196.7353000000003, "ollama_total_duration_ms": 2188.4176, "ollama_load_ms": 244.6498, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 323.1766, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 142.33703801574742, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554744.5370722, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 323.1766, "prefill_cuda_event_ms": null, "kv_decode_ms": 1521.2092, "kv_decode_ms_equiv": 1521.2092, "kv_decode_ms_per_token": 11.884446875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2196.7353000000003, "ollama_total_duration_ms": 2188.4176, "ollama_load_ms": 244.6498, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1521.2092, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 84.14358787732812, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554744.5370722, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 323.1766, "prefill_cuda_event_ms": null, "kv_decode_ms": 1521.2092, "kv_decode_ms_equiv": 1521.2092, "kv_decode_ms_per_token": 11.884446875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2196.7353000000003, "ollama_total_duration_ms": 2188.4176, "ollama_load_ms": 244.6498, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1844.3858, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 94.3403489660352, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554746.7339742, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1464.3699, "kv_decode_ms_equiv": 1464.3699, "kv_decode_ms_per_token": 11.44038984375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1810.7406999988598, "ollama_total_duration_ms": 1806.2784, "ollama_load_ms": 234.2369, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9052, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3863.8578100325904, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554746.7339742, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1464.3699, "kv_decode_ms_equiv": 1464.3699, "kv_decode_ms_per_token": 11.44038984375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1810.7406999988598, "ollama_total_duration_ms": 1806.2784, "ollama_load_ms": 234.2369, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1464.3699, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.4096087334218, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554746.7339742, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1464.3699, "kv_decode_ms_equiv": 1464.3699, "kv_decode_ms_per_token": 11.44038984375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1810.7406999988598, "ollama_total_duration_ms": 1806.2784, "ollama_load_ms": 234.2369, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1476.2750999999998, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 117.8642110809835, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554748.5448012, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.7559, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.813, "kv_decode_ms_equiv": 1462.813, "kv_decode_ms_per_token": 11.4282265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1814.3841000000975, "ollama_total_duration_ms": 1809.6105, "ollama_load_ms": 237.1451, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.7559, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3912.9288272271797, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554748.5448012, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.7559, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.813, "kv_decode_ms_equiv": 1462.813, "kv_decode_ms_per_token": 11.4282265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1814.3841000000975, "ollama_total_duration_ms": 1809.6105, "ollama_load_ms": 237.1451, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1462.813, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.50264046053732, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554748.5448012, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.7559, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.813, "kv_decode_ms_equiv": 1462.813, "kv_decode_ms_per_token": 11.4282265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1814.3841000000975, "ollama_total_duration_ms": 1809.6105, "ollama_load_ms": 237.1451, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1474.5689000000002, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.00058986731646, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554750.3592777, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.6435, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.7665, "kv_decode_ms_equiv": 1462.7665, "kv_decode_ms_per_token": 11.42786328125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1815.2181999994355, "ollama_total_duration_ms": 1808.5786, "ollama_load_ms": 235.347, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.6435, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3638.233084193459, "gen_tokens": 0}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554750.3592777, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.6435, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.7665, "kv_decode_ms_equiv": 1462.7665, "kv_decode_ms_per_token": 11.42786328125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1815.2181999994355, "ollama_total_duration_ms": 1808.5786, "ollama_load_ms": 235.347, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1462.7665, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.505422088898, "gen_tokens": 128}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554750.3592777, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 12.6435, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.7665, "kv_decode_ms_equiv": 1462.7665, "kv_decode_ms_per_token": 11.42786328125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1815.2181999994355, "ollama_total_duration_ms": 1808.5786, "ollama_load_ms": 235.347, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1475.4099999999999, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 117.9333202296311, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554752.174744, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 74.40040000074077, "prefill_cuda_event_ms": null, "kv_decode_ms": 1499.6980000014446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 74.40040000074077, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 120.96709157357206, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554752.174744, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 74.40040000074077, "prefill_cuda_event_ms": null, "kv_decode_ms": 1499.6980000014446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1499.6980000014446, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 85.35051723738826, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554752.174744, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 74.40040000074077, "prefill_cuda_event_ms": null, "kv_decode_ms": 1499.6980000014446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1574.0984000021854, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 87.03394908463778, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554753.7515624, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.32150000052934, "prefill_cuda_event_ms": null, "kv_decode_ms": 1453.4342999995715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 13.32150000052934, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 675.5995946133977, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554753.7515624, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.32150000052934, "prefill_cuda_event_ms": null, "kv_decode_ms": 1453.4342999995715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1453.4342999995715, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.06727624360987, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554753.7515624, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.32150000052934, "prefill_cuda_event_ms": null, "kv_decode_ms": 1453.4342999995715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1466.7558000001009, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 93.40341452884698, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554755.218652, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.267200000380399, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.784700001037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 15.267200000380399, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 589.4990567868211, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554755.218652, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.267200000380399, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.784700001037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1444.784700001037, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.59451515503184, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554755.218652, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.267200000380399, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.784700001037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1460.0519000014174, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 93.83228089348536, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554756.6792235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.530300000842544, "prefill_cuda_event_ms": null, "kv_decode_ms": 1442.38790000054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.530300000842544, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 619.3953324761452, "gen_tokens": 0}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554756.6792235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.530300000842544, "prefill_cuda_event_ms": null, "kv_decode_ms": 1442.38790000054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1442.38790000054, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.74173167977358, "gen_tokens": 128}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554756.6792235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.530300000842544, "prefill_cuda_event_ms": null, "kv_decode_ms": 1442.38790000054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1456.9182000013825, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 94.03410568957818, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554758.1367016, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.380299999378622, "prefill_cuda_event_ms": 15.270751953125, "kv_decode_ms": 591.7974000003596, "kv_decode_cuda_event_ms": 591.7040405273438, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 15.380299999378622, "cuda_event_ms": 15.270751953125, "tokens_total": 17, "tokens_per_s": 1105.3100395107258, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554758.1367016, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.380299999378622, "prefill_cuda_event_ms": 15.270751953125, "kv_decode_ms": 591.7974000003596, "kv_decode_cuda_event_ms": 591.7040405273438, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 591.7974000003596, "cuda_event_ms": 591.7040405273438, "tokens_total": 128, "tokens_per_s": 216.29023716549315, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554758.1367016, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 15.380299999378622, "prefill_cuda_event_ms": 15.270751953125, "kv_decode_ms": 591.7974000003596, "kv_decode_cuda_event_ms": 591.7040405273438, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 607.1776999997383, "cuda_event_ms": 606.9747924804688, "tokens_total": 145, "tokens_per_s": 238.80982453746657, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554758.7479775, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.4570000007079216, "prefill_cuda_event_ms": 3.4145278930664062, "kv_decode_ms": 367.7411000007851, "kv_decode_cuda_event_ms": 367.7132873535156, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 3.4570000007079216, "cuda_event_ms": 3.4145278930664062, "tokens_total": 17, "tokens_per_s": 4917.558575793681, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554758.7479775, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.4570000007079216, "prefill_cuda_event_ms": 3.4145278930664062, "kv_decode_ms": 367.7411000007851, "kv_decode_cuda_event_ms": 367.7132873535156, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 367.7411000007851, "cuda_event_ms": 367.7132873535156, "tokens_total": 128, "tokens_per_s": 348.07096623066263, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554758.7479775, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.4570000007079216, "prefill_cuda_event_ms": 3.4145278930664062, "kv_decode_ms": 367.7411000007851, "kv_decode_cuda_event_ms": 367.7132873535156, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 371.198100001493, "cuda_event_ms": 371.12781524658203, "tokens_total": 145, "tokens_per_s": 390.6269994361954, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554759.1199105, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.9760999989084667, "prefill_cuda_event_ms": 3.9314560890197754, "kv_decode_ms": 356.362700000318, "kv_decode_cuda_event_ms": 356.3378601074219, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 3.9760999989084667, "cuda_event_ms": 3.9314560890197754, "tokens_total": 17, "tokens_per_s": 4275.546390852067, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554759.1199105, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.9760999989084667, "prefill_cuda_event_ms": 3.9314560890197754, "kv_decode_ms": 356.362700000318, "kv_decode_cuda_event_ms": 356.3378601074219, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 356.362700000318, "cuda_event_ms": 356.3378601074219, "tokens_total": 128, "tokens_per_s": 359.1846172449748, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554759.1199105, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.9760999989084667, "prefill_cuda_event_ms": 3.9314560890197754, "kv_decode_ms": 356.362700000318, "kv_decode_cuda_event_ms": 356.3378601074219, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 360.33879999922647, "cuda_event_ms": 360.26931619644165, "tokens_total": 145, "tokens_per_s": 402.3990755375532, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554759.4810555, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.6647999986598734, "prefill_cuda_event_ms": 3.6112959384918213, "kv_decode_ms": 352.211499999612, "kv_decode_cuda_event_ms": 352.1853332519531, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 3.6647999986598734, "cuda_event_ms": 3.6112959384918213, "tokens_total": 17, "tokens_per_s": 4638.725170873303, "gen_tokens": 0}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554759.4810555, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.6647999986598734, "prefill_cuda_event_ms": 3.6112959384918213, "kv_decode_ms": 352.211499999612, "kv_decode_cuda_event_ms": 352.1853332519531, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 352.211499999612, "cuda_event_ms": 352.1853332519531, "tokens_total": 128, "tokens_per_s": 363.4180031036494, "gen_tokens": 128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554759.4810555, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 3.6647999986598734, "prefill_cuda_event_ms": 3.6112959384918213, "kv_decode_ms": 352.211499999612, "kv_decode_cuda_event_ms": 352.1853332519531, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 355.87629999827186, "cuda_event_ms": 355.79662919044495, "tokens_total": 145, "tokens_per_s": 407.4449464623076, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554759.8378503, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.248199999390636, "prefill_cuda_event_ms": 6.190144062042236, "kv_decode_ms": 581.9231999994372, "kv_decode_cuda_event_ms": 581.8685302734375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 6.248199999390636, "cuda_event_ms": 6.190144062042236, "tokens_total": 9, "tokens_per_s": 1440.4148396142468, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554759.8378503, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.248199999390636, "prefill_cuda_event_ms": 6.190144062042236, "kv_decode_ms": 581.9231999994372, "kv_decode_cuda_event_ms": 581.8685302734375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 581.9231999994372, "cuda_event_ms": 581.8685302734375, "tokens_total": 128, "tokens_per_s": 219.9602971665742, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554759.8378503, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.248199999390636, "prefill_cuda_event_ms": 6.190144062042236, "kv_decode_ms": 581.9231999994372, "kv_decode_cuda_event_ms": 581.8685302734375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 588.1713999988278, "cuda_event_ms": 588.0586743354797, "tokens_total": 137, "tokens_per_s": 232.9253003465878, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554760.4287698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.408299999544397, "prefill_cuda_event_ms": 5.352831840515137, "kv_decode_ms": 584.0184000007866, "kv_decode_cuda_event_ms": 583.984130859375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.408299999544397, "cuda_event_ms": 5.352831840515137, "tokens_total": 9, "tokens_per_s": 1664.1088698404624, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554760.4287698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.408299999544397, "prefill_cuda_event_ms": 5.352831840515137, "kv_decode_ms": 584.0184000007866, "kv_decode_cuda_event_ms": 583.984130859375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 584.0184000007866, "cuda_event_ms": 583.984130859375, "tokens_total": 128, "tokens_per_s": 219.171176798244, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554760.4287698, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.408299999544397, "prefill_cuda_event_ms": 5.352831840515137, "kv_decode_ms": 584.0184000007866, "kv_decode_cuda_event_ms": 583.984130859375, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 589.426700000331, "cuda_event_ms": 589.3369626998901, "tokens_total": 137, "tokens_per_s": 232.42924014117966, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554761.0190754, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.394699999669683, "prefill_cuda_event_ms": 5.351103782653809, "kv_decode_ms": 604.0703999988182, "kv_decode_cuda_event_ms": 604.042236328125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.394699999669683, "cuda_event_ms": 5.351103782653809, "tokens_total": 9, "tokens_per_s": 1668.3040763251097, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554761.0190754, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.394699999669683, "prefill_cuda_event_ms": 5.351103782653809, "kv_decode_ms": 604.0703999988182, "kv_decode_cuda_event_ms": 604.042236328125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 604.0703999988182, "cuda_event_ms": 604.042236328125, "tokens_total": 128, "tokens_per_s": 211.89583200939893, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554761.0190754, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.394699999669683, "prefill_cuda_event_ms": 5.351103782653809, "kv_decode_ms": 604.0703999988182, "kv_decode_cuda_event_ms": 604.042236328125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 609.4650999984879, "cuda_event_ms": 609.3933401107788, "tokens_total": 137, "tokens_per_s": 224.787276581284, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554761.6292872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.141900001035538, "prefill_cuda_event_ms": 5.096223831176758, "kv_decode_ms": 588.1485000008979, "kv_decode_cuda_event_ms": 588.1272583007812, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.141900001035538, "cuda_event_ms": 5.096223831176758, "tokens_total": 9, "tokens_per_s": 1750.325754718581, "gen_tokens": 0}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554761.6292872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.141900001035538, "prefill_cuda_event_ms": 5.096223831176758, "kv_decode_ms": 588.1485000008979, "kv_decode_cuda_event_ms": 588.1272583007812, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 588.1485000008979, "cuda_event_ms": 588.1272583007812, "tokens_total": 128, "tokens_per_s": 217.632111617737, "gen_tokens": 128}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554761.6292872, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 5.141900001035538, "prefill_cuda_event_ms": 5.096223831176758, "kv_decode_ms": 588.1485000008979, "kv_decode_cuda_event_ms": 588.1272583007812, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 593.2904000019334, "cuda_event_ms": 593.223482131958, "tokens_total": 137, "tokens_per_s": 230.9155853517157, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554762.2234623, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 468.32850000100734, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.620100000393, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 115.1516999998421, "params_millions_measured": 74.824704, "latency_ms": 468.32850000100734, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 19.217280178295024, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554762.2234623, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 468.32850000100734, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.620100000393, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 115.1516999998421, "params_millions_measured": 74.824704, "latency_ms": 1834.620100000393, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 69.76921271056203, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554762.2234623, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 468.32850000100734, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.620100000393, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 115.1516999998421, "params_millions_measured": 74.824704, "latency_ms": 2302.9486000014003, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 59.48895255409378, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554764.642219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.793700000154786, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.1014000010182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 14.793700000154786, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 608.3670751675263, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554764.642219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.793700000154786, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.1014000010182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1724.1014000010182, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 74.24157303040552, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554764.642219, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 14.793700000154786, "prefill_cuda_event_ms": null, "kv_decode_ms": 1724.1014000010182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1738.895100001173, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 78.78566107863988, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554766.3814852, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.550899999652756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1747.0240999991802, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 15.550899999652756, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 578.7446385868963, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554766.3814852, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.550899999652756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1747.0240999991802, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1747.0240999991802, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.26744948742268, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554766.3814852, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 15.550899999652756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1747.0240999991802, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1762.574999998833, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 77.72718891399839, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554768.1445007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 16.093100000944105, "prefill_cuda_event_ms": null, "kv_decode_ms": 1735.7315999997809, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 16.093100000944105, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 559.2458879564542, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554768.1445007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 16.093100000944105, "prefill_cuda_event_ms": null, "kv_decode_ms": 1735.7315999997809, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1735.7315999997809, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.7441203467265, "gen_tokens": 128}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554768.1445007, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 16.093100000944105, "prefill_cuda_event_ms": null, "kv_decode_ms": 1735.7315999997809, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1751.824700000725, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 78.20417191283083, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554769.8967683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 19.063800000367337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1751.8361999991612, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.063800000367337, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 891.7424647589899, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554769.8967683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 19.063800000367337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1751.8361999991612, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1751.8361999991612, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.06619191911966, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554769.8967683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 19.063800000367337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1751.8361999991612, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1770.8999999995285, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 81.87927042748807, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554771.668195, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.05440000013914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1793.7683000000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.05440000013914, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 807.4321756919054, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554771.668195, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.05440000013914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1793.7683000000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1793.7683000000106, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 71.35815701503881, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554771.668195, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.05440000013914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1793.7683000000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1814.8227000001498, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 79.89761203669539, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554773.4834359, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.29650000097172, "prefill_cuda_event_ms": null, "kv_decode_ms": 1766.949900000327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.29650000097172, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 798.2532340630771, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554773.4834359, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.29650000097172, "prefill_cuda_event_ms": null, "kv_decode_ms": 1766.949900000327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1766.949900000327, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 72.44121635818668, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554773.4834359, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 21.29650000097172, "prefill_cuda_event_ms": null, "kv_decode_ms": 1766.949900000327, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1788.2464000012988, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 81.08502273506308, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554775.272202, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 17.67930000096385, "prefill_cuda_event_ms": null, "kv_decode_ms": 1763.1983999999647, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 17.67930000096385, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 961.5765329551049, "gen_tokens": 0}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554775.272202, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 17.67930000096385, "prefill_cuda_event_ms": null, "kv_decode_ms": 1763.1983999999647, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1763.1983999999647, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 72.59534718271215, "gen_tokens": 128}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554775.272202, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 17.67930000096385, "prefill_cuda_event_ms": null, "kv_decode_ms": 1763.1983999999647, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1780.8777000009286, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 81.42052651898803, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554777.0535593, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.902599999302765, "prefill_cuda_event_ms": null, "kv_decode_ms": 1208.7735999994038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 11.902599999302765, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 756.1373145806131, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554777.0535593, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.902599999302765, "prefill_cuda_event_ms": null, "kv_decode_ms": 1208.7735999994038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1208.7735999994038, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 105.89245165518435, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554777.0535593, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 11.902599999302765, "prefill_cuda_event_ms": null, "kv_decode_ms": 1208.7735999994038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1220.6761999987066, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 112.23287551616487, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554778.27675, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.253500001155771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1242.6507000000129, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.253500001155771, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 679.0659070596563, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554778.27675, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.253500001155771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1242.6507000000129, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1242.6507000000129, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 103.00561533502429, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554778.27675, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.253500001155771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1242.6507000000129, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1255.9042000011686, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 109.08475343889488, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554779.5331423, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 12.821999998777756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1201.8355000000156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.821999998777756, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 701.9185775119261, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554779.5331423, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 12.821999998777756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1201.8355000000156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1201.8355000000156, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 106.50376028998839, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554779.5331423, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 12.821999998777756, "prefill_cuda_event_ms": null, "kv_decode_ms": 1201.8355000000156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1214.6574999987934, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 112.7889960751373, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554780.7484474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.269899998704204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1205.0939000000653, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.269899998704204, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 678.2266634171202, "gen_tokens": 0}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554780.7484474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.269899998704204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1205.0939000000653, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1205.0939000000653, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 106.2157894915849, "gen_tokens": 128}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554780.7484474, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 13.269899998704204, "prefill_cuda_event_ms": null, "kv_decode_ms": 1205.0939000000653, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1218.3637999987695, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 112.4458884941742, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554781.9672134, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.714799999826937, "prefill_cuda_event_ms": 6.6190080642700195, "kv_decode_ms": 328.60340000115684, "kv_decode_cuda_event_ms": 328.4613037109375, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 6.714799999826937, "cuda_event_ms": 6.6190080642700195, "tokens_total": 9, "tokens_per_s": 1340.322868921183, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554781.9672134, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.714799999826937, "prefill_cuda_event_ms": 6.6190080642700195, "kv_decode_ms": 328.60340000115684, "kv_decode_cuda_event_ms": 328.4613037109375, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 328.60340000115684, "cuda_event_ms": 328.4613037109375, "tokens_total": 128, "tokens_per_s": 389.5273146886167, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554781.9672134, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 6.714799999826937, "prefill_cuda_event_ms": 6.6190080642700195, "kv_decode_ms": 328.60340000115684, "kv_decode_cuda_event_ms": 328.4613037109375, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 335.3182000009838, "cuda_event_ms": 335.0803117752075, "tokens_total": 137, "tokens_per_s": 408.567146070801, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554782.3046293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.7319999990140786, "prefill_cuda_event_ms": 2.663935899734497, "kv_decode_ms": 237.11530000036873, "kv_decode_cuda_event_ms": 237.07647705078125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.7319999990140786, "cuda_event_ms": 2.663935899734497, "tokens_total": 9, "tokens_per_s": 3294.289898699821, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554782.3046293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.7319999990140786, "prefill_cuda_event_ms": 2.663935899734497, "kv_decode_ms": 237.11530000036873, "kv_decode_cuda_event_ms": 237.07647705078125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 237.11530000036873, "cuda_event_ms": 237.07647705078125, "tokens_total": 128, "tokens_per_s": 539.8217660345028, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554782.3046293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.7319999990140786, "prefill_cuda_event_ms": 2.663935899734497, "kv_decode_ms": 237.11530000036873, "kv_decode_cuda_event_ms": 237.07647705078125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 239.8472999993828, "cuda_event_ms": 239.74041295051575, "tokens_total": 137, "tokens_per_s": 571.1967572716163, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554782.5452492, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.1943000010651303, "prefill_cuda_event_ms": 2.14083194732666, "kv_decode_ms": 235.72819999935746, "kv_decode_cuda_event_ms": 235.68707275390625, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.1943000010651303, "cuda_event_ms": 2.14083194732666, "tokens_total": 9, "tokens_per_s": 4101.535795302068, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554782.5452492, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.1943000010651303, "prefill_cuda_event_ms": 2.14083194732666, "kv_decode_ms": 235.72819999935746, "kv_decode_cuda_event_ms": 235.68707275390625, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 235.72819999935746, "cuda_event_ms": 235.68707275390625, "tokens_total": 128, "tokens_per_s": 542.9982496805596, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554782.5452492, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.1943000010651303, "prefill_cuda_event_ms": 2.14083194732666, "kv_decode_ms": 235.72819999935746, "kv_decode_cuda_event_ms": 235.68707275390625, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 237.9225000004226, "cuda_event_ms": 237.8279047012329, "tokens_total": 137, "tokens_per_s": 575.8177557807969, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554782.7837684, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.3242000006575836, "prefill_cuda_event_ms": 2.267807960510254, "kv_decode_ms": 234.57650000091235, "kv_decode_cuda_event_ms": 234.53497314453125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.3242000006575836, "cuda_event_ms": 2.267807960510254, "tokens_total": 9, "tokens_per_s": 3872.3001451913083, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554782.7837684, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.3242000006575836, "prefill_cuda_event_ms": 2.267807960510254, "kv_decode_ms": 234.57650000091235, "kv_decode_cuda_event_ms": 234.53497314453125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 234.57650000091235, "cuda_event_ms": 234.53497314453125, "tokens_total": 128, "tokens_per_s": 545.6642076231087, "gen_tokens": 128}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554782.7837684, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 128, "prefill_ms": 2.3242000006575836, "prefill_cuda_event_ms": 2.267807960510254, "kv_decode_ms": 234.57650000091235, "kv_decode_cuda_event_ms": 234.53497314453125, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 236.90070000156993, "cuda_event_ms": 236.8027811050415, "tokens_total": 137, "tokens_per_s": 578.3013726810099, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554783.021326, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.308400000809343, "prefill_cuda_event_ms": 4.253632068634033, "kv_decode_ms": 439.9202000004152, "kv_decode_cuda_event_ms": 439.8636169433594, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.308400000809343, "cuda_event_ms": 4.253632068634033, "tokens_total": 17, "tokens_per_s": 3945.780335346419, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554783.021326, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.308400000809343, "prefill_cuda_event_ms": 4.253632068634033, "kv_decode_ms": 439.9202000004152, "kv_decode_cuda_event_ms": 439.8636169433594, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 439.9202000004152, "cuda_event_ms": 439.8636169433594, "tokens_total": 128, "tokens_per_s": 290.96186080993596, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554783.021326, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.308400000809343, "prefill_cuda_event_ms": 4.253632068634033, "kv_decode_ms": 439.9202000004152, "kv_decode_cuda_event_ms": 439.8636169433594, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 444.22860000122455, "cuda_event_ms": 444.1172490119934, "tokens_total": 145, "tokens_per_s": 326.4085202969829, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554783.4681294, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.324199999246048, "prefill_cuda_event_ms": 4.273439884185791, "kv_decode_ms": 448.2114000002184, "kv_decode_cuda_event_ms": 448.174072265625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.324199999246048, "cuda_event_ms": 4.273439884185791, "tokens_total": 17, "tokens_per_s": 3931.3630273724757, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554783.4681294, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.324199999246048, "prefill_cuda_event_ms": 4.273439884185791, "kv_decode_ms": 448.2114000002184, "kv_decode_cuda_event_ms": 448.174072265625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 448.2114000002184, "cuda_event_ms": 448.174072265625, "tokens_total": 128, "tokens_per_s": 285.5795278744307, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554783.4681294, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.324199999246048, "prefill_cuda_event_ms": 4.273439884185791, "kv_decode_ms": 448.2114000002184, "kv_decode_cuda_event_ms": 448.174072265625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 452.53559999946447, "cuda_event_ms": 452.4475121498108, "tokens_total": 145, "tokens_per_s": 320.4167804702472, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554783.9215837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.171399999904679, "prefill_cuda_event_ms": 4.130720138549805, "kv_decode_ms": 433.85660000058124, "kv_decode_cuda_event_ms": 433.8309020996094, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.171399999904679, "cuda_event_ms": 4.130720138549805, "tokens_total": 17, "tokens_per_s": 4075.370379342299, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554783.9215837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.171399999904679, "prefill_cuda_event_ms": 4.130720138549805, "kv_decode_ms": 433.85660000058124, "kv_decode_cuda_event_ms": 433.8309020996094, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 433.85660000058124, "cuda_event_ms": 433.8309020996094, "tokens_total": 128, "tokens_per_s": 295.0283572955408, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554783.9215837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.171399999904679, "prefill_cuda_event_ms": 4.130720138549805, "kv_decode_ms": 433.85660000058124, "kv_decode_cuda_event_ms": 433.8309020996094, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 438.0280000004859, "cuda_event_ms": 437.9616222381592, "tokens_total": 145, "tokens_per_s": 331.0290666346424, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554784.3604348, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.297899999073707, "prefill_cuda_event_ms": 4.241631984710693, "kv_decode_ms": 425.8902000001399, "kv_decode_cuda_event_ms": 425.8365478515625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.297899999073707, "cuda_event_ms": 4.241631984710693, "tokens_total": 17, "tokens_per_s": 3955.4200897330975, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554784.3604348, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.297899999073707, "prefill_cuda_event_ms": 4.241631984710693, "kv_decode_ms": 425.8902000001399, "kv_decode_cuda_event_ms": 425.8365478515625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 425.8902000001399, "cuda_event_ms": 425.8365478515625, "tokens_total": 128, "tokens_per_s": 300.5469484856847, "gen_tokens": 128}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "batch_size_config": 1, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554784.3604348, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 128, "prefill_ms": 4.297899999073707, "prefill_cuda_event_ms": 4.241631984710693, "kv_decode_ms": 425.8902000001399, "kv_decode_cuda_event_ms": 425.8365478515625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 430.1880999992136, "cuda_event_ms": 430.0781798362732, "tokens_total": 145, "tokens_per_s": 337.06185736022235, "gen_tokens": 128}
