{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547407.7415123, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 285.6904000000213, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.26869999999326, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9256.238899999971, "params_millions_measured": 45.1712, "latency_ms": 285.6904000000213, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 59.504974615873444}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547407.7415123, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 285.6904000000213, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.26869999999326, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9256.238899999971, "params_millions_measured": 45.1712, "latency_ms": 149.26869999999326, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 53.594624995061665}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547407.7415123, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 285.6904000000213, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.26869999999326, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 9256.238899999971, "params_millions_measured": 45.1712, "latency_ms": 434.9591000000146, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 57.47666849595551}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547417.4400315, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 36.587300000064715, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.74179999990156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 36.587300000064715, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 464.642102586688}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547417.4400315, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 36.587300000064715, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.74179999990156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 149.74179999990156, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 53.42529607634781}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547417.4400315, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 36.587300000064715, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.74179999990156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 186.32909999996627, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 134.17120567857907}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547417.6271842, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 38.60789999998815, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.64419999997335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 38.60789999998815, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 440.32438956807334}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547417.6271842, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 38.60789999998815, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.64419999997335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 141.64419999997335, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 56.47954522671246}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547417.6271842, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 38.60789999998815, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.64419999997335, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 180.2520999999615, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 138.69463934126338}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547417.8081307, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.409400000171445, "prefill_cuda_event_ms": null, "kv_decode_ms": 121.9797999997354, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 25.409400000171445, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 669.0437397138577}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547417.8081307, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 25.409400000171445, "prefill_cuda_event_ms": null, "kv_decode_ms": 121.9797999997354, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 121.9797999997354, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 65.58462958635243}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547417.8081307, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 25.409400000171445, "prefill_cuda_event_ms": null, "kv_decode_ms": 121.9797999997354, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 147.38919999990685, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 169.61894087230135}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547417.9563828, "prompt_tokens": 92, "prefill_ms": 2298.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 291.2487, "kv_decode_ms_equiv": 291.2487, "kv_decode_ms_per_token": 36.4060875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 2846.091500000057, "ollama_total_duration_ms": 2831.0452, "ollama_load_ms": 235.2613, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2298.147, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 40.03225207090757}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547417.9563828, "prompt_tokens": 92, "prefill_ms": 2298.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 291.2487, "kv_decode_ms_equiv": 291.2487, "kv_decode_ms_per_token": 36.4060875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2846.091500000057, "ollama_total_duration_ms": 2831.0452, "ollama_load_ms": 235.2613, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 291.2487, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.467933762451132}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547417.9563828, "prompt_tokens": 92, "prefill_ms": 2298.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 291.2487, "kv_decode_ms_equiv": 291.2487, "kv_decode_ms_per_token": 36.4060875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2846.091500000057, "ollama_total_duration_ms": 2831.0452, "ollama_load_ms": 235.2613, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2589.3957, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 38.61904922449667}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547420.802659, "prompt_tokens": 92, "prefill_ms": 51.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.6457, "kv_decode_ms_equiv": 286.6457, "kv_decode_ms_per_token": 35.8307125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 584.9570999998832, "ollama_total_duration_ms": 582.2642, "ollama_load_ms": 240.2778, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 51.2794, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1794.0927545954125}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547420.802659, "prompt_tokens": 92, "prefill_ms": 51.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.6457, "kv_decode_ms_equiv": 286.6457, "kv_decode_ms_per_token": 35.8307125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 584.9570999998832, "ollama_total_duration_ms": 582.2642, "ollama_load_ms": 240.2778, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 286.6457, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.909017996781394}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547420.802659, "prompt_tokens": 92, "prefill_ms": 51.2794, "prefill_cuda_event_ms": null, "kv_decode_ms": 286.6457, "kv_decode_ms_equiv": 286.6457, "kv_decode_ms_per_token": 35.8307125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 584.9570999998832, "ollama_total_duration_ms": 582.2642, "ollama_load_ms": 240.2778, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 337.9251, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 295.9235641270802}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547421.3877218, "prompt_tokens": 92, "prefill_ms": 51.7958, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8837, "kv_decode_ms_equiv": 275.8837, "kv_decode_ms_per_token": 34.4854625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 593.4203000001617, "ollama_total_duration_ms": 579.4708, "ollama_load_ms": 234.1162, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 51.7958, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1776.2057927476744}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547421.3877218, "prompt_tokens": 92, "prefill_ms": 51.7958, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8837, "kv_decode_ms_equiv": 275.8837, "kv_decode_ms_per_token": 34.4854625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 593.4203000001617, "ollama_total_duration_ms": 579.4708, "ollama_load_ms": 234.1162, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 275.8837, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 28.997726215793108}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547421.3877218, "prompt_tokens": 92, "prefill_ms": 51.7958, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8837, "kv_decode_ms_equiv": 275.8837, "kv_decode_ms_per_token": 34.4854625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 593.4203000001617, "ollama_total_duration_ms": 579.4708, "ollama_load_ms": 234.1162, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 327.67949999999996, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 305.1762469119979}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547421.981657, "prompt_tokens": 92, "prefill_ms": 56.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.1419, "kv_decode_ms_equiv": 311.1419, "kv_decode_ms_per_token": 38.8927375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 638.6219000000892, "ollama_total_duration_ms": 613.9975, "ollama_load_ms": 243.5482, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.2286, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1636.1780303973421}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547421.981657, "prompt_tokens": 92, "prefill_ms": 56.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.1419, "kv_decode_ms_equiv": 311.1419, "kv_decode_ms_per_token": 38.8927375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 638.6219000000892, "ollama_total_duration_ms": 613.9975, "ollama_load_ms": 243.5482, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 311.1419, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.711741170186333}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547421.981657, "prompt_tokens": 92, "prefill_ms": 56.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.1419, "kv_decode_ms_equiv": 311.1419, "kv_decode_ms_per_token": 38.8927375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 638.6219000000892, "ollama_total_duration_ms": 613.9975, "ollama_load_ms": 243.5482, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 367.3705, "cuda_event_ms": null, "tokens_total": 100, "tokens_per_s": 272.2047633111532}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547422.620461, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 225.22780000008424, "prefill_cuda_event_ms": null, "kv_decode_ms": 296.02139999997235, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 226.11719999986235, "params_millions_measured": 96.08832, "latency_ms": 225.22780000008424, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 39.959543182487394}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547422.620461, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 225.22780000008424, "prefill_cuda_event_ms": null, "kv_decode_ms": 296.02139999997235, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 226.11719999986235, "params_millions_measured": 96.08832, "latency_ms": 296.02139999997235, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.02507318727885}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547422.620461, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 225.22780000008424, "prefill_cuda_event_ms": null, "kv_decode_ms": 296.02139999997235, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 226.11719999986235, "params_millions_measured": 96.08832, "latency_ms": 521.2492000000566, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 32.613958927895055}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547423.3687265, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 51.16310000039448, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.9326999999539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 51.16310000039448, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 175.9080274637504}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547423.3687265, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 51.16310000039448, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.9326999999539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 349.9326999999539, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 22.861538804464555}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547423.3687265, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 51.16310000039448, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.9326999999539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 401.0958000003484, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 42.383889335129496}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547423.7705579, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 53.635400000075606, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.73080000017217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 53.635400000075606, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 167.79962487437984}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547423.7705579, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 53.635400000075606, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.73080000017217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 432.73080000017217, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 18.48724426363184}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547423.7705579, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 53.635400000075606, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.73080000017217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 486.3662000002478, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 34.953086789319116}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547424.2575603, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 69.14649999998801, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.24129999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 69.14649999998801, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 130.15843173554063}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547424.2575603, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 69.14649999998801, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.24129999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 432.24129999998695, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 18.508180500105478}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547424.2575603, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 69.14649999998801, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.24129999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 501.38779999997496, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 33.90589080947093}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547424.759441, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 245.38780000011684, "prefill_cuda_event_ms": 223.44691467285156, "kv_decode_ms": 77.66979999996693, "kv_decode_cuda_event_ms": 77.63455963134766, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 412.0575999995708, "params_millions_measured": 96.08832, "latency_ms": 245.38780000011684, "cuda_event_ms": 223.44691467285156, "tokens_total": 17, "tokens_per_s": 69.2780977701088}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547424.759441, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 245.38780000011684, "prefill_cuda_event_ms": 223.44691467285156, "kv_decode_ms": 77.66979999996693, "kv_decode_cuda_event_ms": 77.63455963134766, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 412.0575999995708, "params_millions_measured": 96.08832, "latency_ms": 77.66979999996693, "cuda_event_ms": 77.63455963134766, "tokens_total": 8, "tokens_per_s": 103.00013647522469}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547424.759441, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 245.38780000011684, "prefill_cuda_event_ms": 223.44691467285156, "kv_decode_ms": 77.66979999996693, "kv_decode_cuda_event_ms": 77.63455963134766, "gpu_peak_mb": 207.1142578125, "hf_load_ms": 412.0575999995708, "params_millions_measured": 96.08832, "latency_ms": 323.05760000008377, "cuda_event_ms": 301.0814743041992, "tokens_total": 25, "tokens_per_s": 77.38558077566823}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547425.500063, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.892399999946065, "prefill_cuda_event_ms": 7.8008317947387695, "kv_decode_ms": 34.99580000016067, "kv_decode_cuda_event_ms": 34.967552185058594, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 7.892399999946065, "cuda_event_ms": 7.8008317947387695, "tokens_total": 17, "tokens_per_s": 2153.970908737035}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547425.500063, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 7.892399999946065, "prefill_cuda_event_ms": 7.8008317947387695, "kv_decode_ms": 34.99580000016067, "kv_decode_cuda_event_ms": 34.967552185058594, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 34.99580000016067, "cuda_event_ms": 34.967552185058594, "tokens_total": 8, "tokens_per_s": 228.59886043363122}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547425.500063, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 7.892399999946065, "prefill_cuda_event_ms": 7.8008317947387695, "kv_decode_ms": 34.99580000016067, "kv_decode_cuda_event_ms": 34.967552185058594, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 42.888200000106735, "cuda_event_ms": 42.76838397979736, "tokens_total": 25, "tokens_per_s": 582.9109172205358}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547425.5440235, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.317699999977776, "prefill_cuda_event_ms": 5.261055946350098, "kv_decode_ms": 35.27140000005602, "kv_decode_cuda_event_ms": 35.234657287597656, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 5.317699999977776, "cuda_event_ms": 5.261055946350098, "tokens_total": 17, "tokens_per_s": 3196.870827626802}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547425.5440235, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.317699999977776, "prefill_cuda_event_ms": 5.261055946350098, "kv_decode_ms": 35.27140000005602, "kv_decode_cuda_event_ms": 35.234657287597656, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 35.27140000005602, "cuda_event_ms": 35.234657287597656, "tokens_total": 8, "tokens_per_s": 226.81265841410587}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547425.5440235, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.317699999977776, "prefill_cuda_event_ms": 5.261055946350098, "kv_decode_ms": 35.27140000005602, "kv_decode_cuda_event_ms": 35.234657287597656, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 40.589100000033795, "cuda_event_ms": 40.495713233947754, "tokens_total": 25, "tokens_per_s": 615.9289070213231}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547425.585408, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.809200000134297, "prefill_cuda_event_ms": 6.758399963378906, "kv_decode_ms": 37.198999999873195, "kv_decode_cuda_event_ms": 37.1671028137207, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 6.809200000134297, "cuda_event_ms": 6.758399963378906, "tokens_total": 17, "tokens_per_s": 2496.622216951288}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547425.585408, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.809200000134297, "prefill_cuda_event_ms": 6.758399963378906, "kv_decode_ms": 37.198999999873195, "kv_decode_cuda_event_ms": 37.1671028137207, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 37.198999999873195, "cuda_event_ms": 37.1671028137207, "tokens_total": 8, "tokens_per_s": 215.0595446121474}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547425.585408, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.809200000134297, "prefill_cuda_event_ms": 6.758399963378906, "kv_decode_ms": 37.198999999873195, "kv_decode_cuda_event_ms": 37.1671028137207, "gpu_peak_mb": 207.1142578125, "params_millions_measured": 96.08832, "latency_ms": 44.00820000000749, "cuda_event_ms": 43.92550277709961, "tokens_total": 25, "tokens_per_s": 568.0759494820453}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547425.6300097, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.122899999809306, "prefill_cuda_event_ms": 6.067200183868408, "kv_decode_ms": 98.57729999976073, "kv_decode_cuda_event_ms": 98.52928161621094, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 355.58979999996154, "params_millions_measured": 51.475968, "latency_ms": 6.122899999809306, "cuda_event_ms": 6.067200183868408, "tokens_total": 17, "tokens_per_s": 2776.4621340426033}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547425.6300097, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.122899999809306, "prefill_cuda_event_ms": 6.067200183868408, "kv_decode_ms": 98.57729999976073, "kv_decode_cuda_event_ms": 98.52928161621094, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 355.58979999996154, "params_millions_measured": 51.475968, "latency_ms": 98.57729999976073, "cuda_event_ms": 98.52928161621094, "tokens_total": 8, "tokens_per_s": 81.15458629947683}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547425.6300097, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.122899999809306, "prefill_cuda_event_ms": 6.067200183868408, "kv_decode_ms": 98.57729999976073, "kv_decode_cuda_event_ms": 98.52928161621094, "gpu_peak_mb": 315.89599609375, "hf_load_ms": 355.58979999996154, "params_millions_measured": 51.475968, "latency_ms": 104.70019999957003, "cuda_event_ms": 104.59648180007935, "tokens_total": 25, "tokens_per_s": 238.77700329228279}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547426.0913365, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.3978999997307255, "prefill_cuda_event_ms": 4.35916805267334, "kv_decode_ms": 38.68449999981749, "kv_decode_cuda_event_ms": 38.629310607910156, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 4.3978999997307255, "cuda_event_ms": 4.35916805267334, "tokens_total": 17, "tokens_per_s": 3865.481252652601}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547426.0913365, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 4.3978999997307255, "prefill_cuda_event_ms": 4.35916805267334, "kv_decode_ms": 38.68449999981749, "kv_decode_cuda_event_ms": 38.629310607910156, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 38.68449999981749, "cuda_event_ms": 38.629310607910156, "tokens_total": 8, "tokens_per_s": 206.80117359763582}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547426.0913365, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 4.3978999997307255, "prefill_cuda_event_ms": 4.35916805267334, "kv_decode_ms": 38.68449999981749, "kv_decode_cuda_event_ms": 38.629310607910156, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 43.082399999548215, "cuda_event_ms": 42.988478660583496, "tokens_total": 25, "tokens_per_s": 580.2833639783801}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547426.135157, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.514700000087032, "prefill_cuda_event_ms": 13.410143852233887, "kv_decode_ms": 38.10669999984384, "kv_decode_cuda_event_ms": 38.05459213256836, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 13.514700000087032, "cuda_event_ms": 13.410143852233887, "tokens_total": 17, "tokens_per_s": 1257.8895572887689}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547426.135157, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.514700000087032, "prefill_cuda_event_ms": 13.410143852233887, "kv_decode_ms": 38.10669999984384, "kv_decode_cuda_event_ms": 38.05459213256836, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 38.10669999984384, "cuda_event_ms": 38.05459213256836, "tokens_total": 8, "tokens_per_s": 209.93683525555306}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547426.135157, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.514700000087032, "prefill_cuda_event_ms": 13.410143852233887, "kv_decode_ms": 38.10669999984384, "kv_decode_cuda_event_ms": 38.05459213256836, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 51.62139999993087, "cuda_event_ms": 51.464735984802246, "tokens_total": 25, "tokens_per_s": 484.2952728913489}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547426.1875157, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.414399999859597, "prefill_cuda_event_ms": 4.376416206359863, "kv_decode_ms": 31.222799999795825, "kv_decode_cuda_event_ms": 31.176544189453125, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 4.414399999859597, "cuda_event_ms": 4.376416206359863, "tokens_total": 17, "tokens_per_s": 3851.032983087327}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547426.1875157, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 4.414399999859597, "prefill_cuda_event_ms": 4.376416206359863, "kv_decode_ms": 31.222799999795825, "kv_decode_cuda_event_ms": 31.176544189453125, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 31.222799999795825, "cuda_event_ms": 31.176544189453125, "tokens_total": 8, "tokens_per_s": 256.22301651524896}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547426.1875157, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 4.414399999859597, "prefill_cuda_event_ms": 4.376416206359863, "kv_decode_ms": 31.222799999795825, "kv_decode_cuda_event_ms": 31.176544189453125, "gpu_peak_mb": 315.89599609375, "params_millions_measured": 51.475968, "latency_ms": 35.63719999965542, "cuda_event_ms": 35.55296039581299, "tokens_total": 25, "tokens_per_s": 701.5141481441226}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547426.2238262, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 142.3608999998578, "prefill_cuda_event_ms": null, "kv_decode_ms": 294.42199999994045, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.4458000000377, "params_millions_measured": 51.475968, "latency_ms": 142.3608999998578, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 63.219605945234896}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547426.2238262, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 142.3608999998578, "prefill_cuda_event_ms": null, "kv_decode_ms": 294.42199999994045, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.4458000000377, "params_millions_measured": 51.475968, "latency_ms": 294.42199999994045, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 27.17188253595729}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547426.2238262, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 142.3608999998578, "prefill_cuda_event_ms": null, "kv_decode_ms": 294.42199999994045, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 146.4458000000377, "params_millions_measured": 51.475968, "latency_ms": 436.78289999979825, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 38.92093760998394}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547426.807946, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 57.65129999963392, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.0317000000323, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 57.65129999963392, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 156.11096367397005}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547426.807946, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 57.65129999963392, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.0317000000323, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 264.0317000000323, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 30.29939208056844}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547426.807946, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 57.65129999963392, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.0317000000323, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 321.6829999996662, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 52.847057506979354}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547427.1306386, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.22670000026119, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8890000000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 46.22670000026119, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 194.6926776072951}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547427.1306386, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 46.22670000026119, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8890000000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 275.8890000000065, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 28.997169151360914}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547427.1306386, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 46.22670000026119, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.8890000000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 322.1157000002677, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 52.776067729656994}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547427.4534602, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.40379999964716, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.61820000018633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 42.40379999964716, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 212.24512897605612}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547427.4534602, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 42.40379999964716, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.61820000018633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 279.61820000018633, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 28.610440951249487}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547427.4534602, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 42.40379999964716, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.61820000018633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 322.0219999998335, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 52.791424188436785}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547427.7764773, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.72790000023451, "prefill_cuda_event_ms": 34.65727996826172, "kv_decode_ms": 25.166299999909825, "kv_decode_cuda_event_ms": 25.133056640625, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 217.69940000012866, "params_millions_measured": 45.1712, "latency_ms": 34.72790000023451, "cuda_event_ms": 34.65727996826172, "tokens_total": 9, "tokens_per_s": 259.15762254381127}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547427.7764773, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 34.72790000023451, "prefill_cuda_event_ms": 34.65727996826172, "kv_decode_ms": 25.166299999909825, "kv_decode_cuda_event_ms": 25.133056640625, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 217.69940000012866, "params_millions_measured": 45.1712, "latency_ms": 25.166299999909825, "cuda_event_ms": 25.133056640625, "tokens_total": 8, "tokens_per_s": 317.8854261464206}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547427.7764773, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 34.72790000023451, "prefill_cuda_event_ms": 34.65727996826172, "kv_decode_ms": 25.166299999909825, "kv_decode_cuda_event_ms": 25.133056640625, "gpu_peak_mb": 408.0185546875, "hf_load_ms": 217.69940000012866, "params_millions_measured": 45.1712, "latency_ms": 59.89420000014434, "cuda_event_ms": 59.79033660888672, "tokens_total": 17, "tokens_per_s": 283.8338269808935}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547428.0548208, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.138799999964249, "prefill_cuda_event_ms": 4.090879917144775, "kv_decode_ms": 26.69819999982792, "kv_decode_cuda_event_ms": 26.63315200805664, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 4.138799999964249, "cuda_event_ms": 4.090879917144775, "tokens_total": 9, "tokens_per_s": 2174.5433459161454}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547428.0548208, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.138799999964249, "prefill_cuda_event_ms": 4.090879917144775, "kv_decode_ms": 26.69819999982792, "kv_decode_cuda_event_ms": 26.63315200805664, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 26.69819999982792, "cuda_event_ms": 26.63315200805664, "tokens_total": 8, "tokens_per_s": 299.64566899834307}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547428.0548208, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.138799999964249, "prefill_cuda_event_ms": 4.090879917144775, "kv_decode_ms": 26.69819999982792, "kv_decode_cuda_event_ms": 26.63315200805664, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 30.83699999979217, "cuda_event_ms": 30.724031925201416, "tokens_total": 17, "tokens_per_s": 551.2857930445431}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547428.0862443, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.1516999999657855, "prefill_cuda_event_ms": 4.0774078369140625, "kv_decode_ms": 26.528699999744276, "kv_decode_cuda_event_ms": 26.49990463256836, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 4.1516999999657855, "cuda_event_ms": 4.0774078369140625, "tokens_total": 9, "tokens_per_s": 2167.7866898075895}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547428.0862443, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.1516999999657855, "prefill_cuda_event_ms": 4.0774078369140625, "kv_decode_ms": 26.528699999744276, "kv_decode_cuda_event_ms": 26.49990463256836, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 26.528699999744276, "cuda_event_ms": 26.49990463256836, "tokens_total": 8, "tokens_per_s": 301.56019707249567}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547428.0862443, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.1516999999657855, "prefill_cuda_event_ms": 4.0774078369140625, "kv_decode_ms": 26.528699999744276, "kv_decode_cuda_event_ms": 26.49990463256836, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 30.680399999710062, "cuda_event_ms": 30.577312469482422, "tokens_total": 17, "tokens_per_s": 554.0996857981205}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547428.1176062, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.9205999996738683, "prefill_cuda_event_ms": 3.8798398971557617, "kv_decode_ms": 27.217899999868678, "kv_decode_cuda_event_ms": 27.176959991455078, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 3.9205999996738683, "cuda_event_ms": 3.8798398971557617, "tokens_total": 9, "tokens_per_s": 2295.567005241202}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547428.1176062, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.9205999996738683, "prefill_cuda_event_ms": 3.8798398971557617, "kv_decode_ms": 27.217899999868678, "kv_decode_cuda_event_ms": 27.176959991455078, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 27.217899999868678, "cuda_event_ms": 27.176959991455078, "tokens_total": 8, "tokens_per_s": 293.9242189896575}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547428.1176062, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.9205999996738683, "prefill_cuda_event_ms": 3.8798398971557617, "kv_decode_ms": 27.217899999868678, "kv_decode_cuda_event_ms": 27.176959991455078, "gpu_peak_mb": 408.0185546875, "params_millions_measured": 45.1712, "latency_ms": 31.138499999542546, "cuda_event_ms": 31.05679988861084, "tokens_total": 17, "tokens_per_s": 545.947942265997}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547428.1492565, "prompt_tokens": 17, "prefill_ms": 8.0851, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.0313, "kv_decode_ms_equiv": 16.0313, "kv_decode_ms_per_token": 2.0039125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 2686.964799999714, "ollama_total_duration_ms": 2623.7558, "ollama_load_ms": 2562.8315, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.0851, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2102.633238920978}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547428.1492565, "prompt_tokens": 17, "prefill_ms": 8.0851, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.0313, "kv_decode_ms_equiv": 16.0313, "kv_decode_ms_per_token": 2.0039125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2686.964799999714, "ollama_total_duration_ms": 2623.7558, "ollama_load_ms": 2562.8315, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.0313, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 499.02378472113924}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547428.1492565, "prompt_tokens": 17, "prefill_ms": 8.0851, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.0313, "kv_decode_ms_equiv": 16.0313, "kv_decode_ms_per_token": 2.0039125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 2686.964799999714, "ollama_total_duration_ms": 2623.7558, "ollama_load_ms": 2562.8315, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 24.116400000000002, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1036.6389676734502}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547430.8363645, "prompt_tokens": 17, "prefill_ms": 2.445, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.518, "kv_decode_ms_equiv": 13.518, "kv_decode_ms_per_token": 1.68975, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 208.12709999972867, "ollama_total_duration_ms": 179.9955, "ollama_load_ms": 153.6904, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.445, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6952.965235173825}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547430.8363645, "prompt_tokens": 17, "prefill_ms": 2.445, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.518, "kv_decode_ms_equiv": 13.518, "kv_decode_ms_per_token": 1.68975, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 208.12709999972867, "ollama_total_duration_ms": 179.9955, "ollama_load_ms": 153.6904, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.518, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 591.8035212309513}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547430.8363645, "prompt_tokens": 17, "prefill_ms": 2.445, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.518, "kv_decode_ms_equiv": 13.518, "kv_decode_ms_per_token": 1.68975, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 208.12709999972867, "ollama_total_duration_ms": 179.9955, "ollama_load_ms": 153.6904, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 15.963000000000001, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1566.1216563302635}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.044722, "prompt_tokens": 17, "prefill_ms": 2.3297, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.7907, "kv_decode_ms_equiv": 11.7907, "kv_decode_ms_per_token": 1.4738375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 193.40490000013233, "ollama_total_duration_ms": 183.4625, "ollama_load_ms": 158.2612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3297, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7297.076876851096}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.044722, "prompt_tokens": 17, "prefill_ms": 2.3297, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.7907, "kv_decode_ms_equiv": 11.7907, "kv_decode_ms_per_token": 1.4738375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 193.40490000013233, "ollama_total_duration_ms": 183.4625, "ollama_load_ms": 158.2612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 11.7907, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 678.5008523666958}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.044722, "prompt_tokens": 17, "prefill_ms": 2.3297, "prefill_cuda_event_ms": null, "kv_decode_ms": 11.7907, "kv_decode_ms_equiv": 11.7907, "kv_decode_ms_per_token": 1.4738375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 193.40490000013233, "ollama_total_duration_ms": 183.4625, "ollama_load_ms": 158.2612, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.1204, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1770.4880881561428}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.2382486, "prompt_tokens": 17, "prefill_ms": 2.3865, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.373, "kv_decode_ms_equiv": 12.373, "kv_decode_ms_per_token": 1.546625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 190.50430000015695, "ollama_total_duration_ms": 178.204, "ollama_load_ms": 151.3092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3865, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7123.4024722396825}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.2382486, "prompt_tokens": 17, "prefill_ms": 2.3865, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.373, "kv_decode_ms_equiv": 12.373, "kv_decode_ms_per_token": 1.546625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 190.50430000015695, "ollama_total_duration_ms": 178.204, "ollama_load_ms": 151.3092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.373, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 646.5691424876749}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.2382486, "prompt_tokens": 17, "prefill_ms": 2.3865, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.373, "kv_decode_ms_equiv": 12.373, "kv_decode_ms_per_token": 1.546625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 190.50430000015695, "ollama_total_duration_ms": 178.204, "ollama_load_ms": 151.3092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.7595, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1693.8243165418885}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.4289498, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.26720000016212, "prefill_cuda_event_ms": 5.172224044799805, "kv_decode_ms": 31.19349999997212, "kv_decode_cuda_event_ms": 31.12928009033203, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 5.26720000016212, "cuda_event_ms": 5.172224044799805, "tokens_total": 9, "tokens_per_s": 1708.6877277724384}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.4289498, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.26720000016212, "prefill_cuda_event_ms": 5.172224044799805, "kv_decode_ms": 31.19349999997212, "kv_decode_cuda_event_ms": 31.12928009033203, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 31.19349999997212, "cuda_event_ms": 31.12928009033203, "tokens_total": 8, "tokens_per_s": 256.46368634514084}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.4289498, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.26720000016212, "prefill_cuda_event_ms": 5.172224044799805, "kv_decode_ms": 31.19349999997212, "kv_decode_cuda_event_ms": 31.12928009033203, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 36.46070000013424, "cuda_event_ms": 36.301504135131836, "tokens_total": 17, "tokens_per_s": 466.2554476446533}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.466868, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.056100000274455, "prefill_cuda_event_ms": 5.0102081298828125, "kv_decode_ms": 31.583400000272377, "kv_decode_cuda_event_ms": 31.520767211914062, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 5.056100000274455, "cuda_event_ms": 5.0102081298828125, "tokens_total": 9, "tokens_per_s": 1780.0280847909382}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.466868, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.056100000274455, "prefill_cuda_event_ms": 5.0102081298828125, "kv_decode_ms": 31.583400000272377, "kv_decode_cuda_event_ms": 31.520767211914062, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 31.583400000272377, "cuda_event_ms": 31.520767211914062, "tokens_total": 8, "tokens_per_s": 253.29761836695883}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.466868, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.056100000274455, "prefill_cuda_event_ms": 5.0102081298828125, "kv_decode_ms": 31.583400000272377, "kv_decode_cuda_event_ms": 31.520767211914062, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 36.63950000054683, "cuda_event_ms": 36.530975341796875, "tokens_total": 17, "tokens_per_s": 463.98013072630033}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.5042455, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.152399999587942, "prefill_cuda_event_ms": 5.104640007019043, "kv_decode_ms": 39.85560000000987, "kv_decode_cuda_event_ms": 39.7913932800293, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 5.152399999587942, "cuda_event_ms": 5.104640007019043, "tokens_total": 9, "tokens_per_s": 1746.7587921589484}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.5042455, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.152399999587942, "prefill_cuda_event_ms": 5.104640007019043, "kv_decode_ms": 39.85560000000987, "kv_decode_cuda_event_ms": 39.7913932800293, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 39.85560000000987, "cuda_event_ms": 39.7913932800293, "tokens_total": 8, "tokens_per_s": 200.72461586321668}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.5042455, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.152399999587942, "prefill_cuda_event_ms": 5.104640007019043, "kv_decode_ms": 39.85560000000987, "kv_decode_cuda_event_ms": 39.7913932800293, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 45.007999999597814, "cuda_event_ms": 44.89603328704834, "tokens_total": 17, "tokens_per_s": 377.7106292248469}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.5502012, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.783800000197516, "prefill_cuda_event_ms": 7.7209601402282715, "kv_decode_ms": 30.377600000065286, "kv_decode_cuda_event_ms": 30.341888427734375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 7.783800000197516, "cuda_event_ms": 7.7209601402282715, "tokens_total": 9, "tokens_per_s": 1156.2475911215117}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.5502012, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 7.783800000197516, "prefill_cuda_event_ms": 7.7209601402282715, "kv_decode_ms": 30.377600000065286, "kv_decode_cuda_event_ms": 30.341888427734375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 30.377600000065286, "cuda_event_ms": 30.341888427734375, "tokens_total": 8, "tokens_per_s": 263.3519435367773}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.5502012, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 7.783800000197516, "prefill_cuda_event_ms": 7.7209601402282715, "kv_decode_ms": 30.377600000065286, "kv_decode_cuda_event_ms": 30.341888427734375, "gpu_peak_mb": 408.22314453125, "params_millions_measured": 96.08832, "latency_ms": 38.1614000002628, "cuda_event_ms": 38.06284856796265, "tokens_total": 17, "tokens_per_s": 445.47631899990375}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547431.5891917, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 69.87010000011651, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.66220000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.01300000013725, "params_millions_measured": 5.03672, "latency_ms": 69.87010000011651, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 243.3086542021788}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547431.5891917, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 69.87010000011651, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.66220000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.01300000013725, "params_millions_measured": 5.03672, "latency_ms": 136.66220000004614, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 58.53849857529953}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547431.5891917, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 69.87010000011651, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.66220000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.01300000013725, "params_millions_measured": 5.03672, "latency_ms": 206.53230000016265, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 121.04644164607818}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547432.034581, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.38709999994171, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.44020000017554, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 27.38709999994171, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 620.730197795173}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547432.034581, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 27.38709999994171, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.44020000017554, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 135.44020000017554, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 59.066658200369105}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547432.034581, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 27.38709999994171, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.44020000017554, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 162.82730000011725, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 153.53690689449496}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547432.1981957, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.22499999972206, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3686999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 31.22499999972206, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 544.4355484435972}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547432.1981957, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 31.22499999972206, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3686999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 148.3686999999918, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 53.91972835241154}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547432.1981957, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 31.22499999972206, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3686999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 179.59369999971386, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 139.20310122259207}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547432.378446, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 32.3551999999836, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.08480000012423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 32.3551999999836, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 525.4178617350107}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547432.378446, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 32.3551999999836, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.08480000012423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 128.08480000012423, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 62.45862116341861}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547432.378446, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 32.3551999999836, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.08480000012423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 160.44000000010783, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 155.8214908999202}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547432.560087, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.761900000034075, "prefill_cuda_event_ms": 10.572799682617188, "kv_decode_ms": 68.5524999998961, "kv_decode_cuda_event_ms": 68.4362564086914, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 417.9575999996814, "params_millions_measured": 74.824704, "latency_ms": 10.761900000034075, "cuda_event_ms": 10.572799682617188, "tokens_total": 9, "tokens_per_s": 836.2835558750318}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547432.560087, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 10.761900000034075, "prefill_cuda_event_ms": 10.572799682617188, "kv_decode_ms": 68.5524999998961, "kv_decode_cuda_event_ms": 68.4362564086914, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 417.9575999996814, "params_millions_measured": 74.824704, "latency_ms": 68.5524999998961, "cuda_event_ms": 68.4362564086914, "tokens_total": 8, "tokens_per_s": 116.69888042029284}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547432.560087, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 10.761900000034075, "prefill_cuda_event_ms": 10.572799682617188, "kv_decode_ms": 68.5524999998961, "kv_decode_cuda_event_ms": 68.4362564086914, "gpu_peak_mb": 557.04931640625, "hf_load_ms": 417.9575999996814, "params_millions_measured": 74.824704, "latency_ms": 79.31439999993017, "cuda_event_ms": 79.0090560913086, "tokens_total": 17, "tokens_per_s": 214.33686694994813}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547433.0586755, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.464900000035414, "prefill_cuda_event_ms": 8.353792190551758, "kv_decode_ms": 65.74360000013257, "kv_decode_cuda_event_ms": 65.617919921875, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 8.464900000035414, "cuda_event_ms": 8.353792190551758, "tokens_total": 9, "tokens_per_s": 1063.2139777153125}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547433.0586755, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 8.464900000035414, "prefill_cuda_event_ms": 8.353792190551758, "kv_decode_ms": 65.74360000013257, "kv_decode_cuda_event_ms": 65.617919921875, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 65.74360000013257, "cuda_event_ms": 65.617919921875, "tokens_total": 8, "tokens_per_s": 121.68484841085471}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547433.0586755, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 8.464900000035414, "prefill_cuda_event_ms": 8.353792190551758, "kv_decode_ms": 65.74360000013257, "kv_decode_cuda_event_ms": 65.617919921875, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 74.20850000016799, "cuda_event_ms": 73.97171211242676, "tokens_total": 17, "tokens_per_s": 229.08426932172887}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547433.1338975, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.550300000024436, "prefill_cuda_event_ms": 8.482815742492676, "kv_decode_ms": 72.65330000018366, "kv_decode_cuda_event_ms": 72.55856323242188, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 8.550300000024436, "cuda_event_ms": 8.482815742492676, "tokens_total": 9, "tokens_per_s": 1052.5946457988935}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547433.1338975, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 8.550300000024436, "prefill_cuda_event_ms": 8.482815742492676, "kv_decode_ms": 72.65330000018366, "kv_decode_cuda_event_ms": 72.55856323242188, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 72.65330000018366, "cuda_event_ms": 72.55856323242188, "tokens_total": 8, "tokens_per_s": 110.11199766534729}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547433.1338975, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 8.550300000024436, "prefill_cuda_event_ms": 8.482815742492676, "kv_decode_ms": 72.65330000018366, "kv_decode_cuda_event_ms": 72.55856323242188, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 81.2036000002081, "cuda_event_ms": 81.04137897491455, "tokens_total": 17, "tokens_per_s": 209.35032436931903}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547433.216332, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.082699999657052, "prefill_cuda_event_ms": 9.973759651184082, "kv_decode_ms": 71.60029999977269, "kv_decode_cuda_event_ms": 71.50281524658203, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 10.082699999657052, "cuda_event_ms": 9.973759651184082, "tokens_total": 9, "tokens_per_s": 892.6180487673065}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547433.216332, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 10.082699999657052, "prefill_cuda_event_ms": 9.973759651184082, "kv_decode_ms": 71.60029999977269, "kv_decode_cuda_event_ms": 71.50281524658203, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 71.60029999977269, "cuda_event_ms": 71.50281524658203, "tokens_total": 8, "tokens_per_s": 111.73137542755265}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547433.216332, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 10.082699999657052, "prefill_cuda_event_ms": 9.973759651184082, "kv_decode_ms": 71.60029999977269, "kv_decode_cuda_event_ms": 71.50281524658203, "gpu_peak_mb": 557.04931640625, "params_millions_measured": 74.824704, "latency_ms": 81.68299999942974, "cuda_event_ms": 81.47657489776611, "tokens_total": 17, "tokens_per_s": 208.12164097937983}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547433.2993546, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 73.94909999993615, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.7267999999567, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 73.94909999993615, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 229.88785529525956}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547433.2993546, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 73.94909999993615, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.7267999999567, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 355.7267999999567, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 22.489168654149683}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547433.2993546, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 73.94909999993615, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.7267999999567, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 429.67589999989286, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 58.18338892175762}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547433.729988, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 80.78799999975672, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.77860000033616, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 80.78799999975672, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 210.42729118249235}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547433.729988, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 80.78799999975672, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.77860000033616, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 394.77860000033616, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 20.264522950314905}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547433.729988, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 80.78799999975672, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.77860000033616, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 475.5666000000929, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 52.568872582715265}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547434.2061794, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 67.08739999976387, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.6546000003509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 67.08739999976387, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 253.40078763016356}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547434.2061794, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 67.08739999976387, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.6546000003509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 397.6546000003509, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 20.117961668223984}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547434.2061794, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 67.08739999976387, "prefill_cuda_event_ms": null, "kv_decode_ms": 397.6546000003509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 464.7420000001148, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 53.7932874584045}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547434.6717672, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 86.54279999973369, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.0439999997143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 86.54279999973369, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 196.4345965239432}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547434.6717672, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 86.54279999973369, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.0439999997143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 392.0439999997143, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 20.405872810209644}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547434.6717672, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 86.54279999973369, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.0439999997143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 478.586799999448, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 52.23712814483984}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547435.1510506, "prompt_tokens": 99, "prefill_ms": 1032.3171, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.7791, "kv_decode_ms_equiv": 307.7791, "kv_decode_ms_per_token": 38.4723875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 10948.25770000034, "ollama_total_duration_ms": 10831.599, "ollama_load_ms": 9423.5777, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1032.3171, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 95.9007653752902}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547435.1510506, "prompt_tokens": 99, "prefill_ms": 1032.3171, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.7791, "kv_decode_ms_equiv": 307.7791, "kv_decode_ms_per_token": 38.4723875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10948.25770000034, "ollama_total_duration_ms": 10831.599, "ollama_load_ms": 9423.5777, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 307.7791, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.992668118140575}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547435.1510506, "prompt_tokens": 99, "prefill_ms": 1032.3171, "prefill_cuda_event_ms": null, "kv_decode_ms": 307.7791, "kv_decode_ms_equiv": 307.7791, "kv_decode_ms_per_token": 38.4723875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 10948.25770000034, "ollama_total_duration_ms": 10831.599, "ollama_load_ms": 9423.5777, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1340.0962, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 79.84501411167348}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547446.1002395, "prompt_tokens": 99, "prefill_ms": 58.1579, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.9882, "kv_decode_ms_equiv": 322.9882, "kv_decode_ms_per_token": 40.373525, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 656.0249999997723, "ollama_total_duration_ms": 638.0464, "ollama_load_ms": 249.1344, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.1579, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1702.2622893880282}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547446.1002395, "prompt_tokens": 99, "prefill_ms": 58.1579, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.9882, "kv_decode_ms_equiv": 322.9882, "kv_decode_ms_per_token": 40.373525, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 656.0249999997723, "ollama_total_duration_ms": 638.0464, "ollama_load_ms": 249.1344, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 322.9882, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 24.768706720555116}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547446.1002395, "prompt_tokens": 99, "prefill_ms": 58.1579, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.9882, "kv_decode_ms_equiv": 322.9882, "kv_decode_ms_per_token": 40.373525, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 656.0249999997723, "ollama_total_duration_ms": 638.0464, "ollama_load_ms": 249.1344, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 381.1461, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 280.7322441446994}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547446.7564557, "prompt_tokens": 99, "prefill_ms": 62.7071, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.2577, "kv_decode_ms_equiv": 317.2577, "kv_decode_ms_per_token": 39.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 655.5747999996129, "ollama_total_duration_ms": 652.1683, "ollama_load_ms": 252.2792, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 62.7071, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1578.768592392249}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547446.7564557, "prompt_tokens": 99, "prefill_ms": 62.7071, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.2577, "kv_decode_ms_equiv": 317.2577, "kv_decode_ms_per_token": 39.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 655.5747999996129, "ollama_total_duration_ms": 652.1683, "ollama_load_ms": 252.2792, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 317.2577, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.216094045944356}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547446.7564557, "prompt_tokens": 99, "prefill_ms": 62.7071, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.2577, "kv_decode_ms_equiv": 317.2577, "kv_decode_ms_per_token": 39.6572125, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 655.5747999996129, "ollama_total_duration_ms": 652.1683, "ollama_load_ms": 252.2792, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 379.96479999999997, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 281.6050328872569}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547447.4122207, "prompt_tokens": 99, "prefill_ms": 56.6187, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.7324, "kv_decode_ms_equiv": 333.7324, "kv_decode_ms_per_token": 41.71655, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.2758000000977, "ollama_total_duration_ms": 649.0533, "ollama_load_ms": 256.5905, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 56.6187, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1748.5389102893569}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547447.4122207, "prompt_tokens": 99, "prefill_ms": 56.6187, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.7324, "kv_decode_ms_equiv": 333.7324, "kv_decode_ms_per_token": 41.71655, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.2758000000977, "ollama_total_duration_ms": 649.0533, "ollama_load_ms": 256.5905, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 333.7324, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 23.971301557775032}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547447.4122207, "prompt_tokens": 99, "prefill_ms": 56.6187, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.7324, "kv_decode_ms_equiv": 333.7324, "kv_decode_ms_per_token": 41.71655, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 652.2758000000977, "ollama_total_duration_ms": 649.0533, "ollama_load_ms": 256.5905, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 390.3511, "cuda_event_ms": null, "tokens_total": 107, "tokens_per_s": 274.1122030910122}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547448.0647447, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 102.37099999994825, "prefill_cuda_event_ms": null, "kv_decode_ms": 260.2217000003293, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 102.37099999994825, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 166.06265446277357}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547448.0647447, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 102.37099999994825, "prefill_cuda_event_ms": null, "kv_decode_ms": 260.2217000003293, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 260.2217000003293, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 30.74301643556197}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547448.0647447, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 102.37099999994825, "prefill_cuda_event_ms": null, "kv_decode_ms": 260.2217000003293, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 362.59270000027755, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 68.9478856027186}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547448.4290414, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 63.45550000014555, "prefill_cuda_event_ms": null, "kv_decode_ms": 222.33140000025742, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 63.45550000014555, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 267.9042793762717}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547448.4290414, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 63.45550000014555, "prefill_cuda_event_ms": null, "kv_decode_ms": 222.33140000025742, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 222.33140000025742, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 35.98232188521611}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547448.4290414, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 63.45550000014555, "prefill_cuda_event_ms": null, "kv_decode_ms": 222.33140000025742, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 285.78690000040297, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 87.47776752526008}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547448.7153625, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 56.11700000008568, "prefill_cuda_event_ms": null, "kv_decode_ms": 224.60840000030657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 56.11700000008568, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 302.93850348333024}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547448.7153625, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 56.11700000008568, "prefill_cuda_event_ms": null, "kv_decode_ms": 224.60840000030657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 224.60840000030657, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 35.617545915420266}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547448.7153625, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 56.11700000008568, "prefill_cuda_event_ms": null, "kv_decode_ms": 224.60840000030657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 280.72540000039226, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 89.0549982294622}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547448.997312, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 51.01569999987987, "prefill_cuda_event_ms": null, "kv_decode_ms": 231.6544000000249, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 51.01569999987987, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 333.2307505344439}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547448.997312, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 51.01569999987987, "prefill_cuda_event_ms": null, "kv_decode_ms": 231.6544000000249, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 231.6544000000249, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 34.534202674324945}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547448.997312, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 51.01569999987987, "prefill_cuda_event_ms": null, "kv_decode_ms": 231.6544000000249, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 282.6700999999048, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 88.44232198597737}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547449.2805147, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 35.393699999985984, "prefill_cuda_event_ms": 35.2993278503418, "kv_decode_ms": 118.23030000005019, "kv_decode_cuda_event_ms": 118.06924438476562, "gpu_peak_mb": 579.662109375, "hf_load_ms": 290.35479999993186, "params_millions_measured": 5.03672, "latency_ms": 35.393699999985984, "cuda_event_ms": 35.2993278503418, "tokens_total": 17, "tokens_per_s": 480.3114678602896}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547449.2805147, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 35.393699999985984, "prefill_cuda_event_ms": 35.2993278503418, "kv_decode_ms": 118.23030000005019, "kv_decode_cuda_event_ms": 118.06924438476562, "gpu_peak_mb": 579.662109375, "hf_load_ms": 290.35479999993186, "params_millions_measured": 5.03672, "latency_ms": 118.23030000005019, "cuda_event_ms": 118.06924438476562, "tokens_total": 8, "tokens_per_s": 67.66454961204195}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547449.2805147, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 35.393699999985984, "prefill_cuda_event_ms": 35.2993278503418, "kv_decode_ms": 118.23030000005019, "kv_decode_cuda_event_ms": 118.06924438476562, "gpu_peak_mb": 579.662109375, "hf_load_ms": 290.35479999993186, "params_millions_measured": 5.03672, "latency_ms": 153.62400000003618, "cuda_event_ms": 153.36857223510742, "tokens_total": 25, "tokens_per_s": 162.73498932454638}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547449.72661, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 17.222399999809568, "prefill_cuda_event_ms": 17.16009521484375, "kv_decode_ms": 109.28430000012668, "kv_decode_cuda_event_ms": 109.22291564941406, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 17.222399999809568, "cuda_event_ms": 17.16009521484375, "tokens_total": 17, "tokens_per_s": 987.0865849235863}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547449.72661, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 17.222399999809568, "prefill_cuda_event_ms": 17.16009521484375, "kv_decode_ms": 109.28430000012668, "kv_decode_cuda_event_ms": 109.22291564941406, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 109.28430000012668, "cuda_event_ms": 109.22291564941406, "tokens_total": 8, "tokens_per_s": 73.2035617192106}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547449.72661, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 17.222399999809568, "prefill_cuda_event_ms": 17.16009521484375, "kv_decode_ms": 109.28430000012668, "kv_decode_cuda_event_ms": 109.22291564941406, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 126.50669999993625, "cuda_event_ms": 126.38301086425781, "tokens_total": 25, "tokens_per_s": 197.6179917744483}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547449.8539834, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.962199999696168, "prefill_cuda_event_ms": 13.889535903930664, "kv_decode_ms": 98.31669999994119, "kv_decode_cuda_event_ms": 98.2630386352539, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 13.962199999696168, "cuda_event_ms": 13.889535903930664, "tokens_total": 17, "tokens_per_s": 1217.573161849131}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547449.8539834, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.962199999696168, "prefill_cuda_event_ms": 13.889535903930664, "kv_decode_ms": 98.31669999994119, "kv_decode_cuda_event_ms": 98.2630386352539, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 98.31669999994119, "cuda_event_ms": 98.2630386352539, "tokens_total": 8, "tokens_per_s": 81.36969609440497}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547449.8539834, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 13.962199999696168, "prefill_cuda_event_ms": 13.889535903930664, "kv_decode_ms": 98.31669999994119, "kv_decode_cuda_event_ms": 98.2630386352539, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 112.27889999963736, "cuda_event_ms": 112.15257453918457, "tokens_total": 25, "tokens_per_s": 222.65982299506626}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547449.9671178, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.459299999889481, "prefill_cuda_event_ms": 9.41875171661377, "kv_decode_ms": 112.20110000022032, "kv_decode_cuda_event_ms": 112.15257263183594, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 9.459299999889481, "cuda_event_ms": 9.41875171661377, "tokens_total": 17, "tokens_per_s": 1797.1731523684227}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547449.9671178, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.459299999889481, "prefill_cuda_event_ms": 9.41875171661377, "kv_decode_ms": 112.20110000022032, "kv_decode_cuda_event_ms": 112.15257263183594, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 112.20110000022032, "cuda_event_ms": 112.15257263183594, "tokens_total": 8, "tokens_per_s": 71.30054874670829}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547449.9671178, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.459299999889481, "prefill_cuda_event_ms": 9.41875171661377, "kv_decode_ms": 112.20110000022032, "kv_decode_cuda_event_ms": 112.15257263183594, "gpu_peak_mb": 579.662109375, "params_millions_measured": 5.03672, "latency_ms": 121.6604000001098, "cuda_event_ms": 121.5713243484497, "tokens_total": 25, "tokens_per_s": 205.49003619893932}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547450.0893772, "prompt_tokens": 25, "prefill_ms": 9.7802, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.1533, "kv_decode_ms_equiv": 16.1533, "kv_decode_ms_per_token": 2.0191625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 1657.038399999692, "ollama_total_duration_ms": 1615.1714, "ollama_load_ms": 1550.4707, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.7802, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2556.1849450931472}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547450.0893772, "prompt_tokens": 25, "prefill_ms": 9.7802, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.1533, "kv_decode_ms_equiv": 16.1533, "kv_decode_ms_per_token": 2.0191625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1657.038399999692, "ollama_total_duration_ms": 1615.1714, "ollama_load_ms": 1550.4707, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.1533, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 495.2548395683853}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547450.0893772, "prompt_tokens": 25, "prefill_ms": 9.7802, "prefill_cuda_event_ms": null, "kv_decode_ms": 16.1533, "kv_decode_ms_equiv": 16.1533, "kv_decode_ms_per_token": 2.0191625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 1657.038399999692, "ollama_total_duration_ms": 1615.1714, "ollama_load_ms": 1550.4707, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 25.933500000000002, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1272.4853953380762}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547451.7465682, "prompt_tokens": 25, "prefill_ms": 2.6629, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8141, "kv_decode_ms_equiv": 12.8141, "kv_decode_ms_per_token": 1.6017625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 216.30149999964488, "ollama_total_duration_ms": 190.9932, "ollama_load_ms": 163.2524, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6629, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9388.260918547448}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547451.7465682, "prompt_tokens": 25, "prefill_ms": 2.6629, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8141, "kv_decode_ms_equiv": 12.8141, "kv_decode_ms_per_token": 1.6017625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 216.30149999964488, "ollama_total_duration_ms": 190.9932, "ollama_load_ms": 163.2524, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 12.8141, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 624.3122810029577}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547451.7465682, "prompt_tokens": 25, "prefill_ms": 2.6629, "prefill_cuda_event_ms": null, "kv_decode_ms": 12.8141, "kv_decode_ms_equiv": 12.8141, "kv_decode_ms_per_token": 1.6017625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 216.30149999964488, "ollama_total_duration_ms": 190.9932, "ollama_load_ms": 163.2524, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 15.477, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 2132.1961620469083}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547451.9632077, "prompt_tokens": 25, "prefill_ms": 3.1819, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.0194, "kv_decode_ms_equiv": 13.0194, "kv_decode_ms_per_token": 1.627425, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 219.91960000013933, "ollama_total_duration_ms": 193.9965, "ollama_load_ms": 165.9811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.1819, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 7856.940821521732}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547451.9632077, "prompt_tokens": 25, "prefill_ms": 3.1819, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.0194, "kv_decode_ms_equiv": 13.0194, "kv_decode_ms_per_token": 1.627425, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 219.91960000013933, "ollama_total_duration_ms": 193.9965, "ollama_load_ms": 165.9811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 13.0194, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 614.4676405978771}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547451.9632077, "prompt_tokens": 25, "prefill_ms": 3.1819, "prefill_cuda_event_ms": null, "kv_decode_ms": 13.0194, "kv_decode_ms_equiv": 13.0194, "kv_decode_ms_per_token": 1.627425, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 219.91960000013933, "ollama_total_duration_ms": 193.9965, "ollama_load_ms": 165.9811, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.2013, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 2036.8735842185506}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547452.1832683, "prompt_tokens": 25, "prefill_ms": 2.6202, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.4266, "kv_decode_ms_equiv": 14.4266, "kv_decode_ms_per_token": 1.803325, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 205.73789999980363, "ollama_total_duration_ms": 183.3099, "ollama_load_ms": 157.9706, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6202, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9541.256392641782}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547452.1832683, "prompt_tokens": 25, "prefill_ms": 2.6202, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.4266, "kv_decode_ms_equiv": 14.4266, "kv_decode_ms_per_token": 1.803325, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 205.73789999980363, "ollama_total_duration_ms": 183.3099, "ollama_load_ms": 157.9706, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 14.4266, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 554.5312131756616}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547452.1832683, "prompt_tokens": 25, "prefill_ms": 2.6202, "prefill_cuda_event_ms": null, "kv_decode_ms": 14.4266, "kv_decode_ms_equiv": 14.4266, "kv_decode_ms_per_token": 1.803325, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 205.73789999980363, "ollama_total_duration_ms": 183.3099, "ollama_load_ms": 157.9706, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 17.0468, "cuda_event_ms": null, "tokens_total": 33, "tokens_per_s": 1935.8471971279066}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547452.3891506, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.14709999980914, "prefill_cuda_event_ms": 38.76224136352539, "kv_decode_ms": 142.36040000014327, "kv_decode_cuda_event_ms": 142.28054809570312, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 39.14709999980914, "cuda_event_ms": 38.76224136352539, "tokens_total": 9, "tokens_per_s": 229.90208725662637}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547452.3891506, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.14709999980914, "prefill_cuda_event_ms": 38.76224136352539, "kv_decode_ms": 142.36040000014327, "kv_decode_cuda_event_ms": 142.28054809570312, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 142.36040000014327, "cuda_event_ms": 142.28054809570312, "tokens_total": 8, "tokens_per_s": 56.19540265405231}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547452.3891506, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.14709999980914, "prefill_cuda_event_ms": 38.76224136352539, "kv_decode_ms": 142.36040000014327, "kv_decode_cuda_event_ms": 142.28054809570312, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 181.50749999995242, "cuda_event_ms": 181.04278945922852, "tokens_total": 17, "tokens_per_s": 93.66004159610185}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547452.5720084, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.523400000096444, "prefill_cuda_event_ms": 16.440223693847656, "kv_decode_ms": 135.31280000006518, "kv_decode_cuda_event_ms": 135.2294464111328, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 16.523400000096444, "cuda_event_ms": 16.440223693847656, "tokens_total": 9, "tokens_per_s": 544.682087218579}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547452.5720084, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 16.523400000096444, "prefill_cuda_event_ms": 16.440223693847656, "kv_decode_ms": 135.31280000006518, "kv_decode_cuda_event_ms": 135.2294464111328, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 135.31280000006518, "cuda_event_ms": 135.2294464111328, "tokens_total": 8, "tokens_per_s": 59.12227076814718}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547452.5720084, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 16.523400000096444, "prefill_cuda_event_ms": 16.440223693847656, "kv_decode_ms": 135.31280000006518, "kv_decode_cuda_event_ms": 135.2294464111328, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 151.83620000016163, "cuda_event_ms": 151.66967010498047, "tokens_total": 17, "tokens_per_s": 111.9627598687395}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547452.724789, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.61490000030608, "prefill_cuda_event_ms": 19.510271072387695, "kv_decode_ms": 136.45749999977852, "kv_decode_cuda_event_ms": 136.35072326660156, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 19.61490000030608, "cuda_event_ms": 19.510271072387695, "tokens_total": 9, "tokens_per_s": 458.8348653248072}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547452.724789, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 19.61490000030608, "prefill_cuda_event_ms": 19.510271072387695, "kv_decode_ms": 136.45749999977852, "kv_decode_cuda_event_ms": 136.35072326660156, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 136.45749999977852, "cuda_event_ms": 136.35072326660156, "tokens_total": 8, "tokens_per_s": 58.62631222184918}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547452.724789, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 19.61490000030608, "prefill_cuda_event_ms": 19.510271072387695, "kv_decode_ms": 136.45749999977852, "kv_decode_cuda_event_ms": 136.35072326660156, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 156.0724000000846, "cuda_event_ms": 155.86099433898926, "tokens_total": 17, "tokens_per_s": 108.92380715610695}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547452.8819785, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 18.02419999967242, "prefill_cuda_event_ms": 17.911808013916016, "kv_decode_ms": 134.61370000004536, "kv_decode_cuda_event_ms": 134.5443878173828, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 18.02419999967242, "cuda_event_ms": 17.911808013916016, "tokens_total": 9, "tokens_per_s": 499.3286803388539}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547452.8819785, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 18.02419999967242, "prefill_cuda_event_ms": 17.911808013916016, "kv_decode_ms": 134.61370000004536, "kv_decode_cuda_event_ms": 134.5443878173828, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 134.61370000004536, "cuda_event_ms": 134.5443878173828, "tokens_total": 8, "tokens_per_s": 59.42931514398092}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547452.8819785, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 18.02419999967242, "prefill_cuda_event_ms": 17.911808013916016, "kv_decode_ms": 134.61370000004536, "kv_decode_cuda_event_ms": 134.5443878173828, "gpu_peak_mb": 578.50146484375, "params_millions_measured": 5.03672, "latency_ms": 152.63789999971777, "cuda_event_ms": 152.45619583129883, "tokens_total": 17, "tokens_per_s": 111.37469789633789}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547453.0358098, "prompt_tokens": 38, "prefill_ms": 22.2675, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.226, "kv_decode_ms_equiv": 83.226, "kv_decode_ms_per_token": 10.40325, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 6261.3123000001, "ollama_total_duration_ms": 6258.5369, "ollama_load_ms": 6141.0238, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.2675, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1706.5229594700797}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547453.0358098, "prompt_tokens": 38, "prefill_ms": 22.2675, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.226, "kv_decode_ms_equiv": 83.226, "kv_decode_ms_per_token": 10.40325, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 6261.3123000001, "ollama_total_duration_ms": 6258.5369, "ollama_load_ms": 6141.0238, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 83.226, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 96.12380746401365}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547453.0358098, "prompt_tokens": 38, "prefill_ms": 22.2675, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.226, "kv_decode_ms_equiv": 83.226, "kv_decode_ms_per_token": 10.40325, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 6261.3123000001, "ollama_total_duration_ms": 6258.5369, "ollama_load_ms": 6141.0238, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 105.4935, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 436.0458227284145}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547459.2972765, "prompt_tokens": 38, "prefill_ms": 11.5684, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.3071, "kv_decode_ms_equiv": 80.3071, "kv_decode_ms_per_token": 10.0383875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 267.39210000005187, "ollama_total_duration_ms": 241.6142, "ollama_load_ms": 140.1589, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5684, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3284.8103454237403}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547459.2972765, "prompt_tokens": 38, "prefill_ms": 11.5684, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.3071, "kv_decode_ms_equiv": 80.3071, "kv_decode_ms_per_token": 10.0383875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 267.39210000005187, "ollama_total_duration_ms": 241.6142, "ollama_load_ms": 140.1589, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.3071, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.61759296500557}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547459.2972765, "prompt_tokens": 38, "prefill_ms": 11.5684, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.3071, "kv_decode_ms_equiv": 80.3071, "kv_decode_ms_per_token": 10.0383875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 267.39210000005187, "ollama_total_duration_ms": 241.6142, "ollama_load_ms": 140.1589, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.8755, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 500.6775473330757}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547459.5647902, "prompt_tokens": 38, "prefill_ms": 11.9729, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4882, "kv_decode_ms_equiv": 80.4882, "kv_decode_ms_per_token": 10.061025, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 249.58330000026763, "ollama_total_duration_ms": 228.2193, "ollama_load_ms": 127.2254, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9729, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3173.8342423305967}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547459.5647902, "prompt_tokens": 38, "prefill_ms": 11.9729, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4882, "kv_decode_ms_equiv": 80.4882, "kv_decode_ms_per_token": 10.061025, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 249.58330000026763, "ollama_total_duration_ms": 228.2193, "ollama_load_ms": 127.2254, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 80.4882, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 99.39345146245039}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547459.5647902, "prompt_tokens": 38, "prefill_ms": 11.9729, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.4882, "kv_decode_ms_equiv": 80.4882, "kv_decode_ms_per_token": 10.061025, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 249.58330000026763, "ollama_total_duration_ms": 228.2193, "ollama_load_ms": 127.2254, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.4611, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 497.5065189577022}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547459.8146217, "prompt_tokens": 38, "prefill_ms": 12.2569, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9451, "kv_decode_ms_equiv": 79.9451, "kv_decode_ms_per_token": 9.9931375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 278.5644000000502, "ollama_total_duration_ms": 254.3116, "ollama_load_ms": 135.2325, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.2569, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3100.2945279801584}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547459.8146217, "prompt_tokens": 38, "prefill_ms": 12.2569, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9451, "kv_decode_ms_equiv": 79.9451, "kv_decode_ms_per_token": 9.9931375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 278.5644000000502, "ollama_total_duration_ms": 254.3116, "ollama_load_ms": 135.2325, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.9451, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.06867212624665}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547459.8146217, "prompt_tokens": 38, "prefill_ms": 12.2569, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.9451, "kv_decode_ms_equiv": 79.9451, "kv_decode_ms_per_token": 9.9931375, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 278.5644000000502, "ollama_total_duration_ms": 254.3116, "ollama_load_ms": 135.2325, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.202, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 498.90457907637585}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.0933547, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.760000000009313, "prefill_cuda_event_ms": 5.639872074127197, "kv_decode_ms": 41.180400000030204, "kv_decode_cuda_event_ms": 41.10054397583008, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 297.7546000001894, "params_millions_measured": 25.016064, "latency_ms": 5.760000000009313, "cuda_event_ms": 5.639872074127197, "tokens_total": 17, "tokens_per_s": 2951.388888884117}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.0933547, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.760000000009313, "prefill_cuda_event_ms": 5.639872074127197, "kv_decode_ms": 41.180400000030204, "kv_decode_cuda_event_ms": 41.10054397583008, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 297.7546000001894, "params_millions_measured": 25.016064, "latency_ms": 41.180400000030204, "cuda_event_ms": 41.10054397583008, "tokens_total": 8, "tokens_per_s": 194.26717564652438}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.0933547, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.760000000009313, "prefill_cuda_event_ms": 5.639872074127197, "kv_decode_ms": 41.180400000030204, "kv_decode_cuda_event_ms": 41.10054397583008, "gpu_peak_mb": 630.97705078125, "hf_load_ms": 297.7546000001894, "params_millions_measured": 25.016064, "latency_ms": 46.94040000003952, "cuda_event_ms": 46.740416049957275, "tokens_total": 25, "tokens_per_s": 532.5902633973923}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.4393275, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.2864999997837, "prefill_cuda_event_ms": 5.170048236846924, "kv_decode_ms": 41.80259999975533, "kv_decode_cuda_event_ms": 41.692161560058594, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 5.2864999997837, "cuda_event_ms": 5.170048236846924, "tokens_total": 17, "tokens_per_s": 3215.7382012097914}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.4393275, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.2864999997837, "prefill_cuda_event_ms": 5.170048236846924, "kv_decode_ms": 41.80259999975533, "kv_decode_cuda_event_ms": 41.692161560058594, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 41.80259999975533, "cuda_event_ms": 41.692161560058594, "tokens_total": 8, "tokens_per_s": 191.37565606079104}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.4393275, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.2864999997837, "prefill_cuda_event_ms": 5.170048236846924, "kv_decode_ms": 41.80259999975533, "kv_decode_cuda_event_ms": 41.692161560058594, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 47.08909999953903, "cuda_event_ms": 46.86220979690552, "tokens_total": 25, "tokens_per_s": 530.9084267961106}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.4876492, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.057399999714107, "prefill_cuda_event_ms": 4.9541120529174805, "kv_decode_ms": 37.084000000049855, "kv_decode_cuda_event_ms": 37.005313873291016, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 5.057399999714107, "cuda_event_ms": 4.9541120529174805, "tokens_total": 17, "tokens_per_s": 3361.4110018904976}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.4876492, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.057399999714107, "prefill_cuda_event_ms": 4.9541120529174805, "kv_decode_ms": 37.084000000049855, "kv_decode_cuda_event_ms": 37.005313873291016, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 37.084000000049855, "cuda_event_ms": 37.005313873291016, "tokens_total": 8, "tokens_per_s": 215.72645884988796}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.4876492, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.057399999714107, "prefill_cuda_event_ms": 4.9541120529174805, "kv_decode_ms": 37.084000000049855, "kv_decode_cuda_event_ms": 37.005313873291016, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 42.14139999976396, "cuda_event_ms": 41.959425926208496, "tokens_total": 25, "tokens_per_s": 593.240851042918}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.5307531, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.219299999680516, "prefill_cuda_event_ms": 5.128191947937012, "kv_decode_ms": 35.653800000090996, "kv_decode_cuda_event_ms": 35.55497741699219, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 5.219299999680516, "cuda_event_ms": 5.128191947937012, "tokens_total": 17, "tokens_per_s": 3257.1417625046665}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.5307531, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.219299999680516, "prefill_cuda_event_ms": 5.128191947937012, "kv_decode_ms": 35.653800000090996, "kv_decode_cuda_event_ms": 35.55497741699219, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 35.653800000090996, "cuda_event_ms": 35.55497741699219, "tokens_total": 8, "tokens_per_s": 224.3800099843378}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.5307531, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.219299999680516, "prefill_cuda_event_ms": 5.128191947937012, "kv_decode_ms": 35.653800000090996, "kv_decode_cuda_event_ms": 35.55497741699219, "gpu_peak_mb": 630.97705078125, "params_millions_measured": 25.016064, "latency_ms": 40.87309999977151, "cuda_event_ms": 40.6831693649292, "tokens_total": 25, "tokens_per_s": 611.6492265118073}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.5724707, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.06710000016756, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.86460000011357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 181.4684999999372, "params_millions_measured": 25.016064, "latency_ms": 42.06710000016756, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 213.94391341366892}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.5724707, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 42.06710000016756, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.86460000011357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 181.4684999999372, "params_millions_measured": 25.016064, "latency_ms": 125.86460000011357, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 63.560365662726305}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.5724707, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 42.06710000016756, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.86460000011357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 181.4684999999372, "params_millions_measured": 25.016064, "latency_ms": 167.93170000028113, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 101.23163166913419}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547460.9231844, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.500099999888334, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6192000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.500099999888334, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 399.99822223210856}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547460.9231844, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.500099999888334, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6192000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 128.6192000002302, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 62.199111796572225}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547460.9231844, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.500099999888334, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6192000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 151.11930000011853, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 112.49390382291783}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.0748298, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.2806000001583, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.82540000012887, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.2806000001583, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 403.9388526312602}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.0748298, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.2806000001583, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.82540000012887, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 117.82540000012887, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 67.89707482419962}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.0748298, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 22.2806000001583, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.82540000012887, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 140.10600000028717, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 121.33670221093426}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.2155776, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.369900000332564, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.8268000004391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 20.369900000332564, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 441.828384030018}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.2155776, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 20.369900000332564, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.8268000004391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 122.8268000004391, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 65.132365249045}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.2155776, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 20.369900000332564, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.8268000004391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 143.19670000077167, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 118.7178196139184}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.3591766, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.089900000082707, "prefill_cuda_event_ms": null, "kv_decode_ms": 114.13319999974192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.089900000082707, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 769.5824788675526}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.3591766, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 22.089900000082707, "prefill_cuda_event_ms": null, "kv_decode_ms": 114.13319999974192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 114.13319999974192, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 70.09353982906016}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.3591766, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 22.089900000082707, "prefill_cuda_event_ms": null, "kv_decode_ms": 114.13319999974192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 136.22309999982463, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 183.5224715927929}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.4959223, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.74120000017865, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.99169999999503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.74120000017865, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 781.9255606801975}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.4959223, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 21.74120000017865, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.99169999999503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 115.99169999999503, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 68.97045219615147}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.4959223, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 21.74120000017865, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.99169999999503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 137.7329000001737, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 181.51073563374092}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.634217, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.216399999957503, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.16999999962718, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.216399999957503, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 801.2669444408124}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.634217, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 21.216399999957503, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.16999999962718, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 113.16999999962718, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 70.69011222078603}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.634217, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 21.216399999957503, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.16999999962718, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 134.38639999958468, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 186.03072930056362}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.7692697, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.2673999997387, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.89429999997446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 20.2673999997387, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 838.7854386955986}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.7692697, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 20.2673999997387, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.89429999997446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 108.89429999997446, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 73.4657369577827}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.7692697, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 20.2673999997387, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.89429999997446, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 129.16169999971316, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 193.55582963104015}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547461.899154, "prompt_tokens": 46, "prefill_ms": 86.4994, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.4621, "kv_decode_ms_equiv": 122.4621, "kv_decode_ms_per_token": 15.3077625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 363.3064999999078, "ollama_total_duration_ms": 346.1302, "ollama_load_ms": 125.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 86.4994, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 531.795596270032}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547461.899154, "prompt_tokens": 46, "prefill_ms": 86.4994, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.4621, "kv_decode_ms_equiv": 122.4621, "kv_decode_ms_per_token": 15.3077625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 363.3064999999078, "ollama_total_duration_ms": 346.1302, "ollama_load_ms": 125.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 122.4621, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 65.3263336166863}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547461.899154, "prompt_tokens": 46, "prefill_ms": 86.4994, "prefill_cuda_event_ms": null, "kv_decode_ms": 122.4621, "kv_decode_ms_equiv": 122.4621, "kv_decode_ms_per_token": 15.3077625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 363.3064999999078, "ollama_total_duration_ms": 346.1302, "ollama_load_ms": 125.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 208.9615, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 258.42080957496955}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547462.262617, "prompt_tokens": 46, "prefill_ms": 11.6011, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.6117, "kv_decode_ms_equiv": 79.6117, "kv_decode_ms_per_token": 9.9514625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 257.58730000006835, "ollama_total_duration_ms": 230.968, "ollama_load_ms": 127.6219, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6011, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3965.141236606873}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547462.262617, "prompt_tokens": 46, "prefill_ms": 11.6011, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.6117, "kv_decode_ms_equiv": 79.6117, "kv_decode_ms_per_token": 9.9514625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 257.58730000006835, "ollama_total_duration_ms": 230.968, "ollama_load_ms": 127.6219, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 79.6117, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 100.48774237957487}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547462.262617, "prompt_tokens": 46, "prefill_ms": 11.6011, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.6117, "kv_decode_ms_equiv": 79.6117, "kv_decode_ms_per_token": 9.9514625, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 257.58730000006835, "ollama_total_duration_ms": 230.968, "ollama_load_ms": 127.6219, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 91.2128, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 592.022172326691}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547462.5203276, "prompt_tokens": 46, "prefill_ms": 11.8941, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.1624, "kv_decode_ms_equiv": 81.1624, "kv_decode_ms_per_token": 10.1453, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 245.17999999989115, "ollama_total_duration_ms": 227.6332, "ollama_load_ms": 123.969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.8941, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3867.463700490159}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547462.5203276, "prompt_tokens": 46, "prefill_ms": 11.8941, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.1624, "kv_decode_ms_equiv": 81.1624, "kv_decode_ms_per_token": 10.1453, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 245.17999999989115, "ollama_total_duration_ms": 227.6332, "ollama_load_ms": 123.969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 81.1624, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 98.5678097247001}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547462.5203276, "prompt_tokens": 46, "prefill_ms": 11.8941, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.1624, "kv_decode_ms_equiv": 81.1624, "kv_decode_ms_per_token": 10.1453, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 245.17999999989115, "ollama_total_duration_ms": 227.6332, "ollama_load_ms": 123.969, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 93.0565, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 580.2926179256688}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547462.7657878, "prompt_tokens": 46, "prefill_ms": 11.1781, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0135, "kv_decode_ms_equiv": 81.0135, "kv_decode_ms_per_token": 10.1266875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 8, "ollama_wall_ms": 229.3955999998616, "ollama_total_duration_ms": 225.8004, "ollama_load_ms": 124.1818, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.1781, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4115.189522369634}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547462.7657878, "prompt_tokens": 46, "prefill_ms": 11.1781, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0135, "kv_decode_ms_equiv": 81.0135, "kv_decode_ms_per_token": 10.1266875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 229.3955999998616, "ollama_total_duration_ms": 225.8004, "ollama_load_ms": 124.1818, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 81.0135, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 98.74897393644271}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547462.7657878, "prompt_tokens": 46, "prefill_ms": 11.1781, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0135, "kv_decode_ms_equiv": 81.0135, "kv_decode_ms_per_token": 10.1266875, "kv_decode_cuda_event_ms": null, "gen_tokens": 8, "gen_tokens_equiv": 8, "ollama_wall_ms": 229.3955999998616, "ollama_total_duration_ms": 225.8004, "ollama_load_ms": 124.1818, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 92.1916, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 585.7366614745812}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547462.9953425, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.74320000008447, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.15699999993376, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 44.74320000008447, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 201.14788392388138}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547462.9953425, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.74320000008447, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.15699999993376, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 212.15699999993376, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 37.70792384885956}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547462.9953425, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.74320000008447, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.15699999993376, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 256.90020000001823, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 66.17355689095919}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547463.2536654, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.40389999990657, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.8959999999206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 44.40389999990657, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 202.68489929981232}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547463.2536654, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.40389999990657, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.8959999999206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 225.8959999999206, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 35.41452703900384}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547463.2536654, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.40389999990657, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.8959999999206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 270.2998999998272, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 62.89310502893589}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547463.5246592, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.77499999984502, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.24250000014945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 44.77499999984502, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 201.00502512632386}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547463.5246592, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.77499999984502, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.24250000014945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 225.24250000014945, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 35.51727582492066}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547463.5246592, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.77499999984502, "prefill_cuda_event_ms": null, "kv_decode_ms": 225.24250000014945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 270.01749999999447, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 62.958882294667376}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547463.7951994, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.2056999999113, "prefill_cuda_event_ms": null, "kv_decode_ms": 208.03860000023633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 39.2056999999113, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 229.55845706160994}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547463.7951994, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.2056999999113, "prefill_cuda_event_ms": null, "kv_decode_ms": 208.03860000023633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 208.03860000023633, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 38.454402211853534}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547463.7951994, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 39.2056999999113, "prefill_cuda_event_ms": null, "kv_decode_ms": 208.03860000023633, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 247.24430000014763, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 68.75790463112739}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.0432696, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.93430000016815, "prefill_cuda_event_ms": 9.820159912109375, "kv_decode_ms": 65.99540000024717, "kv_decode_cuda_event_ms": 65.91657257080078, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 9.93430000016815, "cuda_event_ms": 9.820159912109375, "tokens_total": 17, "tokens_per_s": 1711.2428655982055}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.0432696, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.93430000016815, "prefill_cuda_event_ms": 9.820159912109375, "kv_decode_ms": 65.99540000024717, "kv_decode_cuda_event_ms": 65.91657257080078, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 65.99540000024717, "cuda_event_ms": 65.91657257080078, "tokens_total": 8, "tokens_per_s": 121.22056991805546}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.0432696, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 9.93430000016815, "prefill_cuda_event_ms": 9.820159912109375, "kv_decode_ms": 65.99540000024717, "kv_decode_cuda_event_ms": 65.91657257080078, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 75.92970000041532, "cuda_event_ms": 75.73673248291016, "tokens_total": 25, "tokens_per_s": 329.2519264512207}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.1206968, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.736900000149035, "prefill_cuda_event_ms": 3.6956160068511963, "kv_decode_ms": 20.394499999838445, "kv_decode_cuda_event_ms": 20.354047775268555, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 3.736900000149035, "cuda_event_ms": 3.6956160068511963, "tokens_total": 17, "tokens_per_s": 4549.225293511201}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.1206968, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.736900000149035, "prefill_cuda_event_ms": 3.6956160068511963, "kv_decode_ms": 20.394499999838445, "kv_decode_cuda_event_ms": 20.354047775268555, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 20.394499999838445, "cuda_event_ms": 20.354047775268555, "tokens_total": 8, "tokens_per_s": 392.26261982707945}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.1206968, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.736900000149035, "prefill_cuda_event_ms": 3.6956160068511963, "kv_decode_ms": 20.394499999838445, "kv_decode_cuda_event_ms": 20.354047775268555, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 24.13139999998748, "cuda_event_ms": 24.04966378211975, "tokens_total": 25, "tokens_per_s": 1035.9945962527236}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.1457546, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.359700000146404, "prefill_cuda_event_ms": 3.3024001121520996, "kv_decode_ms": 20.949199999904522, "kv_decode_cuda_event_ms": 20.87321662902832, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 3.359700000146404, "cuda_event_ms": 3.3024001121520996, "tokens_total": 17, "tokens_per_s": 5059.975592838408}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.1457546, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.359700000146404, "prefill_cuda_event_ms": 3.3024001121520996, "kv_decode_ms": 20.949199999904522, "kv_decode_cuda_event_ms": 20.87321662902832, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 20.949199999904522, "cuda_event_ms": 20.87321662902832, "tokens_total": 8, "tokens_per_s": 381.87615756384304}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.1457546, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.359700000146404, "prefill_cuda_event_ms": 3.3024001121520996, "kv_decode_ms": 20.949199999904522, "kv_decode_cuda_event_ms": 20.87321662902832, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 24.308900000050926, "cuda_event_ms": 24.17561674118042, "tokens_total": 25, "tokens_per_s": 1028.4299166127478}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.1708865, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.704100000049948, "prefill_cuda_event_ms": 3.647520065307617, "kv_decode_ms": 19.144699999742443, "kv_decode_cuda_event_ms": 19.106815338134766, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 3.704100000049948, "cuda_event_ms": 3.647520065307617, "tokens_total": 17, "tokens_per_s": 4589.5089224834}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.1708865, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.704100000049948, "prefill_cuda_event_ms": 3.647520065307617, "kv_decode_ms": 19.144699999742443, "kv_decode_cuda_event_ms": 19.106815338134766, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 19.144699999742443, "cuda_event_ms": 19.106815338134766, "tokens_total": 8, "tokens_per_s": 417.87021996205874}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.1708865, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 3.704100000049948, "prefill_cuda_event_ms": 3.647520065307617, "kv_decode_ms": 19.144699999742443, "kv_decode_cuda_event_ms": 19.106815338134766, "gpu_peak_mb": 631.23828125, "params_millions_measured": 74.824704, "latency_ms": 22.84879999979239, "cuda_event_ms": 22.754335403442383, "tokens_total": 25, "tokens_per_s": 1094.1493645279909}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.1946461, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.163599999832513, "prefill_cuda_event_ms": 5.107647895812988, "kv_decode_ms": 31.029700000090088, "kv_decode_cuda_event_ms": 30.9616641998291, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 5.163599999832513, "cuda_event_ms": 5.107647895812988, "tokens_total": 9, "tokens_per_s": 1742.9700209721752}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.1946461, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.163599999832513, "prefill_cuda_event_ms": 5.107647895812988, "kv_decode_ms": 31.029700000090088, "kv_decode_cuda_event_ms": 30.9616641998291, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 31.029700000090088, "cuda_event_ms": 30.9616641998291, "tokens_total": 8, "tokens_per_s": 257.8175103200087}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.1946461, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 5.163599999832513, "prefill_cuda_event_ms": 5.107647895812988, "kv_decode_ms": 31.029700000090088, "kv_decode_cuda_event_ms": 30.9616641998291, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 36.1932999999226, "cuda_event_ms": 36.06931209564209, "tokens_total": 17, "tokens_per_s": 469.7001931306721}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.2319174, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.913700000088284, "prefill_cuda_event_ms": 4.827136039733887, "kv_decode_ms": 32.39519999988261, "kv_decode_cuda_event_ms": 32.323585510253906, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 4.913700000088284, "cuda_event_ms": 4.827136039733887, "tokens_total": 9, "tokens_per_s": 1831.6136515941753}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.2319174, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.913700000088284, "prefill_cuda_event_ms": 4.827136039733887, "kv_decode_ms": 32.39519999988261, "kv_decode_cuda_event_ms": 32.323585510253906, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 32.39519999988261, "cuda_event_ms": 32.323585510253906, "tokens_total": 8, "tokens_per_s": 246.95016545750576}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.2319174, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.913700000088284, "prefill_cuda_event_ms": 4.827136039733887, "kv_decode_ms": 32.39519999988261, "kv_decode_cuda_event_ms": 32.323585510253906, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 37.30889999997089, "cuda_event_ms": 37.15072154998779, "tokens_total": 17, "tokens_per_s": 455.65535301263947}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.2699854, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.596200000378303, "prefill_cuda_event_ms": 4.523007869720459, "kv_decode_ms": 29.989600000135397, "kv_decode_cuda_event_ms": 29.931392669677734, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 4.596200000378303, "cuda_event_ms": 4.523007869720459, "tokens_total": 9, "tokens_per_s": 1958.1393323308885}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.2699854, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.596200000378303, "prefill_cuda_event_ms": 4.523007869720459, "kv_decode_ms": 29.989600000135397, "kv_decode_cuda_event_ms": 29.931392669677734, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 29.989600000135397, "cuda_event_ms": 29.931392669677734, "tokens_total": 8, "tokens_per_s": 266.7591431684278}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.2699854, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.596200000378303, "prefill_cuda_event_ms": 4.523007869720459, "kv_decode_ms": 29.989600000135397, "kv_decode_cuda_event_ms": 29.931392669677734, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 34.5858000005137, "cuda_event_ms": 34.45440053939819, "tokens_total": 17, "tokens_per_s": 491.5312064415888}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.3052487, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.417699999976321, "prefill_cuda_event_ms": 4.374527931213379, "kv_decode_ms": 31.411599999955797, "kv_decode_cuda_event_ms": 31.351743698120117, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 4.417699999976321, "cuda_event_ms": 4.374527931213379, "tokens_total": 9, "tokens_per_s": 2037.259207290726}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.3052487, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.417699999976321, "prefill_cuda_event_ms": 4.374527931213379, "kv_decode_ms": 31.411599999955797, "kv_decode_cuda_event_ms": 31.351743698120117, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 31.411599999955797, "cuda_event_ms": 31.351743698120117, "tokens_total": 8, "tokens_per_s": 254.68298335682545}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.3052487, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 4.417699999976321, "prefill_cuda_event_ms": 4.374527931213379, "kv_decode_ms": 31.411599999955797, "kv_decode_cuda_event_ms": 31.351743698120117, "gpu_peak_mb": 630.365234375, "params_millions_measured": 51.475968, "latency_ms": 35.82929999993212, "cuda_event_ms": 35.726271629333496, "tokens_total": 17, "tokens_per_s": 474.47201033880674}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547464.3416893, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 550.56119999972, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.94359999987864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.4192999998595, "params_millions_measured": 74.824704, "latency_ms": 550.56119999972, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 16.346956523642746}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547464.3416893, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 550.56119999972, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.94359999987864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.4192999998595, "params_millions_measured": 74.824704, "latency_ms": 314.94359999987864, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.401373452272352}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547464.3416893, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 550.56119999972, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.94359999987864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.4192999998595, "params_millions_measured": 74.824704, "latency_ms": 865.5047999995986, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 19.64171660285175}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547465.3873866, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 41.83939999984432, "prefill_cuda_event_ms": null, "kv_decode_ms": 303.2327000000805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 41.83939999984432, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 215.10824725099997}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547465.3873866, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 41.83939999984432, "prefill_cuda_event_ms": null, "kv_decode_ms": 303.2327000000805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 303.2327000000805, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 26.382378945271654}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547465.3873866, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 41.83939999984432, "prefill_cuda_event_ms": null, "kv_decode_ms": 303.2327000000805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 345.07209999992483, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 49.265066633911296}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547465.7330363, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.732799999972485, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.385099999934, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 46.732799999972485, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 192.58422350052425}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547465.7330363, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 46.732799999972485, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.385099999934, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 316.385099999934, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.285640821902383}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547465.7330363, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 46.732799999972485, "prefill_cuda_event_ms": null, "kv_decode_ms": 316.385099999934, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 363.1178999999065, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 46.816750151959944}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547466.096712, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.35639999974228, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.5245000000614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 44.35639999974228, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 202.90194876167345}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547466.096712, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.35639999974228, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.5245000000614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 314.5245000000614, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.435220467716945}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547466.096712, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 44.35639999974228, "prefill_cuda_event_ms": null, "kv_decode_ms": 314.5245000000614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 358.88089999980366, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 47.36947550011522}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547466.4564016, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 60.560399999758374, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.7839000002132, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 60.560399999758374, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 280.71148803620565}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547466.4564016, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 60.560399999758374, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.7839000002132, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 327.7839000002132, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 24.406323800512464}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547466.4564016, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 60.560399999758374, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.7839000002132, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 388.34429999997155, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 64.37586440692404}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547466.8455303, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 67.76200000012977, "prefill_cuda_event_ms": null, "kv_decode_ms": 337.5186000002941, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 67.76200000012977, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 250.87807325591692}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547466.8455303, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 67.76200000012977, "prefill_cuda_event_ms": null, "kv_decode_ms": 337.5186000002941, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 337.5186000002941, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 23.70239743822423}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547466.8455303, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 67.76200000012977, "prefill_cuda_event_ms": null, "kv_decode_ms": 337.5186000002941, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 405.2806000004239, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 61.68565680166742}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547467.251918, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.67480000025171, "prefill_cuda_event_ms": null, "kv_decode_ms": 332.83769999979995, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 50.67480000025171, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 335.4724636291719}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547467.251918, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 50.67480000025171, "prefill_cuda_event_ms": null, "kv_decode_ms": 332.83769999979995, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 332.83769999979995, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 24.03573873994685}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547467.251918, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 50.67480000025171, "prefill_cuda_event_ms": null, "kv_decode_ms": 332.83769999979995, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 383.51250000005166, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 65.18692350313648}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547467.6360328, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 64.48409999984506, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.5380000001933, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 64.48409999984506, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 263.6308795507861}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547467.6360328, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 64.48409999984506, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.5380000001933, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 319.5380000001933, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 25.036145935679517}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547467.6360328, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 64.48409999984506, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.5380000001933, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 384.02210000003834, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 65.10042000186318}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.0214577, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 61.67069999992236, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.58259999967777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 61.67069999992236, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 145.93640091666433}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.0214577, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 61.67069999992236, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.58259999967777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 212.58259999967777, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 37.6324308763376}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.0214577, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 61.67069999992236, "prefill_cuda_event_ms": null, "kv_decode_ms": 212.58259999967777, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 274.25329999960013, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 61.98649204959352}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.2964613, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.98610000037661, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.84059999999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 36.98610000037661, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 243.33465815288332}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.2964613, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 36.98610000037661, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.84059999999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 142.84059999999954, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 56.00648555102699}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.2964613, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 36.98610000037661, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.84059999999954, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 179.82670000037615, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 94.53546108539189}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.477028, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.932999999706226, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4664000002449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 33.932999999706226, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 265.2285385930486}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.477028, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 33.932999999706226, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4664000002449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 136.4664000002449, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 58.62248875903258}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.477028, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 33.932999999706226, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.4664000002449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 170.39939999995113, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 99.76560950334846}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.6480355, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 30.526000000008935, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.9846999997717, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 30.526000000008935, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 294.83063617890866}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.6480355, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 30.526000000008935, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.9846999997717, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 130.9846999997717, "cuda_event_ms": null, "tokens_total": 8, "tokens_per_s": 61.07583557479571}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.6480355, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 30.526000000008935, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.9846999997717, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 161.51069999978063, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 105.25618426533407}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.810108, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.750100000161183, "prefill_cuda_event_ms": 3.668992042541504, "kv_decode_ms": 13.582900000074005, "kv_decode_cuda_event_ms": 13.530112266540527, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 3.750100000161183, "cuda_event_ms": 3.668992042541504, "tokens_total": 9, "tokens_per_s": 2399.93600160347}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.810108, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.750100000161183, "prefill_cuda_event_ms": 3.668992042541504, "kv_decode_ms": 13.582900000074005, "kv_decode_cuda_event_ms": 13.530112266540527, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 13.582900000074005, "cuda_event_ms": 13.530112266540527, "tokens_total": 8, "tokens_per_s": 588.9758446249632}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.810108, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.750100000161183, "prefill_cuda_event_ms": 3.668992042541504, "kv_decode_ms": 13.582900000074005, "kv_decode_cuda_event_ms": 13.530112266540527, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 17.333000000235188, "cuda_event_ms": 17.19910430908203, "tokens_total": 17, "tokens_per_s": 980.7880920653857}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.8287148, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.1567000000904955, "prefill_cuda_event_ms": 3.0996479988098145, "kv_decode_ms": 16.766899999765883, "kv_decode_cuda_event_ms": 16.732160568237305, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 3.1567000000904955, "cuda_event_ms": 3.0996479988098145, "tokens_total": 9, "tokens_per_s": 2851.078658010578}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.8287148, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.1567000000904955, "prefill_cuda_event_ms": 3.0996479988098145, "kv_decode_ms": 16.766899999765883, "kv_decode_cuda_event_ms": 16.732160568237305, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 16.766899999765883, "cuda_event_ms": 16.732160568237305, "tokens_total": 8, "tokens_per_s": 477.1305369574402}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.8287148, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 3.1567000000904955, "prefill_cuda_event_ms": 3.0996479988098145, "kv_decode_ms": 16.766899999765883, "kv_decode_cuda_event_ms": 16.732160568237305, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 19.92359999985638, "cuda_event_ms": 19.83180856704712, "tokens_total": 17, "tokens_per_s": 853.259451109365}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.8498769, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.653600000030565, "prefill_cuda_event_ms": 2.6112000942230225, "kv_decode_ms": 16.97150000018155, "kv_decode_cuda_event_ms": 16.930816650390625, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 2.653600000030565, "cuda_event_ms": 2.6112000942230225, "tokens_total": 9, "tokens_per_s": 3391.61893273151}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.8498769, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.653600000030565, "prefill_cuda_event_ms": 2.6112000942230225, "kv_decode_ms": 16.97150000018155, "kv_decode_cuda_event_ms": 16.930816650390625, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 16.97150000018155, "cuda_event_ms": 16.930816650390625, "tokens_total": 8, "tokens_per_s": 471.3784874592359}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.8498769, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.653600000030565, "prefill_cuda_event_ms": 2.6112000942230225, "kv_decode_ms": 16.97150000018155, "kv_decode_cuda_event_ms": 16.930816650390625, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 19.625100000212115, "cuda_event_ms": 19.542016744613647, "tokens_total": 17, "tokens_per_s": 866.2376242575201}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.870764, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.86099999993894, "prefill_cuda_event_ms": 2.810879945755005, "kv_decode_ms": 17.207900000357768, "kv_decode_cuda_event_ms": 17.153024673461914, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 2.86099999993894, "cuda_event_ms": 2.810879945755005, "tokens_total": 9, "tokens_per_s": 3145.7532332024043}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.870764, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.86099999993894, "prefill_cuda_event_ms": 2.810879945755005, "kv_decode_ms": 17.207900000357768, "kv_decode_cuda_event_ms": 17.153024673461914, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 17.207900000357768, "cuda_event_ms": 17.153024673461914, "tokens_total": 8, "tokens_per_s": 464.9027481467043}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 8, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.870764, "prompt_tokens": 9, "gen_tokens": 8, "prefill_ms": 2.86099999993894, "prefill_cuda_event_ms": 2.810879945755005, "kv_decode_ms": 17.207900000357768, "kv_decode_cuda_event_ms": 17.153024673461914, "gpu_peak_mb": 630.1748046875, "params_millions_measured": 25.016064, "latency_ms": 20.068900000296708, "cuda_event_ms": 19.96390461921692, "tokens_total": 17, "tokens_per_s": 847.0818031754936}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.8921845, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.669800000054238, "prefill_cuda_event_ms": 6.614016056060791, "kv_decode_ms": 35.73679999999513, "kv_decode_cuda_event_ms": 35.67308807373047, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 6.669800000054238, "cuda_event_ms": 6.614016056060791, "tokens_total": 17, "tokens_per_s": 2548.8020630096494}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.8921845, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.669800000054238, "prefill_cuda_event_ms": 6.614016056060791, "kv_decode_ms": 35.73679999999513, "kv_decode_cuda_event_ms": 35.67308807373047, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 35.73679999999513, "cuda_event_ms": 35.67308807373047, "tokens_total": 8, "tokens_per_s": 223.85887936248042}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.8921845, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.669800000054238, "prefill_cuda_event_ms": 6.614016056060791, "kv_decode_ms": 35.73679999999513, "kv_decode_cuda_event_ms": 35.67308807373047, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 42.406600000049366, "cuda_event_ms": 42.28710412979126, "tokens_total": 25, "tokens_per_s": 589.5308749102945}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.936467, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.638099999941915, "prefill_cuda_event_ms": 5.5808000564575195, "kv_decode_ms": 35.81700000040655, "kv_decode_cuda_event_ms": 35.75193786621094, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 5.638099999941915, "cuda_event_ms": 5.5808000564575195, "tokens_total": 17, "tokens_per_s": 3015.200156112013}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.936467, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.638099999941915, "prefill_cuda_event_ms": 5.5808000564575195, "kv_decode_ms": 35.81700000040655, "kv_decode_cuda_event_ms": 35.75193786621094, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 35.81700000040655, "cuda_event_ms": 35.75193786621094, "tokens_total": 8, "tokens_per_s": 223.35762347235095}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.936467, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.638099999941915, "prefill_cuda_event_ms": 5.5808000564575195, "kv_decode_ms": 35.81700000040655, "kv_decode_cuda_event_ms": 35.75193786621094, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 41.455100000348466, "cuda_event_ms": 41.33273792266846, "tokens_total": 25, "tokens_per_s": 603.062108155326}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547468.9797864, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.5776000003788795, "prefill_cuda_event_ms": 5.519360065460205, "kv_decode_ms": 35.7970000000023, "kv_decode_cuda_event_ms": 35.757057189941406, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 5.5776000003788795, "cuda_event_ms": 5.519360065460205, "tokens_total": 17, "tokens_per_s": 3047.905909144652}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547468.9797864, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.5776000003788795, "prefill_cuda_event_ms": 5.519360065460205, "kv_decode_ms": 35.7970000000023, "kv_decode_cuda_event_ms": 35.757057189941406, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 35.7970000000023, "cuda_event_ms": 35.757057189941406, "tokens_total": 8, "tokens_per_s": 223.48241472747677}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547468.9797864, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 5.5776000003788795, "prefill_cuda_event_ms": 5.519360065460205, "kv_decode_ms": 35.7970000000023, "kv_decode_cuda_event_ms": 35.757057189941406, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 41.37460000038118, "cuda_event_ms": 41.27641725540161, "tokens_total": 25, "tokens_per_s": 604.2354487963552}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547469.0223792, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.182199999784643, "prefill_cuda_event_ms": 6.129663944244385, "kv_decode_ms": 23.81889999969644, "kv_decode_cuda_event_ms": 23.747583389282227, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 6.182199999784643, "cuda_event_ms": 6.129663944244385, "tokens_total": 17, "tokens_per_s": 2749.830157644883}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547469.0223792, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.182199999784643, "prefill_cuda_event_ms": 6.129663944244385, "kv_decode_ms": 23.81889999969644, "kv_decode_cuda_event_ms": 23.747583389282227, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 23.81889999969644, "cuda_event_ms": 23.747583389282227, "tokens_total": 8, "tokens_per_s": 335.86773529012487}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 8, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547469.0223792, "prompt_tokens": 17, "gen_tokens": 8, "prefill_ms": 6.182199999784643, "prefill_cuda_event_ms": 6.129663944244385, "kv_decode_ms": 23.81889999969644, "kv_decode_cuda_event_ms": 23.747583389282227, "gpu_peak_mb": 631.15966796875, "params_millions_measured": 45.1712, "latency_ms": 30.001099999481085, "cuda_event_ms": 29.87724733352661, "tokens_total": 25, "tokens_per_s": 833.3027789125204}
