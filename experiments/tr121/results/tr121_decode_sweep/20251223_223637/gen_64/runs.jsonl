{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547626.1826282, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 109.3591999997443, "prefill_cuda_event_ms": null, "kv_decode_ms": 1518.4170000002268, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7289.86040000018, "params_millions_measured": 45.1712, "latency_ms": 109.3591999997443, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 155.45102744021307}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547626.1826282, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 109.3591999997443, "prefill_cuda_event_ms": null, "kv_decode_ms": 1518.4170000002268, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7289.86040000018, "params_millions_measured": 45.1712, "latency_ms": 1518.4170000002268, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 42.149159288911044}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547626.1826282, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 109.3591999997443, "prefill_cuda_event_ms": null, "kv_decode_ms": 1518.4170000002268, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7289.86040000018, "params_millions_measured": 45.1712, "latency_ms": 1627.7761999999711, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 49.76114038281272}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547635.1027837, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.682200000323064, "prefill_cuda_event_ms": null, "kv_decode_ms": 1625.836999999592, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 26.682200000323064, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 637.1288724240942}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547635.1027837, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.682200000323064, "prefill_cuda_event_ms": null, "kv_decode_ms": 1625.836999999592, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1625.836999999592, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.36433972164249}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547635.1027837, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.682200000323064, "prefill_cuda_event_ms": null, "kv_decode_ms": 1625.836999999592, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1652.5191999999151, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 49.01607194639806}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547636.7557318, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.82559999973455, "prefill_cuda_event_ms": null, "kv_decode_ms": 1274.0752999998222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 25.82559999973455, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 658.261569921889}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547636.7557318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.82559999973455, "prefill_cuda_event_ms": null, "kv_decode_ms": 1274.0752999998222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1274.0752999998222, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.23250980535368}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547636.7557318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.82559999973455, "prefill_cuda_event_ms": null, "kv_decode_ms": 1274.0752999998222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1299.9008999995567, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 62.31244243313288}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547638.0561213, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 34.77500000008149, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.939400000294, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 34.77500000008149, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 488.85693745392274}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547638.0561213, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.77500000008149, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.939400000294, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1444.939400000294, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 44.29251496636259}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547638.0561213, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.77500000008149, "prefill_cuda_event_ms": null, "kv_decode_ms": 1444.939400000294, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1479.7144000003755, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 54.74029312682194}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547639.5367355, "prompt_tokens": 92, "prefill_ms": 836.4702, "prefill_cuda_event_ms": null, "kv_decode_ms": 2912.8116, "kv_decode_ms_equiv": 2912.8116, "kv_decode_ms_per_token": 45.51268125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 12980.21400000016, "ollama_total_duration_ms": 12834.8826, "ollama_load_ms": 8988.3326, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 836.4702, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 109.9859863507391}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547639.5367355, "prompt_tokens": 92, "prefill_ms": 836.4702, "prefill_cuda_event_ms": null, "kv_decode_ms": 2912.8116, "kv_decode_ms_equiv": 2912.8116, "kv_decode_ms_per_token": 45.51268125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 12980.21400000016, "ollama_total_duration_ms": 12834.8826, "ollama_load_ms": 8988.3326, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2912.8116, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.971898216829402}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547639.5367355, "prompt_tokens": 92, "prefill_ms": 836.4702, "prefill_cuda_event_ms": null, "kv_decode_ms": 2912.8116, "kv_decode_ms_equiv": 2912.8116, "kv_decode_ms_per_token": 45.51268125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 12980.21400000016, "ollama_total_duration_ms": 12834.8826, "ollama_load_ms": 8988.3326, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3749.2817999999997, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 41.60796875817657}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547652.5171516, "prompt_tokens": 92, "prefill_ms": 58.742, "prefill_cuda_event_ms": null, "kv_decode_ms": 3164.7339, "kv_decode_ms_equiv": 3164.7339, "kv_decode_ms_per_token": 49.4489671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3499.96259999989, "ollama_total_duration_ms": 3485.9818, "ollama_load_ms": 236.9836, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 58.742, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1566.1707126076744}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547652.5171516, "prompt_tokens": 92, "prefill_ms": 58.742, "prefill_cuda_event_ms": null, "kv_decode_ms": 3164.7339, "kv_decode_ms_equiv": 3164.7339, "kv_decode_ms_per_token": 49.4489671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3499.96259999989, "ollama_total_duration_ms": 3485.9818, "ollama_load_ms": 236.9836, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3164.7339, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.22286929084306}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547652.5171516, "prompt_tokens": 92, "prefill_ms": 58.742, "prefill_cuda_event_ms": null, "kv_decode_ms": 3164.7339, "kv_decode_ms_equiv": 3164.7339, "kv_decode_ms_per_token": 49.4489671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3499.96259999989, "ollama_total_duration_ms": 3485.9818, "ollama_load_ms": 236.9836, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3223.4759000000004, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 48.3949639580057}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547656.0172706, "prompt_tokens": 92, "prefill_ms": 65.6112, "prefill_cuda_event_ms": null, "kv_decode_ms": 3634.3052, "kv_decode_ms_equiv": 3634.3052, "kv_decode_ms_per_token": 56.78601875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3984.7012000000177, "ollama_total_duration_ms": 3981.0657, "ollama_load_ms": 254.4376, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 65.6112, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1402.1996244543616}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547656.0172706, "prompt_tokens": 92, "prefill_ms": 65.6112, "prefill_cuda_event_ms": null, "kv_decode_ms": 3634.3052, "kv_decode_ms_equiv": 3634.3052, "kv_decode_ms_per_token": 56.78601875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3984.7012000000177, "ollama_total_duration_ms": 3981.0657, "ollama_load_ms": 254.4376, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3634.3052, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.60996847485456}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547656.0172706, "prompt_tokens": 92, "prefill_ms": 65.6112, "prefill_cuda_event_ms": null, "kv_decode_ms": 3634.3052, "kv_decode_ms_equiv": 3634.3052, "kv_decode_ms_per_token": 56.78601875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3984.7012000000177, "ollama_total_duration_ms": 3981.0657, "ollama_load_ms": 254.4376, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3699.9163999999996, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 42.16311482064838}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547660.0028915, "prompt_tokens": 92, "prefill_ms": 63.458, "prefill_cuda_event_ms": null, "kv_decode_ms": 3543.194, "kv_decode_ms_equiv": 3543.194, "kv_decode_ms_per_token": 55.36240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3879.0718999998717, "ollama_total_duration_ms": 3875.34, "ollama_load_ms": 249.1458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 63.458, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1449.7778057928078}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547660.0028915, "prompt_tokens": 92, "prefill_ms": 63.458, "prefill_cuda_event_ms": null, "kv_decode_ms": 3543.194, "kv_decode_ms_equiv": 3543.194, "kv_decode_ms_per_token": 55.36240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3879.0718999998717, "ollama_total_duration_ms": 3875.34, "ollama_load_ms": 249.1458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3543.194, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.062798706477828}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547660.0028915, "prompt_tokens": 92, "prefill_ms": 63.458, "prefill_cuda_event_ms": null, "kv_decode_ms": 3543.194, "kv_decode_ms_equiv": 3543.194, "kv_decode_ms_per_token": 55.36240625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3879.0718999998717, "ollama_total_duration_ms": 3875.34, "ollama_load_ms": 249.1458, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3606.652, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 43.25341064233533}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547663.8821368, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 749.997899999471, "prefill_cuda_event_ms": null, "kv_decode_ms": 2012.2977000000901, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.55089999971096, "params_millions_measured": 96.08832, "latency_ms": 749.997899999471, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 12.000033600102546}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547663.8821368, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 749.997899999471, "prefill_cuda_event_ms": null, "kv_decode_ms": 2012.2977000000901, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.55089999971096, "params_millions_measured": 96.08832, "latency_ms": 2012.2977000000901, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.80443927357127}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547663.8821368, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 749.997899999471, "prefill_cuda_event_ms": null, "kv_decode_ms": 2012.2977000000901, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 301.55089999971096, "params_millions_measured": 96.08832, "latency_ms": 2762.295599999561, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 26.427294747170286}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547666.9466152, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.81820000070002, "prefill_cuda_event_ms": null, "kv_decode_ms": 2063.8646999996126, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.81820000070002, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 258.4855047021114}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547666.9466152, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.81820000070002, "prefill_cuda_event_ms": null, "kv_decode_ms": 2063.8646999996126, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2063.8646999996126, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.009784701493277}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547666.9466152, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.81820000070002, "prefill_cuda_event_ms": null, "kv_decode_ms": 2063.8646999996126, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2098.6829000003127, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 34.783720780299454}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547669.0461655, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 43.341599999621394, "prefill_cuda_event_ms": null, "kv_decode_ms": 2383.111099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 43.341599999621394, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 207.65269394943007}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547669.0461655, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 43.341599999621394, "prefill_cuda_event_ms": null, "kv_decode_ms": 2383.111099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2383.111099999951, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.855651001752}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547669.0461655, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 43.341599999621394, "prefill_cuda_event_ms": null, "kv_decode_ms": 2383.111099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2426.4526999995724, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 30.085070275638536}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547671.473206, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 60.131299999738985, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.3419999999824, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 60.131299999738985, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 149.67246675257422}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547671.473206, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 60.131299999738985, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.3419999999824, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2570.3419999999824, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.899410273029986}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547671.473206, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 60.131299999738985, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.3419999999824, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2630.4732999997213, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 27.751659748839774}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547674.1041937, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 713.9144999991913, "prefill_cuda_event_ms": 609.626953125, "kv_decode_ms": 449.95229999949515, "kv_decode_cuda_event_ms": 449.8227233886719, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 425.9263000003557, "params_millions_measured": 96.08832, "latency_ms": 713.9144999991913, "cuda_event_ms": 609.626953125, "tokens_total": 17, "tokens_per_s": 23.812375291465933}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547674.1041937, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 713.9144999991913, "prefill_cuda_event_ms": 609.626953125, "kv_decode_ms": 449.95229999949515, "kv_decode_cuda_event_ms": 449.8227233886719, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 425.9263000003557, "params_millions_measured": 96.08832, "latency_ms": 449.95229999949515, "cuda_event_ms": 449.8227233886719, "tokens_total": 64, "tokens_per_s": 142.23729937611566}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547674.1041937, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 713.9144999991913, "prefill_cuda_event_ms": 609.626953125, "kv_decode_ms": 449.95229999949515, "kv_decode_cuda_event_ms": 449.8227233886719, "gpu_peak_mb": 208.4267578125, "hf_load_ms": 425.9263000003557, "params_millions_measured": 96.08832, "latency_ms": 1163.8667999986865, "cuda_event_ms": 1059.4496765136719, "tokens_total": 81, "tokens_per_s": 69.59559289782251}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547675.6974368, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.397300000477117, "prefill_cuda_event_ms": 10.323840141296387, "kv_decode_ms": 242.45340000015858, "kv_decode_cuda_event_ms": 242.41763305664062, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 10.397300000477117, "cuda_event_ms": 10.323840141296387, "tokens_total": 17, "tokens_per_s": 1635.0398660440592}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547675.6974368, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.397300000477117, "prefill_cuda_event_ms": 10.323840141296387, "kv_decode_ms": 242.45340000015858, "kv_decode_cuda_event_ms": 242.41763305664062, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 242.45340000015858, "cuda_event_ms": 242.41763305664062, "tokens_total": 64, "tokens_per_s": 263.96825121841204}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547675.6974368, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.397300000477117, "prefill_cuda_event_ms": 10.323840141296387, "kv_decode_ms": 242.45340000015858, "kv_decode_cuda_event_ms": 242.41763305664062, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 252.8507000006357, "cuda_event_ms": 252.741473197937, "tokens_total": 81, "tokens_per_s": 320.347145567706}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547675.9511542, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.721899999822199, "prefill_cuda_event_ms": 4.671520233154297, "kv_decode_ms": 243.9936000000671, "kv_decode_cuda_event_ms": 243.94239807128906, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 4.721899999822199, "cuda_event_ms": 4.671520233154297, "tokens_total": 17, "tokens_per_s": 3600.2456639573325}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547675.9511542, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.721899999822199, "prefill_cuda_event_ms": 4.671520233154297, "kv_decode_ms": 243.9936000000671, "kv_decode_cuda_event_ms": 243.94239807128906, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 243.9936000000671, "cuda_event_ms": 243.94239807128906, "tokens_total": 64, "tokens_per_s": 262.3019620186038}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547675.9511542, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.721899999822199, "prefill_cuda_event_ms": 4.671520233154297, "kv_decode_ms": 243.9936000000671, "kv_decode_cuda_event_ms": 243.94239807128906, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 248.7154999998893, "cuda_event_ms": 248.61391830444336, "tokens_total": 81, "tokens_per_s": 325.67330946417115}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547676.2005534, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.788100000041595, "prefill_cuda_event_ms": 4.746240139007568, "kv_decode_ms": 249.40690000039467, "kv_decode_cuda_event_ms": 249.37762451171875, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 4.788100000041595, "cuda_event_ms": 4.746240139007568, "tokens_total": 17, "tokens_per_s": 3550.4688707112045}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547676.2005534, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.788100000041595, "prefill_cuda_event_ms": 4.746240139007568, "kv_decode_ms": 249.40690000039467, "kv_decode_cuda_event_ms": 249.37762451171875, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 249.40690000039467, "cuda_event_ms": 249.37762451171875, "tokens_total": 64, "tokens_per_s": 256.60877866610235}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547676.2005534, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.788100000041595, "prefill_cuda_event_ms": 4.746240139007568, "kv_decode_ms": 249.40690000039467, "kv_decode_cuda_event_ms": 249.37762451171875, "gpu_peak_mb": 208.4267578125, "params_millions_measured": 96.08832, "latency_ms": 254.19500000043627, "cuda_event_ms": 254.12386465072632, "tokens_total": 81, "tokens_per_s": 318.6530026155549}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547676.45551, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 18.995600000380364, "prefill_cuda_event_ms": 18.879487991333008, "kv_decode_ms": 614.2633999997997, "kv_decode_cuda_event_ms": 614.1992797851562, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 366.838099999768, "params_millions_measured": 51.475968, "latency_ms": 18.995600000380364, "cuda_event_ms": 18.879487991333008, "tokens_total": 17, "tokens_per_s": 894.9440922981952}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547676.45551, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.995600000380364, "prefill_cuda_event_ms": 18.879487991333008, "kv_decode_ms": 614.2633999997997, "kv_decode_cuda_event_ms": 614.1992797851562, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 366.838099999768, "params_millions_measured": 51.475968, "latency_ms": 614.2633999997997, "cuda_event_ms": 614.1992797851562, "tokens_total": 64, "tokens_per_s": 104.18983126785817}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547676.45551, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.995600000380364, "prefill_cuda_event_ms": 18.879487991333008, "kv_decode_ms": 614.2633999997997, "kv_decode_cuda_event_ms": 614.1992797851562, "gpu_peak_mb": 316.77099609375, "hf_load_ms": 366.838099999768, "params_millions_measured": 51.475968, "latency_ms": 633.2590000001801, "cuda_event_ms": 633.0787677764893, "tokens_total": 81, "tokens_per_s": 127.90974940739407}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547677.457686, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.455899999513349, "prefill_cuda_event_ms": 7.400447845458984, "kv_decode_ms": 287.8853000001982, "kv_decode_cuda_event_ms": 287.824951171875, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 7.455899999513349, "cuda_event_ms": 7.400447845458984, "tokens_total": 17, "tokens_per_s": 2280.073498988667}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547677.457686, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.455899999513349, "prefill_cuda_event_ms": 7.400447845458984, "kv_decode_ms": 287.8853000001982, "kv_decode_cuda_event_ms": 287.824951171875, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 287.8853000001982, "cuda_event_ms": 287.824951171875, "tokens_total": 64, "tokens_per_s": 222.31076057011575}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547677.457686, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.455899999513349, "prefill_cuda_event_ms": 7.400447845458984, "kv_decode_ms": 287.8853000001982, "kv_decode_cuda_event_ms": 287.824951171875, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 295.34119999971153, "cuda_event_ms": 295.225399017334, "tokens_total": 81, "tokens_per_s": 274.25906036841155}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547677.7539907, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.899099999420287, "prefill_cuda_event_ms": 4.849664211273193, "kv_decode_ms": 261.80699999986246, "kv_decode_cuda_event_ms": 261.760009765625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 4.899099999420287, "cuda_event_ms": 4.849664211273193, "tokens_total": 17, "tokens_per_s": 3470.0251070628524}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547677.7539907, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.899099999420287, "prefill_cuda_event_ms": 4.849664211273193, "kv_decode_ms": 261.80699999986246, "kv_decode_cuda_event_ms": 261.760009765625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 261.80699999986246, "cuda_event_ms": 261.760009765625, "tokens_total": 64, "tokens_per_s": 244.45488470527383}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547677.7539907, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.899099999420287, "prefill_cuda_event_ms": 4.849664211273193, "kv_decode_ms": 261.80699999986246, "kv_decode_cuda_event_ms": 261.760009765625, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 266.70609999928274, "cuda_event_ms": 266.6096739768982, "tokens_total": 81, "tokens_per_s": 303.7050896106907}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547678.021458, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.025999999816122, "prefill_cuda_event_ms": 4.975615978240967, "kv_decode_ms": 261.91779999953724, "kv_decode_cuda_event_ms": 261.8643798828125, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.025999999816122, "cuda_event_ms": 4.975615978240967, "tokens_total": 17, "tokens_per_s": 3382.4114605296363}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547678.021458, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.025999999816122, "prefill_cuda_event_ms": 4.975615978240967, "kv_decode_ms": 261.91779999953724, "kv_decode_cuda_event_ms": 261.8643798828125, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 261.91779999953724, "cuda_event_ms": 261.8643798828125, "tokens_total": 64, "tokens_per_s": 244.35147210351138}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547678.021458, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.025999999816122, "prefill_cuda_event_ms": 4.975615978240967, "kv_decode_ms": 261.91779999953724, "kv_decode_cuda_event_ms": 261.8643798828125, "gpu_peak_mb": 316.77099609375, "params_millions_measured": 51.475968, "latency_ms": 266.94379999935336, "cuda_event_ms": 266.83999586105347, "tokens_total": 81, "tokens_per_s": 303.43465553497106}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547678.2890146, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 102.33700000026147, "prefill_cuda_event_ms": null, "kv_decode_ms": 1949.8632999993788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.3985000003304, "params_millions_measured": 51.475968, "latency_ms": 102.33700000026147, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 87.94473162176931}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547678.2890146, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 102.33700000026147, "prefill_cuda_event_ms": null, "kv_decode_ms": 1949.8632999993788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.3985000003304, "params_millions_measured": 51.475968, "latency_ms": 1949.8632999993788, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.82281378393059}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547678.2890146, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 102.33700000026147, "prefill_cuda_event_ms": null, "kv_decode_ms": 1949.8632999993788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 143.3985000003304, "params_millions_measured": 51.475968, "latency_ms": 2052.2002999996403, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 35.571576517171735}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547680.4853556, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 35.92260000004899, "prefill_cuda_event_ms": null, "kv_decode_ms": 1539.790200000425, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 35.92260000004899, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 250.53865811460548}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547680.4853556, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.92260000004899, "prefill_cuda_event_ms": null, "kv_decode_ms": 1539.790200000425, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1539.790200000425, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.56410399285716}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547680.4853556, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.92260000004899, "prefill_cuda_event_ms": null, "kv_decode_ms": 1539.790200000425, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1575.712800000474, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 46.328239511653415}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547682.0621068, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 30.121800000415533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1353.3363999995345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.121800000415533, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 298.78692508003655}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547682.0621068, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.121800000415533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1353.3363999995345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1353.3363999995345, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.29053323329072}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547682.0621068, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.121800000415533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1353.3363999995345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1383.45819999995, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.76632138217305}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547683.4465134, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 30.239200000323763, "prefill_cuda_event_ms": null, "kv_decode_ms": 1356.4453999997568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.239200000323763, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 297.626921343939}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547683.4465134, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.239200000323763, "prefill_cuda_event_ms": null, "kv_decode_ms": 1356.4453999997568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1356.4453999997568, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.182142384803306}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547683.4465134, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.239200000323763, "prefill_cuda_event_ms": null, "kv_decode_ms": 1356.4453999997568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1386.6846000000805, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.64354994639427}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547684.833871, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 63.37000000075932, "prefill_cuda_event_ms": 63.278079986572266, "kv_decode_ms": 319.45880000057514, "kv_decode_cuda_event_ms": 319.39276123046875, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 312.19920000057755, "params_millions_measured": 45.1712, "latency_ms": 63.37000000075932, "cuda_event_ms": 63.278079986572266, "tokens_total": 9, "tokens_per_s": 142.0230392913391}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547684.833871, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 63.37000000075932, "prefill_cuda_event_ms": 63.278079986572266, "kv_decode_ms": 319.45880000057514, "kv_decode_cuda_event_ms": 319.39276123046875, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 312.19920000057755, "params_millions_measured": 45.1712, "latency_ms": 319.45880000057514, "cuda_event_ms": 319.39276123046875, "tokens_total": 64, "tokens_per_s": 200.33882303409635}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547684.833871, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 63.37000000075932, "prefill_cuda_event_ms": 63.278079986572266, "kv_decode_ms": 319.45880000057514, "kv_decode_cuda_event_ms": 319.39276123046875, "gpu_peak_mb": 408.6748046875, "hf_load_ms": 312.19920000057755, "params_millions_measured": 45.1712, "latency_ms": 382.82880000133446, "cuda_event_ms": 382.670841217041, "tokens_total": 73, "tokens_per_s": 190.68575822860123}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547685.5304239, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.064200000560959, "prefill_cuda_event_ms": 5.021696090698242, "kv_decode_ms": 189.85520000023826, "kv_decode_cuda_event_ms": 189.8250274658203, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 5.064200000560959, "cuda_event_ms": 5.021696090698242, "tokens_total": 9, "tokens_per_s": 1777.1809958143585}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547685.5304239, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.064200000560959, "prefill_cuda_event_ms": 5.021696090698242, "kv_decode_ms": 189.85520000023826, "kv_decode_cuda_event_ms": 189.8250274658203, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 189.85520000023826, "cuda_event_ms": 189.8250274658203, "tokens_total": 64, "tokens_per_s": 337.09901019260826}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547685.5304239, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.064200000560959, "prefill_cuda_event_ms": 5.021696090698242, "kv_decode_ms": 189.85520000023826, "kv_decode_cuda_event_ms": 189.8250274658203, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 194.91940000079921, "cuda_event_ms": 194.84672355651855, "tokens_total": 73, "tokens_per_s": 374.51377338377137}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547685.726366, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.52030000067316, "prefill_cuda_event_ms": 3.4805760383605957, "kv_decode_ms": 183.8737999996738, "kv_decode_cuda_event_ms": 183.84588623046875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 3.52030000067316, "cuda_event_ms": 3.4805760383605957, "tokens_total": 9, "tokens_per_s": 2556.6002892591537}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547685.726366, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.52030000067316, "prefill_cuda_event_ms": 3.4805760383605957, "kv_decode_ms": 183.8737999996738, "kv_decode_cuda_event_ms": 183.84588623046875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 183.8737999996738, "cuda_event_ms": 183.84588623046875, "tokens_total": 64, "tokens_per_s": 348.06481401979806}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547685.726366, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.52030000067316, "prefill_cuda_event_ms": 3.4805760383605957, "kv_decode_ms": 183.8737999996738, "kv_decode_cuda_event_ms": 183.84588623046875, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 187.39410000034695, "cuda_event_ms": 187.32646226882935, "tokens_total": 73, "tokens_per_s": 389.5533530664244}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547685.9142356, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.5929000005126, "prefill_cuda_event_ms": 3.5481600761413574, "kv_decode_ms": 212.48209999976098, "kv_decode_cuda_event_ms": 212.4259490966797, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 3.5929000005126, "cuda_event_ms": 3.5481600761413574, "tokens_total": 9, "tokens_per_s": 2504.940298565495}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547685.9142356, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5929000005126, "prefill_cuda_event_ms": 3.5481600761413574, "kv_decode_ms": 212.48209999976098, "kv_decode_cuda_event_ms": 212.4259490966797, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 212.48209999976098, "cuda_event_ms": 212.4259490966797, "tokens_total": 64, "tokens_per_s": 301.2018424143586}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547685.9142356, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5929000005126, "prefill_cuda_event_ms": 3.5481600761413574, "kv_decode_ms": 212.48209999976098, "kv_decode_cuda_event_ms": 212.4259490966797, "gpu_peak_mb": 408.6748046875, "params_millions_measured": 45.1712, "latency_ms": 216.07500000027358, "cuda_event_ms": 215.97410917282104, "tokens_total": 73, "tokens_per_s": 337.84565544328393}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547686.1308806, "prompt_tokens": 17, "prefill_ms": 9.7721, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.8922, "kv_decode_ms_equiv": 143.38110270270272, "kv_decode_ms_per_token": 2.24032972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2805.6219000000056, "ollama_total_duration_ms": 2738.6631, "ollama_load_ms": 2595.5436, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 9.7721, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1739.6465447549656}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547686.1308806, "prompt_tokens": 17, "prefill_ms": 9.7721, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.8922, "kv_decode_ms_equiv": 143.38110270270272, "kv_decode_ms_per_token": 2.24032972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2805.6219000000056, "ollama_total_duration_ms": 2738.6631, "ollama_load_ms": 2595.5436, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 143.38110270270272, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 446.3628664699453}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547686.1308806, "prompt_tokens": 17, "prefill_ms": 9.7721, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.8922, "kv_decode_ms_equiv": 143.38110270270272, "kv_decode_ms_per_token": 2.24032972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2805.6219000000056, "ollama_total_duration_ms": 2738.6631, "ollama_load_ms": 2595.5436, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 153.1532027027027, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 528.8821818322352}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547688.936795, "prompt_tokens": 17, "prefill_ms": 2.5058, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2732, "kv_decode_ms_equiv": 139.03093333333334, "kv_decode_ms_per_token": 2.1723583333333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.9723000000886, "ollama_total_duration_ms": 283.3264, "ollama_load_ms": 149.3615, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.5058, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6784.2605156038}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547688.936795, "prompt_tokens": 17, "prefill_ms": 2.5058, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2732, "kv_decode_ms_equiv": 139.03093333333334, "kv_decode_ms_per_token": 2.1723583333333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.9723000000886, "ollama_total_duration_ms": 283.3264, "ollama_load_ms": 149.3615, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.03093333333334, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 460.32921210819273}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547688.936795, "prompt_tokens": 17, "prefill_ms": 2.5058, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2732, "kv_decode_ms_equiv": 139.03093333333334, "kv_decode_ms_per_token": 2.1723583333333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 316.9723000000886, "ollama_total_duration_ms": 283.3264, "ollama_load_ms": 149.3615, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 141.53673333333333, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 572.2895964345652}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547689.2539346, "prompt_tokens": 17, "prefill_ms": 2.1788, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9222, "kv_decode_ms_equiv": 139.89626666666666, "kv_decode_ms_per_token": 2.1858791666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 321.9818000006853, "ollama_total_duration_ms": 308.8796, "ollama_load_ms": 168.1696, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1788, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7802.460069763173}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547689.2539346, "prompt_tokens": 17, "prefill_ms": 2.1788, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9222, "kv_decode_ms_equiv": 139.89626666666666, "kv_decode_ms_per_token": 2.1858791666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 321.9818000006853, "ollama_total_duration_ms": 308.8796, "ollama_load_ms": 168.1696, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.89626666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 457.4818293935888}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547689.2539346, "prompt_tokens": 17, "prefill_ms": 2.1788, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9222, "kv_decode_ms_equiv": 139.89626666666666, "kv_decode_ms_per_token": 2.1858791666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 321.9818000006853, "ollama_total_duration_ms": 308.8796, "ollama_load_ms": 168.1696, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.07506666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 570.1211472245188}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547689.5760381, "prompt_tokens": 17, "prefill_ms": 1.8157, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4526, "kv_decode_ms_equiv": 136.60346666666666, "kv_decode_ms_per_token": 2.1344291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 289.64520000045013, "ollama_total_duration_ms": 273.4241, "ollama_load_ms": 141.6522, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 1.8157, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 9362.780194966128}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547689.5760381, "prompt_tokens": 17, "prefill_ms": 1.8157, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4526, "kv_decode_ms_equiv": 136.60346666666666, "kv_decode_ms_per_token": 2.1344291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 289.64520000045013, "ollama_total_duration_ms": 273.4241, "ollama_load_ms": 141.6522, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 136.60346666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 468.50933992890367}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547689.5760381, "prompt_tokens": 17, "prefill_ms": 1.8157, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4526, "kv_decode_ms_equiv": 136.60346666666666, "kv_decode_ms_per_token": 2.1344291666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 289.64520000045013, "ollama_total_duration_ms": 273.4241, "ollama_load_ms": 141.6522, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 138.41916666666665, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 585.1790756337936}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547689.8660421, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.633600000393926, "prefill_cuda_event_ms": 8.532992362976074, "kv_decode_ms": 551.5224000000671, "kv_decode_cuda_event_ms": 551.4915771484375, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 8.633600000393926, "cuda_event_ms": 8.532992362976074, "tokens_total": 9, "tokens_per_s": 1042.4388435402793}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547689.8660421, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.633600000393926, "prefill_cuda_event_ms": 8.532992362976074, "kv_decode_ms": 551.5224000000671, "kv_decode_cuda_event_ms": 551.4915771484375, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 551.5224000000671, "cuda_event_ms": 551.4915771484375, "tokens_total": 64, "tokens_per_s": 116.04243091484989}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547689.8660421, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.633600000393926, "prefill_cuda_event_ms": 8.532992362976074, "kv_decode_ms": 551.5224000000671, "kv_decode_cuda_event_ms": 551.4915771484375, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 560.1560000004611, "cuda_event_ms": 560.0245695114136, "tokens_total": 73, "tokens_per_s": 130.32083919468846}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547690.4274604, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.667999999895983, "prefill_cuda_event_ms": 10.618751525878906, "kv_decode_ms": 427.6134000001548, "kv_decode_cuda_event_ms": 427.5710754394531, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 10.667999999895983, "cuda_event_ms": 10.618751525878906, "tokens_total": 9, "tokens_per_s": 843.6445444401719}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547690.4274604, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.667999999895983, "prefill_cuda_event_ms": 10.618751525878906, "kv_decode_ms": 427.6134000001548, "kv_decode_cuda_event_ms": 427.5710754394531, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 427.6134000001548, "cuda_event_ms": 427.5710754394531, "tokens_total": 64, "tokens_per_s": 149.6679009590832}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547690.4274604, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.667999999895983, "prefill_cuda_event_ms": 10.618751525878906, "kv_decode_ms": 427.6134000001548, "kv_decode_cuda_event_ms": 427.5710754394531, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 438.2814000000508, "cuda_event_ms": 438.18982696533203, "tokens_total": 73, "tokens_per_s": 166.55965779061475}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547690.8664236, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.440400000021327, "prefill_cuda_event_ms": 4.398079872131348, "kv_decode_ms": 429.30840000008175, "kv_decode_cuda_event_ms": 429.1061706542969, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 4.440400000021327, "cuda_event_ms": 4.398079872131348, "tokens_total": 9, "tokens_per_s": 2026.8444284201362}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547690.8664236, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.440400000021327, "prefill_cuda_event_ms": 4.398079872131348, "kv_decode_ms": 429.30840000008175, "kv_decode_cuda_event_ms": 429.1061706542969, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 429.30840000008175, "cuda_event_ms": 429.1061706542969, "tokens_total": 64, "tokens_per_s": 149.07698055753815}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547690.8664236, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.440400000021327, "prefill_cuda_event_ms": 4.398079872131348, "kv_decode_ms": 429.30840000008175, "kv_decode_cuda_event_ms": 429.1061706542969, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 433.7488000001031, "cuda_event_ms": 433.5042505264282, "tokens_total": 73, "tokens_per_s": 168.3001774298457}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547691.3007498, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.363399999732792, "prefill_cuda_event_ms": 10.31884765625, "kv_decode_ms": 490.75970000012603, "kv_decode_cuda_event_ms": 490.71612548828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 10.363399999732792, "cuda_event_ms": 10.31884765625, "tokens_total": 9, "tokens_per_s": 868.4408591998817}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547691.3007498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.363399999732792, "prefill_cuda_event_ms": 10.31884765625, "kv_decode_ms": 490.75970000012603, "kv_decode_cuda_event_ms": 490.71612548828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 490.75970000012603, "cuda_event_ms": 490.71612548828125, "tokens_total": 64, "tokens_per_s": 130.4100560824036}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547691.3007498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.363399999732792, "prefill_cuda_event_ms": 10.31884765625, "kv_decode_ms": 490.75970000012603, "kv_decode_cuda_event_ms": 490.71612548828125, "gpu_peak_mb": 409.53564453125, "params_millions_measured": 96.08832, "latency_ms": 501.1230999998588, "cuda_event_ms": 501.03497314453125, "tokens_total": 73, "tokens_per_s": 145.6727897796381}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547691.8026536, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 48.31560000002355, "prefill_cuda_event_ms": null, "kv_decode_ms": 991.4370000005874, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 242.43959999967046, "params_millions_measured": 5.03672, "latency_ms": 48.31560000002355, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 351.8532316682751}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547691.8026536, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 48.31560000002355, "prefill_cuda_event_ms": null, "kv_decode_ms": 991.4370000005874, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 242.43959999967046, "params_millions_measured": 5.03672, "latency_ms": 991.4370000005874, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.5527653294784}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547691.8026536, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 48.31560000002355, "prefill_cuda_event_ms": null, "kv_decode_ms": 991.4370000005874, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 242.43959999967046, "params_millions_measured": 5.03672, "latency_ms": 1039.752600000611, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 77.903147344813}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547693.0868974, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.95629999945959, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.0089999998745, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 27.95629999945959, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 608.0919148931947}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547693.0868974, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.95629999945959, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.0089999998745, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 913.0089999998745, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.09788512490982}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547693.0868974, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.95629999945959, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.0089999998745, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 940.9652999993341, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 86.08181406908132}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547694.028581, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.270599999705155, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.187999999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 23.270599999705155, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 730.535525522135}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547694.028581, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.270599999705155, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.187999999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 857.187999999951, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.66273442932433}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547694.028581, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.270599999705155, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.187999999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 880.4585999996561, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 91.99751129698959}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547694.9098032, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.13010000084614, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.6300000002666, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.13010000084614, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 804.5394957581482}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547694.9098032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.13010000084614, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.6300000002666, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 883.6300000002666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.42850514353371}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547694.9098032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.13010000084614, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.6300000002666, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 904.7601000011127, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 89.52649437116024}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547695.8149107, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.48839999991469, "prefill_cuda_event_ms": 13.400064468383789, "kv_decode_ms": 241.7144000000917, "kv_decode_cuda_event_ms": 241.6558074951172, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 439.44009999995615, "params_millions_measured": 74.824704, "latency_ms": 13.48839999991469, "cuda_event_ms": 13.400064468383789, "tokens_total": 9, "tokens_per_s": 667.2399988180156}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547695.8149107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.48839999991469, "prefill_cuda_event_ms": 13.400064468383789, "kv_decode_ms": 241.7144000000917, "kv_decode_cuda_event_ms": 241.6558074951172, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 439.44009999995615, "params_millions_measured": 74.824704, "latency_ms": 241.7144000000917, "cuda_event_ms": 241.6558074951172, "tokens_total": 64, "tokens_per_s": 264.7752885222218}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547695.8149107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.48839999991469, "prefill_cuda_event_ms": 13.400064468383789, "kv_decode_ms": 241.7144000000917, "kv_decode_cuda_event_ms": 241.6558074951172, "gpu_peak_mb": 557.86962890625, "hf_load_ms": 439.44009999995615, "params_millions_measured": 74.824704, "latency_ms": 255.2028000000064, "cuda_event_ms": 255.05587196350098, "tokens_total": 73, "tokens_per_s": 286.0470182928956}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547696.5114653, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.6319999999250285, "prefill_cuda_event_ms": 3.575808048248291, "kv_decode_ms": 165.4617000003782, "kv_decode_cuda_event_ms": 165.42311096191406, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 3.6319999999250285, "cuda_event_ms": 3.575808048248291, "tokens_total": 9, "tokens_per_s": 2477.9735683330887}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547696.5114653, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.6319999999250285, "prefill_cuda_event_ms": 3.575808048248291, "kv_decode_ms": 165.4617000003782, "kv_decode_cuda_event_ms": 165.42311096191406, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 165.4617000003782, "cuda_event_ms": 165.42311096191406, "tokens_total": 64, "tokens_per_s": 386.7964610532451}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547696.5114653, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.6319999999250285, "prefill_cuda_event_ms": 3.575808048248291, "kv_decode_ms": 165.4617000003782, "kv_decode_cuda_event_ms": 165.42311096191406, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 169.09370000030322, "cuda_event_ms": 168.99891901016235, "tokens_total": 73, "tokens_per_s": 431.71330451618894}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547696.6813743, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.52039999961562, "prefill_cuda_event_ms": 3.4754559993743896, "kv_decode_ms": 167.39810000035504, "kv_decode_cuda_event_ms": 167.3543701171875, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 3.52039999961562, "cuda_event_ms": 3.4754559993743896, "tokens_total": 9, "tokens_per_s": 2556.527667589671}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547696.6813743, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.52039999961562, "prefill_cuda_event_ms": 3.4754559993743896, "kv_decode_ms": 167.39810000035504, "kv_decode_cuda_event_ms": 167.3543701171875, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 167.39810000035504, "cuda_event_ms": 167.3543701171875, "tokens_total": 64, "tokens_per_s": 382.32214105096926}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547696.6813743, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.52039999961562, "prefill_cuda_event_ms": 3.4754559993743896, "kv_decode_ms": 167.39810000035504, "kv_decode_cuda_event_ms": 167.3543701171875, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 170.91849999997066, "cuda_event_ms": 170.8298261165619, "tokens_total": 73, "tokens_per_s": 427.1041461281987}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547696.8529444, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.5366000001886277, "prefill_cuda_event_ms": 3.481600046157837, "kv_decode_ms": 164.52100000060454, "kv_decode_cuda_event_ms": 164.48614501953125, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 3.5366000001886277, "cuda_event_ms": 3.481600046157837, "tokens_total": 9, "tokens_per_s": 2544.8170557936937}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547696.8529444, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5366000001886277, "prefill_cuda_event_ms": 3.481600046157837, "kv_decode_ms": 164.52100000060454, "kv_decode_cuda_event_ms": 164.48614501953125, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 164.52100000060454, "cuda_event_ms": 164.48614501953125, "tokens_total": 64, "tokens_per_s": 389.00809015119546}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547696.8529444, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.5366000001886277, "prefill_cuda_event_ms": 3.481600046157837, "kv_decode_ms": 164.52100000060454, "kv_decode_cuda_event_ms": 164.48614501953125, "gpu_peak_mb": 557.86962890625, "params_millions_measured": 74.824704, "latency_ms": 168.05760000079317, "cuda_event_ms": 167.9677450656891, "tokens_total": 73, "tokens_per_s": 434.3748809911332}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547697.0216827, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 59.41979999988689, "prefill_cuda_event_ms": null, "kv_decode_ms": 2433.2131000001027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 59.41979999988689, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 286.099919555979}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547697.0216827, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 59.41979999988689, "prefill_cuda_event_ms": null, "kv_decode_ms": 2433.2131000001027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2433.2131000001027, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.302669503134474}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547697.0216827, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 59.41979999988689, "prefill_cuda_event_ms": null, "kv_decode_ms": 2433.2131000001027, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2492.6328999999896, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 32.49575980482338}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547699.5152032, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 63.84810000054131, "prefill_cuda_event_ms": null, "kv_decode_ms": 2333.604700000251, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 63.84810000054131, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 266.2569442137804}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547699.5152032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 63.84810000054131, "prefill_cuda_event_ms": null, "kv_decode_ms": 2333.604700000251, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2333.604700000251, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.42538185665855}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547699.5152032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 63.84810000054131, "prefill_cuda_event_ms": null, "kv_decode_ms": 2333.604700000251, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2397.452800000792, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 33.78585805734037}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547701.9132702, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 60.892100000273786, "prefill_cuda_event_ms": null, "kv_decode_ms": 2273.8706000000093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 60.892100000273786, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 279.18235698758235}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547701.9132702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.892100000273786, "prefill_cuda_event_ms": null, "kv_decode_ms": 2273.8706000000093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2273.8706000000093, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.14584084072319}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547701.9132702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.892100000273786, "prefill_cuda_event_ms": null, "kv_decode_ms": 2273.8706000000093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2334.762700000283, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 34.69303325772258}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547704.248599, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 44.6059999994759, "prefill_cuda_event_ms": null, "kv_decode_ms": 2562.5946000000113, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 44.6059999994759, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 381.1146482580761}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547704.248599, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 44.6059999994759, "prefill_cuda_event_ms": null, "kv_decode_ms": 2562.5946000000113, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2562.5946000000113, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.97468776372186}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547704.248599, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 44.6059999994759, "prefill_cuda_event_ms": null, "kv_decode_ms": 2562.5946000000113, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2607.200599999487, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 31.067805062646862}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547706.856405, "prompt_tokens": 99, "prefill_ms": 1018.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 3201.2612, "kv_decode_ms_equiv": 3201.2612, "kv_decode_ms_per_token": 50.01970625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 14365.416799999366, "ollama_total_duration_ms": 14216.6495, "ollama_load_ms": 9900.6633, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1018.3132, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 97.21959805686501}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547706.856405, "prompt_tokens": 99, "prefill_ms": 1018.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 3201.2612, "kv_decode_ms_equiv": 3201.2612, "kv_decode_ms_per_token": 50.01970625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14365.416799999366, "ollama_total_duration_ms": 14216.6495, "ollama_load_ms": 9900.6633, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3201.2612, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.992120605466372}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547706.856405, "prompt_tokens": 99, "prefill_ms": 1018.3132, "prefill_cuda_event_ms": null, "kv_decode_ms": 3201.2612, "kv_decode_ms_equiv": 3201.2612, "kv_decode_ms_per_token": 50.01970625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14365.416799999366, "ollama_total_duration_ms": 14216.6495, "ollama_load_ms": 9900.6633, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 4219.5743999999995, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 38.62948831995947}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547721.2226088, "prompt_tokens": 99, "prefill_ms": 61.3462, "prefill_cuda_event_ms": null, "kv_decode_ms": 3616.0981, "kv_decode_ms_equiv": 3616.0981, "kv_decode_ms_per_token": 56.5015328125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3959.764900000664, "ollama_total_duration_ms": 3956.7804, "ollama_load_ms": 261.8838, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.3462, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1613.7918893101773}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547721.2226088, "prompt_tokens": 99, "prefill_ms": 61.3462, "prefill_cuda_event_ms": null, "kv_decode_ms": 3616.0981, "kv_decode_ms_equiv": 3616.0981, "kv_decode_ms_per_token": 56.5015328125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3959.764900000664, "ollama_total_duration_ms": 3956.7804, "ollama_load_ms": 261.8838, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3616.0981, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.69863489046384}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547721.2226088, "prompt_tokens": 99, "prefill_ms": 61.3462, "prefill_cuda_event_ms": null, "kv_decode_ms": 3616.0981, "kv_decode_ms_equiv": 3616.0981, "kv_decode_ms_per_token": 56.5015328125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3959.764900000664, "ollama_total_duration_ms": 3956.7804, "ollama_load_ms": 261.8838, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3677.4443, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 44.324260737273434}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547725.182545, "prompt_tokens": 99, "prefill_ms": 72.2725, "prefill_cuda_event_ms": null, "kv_decode_ms": 3868.1117, "kv_decode_ms_equiv": 3868.1117, "kv_decode_ms_per_token": 60.4392453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4213.167500000054, "ollama_total_duration_ms": 4198.4204, "ollama_load_ms": 234.4305, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 72.2725, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1369.8156283510327}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547725.182545, "prompt_tokens": 99, "prefill_ms": 72.2725, "prefill_cuda_event_ms": null, "kv_decode_ms": 3868.1117, "kv_decode_ms_equiv": 3868.1117, "kv_decode_ms_per_token": 60.4392453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4213.167500000054, "ollama_total_duration_ms": 4198.4204, "ollama_load_ms": 234.4305, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3868.1117, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.545540812588218}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547725.182545, "prompt_tokens": 99, "prefill_ms": 72.2725, "prefill_cuda_event_ms": null, "kv_decode_ms": 3868.1117, "kv_decode_ms_equiv": 3868.1117, "kv_decode_ms_per_token": 60.4392453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4213.167500000054, "ollama_total_duration_ms": 4198.4204, "ollama_load_ms": 234.4305, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3940.3842, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 41.36652461452871}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547729.3959408, "prompt_tokens": 99, "prefill_ms": 67.3786, "prefill_cuda_event_ms": null, "kv_decode_ms": 3744.8109, "kv_decode_ms_equiv": 3744.8109, "kv_decode_ms_per_token": 58.5126703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4097.503799999686, "ollama_total_duration_ms": 4078.6656, "ollama_load_ms": 252.6827, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 67.3786, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1469.309246556028}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547729.3959408, "prompt_tokens": 99, "prefill_ms": 67.3786, "prefill_cuda_event_ms": null, "kv_decode_ms": 3744.8109, "kv_decode_ms_equiv": 3744.8109, "kv_decode_ms_per_token": 58.5126703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4097.503799999686, "ollama_total_duration_ms": 4078.6656, "ollama_load_ms": 252.6827, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3744.8109, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.090315561728364}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547729.3959408, "prompt_tokens": 99, "prefill_ms": 67.3786, "prefill_cuda_event_ms": null, "kv_decode_ms": 3744.8109, "kv_decode_ms_equiv": 3744.8109, "kv_decode_ms_per_token": 58.5126703125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4097.503799999686, "ollama_total_duration_ms": 4078.6656, "ollama_load_ms": 252.6827, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3812.1895, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 42.757580650174916}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547733.4936652, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.42429999957676, "prefill_cuda_event_ms": null, "kv_decode_ms": 1458.0070000001797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 50.42429999957676, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 337.1390381253223}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547733.4936652, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.42429999957676, "prefill_cuda_event_ms": null, "kv_decode_ms": 1458.0070000001797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1458.0070000001797, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 43.895536852698314}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547733.4936652, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.42429999957676, "prefill_cuda_event_ms": null, "kv_decode_ms": 1458.0070000001797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1508.4312999997564, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 53.69816974761335}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547735.0034356, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 38.11310000037338, "prefill_cuda_event_ms": null, "kv_decode_ms": 1659.0132999999696, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 38.11310000037338, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 446.04086258618315}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547735.0034356, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.11310000037338, "prefill_cuda_event_ms": null, "kv_decode_ms": 1659.0132999999696, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1659.0132999999696, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 38.57714703070866}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547735.0034356, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.11310000037338, "prefill_cuda_event_ms": null, "kv_decode_ms": 1659.0132999999696, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1697.126400000343, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 47.72773554166834}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547736.7011945, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 35.987300000670075, "prefill_cuda_event_ms": null, "kv_decode_ms": 1887.8016999997271, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 35.987300000670075, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 472.3888705094148}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547736.7011945, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.987300000670075, "prefill_cuda_event_ms": null, "kv_decode_ms": 1887.8016999997271, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1887.8016999997271, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.901865858055565}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547736.7011945, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.987300000670075, "prefill_cuda_event_ms": null, "kv_decode_ms": 1887.8016999997271, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1923.7890000003972, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 42.104409579212316}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547738.6258104, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 32.92310000051657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1716.3441000002422, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 32.92310000051657, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 516.3547782478948}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547738.6258104, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 32.92310000051657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1716.3441000002422, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1716.3441000002422, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 37.28856002708954}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547738.6258104, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 32.92310000051657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1716.3441000002422, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1749.2672000007587, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 46.305104217334474}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547740.375679, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.691000000108033, "prefill_cuda_event_ms": 13.201408386230469, "kv_decode_ms": 377.8191000001243, "kv_decode_cuda_event_ms": 377.37994384765625, "gpu_peak_mb": 579.873046875, "hf_load_ms": 274.13109999997687, "params_millions_measured": 5.03672, "latency_ms": 13.691000000108033, "cuda_event_ms": 13.201408386230469, "tokens_total": 17, "tokens_per_s": 1241.6916222237862}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547740.375679, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.691000000108033, "prefill_cuda_event_ms": 13.201408386230469, "kv_decode_ms": 377.8191000001243, "kv_decode_cuda_event_ms": 377.37994384765625, "gpu_peak_mb": 579.873046875, "hf_load_ms": 274.13109999997687, "params_millions_measured": 5.03672, "latency_ms": 377.8191000001243, "cuda_event_ms": 377.37994384765625, "tokens_total": 64, "tokens_per_s": 169.3932360750924}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547740.375679, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.691000000108033, "prefill_cuda_event_ms": 13.201408386230469, "kv_decode_ms": 377.8191000001243, "kv_decode_cuda_event_ms": 377.37994384765625, "gpu_peak_mb": 579.873046875, "hf_load_ms": 274.13109999997687, "params_millions_measured": 5.03672, "latency_ms": 391.51010000023234, "cuda_event_ms": 390.5813522338867, "tokens_total": 81, "tokens_per_s": 206.89121430060663}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547741.0436218, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.411800000227231, "prefill_cuda_event_ms": 7.341119766235352, "kv_decode_ms": 398.39000000029046, "kv_decode_cuda_event_ms": 398.33673095703125, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 7.411800000227231, "cuda_event_ms": 7.341119766235352, "tokens_total": 17, "tokens_per_s": 2293.6398714858487}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547741.0436218, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.411800000227231, "prefill_cuda_event_ms": 7.341119766235352, "kv_decode_ms": 398.39000000029046, "kv_decode_cuda_event_ms": 398.33673095703125, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 398.39000000029046, "cuda_event_ms": 398.33673095703125, "tokens_total": 64, "tokens_per_s": 160.64660257524872}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547741.0436218, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.411800000227231, "prefill_cuda_event_ms": 7.341119766235352, "kv_decode_ms": 398.39000000029046, "kv_decode_cuda_event_ms": 398.33673095703125, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 405.8018000005177, "cuda_event_ms": 405.6778507232666, "tokens_total": 81, "tokens_per_s": 199.60483171808667}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547741.450197, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.832099999883212, "prefill_cuda_event_ms": 6.776832103729248, "kv_decode_ms": 379.8296000004484, "kv_decode_cuda_event_ms": 379.7767639160156, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 6.832099999883212, "cuda_event_ms": 6.776832103729248, "tokens_total": 17, "tokens_per_s": 2488.253977589701}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547741.450197, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.832099999883212, "prefill_cuda_event_ms": 6.776832103729248, "kv_decode_ms": 379.8296000004484, "kv_decode_cuda_event_ms": 379.7767639160156, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 379.8296000004484, "cuda_event_ms": 379.7767639160156, "tokens_total": 64, "tokens_per_s": 168.49661005862745}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547741.450197, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.832099999883212, "prefill_cuda_event_ms": 6.776832103729248, "kv_decode_ms": 379.8296000004484, "kv_decode_cuda_event_ms": 379.7767639160156, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 386.6617000003316, "cuda_event_ms": 386.5535960197449, "tokens_total": 81, "tokens_per_s": 209.48544942498967}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547741.8375463, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.684799999675306, "prefill_cuda_event_ms": 6.627327919006348, "kv_decode_ms": 375.18120000004274, "kv_decode_cuda_event_ms": 375.1393127441406, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 6.684799999675306, "cuda_event_ms": 6.627327919006348, "tokens_total": 17, "tokens_per_s": 2543.082814867419}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547741.8375463, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.684799999675306, "prefill_cuda_event_ms": 6.627327919006348, "kv_decode_ms": 375.18120000004274, "kv_decode_cuda_event_ms": 375.1393127441406, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 375.18120000004274, "cuda_event_ms": 375.1393127441406, "tokens_total": 64, "tokens_per_s": 170.58424036170445}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547741.8375463, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.684799999675306, "prefill_cuda_event_ms": 6.627327919006348, "kv_decode_ms": 375.18120000004274, "kv_decode_cuda_event_ms": 375.1393127441406, "gpu_peak_mb": 579.873046875, "params_millions_measured": 5.03672, "latency_ms": 381.86599999971804, "cuda_event_ms": 381.766640663147, "tokens_total": 81, "tokens_per_s": 212.11629210262188}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547742.2200744, "prompt_tokens": 25, "prefill_ms": 8.3177, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.8968, "kv_decode_ms_equiv": 141.8968, "kv_decode_ms_per_token": 2.2171375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1803.3026000002792, "ollama_total_duration_ms": 1718.6657, "ollama_load_ms": 1495.6142, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.3177, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3005.638577972276}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547742.2200744, "prompt_tokens": 25, "prefill_ms": 8.3177, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.8968, "kv_decode_ms_equiv": 141.8968, "kv_decode_ms_per_token": 2.2171375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1803.3026000002792, "ollama_total_duration_ms": 1718.6657, "ollama_load_ms": 1495.6142, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 141.8968, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 451.03201763535185}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547742.2200744, "prompt_tokens": 25, "prefill_ms": 8.3177, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.8968, "kv_decode_ms_equiv": 141.8968, "kv_decode_ms_per_token": 2.2171375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1803.3026000002792, "ollama_total_duration_ms": 1718.6657, "ollama_load_ms": 1495.6142, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 150.21450000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 592.4860782414479}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547744.0236056, "prompt_tokens": 25, "prefill_ms": 2.4514, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.8633, "kv_decode_ms_equiv": 137.8633, "kv_decode_ms_per_token": 2.1541140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 386.16689999980736, "ollama_total_duration_ms": 334.2002, "ollama_load_ms": 156.9664, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4514, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10198.254058905117}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547744.0236056, "prompt_tokens": 25, "prefill_ms": 2.4514, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.8633, "kv_decode_ms_equiv": 137.8633, "kv_decode_ms_per_token": 2.1541140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 386.16689999980736, "ollama_total_duration_ms": 334.2002, "ollama_load_ms": 156.9664, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.8633, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 464.22797075073635}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547744.0236056, "prompt_tokens": 25, "prefill_ms": 2.4514, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.8633, "kv_decode_ms_equiv": 137.8633, "kv_decode_ms_per_token": 2.1541140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 386.16689999980736, "ollama_total_duration_ms": 334.2002, "ollama_load_ms": 156.9664, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 140.31470000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 634.2884957883956}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547744.4099095, "prompt_tokens": 25, "prefill_ms": 2.383, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.8013, "kv_decode_ms_equiv": 131.8013, "kv_decode_ms_per_token": 2.0593953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 348.71730000031675, "ollama_total_duration_ms": 312.8541, "ollama_load_ms": 140.2017, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.383, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10490.97775912715}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547744.4099095, "prompt_tokens": 25, "prefill_ms": 2.383, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.8013, "kv_decode_ms_equiv": 131.8013, "kv_decode_ms_per_token": 2.0593953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 348.71730000031675, "ollama_total_duration_ms": 312.8541, "ollama_load_ms": 140.2017, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.8013, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 485.5794290344632}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547744.4099095, "prompt_tokens": 25, "prefill_ms": 2.383, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.8013, "kv_decode_ms_equiv": 131.8013, "kv_decode_ms_per_token": 2.0593953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 348.71730000031675, "ollama_total_duration_ms": 312.8541, "ollama_load_ms": 140.2017, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.1843, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 663.2668650505312}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547744.758761, "prompt_tokens": 25, "prefill_ms": 2.2887, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.3307, "kv_decode_ms_equiv": 133.3307, "kv_decode_ms_per_token": 2.0832921875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.01130000022386, "ollama_total_duration_ms": 332.4307, "ollama_load_ms": 154.823, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2887, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10923.231528815486}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547744.758761, "prompt_tokens": 25, "prefill_ms": 2.2887, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.3307, "kv_decode_ms_equiv": 133.3307, "kv_decode_ms_per_token": 2.0832921875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.01130000022386, "ollama_total_duration_ms": 332.4307, "ollama_load_ms": 154.823, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 133.3307, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 480.0094801872337}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547744.758761, "prompt_tokens": 25, "prefill_ms": 2.2887, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.3307, "kv_decode_ms_equiv": 133.3307, "kv_decode_ms_per_token": 2.0832921875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.01130000022386, "ollama_total_duration_ms": 332.4307, "ollama_load_ms": 154.823, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.6194, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 656.2482948604699}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547745.1190603, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.936799999821233, "prefill_cuda_event_ms": 21.797887802124023, "kv_decode_ms": 796.3079000001017, "kv_decode_cuda_event_ms": 795.9744873046875, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 21.936799999821233, "cuda_event_ms": 21.797887802124023, "tokens_total": 9, "tokens_per_s": 410.2695014803136}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547745.1190603, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.936799999821233, "prefill_cuda_event_ms": 21.797887802124023, "kv_decode_ms": 796.3079000001017, "kv_decode_cuda_event_ms": 795.9744873046875, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 796.3079000001017, "cuda_event_ms": 795.9744873046875, "tokens_total": 64, "tokens_per_s": 80.37092185069598}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547745.1190603, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.936799999821233, "prefill_cuda_event_ms": 21.797887802124023, "kv_decode_ms": 796.3079000001017, "kv_decode_cuda_event_ms": 795.9744873046875, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 818.2446999999229, "cuda_event_ms": 817.7723751068115, "tokens_total": 73, "tokens_per_s": 89.21536552574906}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547745.9391189, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 18.14569999987725, "prefill_cuda_event_ms": 18.04070472717285, "kv_decode_ms": 857.9739000006157, "kv_decode_cuda_event_ms": 857.9122924804688, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 18.14569999987725, "cuda_event_ms": 18.04070472717285, "tokens_total": 9, "tokens_per_s": 495.98527475164263}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547745.9391189, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.14569999987725, "prefill_cuda_event_ms": 18.04070472717285, "kv_decode_ms": 857.9739000006157, "kv_decode_cuda_event_ms": 857.9122924804688, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 857.9739000006157, "cuda_event_ms": 857.9122924804688, "tokens_total": 64, "tokens_per_s": 74.59434372065873}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547745.9391189, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.14569999987725, "prefill_cuda_event_ms": 18.04070472717285, "kv_decode_ms": 857.9739000006157, "kv_decode_cuda_event_ms": 857.9122924804688, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 876.1196000004929, "cuda_event_ms": 875.9529972076416, "tokens_total": 73, "tokens_per_s": 83.32195741307343}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547746.8160787, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 14.796899999964808, "prefill_cuda_event_ms": 14.693375587463379, "kv_decode_ms": 648.7876000001052, "kv_decode_cuda_event_ms": 648.7307739257812, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 14.796899999964808, "cuda_event_ms": 14.693375587463379, "tokens_total": 9, "tokens_per_s": 608.2355087904497}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547746.8160787, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 14.796899999964808, "prefill_cuda_event_ms": 14.693375587463379, "kv_decode_ms": 648.7876000001052, "kv_decode_cuda_event_ms": 648.7307739257812, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 648.7876000001052, "cuda_event_ms": 648.7307739257812, "tokens_total": 64, "tokens_per_s": 98.64553514892953}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547746.8160787, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 14.796899999964808, "prefill_cuda_event_ms": 14.693375587463379, "kv_decode_ms": 648.7876000001052, "kv_decode_cuda_event_ms": 648.7307739257812, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 663.58450000007, "cuda_event_ms": 663.4241495132446, "tokens_total": 73, "tokens_per_s": 110.00859724721163}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547747.4803793, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.36370000019815, "prefill_cuda_event_ms": 10.31167984008789, "kv_decode_ms": 703.9269000006243, "kv_decode_cuda_event_ms": 703.8688354492188, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 10.36370000019815, "cuda_event_ms": 10.31167984008789, "tokens_total": 9, "tokens_per_s": 868.4157202377455}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547747.4803793, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.36370000019815, "prefill_cuda_event_ms": 10.31167984008789, "kv_decode_ms": 703.9269000006243, "kv_decode_cuda_event_ms": 703.8688354492188, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 703.9269000006243, "cuda_event_ms": 703.8688354492188, "tokens_total": 64, "tokens_per_s": 90.91853145538725}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547747.4803793, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.36370000019815, "prefill_cuda_event_ms": 10.31167984008789, "kv_decode_ms": 703.9269000006243, "kv_decode_cuda_event_ms": 703.8688354492188, "gpu_peak_mb": 578.70068359375, "params_millions_measured": 5.03672, "latency_ms": 714.2906000008225, "cuda_event_ms": 714.1805152893066, "tokens_total": 73, "tokens_per_s": 102.19930095666378}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547748.195424, "prompt_tokens": 38, "prefill_ms": 24.3137, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.8823, "kv_decode_ms_equiv": 723.8990628571428, "kv_decode_ms_per_token": 11.310922857142856, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 6929.9913999993805, "ollama_total_duration_ms": 6915.3883, "ollama_load_ms": 6455.0976, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 24.3137, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1562.9048643357448}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547748.195424, "prompt_tokens": 38, "prefill_ms": 24.3137, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.8823, "kv_decode_ms_equiv": 723.8990628571428, "kv_decode_ms_per_token": 11.310922857142856, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6929.9913999993805, "ollama_total_duration_ms": 6915.3883, "ollama_load_ms": 6455.0976, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 723.8990628571428, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.41011583493378}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547748.195424, "prompt_tokens": 38, "prefill_ms": 24.3137, "prefill_cuda_event_ms": null, "kv_decode_ms": 395.8823, "kv_decode_ms_equiv": 723.8990628571428, "kv_decode_ms_per_token": 11.310922857142856, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6929.9913999993805, "ollama_total_duration_ms": 6915.3883, "ollama_load_ms": 6455.0976, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 748.2127628571428, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 136.32485980391516}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547755.1255434, "prompt_tokens": 38, "prefill_ms": 11.7186, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.9531, "kv_decode_ms_equiv": 725.8570971428571, "kv_decode_ms_per_token": 11.341517142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 588.6116000001493, "ollama_total_duration_ms": 574.091, "ollama_load_ms": 134.6902, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.7186, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3242.7081733312853}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547755.1255434, "prompt_tokens": 38, "prefill_ms": 11.7186, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.9531, "kv_decode_ms_equiv": 725.8570971428571, "kv_decode_ms_per_token": 11.341517142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 588.6116000001493, "ollama_total_duration_ms": 574.091, "ollama_load_ms": 134.6902, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 725.8570971428571, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.1716253129148}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547755.1255434, "prompt_tokens": 38, "prefill_ms": 11.7186, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.9531, "kv_decode_ms_equiv": 725.8570971428571, "kv_decode_ms_per_token": 11.341517142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 588.6116000001493, "ollama_total_duration_ms": 574.091, "ollama_load_ms": 134.6902, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 737.5756971428572, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 138.2908905419699}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547755.7143838, "prompt_tokens": 38, "prefill_ms": 11.4914, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.5413, "kv_decode_ms_equiv": 708.6469485714285, "kv_decode_ms_per_token": 11.072608571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 591.4235999998709, "ollama_total_duration_ms": 576.5565, "ollama_load_ms": 138.8879, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4914, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3306.820752910872}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547755.7143838, "prompt_tokens": 38, "prefill_ms": 11.4914, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.5413, "kv_decode_ms_equiv": 708.6469485714285, "kv_decode_ms_per_token": 11.072608571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 591.4235999998709, "ollama_total_duration_ms": 576.5565, "ollama_load_ms": 138.8879, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 708.6469485714285, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.31295503214754}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547755.7143838, "prompt_tokens": 38, "prefill_ms": 11.4914, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.5413, "kv_decode_ms_equiv": 708.6469485714285, "kv_decode_ms_per_token": 11.072608571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 591.4235999998709, "ollama_total_duration_ms": 576.5565, "ollama_load_ms": 138.8879, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.1383485714285, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.63945053383432}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547756.3059738, "prompt_tokens": 38, "prefill_ms": 11.4122, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8684, "kv_decode_ms_equiv": 720.2165028571429, "kv_decode_ms_per_token": 11.253382857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 605.1447999998345, "ollama_total_duration_ms": 578.3111, "ollama_load_ms": 137.4108, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4122, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3329.7698953751246}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547756.3059738, "prompt_tokens": 38, "prefill_ms": 11.4122, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8684, "kv_decode_ms_equiv": 720.2165028571429, "kv_decode_ms_per_token": 11.253382857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 605.1447999998345, "ollama_total_duration_ms": 578.3111, "ollama_load_ms": 137.4108, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.2165028571429, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.86216817596943}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547756.3059738, "prompt_tokens": 38, "prefill_ms": 11.4122, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8684, "kv_decode_ms_equiv": 720.2165028571429, "kv_decode_ms_per_token": 11.253382857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 605.1447999998345, "ollama_total_duration_ms": 578.3111, "ollama_load_ms": 137.4108, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 731.6287028571429, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 139.41497866564214}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547756.9112651, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.857200000216835, "prefill_cuda_event_ms": 7.729152202606201, "kv_decode_ms": 287.37100000034843, "kv_decode_cuda_event_ms": 287.2944641113281, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 314.4027999996979, "params_millions_measured": 25.016064, "latency_ms": 7.857200000216835, "cuda_event_ms": 7.729152202606201, "tokens_total": 17, "tokens_per_s": 2163.6206281539035}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547756.9112651, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.857200000216835, "prefill_cuda_event_ms": 7.729152202606201, "kv_decode_ms": 287.37100000034843, "kv_decode_cuda_event_ms": 287.2944641113281, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 314.4027999996979, "params_millions_measured": 25.016064, "latency_ms": 287.37100000034843, "cuda_event_ms": 287.2944641113281, "tokens_total": 64, "tokens_per_s": 222.70862404321383}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547756.9112651, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.857200000216835, "prefill_cuda_event_ms": 7.729152202606201, "kv_decode_ms": 287.37100000034843, "kv_decode_cuda_event_ms": 287.2944641113281, "gpu_peak_mb": 631.22314453125, "hf_load_ms": 314.4027999996979, "params_millions_measured": 25.016064, "latency_ms": 295.22820000056527, "cuda_event_ms": 295.0236163139343, "tokens_total": 81, "tokens_per_s": 274.3640343295285}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547757.5220623, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.835700000716315, "prefill_cuda_event_ms": 5.739520072937012, "kv_decode_ms": 251.9611999996414, "kv_decode_cuda_event_ms": 251.90809631347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 5.835700000716315, "cuda_event_ms": 5.739520072937012, "tokens_total": 17, "tokens_per_s": 2913.1038260899804}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547757.5220623, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.835700000716315, "prefill_cuda_event_ms": 5.739520072937012, "kv_decode_ms": 251.9611999996414, "kv_decode_cuda_event_ms": 251.90809631347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 251.9611999996414, "cuda_event_ms": 251.90809631347656, "tokens_total": 64, "tokens_per_s": 254.0073630387976}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547757.5220623, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.835700000716315, "prefill_cuda_event_ms": 5.739520072937012, "kv_decode_ms": 251.9611999996414, "kv_decode_cuda_event_ms": 251.90809631347656, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 257.7969000003577, "cuda_event_ms": 257.6476163864136, "tokens_total": 81, "tokens_per_s": 314.2008301879798}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547757.7804554, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.015599999547703, "prefill_cuda_event_ms": 3.957632064819336, "kv_decode_ms": 209.9616999994396, "kv_decode_cuda_event_ms": 209.89234924316406, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 4.015599999547703, "cuda_event_ms": 3.957632064819336, "tokens_total": 17, "tokens_per_s": 4233.489391850481}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547757.7804554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.015599999547703, "prefill_cuda_event_ms": 3.957632064819336, "kv_decode_ms": 209.9616999994396, "kv_decode_cuda_event_ms": 209.89234924316406, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 209.9616999994396, "cuda_event_ms": 209.89234924316406, "tokens_total": 64, "tokens_per_s": 304.8174976682453}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547757.7804554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.015599999547703, "prefill_cuda_event_ms": 3.957632064819336, "kv_decode_ms": 209.9616999994396, "kv_decode_cuda_event_ms": 209.89234924316406, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 213.9772999989873, "cuda_event_ms": 213.8499813079834, "tokens_total": 81, "tokens_per_s": 378.54482695306166}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547757.9949882, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.443500000183121, "prefill_cuda_event_ms": 5.345280170440674, "kv_decode_ms": 221.19670000029146, "kv_decode_cuda_event_ms": 221.09727478027344, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 5.443500000183121, "cuda_event_ms": 5.345280170440674, "tokens_total": 17, "tokens_per_s": 3122.990722775441}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547757.9949882, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.443500000183121, "prefill_cuda_event_ms": 5.345280170440674, "kv_decode_ms": 221.19670000029146, "kv_decode_cuda_event_ms": 221.09727478027344, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 221.19670000029146, "cuda_event_ms": 221.09727478027344, "tokens_total": 64, "tokens_per_s": 289.3352387260555}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547757.9949882, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.443500000183121, "prefill_cuda_event_ms": 5.345280170440674, "kv_decode_ms": 221.19670000029146, "kv_decode_cuda_event_ms": 221.09727478027344, "gpu_peak_mb": 631.22314453125, "params_millions_measured": 25.016064, "latency_ms": 226.64020000047458, "cuda_event_ms": 226.4425549507141, "tokens_total": 81, "tokens_per_s": 357.3946722595126}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547758.2225614, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 40.91299999981857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.4594999998662, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 170.21380000005593, "params_millions_measured": 25.016064, "latency_ms": 40.91299999981857, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 219.97897978735148}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547758.2225614, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 40.91299999981857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.4594999998662, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 170.21380000005593, "params_millions_measured": 25.016064, "latency_ms": 1084.4594999998662, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.01557411780513}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547758.2225614, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 40.91299999981857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.4594999998662, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 170.21380000005593, "params_millions_measured": 25.016064, "latency_ms": 1125.3724999996848, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 64.86741056851882}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547759.5186799, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 23.164700000052107, "prefill_cuda_event_ms": null, "kv_decode_ms": 1060.5932999997094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 23.164700000052107, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 388.5221910916073}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547759.5186799, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 23.164700000052107, "prefill_cuda_event_ms": null, "kv_decode_ms": 1060.5932999997094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1060.5932999997094, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.343583162384235}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547759.5186799, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 23.164700000052107, "prefill_cuda_event_ms": null, "kv_decode_ms": 1060.5932999997094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1083.7579999997615, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 67.35821096593156}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547760.6030948, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.545299999772396, "prefill_cuda_event_ms": null, "kv_decode_ms": 1082.9925999996703, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 24.545299999772396, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 366.668975326578}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547760.6030948, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.545299999772396, "prefill_cuda_event_ms": null, "kv_decode_ms": 1082.9925999996703, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1082.9925999996703, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.09550997857186}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547760.6030948, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.545299999772396, "prefill_cuda_event_ms": null, "kv_decode_ms": 1082.9925999996703, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1107.5378999994427, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 65.91196563118673}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547761.7112265, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.96129999983532, "prefill_cuda_event_ms": null, "kv_decode_ms": 1069.5580999999947, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 25.96129999983532, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 346.6698508956443}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547761.7112265, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.96129999983532, "prefill_cuda_event_ms": null, "kv_decode_ms": 1069.5580999999947, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1069.5580999999947, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.83779656289856}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547761.7112265, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.96129999983532, "prefill_cuda_event_ms": null, "kv_decode_ms": 1069.5580999999947, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1095.51939999983, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 66.63505913269206}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547762.8074157, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.141000000279746, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.0414999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 26.141000000279746, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 650.3194215912963}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547762.8074157, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.141000000279746, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.0414999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1029.0414999999484, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.193798792374466}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547762.8074157, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.141000000279746, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.0414999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1055.1825000002282, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 76.76397210907353}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547763.8634558, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.58640000016021, "prefill_cuda_event_ms": null, "kv_decode_ms": 1026.2185999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.58640000016021, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 787.5328910737237}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547763.8634558, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.58640000016021, "prefill_cuda_event_ms": null, "kv_decode_ms": 1026.2185999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1026.2185999999929, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.36488015321535}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547763.8634558, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.58640000016021, "prefill_cuda_event_ms": null, "kv_decode_ms": 1026.2185999999929, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1047.805000000153, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 77.30446027647145}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547764.9124703, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.011300000187475, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.868899999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 25.011300000187475, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 679.6927788588588}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547764.9124703, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.011300000187475, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.868899999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1064.868899999965, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.101295098393905}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547764.9124703, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.011300000187475, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.868899999965, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1089.8802000001524, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 74.3200949975866}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547766.0030377, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 24.317899999914516, "prefill_cuda_event_ms": null, "kv_decode_ms": 1061.5764000003765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 24.317899999914516, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 699.0735219759831}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547766.0030377, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.317899999914516, "prefill_cuda_event_ms": null, "kv_decode_ms": 1061.5764000003765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1061.5764000003765, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.28770044245266}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547766.0030377, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.317899999914516, "prefill_cuda_event_ms": null, "kv_decode_ms": 1061.5764000003765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1085.894300000291, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 74.59289545951047}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547767.0899239, "prompt_tokens": 46, "prefill_ms": 368.334, "prefill_cuda_event_ms": null, "kv_decode_ms": 872.595, "kv_decode_ms_equiv": 872.595, "kv_decode_ms_per_token": 13.634296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1444.99149999956, "ollama_total_duration_ms": 1440.0097, "ollama_load_ms": 138.1314, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 368.334, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 124.88665178886554}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547767.0899239, "prompt_tokens": 46, "prefill_ms": 368.334, "prefill_cuda_event_ms": null, "kv_decode_ms": 872.595, "kv_decode_ms_equiv": 872.595, "kv_decode_ms_per_token": 13.634296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1444.99149999956, "ollama_total_duration_ms": 1440.0097, "ollama_load_ms": 138.1314, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 872.595, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.34444960147606}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547767.0899239, "prompt_tokens": 46, "prefill_ms": 368.334, "prefill_cuda_event_ms": null, "kv_decode_ms": 872.595, "kv_decode_ms_equiv": 872.595, "kv_decode_ms_per_token": 13.634296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1444.99149999956, "ollama_total_duration_ms": 1440.0097, "ollama_load_ms": 138.1314, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1240.929, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 88.64326645601803}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547768.5350237, "prompt_tokens": 46, "prefill_ms": 13.1014, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.0827, "kv_decode_ms_equiv": 749.0827, "kv_decode_ms_per_token": 11.7044171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.952699999747, "ollama_total_duration_ms": 951.7693, "ollama_load_ms": 130.0426, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 13.1014, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3511.075152273803}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547768.5350237, "prompt_tokens": 46, "prefill_ms": 13.1014, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.0827, "kv_decode_ms_equiv": 749.0827, "kv_decode_ms_per_token": 11.7044171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.952699999747, "ollama_total_duration_ms": 951.7693, "ollama_load_ms": 130.0426, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 749.0827, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 85.4378294946606}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547768.5350237, "prompt_tokens": 46, "prefill_ms": 13.1014, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.0827, "kv_decode_ms_equiv": 749.0827, "kv_decode_ms_per_token": 11.7044171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.952699999747, "ollama_total_duration_ms": 951.7693, "ollama_load_ms": 130.0426, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 762.1841000000001, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 144.32208701283588}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547769.507097, "prompt_tokens": 46, "prefill_ms": 11.807, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.5796, "kv_decode_ms_equiv": 736.5796, "kv_decode_ms_per_token": 11.50905625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6801000005871, "ollama_total_duration_ms": 936.3928, "ollama_load_ms": 134.5506, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.807, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3895.9939019225885}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547769.507097, "prompt_tokens": 46, "prefill_ms": 11.807, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.5796, "kv_decode_ms_equiv": 736.5796, "kv_decode_ms_per_token": 11.50905625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6801000005871, "ollama_total_duration_ms": 936.3928, "ollama_load_ms": 134.5506, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 736.5796, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 86.8880973624575}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547769.507097, "prompt_tokens": 46, "prefill_ms": 11.807, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.5796, "kv_decode_ms_equiv": 736.5796, "kv_decode_ms_per_token": 11.50905625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6801000005871, "ollama_total_duration_ms": 936.3928, "ollama_load_ms": 134.5506, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 748.3866, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 146.9828561868959}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547770.4619434, "prompt_tokens": 46, "prefill_ms": 12.2493, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.3603, "kv_decode_ms_equiv": 732.3603, "kv_decode_ms_per_token": 11.4431296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6871000002284, "ollama_total_duration_ms": 930.8098, "ollama_load_ms": 137.5216, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.2493, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3755.3166303380603}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547770.4619434, "prompt_tokens": 46, "prefill_ms": 12.2493, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.3603, "kv_decode_ms_equiv": 732.3603, "kv_decode_ms_per_token": 11.4431296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6871000002284, "ollama_total_duration_ms": 930.8098, "ollama_load_ms": 137.5216, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 732.3603, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.38868013462772}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547770.4619434, "prompt_tokens": 46, "prefill_ms": 12.2493, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.3603, "kv_decode_ms_equiv": 732.3603, "kv_decode_ms_per_token": 11.4431296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 954.6871000002284, "ollama_total_duration_ms": 930.8098, "ollama_load_ms": 137.5216, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 744.6096, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.72842036954668}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547771.4168074, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 100.61729999961244, "prefill_cuda_event_ms": null, "kv_decode_ms": 1936.561399999846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 100.61729999961244, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 89.44783849332735}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547771.4168074, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 100.61729999961244, "prefill_cuda_event_ms": null, "kv_decode_ms": 1936.561399999846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1936.561399999846, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.048267924789315}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547771.4168074, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 100.61729999961244, "prefill_cuda_event_ms": null, "kv_decode_ms": 1936.561399999846, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2037.1786999994583, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 35.83387161863582}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547773.4557793, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 41.305100000499806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1913.7096000004021, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 41.305100000499806, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 217.89076893388702}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547773.4557793, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 41.305100000499806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1913.7096000004021, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1913.7096000004021, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.442900636536784}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547773.4557793, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 41.305100000499806, "prefill_cuda_event_ms": null, "kv_decode_ms": 1913.7096000004021, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1955.014700000902, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 37.339872687385075}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547775.4115965, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.57479999987845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.2493999998042, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 38.57479999987845, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 233.31294005486376}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547775.4115965, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.57479999987845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.2493999998042, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1923.2493999998042, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.27701545104161}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547775.4115965, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.57479999987845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1923.2493999998042, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1961.8241999996826, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 37.21026583320351}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547777.3739445, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 37.85280000010971, "prefill_cuda_event_ms": null, "kv_decode_ms": 1924.025399999664, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 37.85280000010971, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 237.76312452378463}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547777.3739445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.85280000010971, "prefill_cuda_event_ms": null, "kv_decode_ms": 1924.025399999664, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1924.025399999664, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.263594129272505}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547777.3739445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.85280000010971, "prefill_cuda_event_ms": null, "kv_decode_ms": 1924.025399999664, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1961.8781999997736, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 37.209241633863115}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547779.3363602, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 15.580899999804387, "prefill_cuda_event_ms": 15.465472221374512, "kv_decode_ms": 302.3728000007395, "kv_decode_cuda_event_ms": 302.2550964355469, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 15.580899999804387, "cuda_event_ms": 15.465472221374512, "tokens_total": 17, "tokens_per_s": 1091.0794626891534}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547779.3363602, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.580899999804387, "prefill_cuda_event_ms": 15.465472221374512, "kv_decode_ms": 302.3728000007395, "kv_decode_cuda_event_ms": 302.2550964355469, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 302.3728000007395, "cuda_event_ms": 302.2550964355469, "tokens_total": 64, "tokens_per_s": 211.6592497732715}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547779.3363602, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.580899999804387, "prefill_cuda_event_ms": 15.465472221374512, "kv_decode_ms": 302.3728000007395, "kv_decode_cuda_event_ms": 302.2550964355469, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 317.9537000005439, "cuda_event_ms": 317.7205686569214, "tokens_total": 81, "tokens_per_s": 254.75407268373175}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547779.6572716, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.6562000004778383, "prefill_cuda_event_ms": 3.601408004760742, "kv_decode_ms": 158.43820000009146, "kv_decode_cuda_event_ms": 158.4114532470703, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 3.6562000004778383, "cuda_event_ms": 3.601408004760742, "tokens_total": 17, "tokens_per_s": 4649.636233733993}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547779.6572716, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.6562000004778383, "prefill_cuda_event_ms": 3.601408004760742, "kv_decode_ms": 158.43820000009146, "kv_decode_cuda_event_ms": 158.4114532470703, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 158.43820000009146, "cuda_event_ms": 158.4114532470703, "tokens_total": 64, "tokens_per_s": 403.9429884962279}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547779.6572716, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.6562000004778383, "prefill_cuda_event_ms": 3.601408004760742, "kv_decode_ms": 158.43820000009146, "kv_decode_cuda_event_ms": 158.4114532470703, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 162.0944000005693, "cuda_event_ms": 162.01286125183105, "tokens_total": 81, "tokens_per_s": 499.70881165367535}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547779.820121, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.501400000459398, "prefill_cuda_event_ms": 3.4536960124969482, "kv_decode_ms": 156.3350000005812, "kv_decode_cuda_event_ms": 156.30335998535156, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 3.501400000459398, "cuda_event_ms": 3.4536960124969482, "tokens_total": 17, "tokens_per_s": 4855.200776195102}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547779.820121, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.501400000459398, "prefill_cuda_event_ms": 3.4536960124969482, "kv_decode_ms": 156.3350000005812, "kv_decode_cuda_event_ms": 156.30335998535156, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 156.3350000005812, "cuda_event_ms": 156.30335998535156, "tokens_total": 64, "tokens_per_s": 409.3772987479583}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547779.820121, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.501400000459398, "prefill_cuda_event_ms": 3.4536960124969482, "kv_decode_ms": 156.3350000005812, "kv_decode_cuda_event_ms": 156.30335998535156, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 159.8364000010406, "cuda_event_ms": 159.7570559978485, "tokens_total": 81, "tokens_per_s": 506.7681704509902}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547779.9805398, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.3726000001479406, "prefill_cuda_event_ms": 3.3310720920562744, "kv_decode_ms": 165.9492999997383, "kv_decode_cuda_event_ms": 165.91258239746094, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 3.3726000001479406, "cuda_event_ms": 3.3310720920562744, "tokens_total": 17, "tokens_per_s": 5040.621478756534}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547779.9805398, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.3726000001479406, "prefill_cuda_event_ms": 3.3310720920562744, "kv_decode_ms": 165.9492999997383, "kv_decode_cuda_event_ms": 165.91258239746094, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 165.9492999997383, "cuda_event_ms": 165.91258239746094, "tokens_total": 64, "tokens_per_s": 385.6599575900648}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547779.9805398, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.3726000001479406, "prefill_cuda_event_ms": 3.3310720920562744, "kv_decode_ms": 165.9492999997383, "kv_decode_cuda_event_ms": 165.91258239746094, "gpu_peak_mb": 632.05859375, "params_millions_measured": 74.824704, "latency_ms": 169.32189999988623, "cuda_event_ms": 169.2436544895172, "tokens_total": 81, "tokens_per_s": 478.3787566762151}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547780.1505294, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.334499999662512, "prefill_cuda_event_ms": 5.291007995605469, "kv_decode_ms": 268.0556000004799, "kv_decode_cuda_event_ms": 267.9789733886719, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.334499999662512, "cuda_event_ms": 5.291007995605469, "tokens_total": 9, "tokens_per_s": 1687.1309402135882}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547780.1505294, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.334499999662512, "prefill_cuda_event_ms": 5.291007995605469, "kv_decode_ms": 268.0556000004799, "kv_decode_cuda_event_ms": 267.9789733886719, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 268.0556000004799, "cuda_event_ms": 267.9789733886719, "tokens_total": 64, "tokens_per_s": 238.75643709695086}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547780.1505294, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.334499999662512, "prefill_cuda_event_ms": 5.291007995605469, "kv_decode_ms": 268.0556000004799, "kv_decode_cuda_event_ms": 267.9789733886719, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 273.3901000001424, "cuda_event_ms": 273.26998138427734, "tokens_total": 73, "tokens_per_s": 267.0177157108541}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547780.4246476, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.168699999558157, "prefill_cuda_event_ms": 5.111807823181152, "kv_decode_ms": 269.57880000009027, "kv_decode_cuda_event_ms": 269.5250244140625, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.168699999558157, "cuda_event_ms": 5.111807823181152, "tokens_total": 9, "tokens_per_s": 1741.2502178051268}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547780.4246476, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.168699999558157, "prefill_cuda_event_ms": 5.111807823181152, "kv_decode_ms": 269.57880000009027, "kv_decode_cuda_event_ms": 269.5250244140625, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 269.57880000009027, "cuda_event_ms": 269.5250244140625, "tokens_total": 64, "tokens_per_s": 237.40739256936587}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547780.4246476, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.168699999558157, "prefill_cuda_event_ms": 5.111807823181152, "kv_decode_ms": 269.57880000009027, "kv_decode_cuda_event_ms": 269.5250244140625, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 274.7474999996484, "cuda_event_ms": 274.63683223724365, "tokens_total": 73, "tokens_per_s": 265.6985049912862}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547780.7001152, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.185500000152388, "prefill_cuda_event_ms": 5.140480041503906, "kv_decode_ms": 262.80640000004496, "kv_decode_cuda_event_ms": 262.7489013671875, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.185500000152388, "cuda_event_ms": 5.140480041503906, "tokens_total": 9, "tokens_per_s": 1735.6089094080637}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547780.7001152, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.185500000152388, "prefill_cuda_event_ms": 5.140480041503906, "kv_decode_ms": 262.80640000004496, "kv_decode_cuda_event_ms": 262.7489013671875, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 262.80640000004496, "cuda_event_ms": 262.7489013671875, "tokens_total": 64, "tokens_per_s": 243.52527183504301}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547780.7001152, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.185500000152388, "prefill_cuda_event_ms": 5.140480041503906, "kv_decode_ms": 262.80640000004496, "kv_decode_cuda_event_ms": 262.7489013671875, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 267.99190000019735, "cuda_event_ms": 267.8893814086914, "tokens_total": 73, "tokens_per_s": 272.3962925743138}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547780.9686906, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.870099999607191, "prefill_cuda_event_ms": 4.81382417678833, "kv_decode_ms": 255.0216999998156, "kv_decode_cuda_event_ms": 254.95449829101562, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 4.870099999607191, "cuda_event_ms": 4.81382417678833, "tokens_total": 9, "tokens_per_s": 1848.0113346185735}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547780.9686906, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.870099999607191, "prefill_cuda_event_ms": 4.81382417678833, "kv_decode_ms": 255.0216999998156, "kv_decode_cuda_event_ms": 254.95449829101562, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 255.0216999998156, "cuda_event_ms": 254.95449829101562, "tokens_total": 64, "tokens_per_s": 250.95903603515416}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547780.9686906, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.870099999607191, "prefill_cuda_event_ms": 4.81382417678833, "kv_decode_ms": 255.0216999998156, "kv_decode_cuda_event_ms": 254.95449829101562, "gpu_peak_mb": 631.240234375, "params_millions_measured": 51.475968, "latency_ms": 259.8917999994228, "cuda_event_ms": 259.76832246780396, "tokens_total": 73, "tokens_per_s": 280.8861226101098}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547781.2294025, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 522.4748999999065, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.104100000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 134.45859999956156, "params_millions_measured": 74.824704, "latency_ms": 522.4748999999065, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 17.225707876113496}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547781.2294025, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 522.4748999999065, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.104100000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 134.45859999956156, "params_millions_measured": 74.824704, "latency_ms": 2249.104100000295, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.455774901655996}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547781.2294025, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 522.4748999999065, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.104100000295, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 134.45859999956156, "params_millions_measured": 74.824704, "latency_ms": 2771.5790000002016, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 26.33877656021881}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547784.1362634, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.10050000042975, "prefill_cuda_event_ms": null, "kv_decode_ms": 1837.3242999996364, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 36.10050000042975, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 249.30402625705634}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547784.1362634, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.10050000042975, "prefill_cuda_event_ms": null, "kv_decode_ms": 1837.3242999996364, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1837.3242999996364, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 34.833262696200485}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547784.1362634, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.10050000042975, "prefill_cuda_event_ms": null, "kv_decode_ms": 1837.3242999996364, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1873.4248000000662, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 38.96606898766229}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547786.0103934, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.3948000003802, "prefill_cuda_event_ms": null, "kv_decode_ms": 2060.209400000531, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 36.3948000003802, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 247.28807411789543}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547786.0103934, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.3948000003802, "prefill_cuda_event_ms": null, "kv_decode_ms": 2060.209400000531, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2060.209400000531, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.064803412693635}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547786.0103934, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.3948000003802, "prefill_cuda_event_ms": null, "kv_decode_ms": 2060.209400000531, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2096.604200000911, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 34.81820746136456}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547788.1074817, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.15019999990909, "prefill_cuda_event_ms": null, "kv_decode_ms": 2057.796000000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 33.15019999990909, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 271.4915747122093}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547788.1074817, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.15019999990909, "prefill_cuda_event_ms": null, "kv_decode_ms": 2057.796000000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2057.796000000053, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.10123646853155}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547788.1074817, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.15019999990909, "prefill_cuda_event_ms": null, "kv_decode_ms": 2057.796000000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2090.946199999962, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 34.91242385863458}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547790.1989741, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 47.80010000013135, "prefill_cuda_event_ms": null, "kv_decode_ms": 2163.0581999997958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 47.80010000013135, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 355.6477915308396}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547790.1989741, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.80010000013135, "prefill_cuda_event_ms": null, "kv_decode_ms": 2163.0581999997958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2163.0581999997958, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.587738323456133}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547790.1989741, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.80010000013135, "prefill_cuda_event_ms": null, "kv_decode_ms": 2163.0581999997958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2210.858299999927, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 36.63735482278655}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547792.410754, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 52.57849999998143, "prefill_cuda_event_ms": null, "kv_decode_ms": 1943.4301000001142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 52.57849999998143, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 323.3260743460921}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547792.410754, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 52.57849999998143, "prefill_cuda_event_ms": null, "kv_decode_ms": 1943.4301000001142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1943.4301000001142, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.931464836320195}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547792.410754, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 52.57849999998143, "prefill_cuda_event_ms": null, "kv_decode_ms": 1943.4301000001142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1996.0086000000956, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 40.58098747670532}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547794.4074228, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.056299999596376, "prefill_cuda_event_ms": null, "kv_decode_ms": 2055.5489999997008, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 50.056299999596376, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 339.61759059573075}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547794.4074228, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.056299999596376, "prefill_cuda_event_ms": null, "kv_decode_ms": 2055.5489999997008, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2055.5489999997008, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.13523443129272}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547794.4074228, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.056299999596376, "prefill_cuda_event_ms": null, "kv_decode_ms": 2055.5489999997008, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2105.605299999297, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 38.46874815523452}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547796.5135214, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 41.134799999781535, "prefill_cuda_event_ms": null, "kv_decode_ms": 2081.4572999997836, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 41.134799999781535, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 413.2753775414074}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547796.5135214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.134799999781535, "prefill_cuda_event_ms": null, "kv_decode_ms": 2081.4572999997836, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2081.4572999997836, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.747688170209717}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547796.5135214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.134799999781535, "prefill_cuda_event_ms": null, "kv_decode_ms": 2081.4572999997836, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2122.592099999565, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 38.160888283724695}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547798.6366537, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.43560000000434, "prefill_cuda_event_ms": null, "kv_decode_ms": 1204.012000000148, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 36.43560000000434, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 247.01116490462425}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547798.6366537, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.43560000000434, "prefill_cuda_event_ms": null, "kv_decode_ms": 1204.012000000148, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1204.012000000148, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 53.15561638919889}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547798.6366537, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.43560000000434, "prefill_cuda_event_ms": null, "kv_decode_ms": 1204.012000000148, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1240.4476000001523, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.84972488962133}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547799.8782318, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.937100000192004, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.303399999968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 25.937100000192004, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 346.99330302668284}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547799.8782318, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.937100000192004, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.303399999968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 843.303399999968, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.89202178006448}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547799.8782318, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.937100000192004, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.303399999968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 869.24050000016, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.98136073961875}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547800.7481449, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.64459999999235, "prefill_cuda_event_ms": null, "kv_decode_ms": 810.8029000004535, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 19.64459999999235, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 458.14116856558564}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547800.7481449, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.64459999999235, "prefill_cuda_event_ms": null, "kv_decode_ms": 810.8029000004535, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 810.8029000004535, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 78.93410346702534}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547800.7481449, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.64459999999235, "prefill_cuda_event_ms": null, "kv_decode_ms": 810.8029000004535, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 830.4475000004459, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 87.90441298211002}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547801.5793023, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.918000000165193, "prefill_cuda_event_ms": null, "kv_decode_ms": 876.5098000003491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 20.918000000165193, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 430.2514580709879}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547801.5793023, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.918000000165193, "prefill_cuda_event_ms": null, "kv_decode_ms": 876.5098000003491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 876.5098000003491, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.01686758091526}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547801.5793023, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.918000000165193, "prefill_cuda_event_ms": null, "kv_decode_ms": 876.5098000003491, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 897.4278000005143, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 81.34359109441246}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547802.4774756, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.584000000657397, "prefill_cuda_event_ms": 6.508543968200684, "kv_decode_ms": 185.85910000001604, "kv_decode_cuda_event_ms": 185.76690673828125, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 6.584000000657397, "cuda_event_ms": 6.508543968200684, "tokens_total": 9, "tokens_per_s": 1366.9501821235374}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547802.4774756, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.584000000657397, "prefill_cuda_event_ms": 6.508543968200684, "kv_decode_ms": 185.85910000001604, "kv_decode_cuda_event_ms": 185.76690673828125, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 185.85910000001604, "cuda_event_ms": 185.76690673828125, "tokens_total": 64, "tokens_per_s": 344.3468735186734}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547802.4774756, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.584000000657397, "prefill_cuda_event_ms": 6.508543968200684, "kv_decode_ms": 185.85910000001604, "kv_decode_cuda_event_ms": 185.76690673828125, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 192.44310000067344, "cuda_event_ms": 192.27545070648193, "tokens_total": 73, "tokens_per_s": 379.3329041142267}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547802.6721408, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.819699999439763, "prefill_cuda_event_ms": 3.7693440914154053, "kv_decode_ms": 118.07169999974576, "kv_decode_cuda_event_ms": 118.0047378540039, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 3.819699999439763, "cuda_event_ms": 3.7693440914154053, "tokens_total": 9, "tokens_per_s": 2356.205985108787}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547802.6721408, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.819699999439763, "prefill_cuda_event_ms": 3.7693440914154053, "kv_decode_ms": 118.07169999974576, "kv_decode_cuda_event_ms": 118.0047378540039, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 118.07169999974576, "cuda_event_ms": 118.0047378540039, "tokens_total": 64, "tokens_per_s": 542.0435210142465}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547802.6721408, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.819699999439763, "prefill_cuda_event_ms": 3.7693440914154053, "kv_decode_ms": 118.07169999974576, "kv_decode_cuda_event_ms": 118.0047378540039, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 121.89139999918552, "cuda_event_ms": 121.77408194541931, "tokens_total": 73, "tokens_per_s": 598.893769375754}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547802.7953038, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.4861000001692446, "prefill_cuda_event_ms": 2.4219839572906494, "kv_decode_ms": 117.34599999999773, "kv_decode_cuda_event_ms": 117.27974700927734, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 2.4861000001692446, "cuda_event_ms": 2.4219839572906494, "tokens_total": 9, "tokens_per_s": 3620.12791093975}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547802.7953038, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.4861000001692446, "prefill_cuda_event_ms": 2.4219839572906494, "kv_decode_ms": 117.34599999999773, "kv_decode_cuda_event_ms": 117.27974700927734, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 117.34599999999773, "cuda_event_ms": 117.27974700927734, "tokens_total": 64, "tokens_per_s": 545.3956675131767}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547802.7953038, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.4861000001692446, "prefill_cuda_event_ms": 2.4219839572906494, "kv_decode_ms": 117.34599999999773, "kv_decode_cuda_event_ms": 117.27974700927734, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 119.83210000016697, "cuda_event_ms": 119.701730966568, "tokens_total": 73, "tokens_per_s": 609.1856856376403}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547802.915817, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.3554999997941195, "prefill_cuda_event_ms": 2.2896640300750732, "kv_decode_ms": 117.52630000046338, "kv_decode_cuda_event_ms": 117.47122955322266, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 2.3554999997941195, "cuda_event_ms": 2.2896640300750732, "tokens_total": 9, "tokens_per_s": 3820.844831579978}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547802.915817, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.3554999997941195, "prefill_cuda_event_ms": 2.2896640300750732, "kv_decode_ms": 117.52630000046338, "kv_decode_cuda_event_ms": 117.47122955322266, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 117.52630000046338, "cuda_event_ms": 117.47122955322266, "tokens_total": 64, "tokens_per_s": 544.5589625449594}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547802.915817, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.3554999997941195, "prefill_cuda_event_ms": 2.2896640300750732, "kv_decode_ms": 117.52630000046338, "kv_decode_cuda_event_ms": 117.47122955322266, "gpu_peak_mb": 630.4208984375, "params_millions_measured": 25.016064, "latency_ms": 119.8818000002575, "cuda_event_ms": 119.76089358329773, "tokens_total": 73, "tokens_per_s": 608.9331324675071}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547803.0363724, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.799399999479647, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 211.1534000005122, "kv_decode_cuda_event_ms": 211.10765075683594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 3.799399999479647, "cuda_event_ms": 3.7498879432678223, "tokens_total": 17, "tokens_per_s": 4474.390693880156}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547803.0363724, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.799399999479647, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 211.1534000005122, "kv_decode_cuda_event_ms": 211.10765075683594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 211.1534000005122, "cuda_event_ms": 211.10765075683594, "tokens_total": 64, "tokens_per_s": 303.09717958529086}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547803.0363724, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.799399999479647, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 211.1534000005122, "kv_decode_cuda_event_ms": 211.10765075683594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 214.95279999999184, "cuda_event_ms": 214.85753870010376, "tokens_total": 81, "tokens_per_s": 376.8269126989882}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547803.2521434, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.297499999665888, "prefill_cuda_event_ms": 4.25161600112915, "kv_decode_ms": 206.16900000004534, "kv_decode_cuda_event_ms": 206.14134216308594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.297499999665888, "cuda_event_ms": 4.25161600112915, "tokens_total": 17, "tokens_per_s": 3955.7882492895114}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547803.2521434, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.297499999665888, "prefill_cuda_event_ms": 4.25161600112915, "kv_decode_ms": 206.16900000004534, "kv_decode_cuda_event_ms": 206.14134216308594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 206.16900000004534, "cuda_event_ms": 206.14134216308594, "tokens_total": 64, "tokens_per_s": 310.42494264407316}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547803.2521434, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.297499999665888, "prefill_cuda_event_ms": 4.25161600112915, "kv_decode_ms": 206.16900000004534, "kv_decode_cuda_event_ms": 206.14134216308594, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 210.46649999971123, "cuda_event_ms": 210.3929581642151, "tokens_total": 81, "tokens_per_s": 384.85934816282463}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547803.4631937, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.022400000394555, "prefill_cuda_event_ms": 3.9823360443115234, "kv_decode_ms": 208.01829999982147, "kv_decode_cuda_event_ms": 207.95692443847656, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.022400000394555, "cuda_event_ms": 3.9823360443115234, "tokens_total": 17, "tokens_per_s": 4226.332537373826}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547803.4631937, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.022400000394555, "prefill_cuda_event_ms": 3.9823360443115234, "kv_decode_ms": 208.01829999982147, "kv_decode_cuda_event_ms": 207.95692443847656, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 208.01829999982147, "cuda_event_ms": 207.95692443847656, "tokens_total": 64, "tokens_per_s": 307.665239068173}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547803.4631937, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.022400000394555, "prefill_cuda_event_ms": 3.9823360443115234, "kv_decode_ms": 208.01829999982147, "kv_decode_cuda_event_ms": 207.95692443847656, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 212.04070000021602, "cuda_event_ms": 211.9392604827881, "tokens_total": 81, "tokens_per_s": 382.0021344954883}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547803.6758807, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.0014000005612615, "prefill_cuda_event_ms": 3.94649600982666, "kv_decode_ms": 207.44959999956336, "kv_decode_cuda_event_ms": 207.3896942138672, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 4.0014000005612615, "cuda_event_ms": 3.94649600982666, "tokens_total": 17, "tokens_per_s": 4248.513019846921}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547803.6758807, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.0014000005612615, "prefill_cuda_event_ms": 3.94649600982666, "kv_decode_ms": 207.44959999956336, "kv_decode_cuda_event_ms": 207.3896942138672, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 207.44959999956336, "cuda_event_ms": 207.3896942138672, "tokens_total": 64, "tokens_per_s": 308.5086690942509}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547803.6758807, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.0014000005612615, "prefill_cuda_event_ms": 3.94649600982666, "kv_decode_ms": 207.44959999956336, "kv_decode_cuda_event_ms": 207.3896942138672, "gpu_peak_mb": 631.81591796875, "params_millions_measured": 45.1712, "latency_ms": 211.45100000012462, "cuda_event_ms": 211.33619022369385, "tokens_total": 81, "tokens_per_s": 383.06747189633654}
