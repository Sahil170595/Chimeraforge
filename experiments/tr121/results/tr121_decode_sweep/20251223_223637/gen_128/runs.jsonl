{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547823.8236904, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 106.85929999999644, "prefill_cuda_event_ms": null, "kv_decode_ms": 2733.126099999936, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7553.77210000006, "params_millions_measured": 45.1712, "latency_ms": 106.85929999999644, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 159.0876975611909}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547823.8236904, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 106.85929999999644, "prefill_cuda_event_ms": null, "kv_decode_ms": 2733.126099999936, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7553.77210000006, "params_millions_measured": 45.1712, "latency_ms": 2733.126099999936, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 46.832819019950456}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547823.8236904, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 106.85929999999644, "prefill_cuda_event_ms": null, "kv_decode_ms": 2733.126099999936, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7553.77210000006, "params_millions_measured": 45.1712, "latency_ms": 2839.9853999999323, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 51.056600502243235}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547834.2200282, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.442600000445964, "prefill_cuda_event_ms": null, "kv_decode_ms": 2550.129299999753, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 31.442600000445964, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 540.6677564755739}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547834.2200282, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 31.442600000445964, "prefill_cuda_event_ms": null, "kv_decode_ms": 2550.129299999753, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2550.129299999753, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 50.193533323981804}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547834.2200282, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 31.442600000445964, "prefill_cuda_event_ms": null, "kv_decode_ms": 2550.129299999753, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2581.571900000199, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 56.16732968002511}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547836.8022704, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.41240000068501, "prefill_cuda_event_ms": null, "kv_decode_ms": 2605.7120999994368, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 27.41240000068501, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 620.1573010599285}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547836.8022704, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 27.41240000068501, "prefill_cuda_event_ms": null, "kv_decode_ms": 2605.7120999994368, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2605.7120999994368, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 49.12284822257519}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547836.8022704, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 27.41240000068501, "prefill_cuda_event_ms": null, "kv_decode_ms": 2605.7120999994368, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2633.1245000001218, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 55.067658213651995}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547839.4359882, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 32.98240000003716, "prefill_cuda_event_ms": null, "kv_decode_ms": 3230.242399999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 32.98240000003716, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 515.4264092358605}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547839.4359882, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 32.98240000003716, "prefill_cuda_event_ms": null, "kv_decode_ms": 3230.242399999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 3230.242399999952, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 39.62550921875148}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547839.4359882, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 32.98240000003716, "prefill_cuda_event_ms": null, "kv_decode_ms": 3230.242399999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 3263.224799999989, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 44.43457281888777}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547842.6999204, "prompt_tokens": 92, "prefill_ms": 822.2523, "prefill_cuda_event_ms": null, "kv_decode_ms": 6081.896, "kv_decode_ms_equiv": 6081.896, "kv_decode_ms_per_token": 47.5148125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 16432.816800000182, "ollama_total_duration_ms": 16304.7145, "ollama_load_ms": 9219.5129, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 822.2523, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 111.8877989152478}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547842.6999204, "prompt_tokens": 92, "prefill_ms": 822.2523, "prefill_cuda_event_ms": null, "kv_decode_ms": 6081.896, "kv_decode_ms_equiv": 6081.896, "kv_decode_ms_per_token": 47.5148125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 16432.816800000182, "ollama_total_duration_ms": 16304.7145, "ollama_load_ms": 9219.5129, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6081.896, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 21.04606852862989}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547842.6999204, "prompt_tokens": 92, "prefill_ms": 822.2523, "prefill_cuda_event_ms": null, "kv_decode_ms": 6081.896, "kv_decode_ms_equiv": 6081.896, "kv_decode_ms_per_token": 47.5148125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 16432.816800000182, "ollama_total_duration_ms": 16304.7145, "ollama_load_ms": 9219.5129, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6904.1483, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 31.86490070035141}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547859.132899, "prompt_tokens": 92, "prefill_ms": 61.7148, "prefill_cuda_event_ms": null, "kv_decode_ms": 7333.5018, "kv_decode_ms_equiv": 7333.5018, "kv_decode_ms_per_token": 57.2929828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7743.8867999999275, "ollama_total_duration_ms": 7729.2166, "ollama_load_ms": 290.3372, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.7148, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1490.7283180047573}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547859.132899, "prompt_tokens": 92, "prefill_ms": 61.7148, "prefill_cuda_event_ms": null, "kv_decode_ms": 7333.5018, "kv_decode_ms_equiv": 7333.5018, "kv_decode_ms_per_token": 57.2929828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7743.8867999999275, "ollama_total_duration_ms": 7729.2166, "ollama_load_ms": 290.3372, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7333.5018, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.454144485244417}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547859.132899, "prompt_tokens": 92, "prefill_ms": 61.7148, "prefill_cuda_event_ms": null, "kv_decode_ms": 7333.5018, "kv_decode_ms_equiv": 7333.5018, "kv_decode_ms_per_token": 57.2929828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7743.8867999999275, "ollama_total_duration_ms": 7729.2166, "ollama_load_ms": 290.3372, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7395.2166, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 29.748959618032014}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547866.876959, "prompt_tokens": 92, "prefill_ms": 65.1733, "prefill_cuda_event_ms": null, "kv_decode_ms": 7151.1204, "kv_decode_ms_equiv": 7151.1204, "kv_decode_ms_per_token": 55.868128125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7484.897000000274, "ollama_total_duration_ms": 7481.5839, "ollama_load_ms": 235.5268, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 65.1733, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1411.6210165819436}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547866.876959, "prompt_tokens": 92, "prefill_ms": 65.1733, "prefill_cuda_event_ms": null, "kv_decode_ms": 7151.1204, "kv_decode_ms_equiv": 7151.1204, "kv_decode_ms_per_token": 55.868128125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7484.897000000274, "ollama_total_duration_ms": 7481.5839, "ollama_load_ms": 235.5268, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7151.1204, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.89929309538684}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547866.876959, "prompt_tokens": 92, "prefill_ms": 65.1733, "prefill_cuda_event_ms": null, "kv_decode_ms": 7151.1204, "kv_decode_ms_equiv": 7151.1204, "kv_decode_ms_per_token": 55.868128125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7484.897000000274, "ollama_total_duration_ms": 7481.5839, "ollama_load_ms": 235.5268, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7216.2937, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 30.486564037713705}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547874.3629224, "prompt_tokens": 92, "prefill_ms": 59.5824, "prefill_cuda_event_ms": null, "kv_decode_ms": 7143.8398, "kv_decode_ms_equiv": 7143.8398, "kv_decode_ms_per_token": 55.8112484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7492.379100000107, "ollama_total_duration_ms": 7478.0901, "ollama_load_ms": 237.119, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 59.5824, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1544.0801310454094}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547874.3629224, "prompt_tokens": 92, "prefill_ms": 59.5824, "prefill_cuda_event_ms": null, "kv_decode_ms": 7143.8398, "kv_decode_ms_equiv": 7143.8398, "kv_decode_ms_per_token": 55.8112484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7492.379100000107, "ollama_total_duration_ms": 7478.0901, "ollama_load_ms": 237.119, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7143.8398, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.91753504886826}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547874.3629224, "prompt_tokens": 92, "prefill_ms": 59.5824, "prefill_cuda_event_ms": null, "kv_decode_ms": 7143.8398, "kv_decode_ms_equiv": 7143.8398, "kv_decode_ms_per_token": 55.8112484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7492.379100000107, "ollama_total_duration_ms": 7478.0901, "ollama_load_ms": 237.119, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7203.4222, "cuda_event_ms": null, "tokens_total": 220, "tokens_per_s": 30.541039229937123}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547881.8555298, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 742.0178000002124, "prefill_cuda_event_ms": null, "kv_decode_ms": 4313.73480000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.5338999996966, "params_millions_measured": 96.08832, "latency_ms": 742.0178000002124, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 12.129089086538656}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547881.8555298, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 742.0178000002124, "prefill_cuda_event_ms": null, "kv_decode_ms": 4313.73480000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.5338999996966, "params_millions_measured": 96.08832, "latency_ms": 4313.73480000002, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 29.672663233724847}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547881.8555298, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 742.0178000002124, "prefill_cuda_event_ms": null, "kv_decode_ms": 4313.73480000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 289.5338999996966, "params_millions_measured": 96.08832, "latency_ms": 5055.752600000233, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 27.097844938060003}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547887.2020254, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 48.608400000375696, "prefill_cuda_event_ms": null, "kv_decode_ms": 5425.881300000583, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 48.608400000375696, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 185.15318339896888}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547887.2020254, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 48.608400000375696, "prefill_cuda_event_ms": null, "kv_decode_ms": 5425.881300000583, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5425.881300000583, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 23.590637708935184}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547887.2020254, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 48.608400000375696, "prefill_cuda_event_ms": null, "kv_decode_ms": 5425.881300000583, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5474.489700000959, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 25.02516353258935}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547892.679267, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 54.55270000038581, "prefill_cuda_event_ms": null, "kv_decode_ms": 5366.496200000256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 54.55270000038581, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 164.97808540982115}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547892.679267, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 54.55270000038581, "prefill_cuda_event_ms": null, "kv_decode_ms": 5366.496200000256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5366.496200000256, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 23.85168930148388}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547892.679267, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 54.55270000038581, "prefill_cuda_event_ms": null, "kv_decode_ms": 5366.496200000256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5421.048900000642, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 25.27186205606516}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547898.1008809, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 56.604099999276514, "prefill_cuda_event_ms": null, "kv_decode_ms": 5482.255500000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 56.604099999276514, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 158.99908310731968}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547898.1008809, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 56.604099999276514, "prefill_cuda_event_ms": null, "kv_decode_ms": 5482.255500000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5482.255500000065, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 23.348054464079336}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547898.1008809, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 56.604099999276514, "prefill_cuda_event_ms": null, "kv_decode_ms": 5482.255500000065, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5538.859599999341, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 24.734333399607436}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547903.6402159, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 878.1260999994629, "prefill_cuda_event_ms": 749.0723876953125, "kv_decode_ms": 689.0714999999545, "kv_decode_cuda_event_ms": 688.9953002929688, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 426.0260000000926, "params_millions_measured": 96.08832, "latency_ms": 878.1260999994629, "cuda_event_ms": 749.0723876953125, "tokens_total": 17, "tokens_per_s": 19.35940635406509}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547903.6402159, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 878.1260999994629, "prefill_cuda_event_ms": 749.0723876953125, "kv_decode_ms": 689.0714999999545, "kv_decode_cuda_event_ms": 688.9953002929688, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 426.0260000000926, "params_millions_measured": 96.08832, "latency_ms": 689.0714999999545, "cuda_event_ms": 688.9953002929688, "tokens_total": 128, "tokens_per_s": 185.75721097158777}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547903.6402159, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 878.1260999994629, "prefill_cuda_event_ms": 749.0723876953125, "kv_decode_ms": 689.0714999999545, "kv_decode_cuda_event_ms": 688.9953002929688, "gpu_peak_mb": 209.9267578125, "hf_load_ms": 426.0260000000926, "params_millions_measured": 96.08832, "latency_ms": 1567.1975999994174, "cuda_event_ms": 1438.0676879882812, "tokens_total": 145, "tokens_per_s": 92.52183642959503}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547905.6398225, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.469500000610424, "prefill_cuda_event_ms": 13.34067153930664, "kv_decode_ms": 516.2523999997575, "kv_decode_cuda_event_ms": 516.2247314453125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 13.469500000610424, "cuda_event_ms": 13.34067153930664, "tokens_total": 17, "tokens_per_s": 1262.1106944748935}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547905.6398225, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 13.469500000610424, "prefill_cuda_event_ms": 13.34067153930664, "kv_decode_ms": 516.2523999997575, "kv_decode_cuda_event_ms": 516.2247314453125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 516.2523999997575, "cuda_event_ms": 516.2247314453125, "tokens_total": 128, "tokens_per_s": 247.94073596570232}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547905.6398225, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 13.469500000610424, "prefill_cuda_event_ms": 13.34067153930664, "kv_decode_ms": 516.2523999997575, "kv_decode_cuda_event_ms": 516.2247314453125, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 529.7219000003679, "cuda_event_ms": 529.5654029846191, "tokens_total": 145, "tokens_per_s": 273.7285356710744}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547906.1706176, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.222499999945285, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 519.0452000006189, "kv_decode_cuda_event_ms": 519.0113525390625, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 5.222499999945285, "cuda_event_ms": 5.174272060394287, "tokens_total": 17, "tokens_per_s": 3255.1460029062914}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547906.1706176, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.222499999945285, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 519.0452000006189, "kv_decode_cuda_event_ms": 519.0113525390625, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 519.0452000006189, "cuda_event_ms": 519.0113525390625, "tokens_total": 128, "tokens_per_s": 246.6066539096159}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547906.1706176, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.222499999945285, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 519.0452000006189, "kv_decode_cuda_event_ms": 519.0113525390625, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 524.2677000005642, "cuda_event_ms": 524.1856245994568, "tokens_total": 145, "tokens_per_s": 276.5762605627697}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547906.6956465, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.994099999748869, "prefill_cuda_event_ms": 4.943871974945068, "kv_decode_ms": 513.9717999991262, "kv_decode_cuda_event_ms": 513.9353637695312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 4.994099999748869, "cuda_event_ms": 4.943871974945068, "tokens_total": 17, "tokens_per_s": 3404.0167399240813}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547906.6956465, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.994099999748869, "prefill_cuda_event_ms": 4.943871974945068, "kv_decode_ms": 513.9717999991262, "kv_decode_cuda_event_ms": 513.9353637695312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 513.9717999991262, "cuda_event_ms": 513.9353637695312, "tokens_total": 128, "tokens_per_s": 249.04090068797083}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547906.6956465, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.994099999748869, "prefill_cuda_event_ms": 4.943871974945068, "kv_decode_ms": 513.9717999991262, "kv_decode_cuda_event_ms": 513.9353637695312, "gpu_peak_mb": 209.9267578125, "params_millions_measured": 96.08832, "latency_ms": 518.9658999988751, "cuda_event_ms": 518.8792357444763, "tokens_total": 145, "tokens_per_s": 279.40178728566616}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547907.2154398, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.625699999764038, "prefill_cuda_event_ms": 10.532575607299805, "kv_decode_ms": 559.5582999994804, "kv_decode_cuda_event_ms": 559.5349731445312, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 363.0558999993809, "params_millions_measured": 51.475968, "latency_ms": 10.625699999764038, "cuda_event_ms": 10.532575607299805, "tokens_total": 17, "tokens_per_s": 1599.89459521514}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547907.2154398, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 10.625699999764038, "prefill_cuda_event_ms": 10.532575607299805, "kv_decode_ms": 559.5582999994804, "kv_decode_cuda_event_ms": 559.5349731445312, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 363.0558999993809, "params_millions_measured": 51.475968, "latency_ms": 559.5582999994804, "cuda_event_ms": 559.5349731445312, "tokens_total": 128, "tokens_per_s": 228.75185659853292}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547907.2154398, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 10.625699999764038, "prefill_cuda_event_ms": 10.532575607299805, "kv_decode_ms": 559.5582999994804, "kv_decode_cuda_event_ms": 559.5349731445312, "gpu_peak_mb": 317.77099609375, "hf_load_ms": 363.0558999993809, "params_millions_measured": 51.475968, "latency_ms": 570.1839999992444, "cuda_event_ms": 570.067548751831, "tokens_total": 145, "tokens_per_s": 254.30387383755445}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547908.151521, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.058599999756552, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 527.5159000002532, "kv_decode_cuda_event_ms": 527.4869384765625, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.058599999756552, "cuda_event_ms": 4.979712009429932, "tokens_total": 17, "tokens_per_s": 3360.6136086700144}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547908.151521, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.058599999756552, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 527.5159000002532, "kv_decode_cuda_event_ms": 527.4869384765625, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 527.5159000002532, "cuda_event_ms": 527.4869384765625, "tokens_total": 128, "tokens_per_s": 242.64671453493358}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547908.151521, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.058599999756552, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 527.5159000002532, "kv_decode_cuda_event_ms": 527.4869384765625, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 532.5745000000097, "cuda_event_ms": 532.4666504859924, "tokens_total": 145, "tokens_per_s": 272.2623783151416}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547908.6847045, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.220699999881617, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 536.8039000004501, "kv_decode_cuda_event_ms": 536.7808227539062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.220699999881617, "cuda_event_ms": 5.174272060394287, "tokens_total": 17, "tokens_per_s": 3256.2683165831186}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547908.6847045, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.220699999881617, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 536.8039000004501, "kv_decode_cuda_event_ms": 536.7808227539062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 536.8039000004501, "cuda_event_ms": 536.7808227539062, "tokens_total": 128, "tokens_per_s": 238.44834212250075}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547908.6847045, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.220699999881617, "prefill_cuda_event_ms": 5.174272060394287, "kv_decode_ms": 536.8039000004501, "kv_decode_cuda_event_ms": 536.7808227539062, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 542.0246000003317, "cuda_event_ms": 541.9550948143005, "tokens_total": 145, "tokens_per_s": 267.5155334276549}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547909.2273848, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.317000000104599, "prefill_cuda_event_ms": 5.2712321281433105, "kv_decode_ms": 569.678599999861, "kv_decode_cuda_event_ms": 569.6266479492188, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 5.317000000104599, "cuda_event_ms": 5.2712321281433105, "tokens_total": 17, "tokens_per_s": 3197.291705786264}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547909.2273848, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.317000000104599, "prefill_cuda_event_ms": 5.2712321281433105, "kv_decode_ms": 569.678599999861, "kv_decode_cuda_event_ms": 569.6266479492188, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 569.678599999861, "cuda_event_ms": 569.6266479492188, "tokens_total": 128, "tokens_per_s": 224.68809605983307}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547909.2273848, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 5.317000000104599, "prefill_cuda_event_ms": 5.2712321281433105, "kv_decode_ms": 569.678599999861, "kv_decode_cuda_event_ms": 569.6266479492188, "gpu_peak_mb": 317.77099609375, "params_millions_measured": 51.475968, "latency_ms": 574.9955999999656, "cuda_event_ms": 574.8978800773621, "tokens_total": 145, "tokens_per_s": 252.17584273689863}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547909.8030121, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 106.20569999991858, "prefill_cuda_event_ms": null, "kv_decode_ms": 3172.7655000004233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.05660000003263, "params_millions_measured": 51.475968, "latency_ms": 106.20569999991858, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 84.74121445465639}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547909.8030121, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 106.20569999991858, "prefill_cuda_event_ms": null, "kv_decode_ms": 3172.7655000004233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.05660000003263, "params_millions_measured": 51.475968, "latency_ms": 3172.7655000004233, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 40.3433534561514}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547909.8030121, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 106.20569999991858, "prefill_cuda_event_ms": null, "kv_decode_ms": 3172.7655000004233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.05660000003263, "params_millions_measured": 51.475968, "latency_ms": 3278.971200000342, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 41.781397775005075}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547913.2396014, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.9376000001721, "prefill_cuda_event_ms": null, "kv_decode_ms": 3328.7549999995463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 33.9376000001721, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 265.1925887497749}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547913.2396014, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 33.9376000001721, "prefill_cuda_event_ms": null, "kv_decode_ms": 3328.7549999995463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3328.7549999995463, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 38.45281494132715}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547913.2396014, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 33.9376000001721, "prefill_cuda_event_ms": null, "kv_decode_ms": 3328.7549999995463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3362.6925999997184, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 40.741160818568865}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547916.6027653, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.041499999933876, "prefill_cuda_event_ms": null, "kv_decode_ms": 3258.6725999999544, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.041499999933876, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 449.06818352067927}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547916.6027653, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 20.041499999933876, "prefill_cuda_event_ms": null, "kv_decode_ms": 3258.6725999999544, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3258.6725999999544, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 39.27979754701402}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547916.6027653, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 20.041499999933876, "prefill_cuda_event_ms": null, "kv_decode_ms": 3258.6725999999544, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3278.7140999998883, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 41.784674058651426}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547919.882379, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.611500000115484, "prefill_cuda_event_ms": null, "kv_decode_ms": 3346.1544000001595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 38.611500000115484, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 233.09117749823452}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547919.882379, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 38.611500000115484, "prefill_cuda_event_ms": null, "kv_decode_ms": 3346.1544000001595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3346.1544000001595, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 38.252867231707505}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547919.882379, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 38.611500000115484, "prefill_cuda_event_ms": null, "kv_decode_ms": 3346.1544000001595, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3384.765900000275, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 40.475472764597654}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547923.267667, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 58.672199999818986, "prefill_cuda_event_ms": 58.5984001159668, "kv_decode_ms": 515.2884999997696, "kv_decode_cuda_event_ms": 515.2235717773438, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 341.2521000000197, "params_millions_measured": 45.1712, "latency_ms": 58.672199999818986, "cuda_event_ms": 58.5984001159668, "tokens_total": 9, "tokens_per_s": 153.39462300762145}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547923.267667, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 58.672199999818986, "prefill_cuda_event_ms": 58.5984001159668, "kv_decode_ms": 515.2884999997696, "kv_decode_cuda_event_ms": 515.2235717773438, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 341.2521000000197, "params_millions_measured": 45.1712, "latency_ms": 515.2884999997696, "cuda_event_ms": 515.2235717773438, "tokens_total": 128, "tokens_per_s": 248.40453454726284}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547923.267667, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 58.672199999818986, "prefill_cuda_event_ms": 58.5984001159668, "kv_decode_ms": 515.2884999997696, "kv_decode_cuda_event_ms": 515.2235717773438, "gpu_peak_mb": 409.4248046875, "hf_load_ms": 341.2521000000197, "params_millions_measured": 45.1712, "latency_ms": 573.9606999995885, "cuda_event_ms": 573.8219718933105, "tokens_total": 137, "tokens_per_s": 238.69230070995837}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547924.184139, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.007399999863992, "prefill_cuda_event_ms": 3.9628798961639404, "kv_decode_ms": 411.05590000006487, "kv_decode_cuda_event_ms": 410.9977722167969, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.007399999863992, "cuda_event_ms": 3.9628798961639404, "tokens_total": 9, "tokens_per_s": 2245.845186481373}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547924.184139, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.007399999863992, "prefill_cuda_event_ms": 3.9628798961639404, "kv_decode_ms": 411.05590000006487, "kv_decode_cuda_event_ms": 410.9977722167969, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 411.05590000006487, "cuda_event_ms": 410.9977722167969, "tokens_total": 128, "tokens_per_s": 311.39317061251234}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547924.184139, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.007399999863992, "prefill_cuda_event_ms": 3.9628798961639404, "kv_decode_ms": 411.05590000006487, "kv_decode_cuda_event_ms": 410.9977722167969, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 415.06329999992886, "cuda_event_ms": 414.9606521129608, "tokens_total": 137, "tokens_per_s": 330.07013629011163}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547924.5999756, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.024100000606268, "prefill_cuda_event_ms": 3.959808111190796, "kv_decode_ms": 411.21339999972406, "kv_decode_cuda_event_ms": 411.1493225097656, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.024100000606268, "cuda_event_ms": 3.959808111190796, "tokens_total": 9, "tokens_per_s": 2236.5249369160974}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547924.5999756, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.024100000606268, "prefill_cuda_event_ms": 3.959808111190796, "kv_decode_ms": 411.21339999972406, "kv_decode_cuda_event_ms": 411.1493225097656, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 411.21339999972406, "cuda_event_ms": 411.1493225097656, "tokens_total": 128, "tokens_per_s": 311.2739030393608}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547924.5999756, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.024100000606268, "prefill_cuda_event_ms": 3.959808111190796, "kv_decode_ms": 411.21339999972406, "kv_decode_cuda_event_ms": 411.1493225097656, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 415.23750000033033, "cuda_event_ms": 415.1091306209564, "tokens_total": 137, "tokens_per_s": 329.9316656127903}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547925.0161157, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.2063999999299995, "prefill_cuda_event_ms": 4.1308159828186035, "kv_decode_ms": 410.3607999995802, "kv_decode_cuda_event_ms": 410.29437255859375, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 4.2063999999299995, "cuda_event_ms": 4.1308159828186035, "tokens_total": 9, "tokens_per_s": 2139.596804904377}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547925.0161157, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.2063999999299995, "prefill_cuda_event_ms": 4.1308159828186035, "kv_decode_ms": 410.3607999995802, "kv_decode_cuda_event_ms": 410.29437255859375, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 410.3607999995802, "cuda_event_ms": 410.29437255859375, "tokens_total": 128, "tokens_per_s": 311.9206317955588}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547925.0161157, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.2063999999299995, "prefill_cuda_event_ms": 4.1308159828186035, "kv_decode_ms": 410.3607999995802, "kv_decode_cuda_event_ms": 410.29437255859375, "gpu_peak_mb": 409.4248046875, "params_millions_measured": 45.1712, "latency_ms": 414.5671999995102, "cuda_event_ms": 414.42518854141235, "tokens_total": 137, "tokens_per_s": 330.46512121596174}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547925.4315038, "prompt_tokens": 17, "prefill_ms": 9.2983, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.0614, "kv_decode_ms_equiv": 287.34754594594597, "kv_decode_ms_per_token": 2.244902702702703, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2783.9756000003035, "ollama_total_duration_ms": 2724.8274, "ollama_load_ms": 2577.7195, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 9.2983, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1828.2911930137768}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547925.4315038, "prompt_tokens": 17, "prefill_ms": 9.2983, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.0614, "kv_decode_ms_equiv": 287.34754594594597, "kv_decode_ms_per_token": 2.244902702702703, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2783.9756000003035, "ollama_total_duration_ms": 2724.8274, "ollama_load_ms": 2577.7195, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 287.34754594594597, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 445.45360420122944}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547925.4315038, "prompt_tokens": 17, "prefill_ms": 9.2983, "prefill_cuda_event_ms": null, "kv_decode_ms": 83.0614, "kv_decode_ms_equiv": 287.34754594594597, "kv_decode_ms_per_token": 2.244902702702703, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2783.9756000003035, "ollama_total_duration_ms": 2724.8274, "ollama_load_ms": 2577.7195, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 296.64584594594595, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 488.79834988965774}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547928.2158253, "prompt_tokens": 17, "prefill_ms": 2.9558, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.9193, "kv_decode_ms_equiv": 274.4514666666667, "kv_decode_ms_per_token": 2.1441520833333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 309.3212999992829, "ollama_total_duration_ms": 282.7264, "ollama_load_ms": 145.6074, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9558, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5751.404019216456}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547928.2158253, "prompt_tokens": 17, "prefill_ms": 2.9558, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.9193, "kv_decode_ms_equiv": 274.4514666666667, "kv_decode_ms_per_token": 2.1441520833333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 309.3212999992829, "ollama_total_duration_ms": 282.7264, "ollama_load_ms": 145.6074, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 274.4514666666667, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 466.384827724246}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547928.2158253, "prompt_tokens": 17, "prefill_ms": 2.9558, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.9193, "kv_decode_ms_equiv": 274.4514666666667, "kv_decode_ms_per_token": 2.1441520833333336, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 309.3212999992829, "ollama_total_duration_ms": 282.7264, "ollama_load_ms": 145.6074, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 277.4072666666667, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 522.6971944258129}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547928.5252724, "prompt_tokens": 17, "prefill_ms": 2.5141, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.0856, "kv_decode_ms_equiv": 274.8949333333333, "kv_decode_ms_per_token": 2.1476166666666665, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 316.58020000031684, "ollama_total_duration_ms": 297.0875, "ollama_load_ms": 161.4457, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.5141, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6761.863092160216}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547928.5252724, "prompt_tokens": 17, "prefill_ms": 2.5141, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.0856, "kv_decode_ms_equiv": 274.8949333333333, "kv_decode_ms_per_token": 2.1476166666666665, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 316.58020000031684, "ollama_total_duration_ms": 297.0875, "ollama_load_ms": 161.4457, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 274.8949333333333, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 465.63244526878634}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547928.5252724, "prompt_tokens": 17, "prefill_ms": 2.5141, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.0856, "kv_decode_ms_equiv": 274.8949333333333, "kv_decode_ms_per_token": 2.1476166666666665, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 316.58020000031684, "ollama_total_duration_ms": 297.0875, "ollama_load_ms": 161.4457, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 277.4090333333333, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 522.6938656527768}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547928.841969, "prompt_tokens": 17, "prefill_ms": 1.9816, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.8239, "kv_decode_ms_equiv": 258.1970666666667, "kv_decode_ms_per_token": 2.0171645833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 292.6299000000654, "ollama_total_duration_ms": 284.4291, "ollama_load_ms": 150.0022, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 1.9816, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 8578.926120306822}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547928.841969, "prompt_tokens": 17, "prefill_ms": 1.9816, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.8239, "kv_decode_ms_equiv": 258.1970666666667, "kv_decode_ms_per_token": 2.0171645833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 292.6299000000654, "ollama_total_duration_ms": 284.4291, "ollama_load_ms": 150.0022, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 258.1970666666667, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 495.7453686538137}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547928.841969, "prompt_tokens": 17, "prefill_ms": 1.9816, "prefill_cuda_event_ms": null, "kv_decode_ms": 96.8239, "kv_decode_ms_equiv": 258.1970666666667, "kv_decode_ms_per_token": 2.0171645833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 292.6299000000654, "ollama_total_duration_ms": 284.4291, "ollama_load_ms": 150.0022, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 260.1786666666667, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 557.3093361484928}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547929.1347628, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.5293999994319165, "prefill_cuda_event_ms": 5.426176071166992, "kv_decode_ms": 479.6504999994795, "kv_decode_cuda_event_ms": 479.61395263671875, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 5.5293999994319165, "cuda_event_ms": 5.426176071166992, "tokens_total": 9, "tokens_per_s": 1627.6630377481551}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547929.1347628, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.5293999994319165, "prefill_cuda_event_ms": 5.426176071166992, "kv_decode_ms": 479.6504999994795, "kv_decode_cuda_event_ms": 479.61395263671875, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 479.6504999994795, "cuda_event_ms": 479.61395263671875, "tokens_total": 128, "tokens_per_s": 266.86097481424264}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547929.1347628, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.5293999994319165, "prefill_cuda_event_ms": 5.426176071166992, "kv_decode_ms": 479.6504999994795, "kv_decode_cuda_event_ms": 479.61395263671875, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 485.1798999989114, "cuda_event_ms": 485.04012870788574, "tokens_total": 137, "tokens_per_s": 282.36948810185123}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547929.6208656, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.821100000299339, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 482.1273999996265, "kv_decode_cuda_event_ms": 482.1059875488281, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 4.821100000299339, "cuda_event_ms": 4.772863864898682, "tokens_total": 9, "tokens_per_s": 1866.7938850970104}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547929.6208656, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.821100000299339, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 482.1273999996265, "kv_decode_cuda_event_ms": 482.1059875488281, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 482.1273999996265, "cuda_event_ms": 482.1059875488281, "tokens_total": 128, "tokens_per_s": 265.4899928942001}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547929.6208656, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.821100000299339, "prefill_cuda_event_ms": 4.772863864898682, "kv_decode_ms": 482.1273999996265, "kv_decode_cuda_event_ms": 482.1059875488281, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 486.94849999992584, "cuda_event_ms": 486.8788514137268, "tokens_total": 137, "tokens_per_s": 281.34392035301653}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547930.1083603, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.458200000044599, "prefill_cuda_event_ms": 4.420608043670654, "kv_decode_ms": 496.451199999683, "kv_decode_cuda_event_ms": 496.42803955078125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 4.458200000044599, "cuda_event_ms": 4.420608043670654, "tokens_total": 9, "tokens_per_s": 2018.7519626553242}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547930.1083603, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.458200000044599, "prefill_cuda_event_ms": 4.420608043670654, "kv_decode_ms": 496.451199999683, "kv_decode_cuda_event_ms": 496.42803955078125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 496.451199999683, "cuda_event_ms": 496.42803955078125, "tokens_total": 128, "tokens_per_s": 257.82997402379476}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547930.1083603, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.458200000044599, "prefill_cuda_event_ms": 4.420608043670654, "kv_decode_ms": 496.451199999683, "kv_decode_cuda_event_ms": 496.42803955078125, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 500.9093999997276, "cuda_event_ms": 500.8486475944519, "tokens_total": 137, "tokens_per_s": 273.5025535557418}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547930.6097453, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.945499999848835, "prefill_cuda_event_ms": 4.902751922607422, "kv_decode_ms": 511.706299999787, "kv_decode_cuda_event_ms": 511.6702575683594, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 4.945499999848835, "cuda_event_ms": 4.902751922607422, "tokens_total": 9, "tokens_per_s": 1819.836214796299}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547930.6097453, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.945499999848835, "prefill_cuda_event_ms": 4.902751922607422, "kv_decode_ms": 511.706299999787, "kv_decode_cuda_event_ms": 511.6702575683594, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 511.706299999787, "cuda_event_ms": 511.6702575683594, "tokens_total": 128, "tokens_per_s": 250.14349051409621}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547930.6097453, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 4.945499999848835, "prefill_cuda_event_ms": 4.902751922607422, "kv_decode_ms": 511.706299999787, "kv_decode_cuda_event_ms": 511.6702575683594, "gpu_peak_mb": 411.03564453125, "params_millions_measured": 96.08832, "latency_ms": 516.6517999996358, "cuda_event_ms": 516.5730094909668, "tokens_total": 137, "tokens_per_s": 265.16892034460454}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547931.1291022, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.647700000605255, "prefill_cuda_event_ms": null, "kv_decode_ms": 1772.0234000007622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 175.89409999982308, "params_millions_measured": 5.03672, "latency_ms": 49.647700000605255, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 342.412639453444}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547931.1291022, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 49.647700000605255, "prefill_cuda_event_ms": null, "kv_decode_ms": 1772.0234000007622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 175.89409999982308, "params_millions_measured": 5.03672, "latency_ms": 1772.0234000007622, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 72.23380910203835}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547931.1291022, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 49.647700000605255, "prefill_cuda_event_ms": null, "kv_decode_ms": 1772.0234000007622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 175.89409999982308, "params_millions_measured": 5.03672, "latency_ms": 1821.6711000013674, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 79.59724452997644}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547933.127571, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.589599999373604, "prefill_cuda_event_ms": null, "kv_decode_ms": 1756.6932999998244, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 27.589599999373604, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 616.1742105860893}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547933.127571, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 27.589599999373604, "prefill_cuda_event_ms": null, "kv_decode_ms": 1756.6932999998244, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1756.6932999998244, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 72.8641704274803}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547933.127571, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 27.589599999373604, "prefill_cuda_event_ms": null, "kv_decode_ms": 1756.6932999998244, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1784.282899999198, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 81.26514018604627}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547934.9129472, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.713800000019546, "prefill_cuda_event_ms": null, "kv_decode_ms": 1690.5563000000257, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 22.713800000019546, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 748.4436774113258}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547934.9129472, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.713800000019546, "prefill_cuda_event_ms": null, "kv_decode_ms": 1690.5563000000257, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1690.5563000000257, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 75.71472183446245}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547934.9129472, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.713800000019546, "prefill_cuda_event_ms": null, "kv_decode_ms": 1690.5563000000257, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1713.2701000000452, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 84.63347372956324}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547936.626961, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.031999999853724, "prefill_cuda_event_ms": null, "kv_decode_ms": 1734.9488999998357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 22.031999999853724, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 771.6049382767278}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547936.626961, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.031999999853724, "prefill_cuda_event_ms": null, "kv_decode_ms": 1734.9488999998357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1734.9488999998357, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 73.77738906316614}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547936.626961, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.031999999853724, "prefill_cuda_event_ms": null, "kv_decode_ms": 1734.9488999998357, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1756.9808999996894, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 82.52793186313274}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547938.3845901, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 14.202899999872898, "prefill_cuda_event_ms": 14.130175590515137, "kv_decode_ms": 498.1379000000743, "kv_decode_cuda_event_ms": 498.08587646484375, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 478.6309999999503, "params_millions_measured": 74.824704, "latency_ms": 14.202899999872898, "cuda_event_ms": 14.130175590515137, "tokens_total": 9, "tokens_per_s": 633.6734047328744}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547938.3845901, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.202899999872898, "prefill_cuda_event_ms": 14.130175590515137, "kv_decode_ms": 498.1379000000743, "kv_decode_cuda_event_ms": 498.08587646484375, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 478.6309999999503, "params_millions_measured": 74.824704, "latency_ms": 498.1379000000743, "cuda_event_ms": 498.08587646484375, "tokens_total": 128, "tokens_per_s": 256.95695910706837}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547938.3845901, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.202899999872898, "prefill_cuda_event_ms": 14.130175590515137, "kv_decode_ms": 498.1379000000743, "kv_decode_cuda_event_ms": 498.08587646484375, "gpu_peak_mb": 558.80712890625, "hf_load_ms": 478.6309999999503, "params_millions_measured": 74.824704, "latency_ms": 512.3407999999472, "cuda_event_ms": 512.2160520553589, "tokens_total": 137, "tokens_per_s": 267.40013678398077}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547939.3768382, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.5241000005044043, "prefill_cuda_event_ms": 3.4785280227661133, "kv_decode_ms": 325.39530000030936, "kv_decode_cuda_event_ms": 325.36883544921875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 3.5241000005044043, "cuda_event_ms": 3.4785280227661133, "tokens_total": 9, "tokens_per_s": 2553.8435341539202}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547939.3768382, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.5241000005044043, "prefill_cuda_event_ms": 3.4785280227661133, "kv_decode_ms": 325.39530000030936, "kv_decode_cuda_event_ms": 325.36883544921875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 325.39530000030936, "cuda_event_ms": 325.36883544921875, "tokens_total": 128, "tokens_per_s": 393.3676976891747}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547939.3768382, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.5241000005044043, "prefill_cuda_event_ms": 3.4785280227661133, "kv_decode_ms": 325.39530000030936, "kv_decode_cuda_event_ms": 325.36883544921875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 328.91940000081377, "cuda_event_ms": 328.84736347198486, "tokens_total": 137, "tokens_per_s": 416.515413805513}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547939.7063615, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.3182000006490853, "prefill_cuda_event_ms": 3.2787840366363525, "kv_decode_ms": 318.38970000080735, "kv_decode_cuda_event_ms": 318.35546875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 3.3182000006490853, "cuda_event_ms": 3.2787840366363525, "tokens_total": 9, "tokens_per_s": 2712.3139045987214}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547939.7063615, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.3182000006490853, "prefill_cuda_event_ms": 3.2787840366363525, "kv_decode_ms": 318.38970000080735, "kv_decode_cuda_event_ms": 318.35546875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 318.38970000080735, "cuda_event_ms": 318.35546875, "tokens_total": 128, "tokens_per_s": 402.0230553930464}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547939.7063615, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.3182000006490853, "prefill_cuda_event_ms": 3.2787840366363525, "kv_decode_ms": 318.38970000080735, "kv_decode_cuda_event_ms": 318.35546875, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 321.70790000145644, "cuda_event_ms": 321.63425278663635, "tokens_total": 137, "tokens_per_s": 425.8521472409592}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547940.0287051, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.3096999995905207, "prefill_cuda_event_ms": 3.2602241039276123, "kv_decode_ms": 316.28749999981665, "kv_decode_cuda_event_ms": 316.2500915527344, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 3.3096999995905207, "cuda_event_ms": 3.2602241039276123, "tokens_total": 9, "tokens_per_s": 2719.279693359969}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547940.0287051, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.3096999995905207, "prefill_cuda_event_ms": 3.2602241039276123, "kv_decode_ms": 316.28749999981665, "kv_decode_cuda_event_ms": 316.2500915527344, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 316.28749999981665, "cuda_event_ms": 316.2500915527344, "tokens_total": 128, "tokens_per_s": 404.69509544346266}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547940.0287051, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 3.3096999995905207, "prefill_cuda_event_ms": 3.2602241039276123, "kv_decode_ms": 316.28749999981665, "kv_decode_cuda_event_ms": 316.2500915527344, "gpu_peak_mb": 558.80712890625, "params_millions_measured": 74.824704, "latency_ms": 319.59719999940717, "cuda_event_ms": 319.510315656662, "tokens_total": 137, "tokens_per_s": 428.66458154281116}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547940.3490367, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 53.2535000002099, "prefill_cuda_event_ms": null, "kv_decode_ms": 4706.235300000117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 53.2535000002099, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 319.2278441779976}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547940.3490367, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 53.2535000002099, "prefill_cuda_event_ms": null, "kv_decode_ms": 4706.235300000117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4706.235300000117, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 27.197960118992953}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547940.3490367, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 53.2535000002099, "prefill_cuda_event_ms": null, "kv_decode_ms": 4706.235300000117, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4759.488800000327, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 30.46545671039084}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547945.109201, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 58.165200000075856, "prefill_cuda_event_ms": null, "kv_decode_ms": 5740.36630000046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 58.165200000075856, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 292.2709798982524}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547945.109201, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 58.165200000075856, "prefill_cuda_event_ms": null, "kv_decode_ms": 5740.36630000046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5740.36630000046, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 22.298228599103467}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547945.109201, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 58.165200000075856, "prefill_cuda_event_ms": null, "kv_decode_ms": 5740.36630000046, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5798.531500000536, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 25.006331344407908}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547950.9086406, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 53.335100000367675, "prefill_cuda_event_ms": null, "kv_decode_ms": 4124.161899999308, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 53.335100000367675, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 318.73944175379455}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547950.9086406, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 53.335100000367675, "prefill_cuda_event_ms": null, "kv_decode_ms": 4124.161899999308, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4124.161899999308, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 31.036608916837498}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547950.9086406, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 53.335100000367675, "prefill_cuda_event_ms": null, "kv_decode_ms": 4124.161899999308, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 4177.496999999676, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 34.709779564177126}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547955.0867798, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 67.13280000076338, "prefill_cuda_event_ms": null, "kv_decode_ms": 5612.458099999458, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 67.13280000076338, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 253.22941989320702}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547955.0867798, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 67.13280000076338, "prefill_cuda_event_ms": null, "kv_decode_ms": 5612.458099999458, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5612.458099999458, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 22.806406340924376}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547955.0867798, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 67.13280000076338, "prefill_cuda_event_ms": null, "kv_decode_ms": 5612.458099999458, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 5679.590900000221, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 25.53000780390615}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547960.7669356, "prompt_tokens": 99, "prefill_ms": 1028.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 6727.2704, "kv_decode_ms_equiv": 6727.2704, "kv_decode_ms_per_token": 52.5568, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 18013.044199999968, "ollama_total_duration_ms": 17878.5093, "ollama_load_ms": 9992.1955, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1028.4189, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 96.26427518980836}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547960.7669356, "prompt_tokens": 99, "prefill_ms": 1028.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 6727.2704, "kv_decode_ms_equiv": 6727.2704, "kv_decode_ms_per_token": 52.5568, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 18013.044199999968, "ollama_total_duration_ms": 17878.5093, "ollama_load_ms": 9992.1955, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 6727.2704, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 19.027033609352166}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547960.7669356, "prompt_tokens": 99, "prefill_ms": 1028.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 6727.2704, "kv_decode_ms_equiv": 6727.2704, "kv_decode_ms_per_token": 52.5568, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 18013.044199999968, "ollama_total_duration_ms": 17878.5093, "ollama_load_ms": 9992.1955, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7755.6893, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 29.26883623355051}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547978.7807815, "prompt_tokens": 99, "prefill_ms": 70.0221, "prefill_cuda_event_ms": null, "kv_decode_ms": 7714.7569, "kv_decode_ms_equiv": 7714.7569, "kv_decode_ms_per_token": 60.27153828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 8104.005299999699, "ollama_total_duration_ms": 8078.3351, "ollama_load_ms": 266.4301, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 70.0221, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1413.8393450067908}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547978.7807815, "prompt_tokens": 99, "prefill_ms": 70.0221, "prefill_cuda_event_ms": null, "kv_decode_ms": 7714.7569, "kv_decode_ms_equiv": 7714.7569, "kv_decode_ms_per_token": 60.27153828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 8104.005299999699, "ollama_total_duration_ms": 8078.3351, "ollama_load_ms": 266.4301, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7714.7569, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 16.591579185080995}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547978.7807815, "prompt_tokens": 99, "prefill_ms": 70.0221, "prefill_cuda_event_ms": null, "kv_decode_ms": 7714.7569, "kv_decode_ms_equiv": 7714.7569, "kv_decode_ms_per_token": 60.27153828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 8104.005299999699, "ollama_total_duration_ms": 8078.3351, "ollama_load_ms": 266.4301, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7784.779, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 29.159466183946904}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547986.8848767, "prompt_tokens": 99, "prefill_ms": 67.2798, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.6546, "kv_decode_ms_equiv": 7505.6546, "kv_decode_ms_per_token": 58.6379265625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7872.849599999427, "ollama_total_duration_ms": 7844.3502, "ollama_load_ms": 251.8234, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 67.2798, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1471.466918748272}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547986.8848767, "prompt_tokens": 99, "prefill_ms": 67.2798, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.6546, "kv_decode_ms_equiv": 7505.6546, "kv_decode_ms_per_token": 58.6379265625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7872.849599999427, "ollama_total_duration_ms": 7844.3502, "ollama_load_ms": 251.8234, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7505.6546, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.05380900421397}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547986.8848767, "prompt_tokens": 99, "prefill_ms": 67.2798, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.6546, "kv_decode_ms_equiv": 7505.6546, "kv_decode_ms_per_token": 58.6379265625, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7872.849599999427, "ollama_total_duration_ms": 7844.3502, "ollama_load_ms": 251.8234, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7572.9344, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 29.97517052306699}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547994.7578995, "prompt_tokens": 99, "prefill_ms": 63.0711, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.3884, "kv_decode_ms_equiv": 7505.3884, "kv_decode_ms_per_token": 58.635846875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 7825.863099999879, "ollama_total_duration_ms": 7822.064, "ollama_load_ms": 232.9642, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 63.0711, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1569.657101271422}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547994.7578995, "prompt_tokens": 99, "prefill_ms": 63.0711, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.3884, "kv_decode_ms_equiv": 7505.3884, "kv_decode_ms_per_token": 58.635846875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7825.863099999879, "ollama_total_duration_ms": 7822.064, "ollama_load_ms": 232.9642, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7505.3884, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 17.054413866176464}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547994.7578995, "prompt_tokens": 99, "prefill_ms": 63.0711, "prefill_cuda_event_ms": null, "kv_decode_ms": 7505.3884, "kv_decode_ms_equiv": 7505.3884, "kv_decode_ms_per_token": 58.635846875, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 7825.863099999879, "ollama_total_duration_ms": 7822.064, "ollama_load_ms": 232.9642, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 7568.4595, "cuda_event_ms": null, "tokens_total": 227, "tokens_per_s": 29.992893534014417}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548002.5840745, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 52.473900000222784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2656.682699999692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 52.473900000222784, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 323.97058346964536}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548002.5840745, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 52.473900000222784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2656.682699999692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2656.682699999692, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 48.18038676580189}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548002.5840745, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 52.473900000222784, "prefill_cuda_event_ms": null, "kv_decode_ms": 2656.682699999692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2709.1565999999148, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 53.52219210953127}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548005.2942073, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.246400000374706, "prefill_cuda_event_ms": null, "kv_decode_ms": 2524.776999999631, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 29.246400000374706, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 581.2681218810587}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548005.2942073, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 29.246400000374706, "prefill_cuda_event_ms": null, "kv_decode_ms": 2524.776999999631, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2524.776999999631, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 50.6975467536415}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548005.2942073, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 29.246400000374706, "prefill_cuda_event_ms": null, "kv_decode_ms": 2524.776999999631, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2554.0234000000055, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 56.77316817065955}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548007.8488238, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 36.885200000142504, "prefill_cuda_event_ms": null, "kv_decode_ms": 2979.5159000004787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 36.885200000142504, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 460.8894624384393}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548007.8488238, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 36.885200000142504, "prefill_cuda_event_ms": null, "kv_decode_ms": 2979.5159000004787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2979.5159000004787, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 42.95999897163812}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548007.8488238, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 36.885200000142504, "prefill_cuda_event_ms": null, "kv_decode_ms": 2979.5159000004787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 3016.401100000621, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 48.07053014268233}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548010.8658652, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 42.135200000302575, "prefill_cuda_event_ms": null, "kv_decode_ms": 2491.9251999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 42.135200000302575, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 403.46313770619156}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548010.8658652, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 42.135200000302575, "prefill_cuda_event_ms": null, "kv_decode_ms": 2491.9251999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2491.9251999999688, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 51.36590777283428}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548010.8658652, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 42.135200000302575, "prefill_cuda_event_ms": null, "kv_decode_ms": 2491.9251999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2534.0604000002713, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 57.22041984476158}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548013.4005444, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.014500000375847, "prefill_cuda_event_ms": 8.931327819824219, "kv_decode_ms": 708.9933000006567, "kv_decode_cuda_event_ms": 708.9522705078125, "gpu_peak_mb": 580.107421875, "hf_load_ms": 191.0211000003983, "params_millions_measured": 5.03672, "latency_ms": 9.014500000375847, "cuda_event_ms": 8.931327819824219, "tokens_total": 17, "tokens_per_s": 1885.8505739964733}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548013.4005444, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.014500000375847, "prefill_cuda_event_ms": 8.931327819824219, "kv_decode_ms": 708.9933000006567, "kv_decode_cuda_event_ms": 708.9522705078125, "gpu_peak_mb": 580.107421875, "hf_load_ms": 191.0211000003983, "params_millions_measured": 5.03672, "latency_ms": 708.9933000006567, "cuda_event_ms": 708.9522705078125, "tokens_total": 128, "tokens_per_s": 180.53767221760972}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548013.4005444, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.014500000375847, "prefill_cuda_event_ms": 8.931327819824219, "kv_decode_ms": 708.9933000006567, "kv_decode_cuda_event_ms": 708.9522705078125, "gpu_peak_mb": 580.107421875, "hf_load_ms": 191.0211000003983, "params_millions_measured": 5.03672, "latency_ms": 718.0078000010326, "cuda_event_ms": 717.8835983276367, "tokens_total": 145, "tokens_per_s": 201.94766686349573}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548014.3113775, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.506300000182819, "prefill_cuda_event_ms": 6.444032192230225, "kv_decode_ms": 760.150999999496, "kv_decode_cuda_event_ms": 760.1193237304688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.506300000182819, "cuda_event_ms": 6.444032192230225, "tokens_total": 17, "tokens_per_s": 2612.8521586035567}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548014.3113775, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.506300000182819, "prefill_cuda_event_ms": 6.444032192230225, "kv_decode_ms": 760.150999999496, "kv_decode_cuda_event_ms": 760.1193237304688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 760.150999999496, "cuda_event_ms": 760.1193237304688, "tokens_total": 128, "tokens_per_s": 168.38759667498283}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548014.3113775, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.506300000182819, "prefill_cuda_event_ms": 6.444032192230225, "kv_decode_ms": 760.150999999496, "kv_decode_cuda_event_ms": 760.1193237304688, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 766.6572999996788, "cuda_event_ms": 766.563355922699, "tokens_total": 145, "tokens_per_s": 189.13274549144808}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548015.078829, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.350699999529752, "prefill_cuda_event_ms": 6.2902398109436035, "kv_decode_ms": 763.705100000152, "kv_decode_cuda_event_ms": 763.6590576171875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.350699999529752, "cuda_event_ms": 6.2902398109436035, "tokens_total": 17, "tokens_per_s": 2676.870266468073}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548015.078829, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.350699999529752, "prefill_cuda_event_ms": 6.2902398109436035, "kv_decode_ms": 763.705100000152, "kv_decode_cuda_event_ms": 763.6590576171875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 763.705100000152, "cuda_event_ms": 763.6590576171875, "tokens_total": 128, "tokens_per_s": 167.60396126721494}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548015.078829, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.350699999529752, "prefill_cuda_event_ms": 6.2902398109436035, "kv_decode_ms": 763.705100000152, "kv_decode_cuda_event_ms": 763.6590576171875, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 770.0557999996818, "cuda_event_ms": 769.9492974281311, "tokens_total": 145, "tokens_per_s": 188.29804281723472}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548015.8495307, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.486799999947834, "prefill_cuda_event_ms": 6.426623821258545, "kv_decode_ms": 764.4549999995434, "kv_decode_cuda_event_ms": 764.42626953125, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 6.486799999947834, "cuda_event_ms": 6.426623821258545, "tokens_total": 17, "tokens_per_s": 2620.7066658655594}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548015.8495307, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.486799999947834, "prefill_cuda_event_ms": 6.426623821258545, "kv_decode_ms": 764.4549999995434, "kv_decode_cuda_event_ms": 764.42626953125, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 764.4549999995434, "cuda_event_ms": 764.42626953125, "tokens_total": 128, "tokens_per_s": 167.43954843656783}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548015.8495307, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 6.486799999947834, "prefill_cuda_event_ms": 6.426623821258545, "kv_decode_ms": 764.4549999995434, "kv_decode_cuda_event_ms": 764.42626953125, "gpu_peak_mb": 580.107421875, "params_millions_measured": 5.03672, "latency_ms": 770.9417999994912, "cuda_event_ms": 770.8528933525085, "tokens_total": 145, "tokens_per_s": 188.08164247949156}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548016.6211224, "prompt_tokens": 25, "prefill_ms": 8.1414, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.4474, "kv_decode_ms_equiv": 281.4474, "kv_decode_ms_per_token": 2.1988078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1994.7145000005548, "ollama_total_duration_ms": 1909.0788, "ollama_load_ms": 1519.2273, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.1414, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3070.7249367430663}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548016.6211224, "prompt_tokens": 25, "prefill_ms": 8.1414, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.4474, "kv_decode_ms_equiv": 281.4474, "kv_decode_ms_per_token": 2.1988078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1994.7145000005548, "ollama_total_duration_ms": 1909.0788, "ollama_load_ms": 1519.2273, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 281.4474, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 454.79190783073494}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548016.6211224, "prompt_tokens": 25, "prefill_ms": 8.1414, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.4474, "kv_decode_ms_equiv": 281.4474, "kv_decode_ms_per_token": 2.1988078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1994.7145000005548, "ollama_total_duration_ms": 1909.0788, "ollama_load_ms": 1519.2273, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 289.5888, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 528.3353499859111}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548018.6160848, "prompt_tokens": 25, "prefill_ms": 2.8756, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.4185, "kv_decode_ms_equiv": 270.4185, "kv_decode_ms_per_token": 2.11264453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 548.4993000000031, "ollama_total_duration_ms": 492.255, "ollama_load_ms": 150.511, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.8756, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8693.837807761858}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548018.6160848, "prompt_tokens": 25, "prefill_ms": 2.8756, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.4185, "kv_decode_ms_equiv": 270.4185, "kv_decode_ms_per_token": 2.11264453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 548.4993000000031, "ollama_total_duration_ms": 492.255, "ollama_load_ms": 150.511, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 270.4185, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 473.34039645956176}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548018.6160848, "prompt_tokens": 25, "prefill_ms": 2.8756, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.4185, "kv_decode_ms_equiv": 270.4185, "kv_decode_ms_per_token": 2.11264453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 548.4993000000031, "ollama_total_duration_ms": 492.255, "ollama_load_ms": 150.511, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 273.2941, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 559.8364545740284}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548019.1647022, "prompt_tokens": 25, "prefill_ms": 2.9466, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.2747, "kv_decode_ms_equiv": 269.2747, "kv_decode_ms_per_token": 2.10370859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 542.6502000000255, "ollama_total_duration_ms": 504.0957, "ollama_load_ms": 159.7786, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.9466, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8484.354849657231}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548019.1647022, "prompt_tokens": 25, "prefill_ms": 2.9466, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.2747, "kv_decode_ms_equiv": 269.2747, "kv_decode_ms_per_token": 2.10370859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 542.6502000000255, "ollama_total_duration_ms": 504.0957, "ollama_load_ms": 159.7786, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 269.2747, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 475.351007725568}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548019.1647022, "prompt_tokens": 25, "prefill_ms": 2.9466, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.2747, "kv_decode_ms_equiv": 269.2747, "kv_decode_ms_per_token": 2.10370859375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 542.6502000000255, "ollama_total_duration_ms": 504.0957, "ollama_load_ms": 159.7786, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 272.2213, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 562.0427203896242}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548019.7074382, "prompt_tokens": 25, "prefill_ms": 3.3957, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.4299, "kv_decode_ms_equiv": 253.4299, "kv_decode_ms_per_token": 1.97992109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 556.7617000006067, "ollama_total_duration_ms": 510.0938, "ollama_load_ms": 172.0614, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.3957, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 7362.252260211443}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548019.7074382, "prompt_tokens": 25, "prefill_ms": 3.3957, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.4299, "kv_decode_ms_equiv": 253.4299, "kv_decode_ms_per_token": 1.97992109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 556.7617000006067, "ollama_total_duration_ms": 510.0938, "ollama_load_ms": 172.0614, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 253.4299, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 505.07063294425797}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548019.7074382, "prompt_tokens": 25, "prefill_ms": 3.3957, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.4299, "kv_decode_ms_equiv": 253.4299, "kv_decode_ms_per_token": 1.97992109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 556.7617000006067, "ollama_total_duration_ms": 510.0938, "ollama_load_ms": 172.0614, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 256.8256, "cuda_event_ms": null, "tokens_total": 153, "tokens_per_s": 595.7350046101324}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548020.2643592, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 29.72719999979745, "prefill_cuda_event_ms": 29.629440307617188, "kv_decode_ms": 754.0951999999379, "kv_decode_cuda_event_ms": 754.060302734375, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 29.72719999979745, "cuda_event_ms": 29.629440307617188, "tokens_total": 9, "tokens_per_s": 302.7530342602507}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548020.2643592, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 29.72719999979745, "prefill_cuda_event_ms": 29.629440307617188, "kv_decode_ms": 754.0951999999379, "kv_decode_cuda_event_ms": 754.060302734375, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 754.0951999999379, "cuda_event_ms": 754.060302734375, "tokens_total": 128, "tokens_per_s": 169.7398418661338}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548020.2643592, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 29.72719999979745, "prefill_cuda_event_ms": 29.629440307617188, "kv_decode_ms": 754.0951999999379, "kv_decode_cuda_event_ms": 754.060302734375, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 783.8223999997354, "cuda_event_ms": 783.6897430419922, "tokens_total": 137, "tokens_per_s": 174.78449199722573}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548021.0495174, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.967700000132027, "prefill_cuda_event_ms": 6.887551784515381, "kv_decode_ms": 751.069699999789, "kv_decode_cuda_event_ms": 751.0241088867188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 6.967700000132027, "cuda_event_ms": 6.887551784515381, "tokens_total": 9, "tokens_per_s": 1291.6744406087323}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548021.0495174, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.967700000132027, "prefill_cuda_event_ms": 6.887551784515381, "kv_decode_ms": 751.069699999789, "kv_decode_cuda_event_ms": 751.0241088867188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 751.069699999789, "cuda_event_ms": 751.0241088867188, "tokens_total": 128, "tokens_per_s": 170.4235971708564}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548021.0495174, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.967700000132027, "prefill_cuda_event_ms": 6.887551784515381, "kv_decode_ms": 751.069699999789, "kv_decode_cuda_event_ms": 751.0241088867188, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 758.037399999921, "cuda_event_ms": 757.9116606712341, "tokens_total": 137, "tokens_per_s": 180.72986900120532}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548021.808346, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.682500000351865, "prefill_cuda_event_ms": 6.618207931518555, "kv_decode_ms": 772.877100000187, "kv_decode_cuda_event_ms": 772.8261108398438, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 6.682500000351865, "cuda_event_ms": 6.618207931518555, "tokens_total": 9, "tokens_per_s": 1346.8013467304313}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548021.808346, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.682500000351865, "prefill_cuda_event_ms": 6.618207931518555, "kv_decode_ms": 772.877100000187, "kv_decode_cuda_event_ms": 772.8261108398438, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 772.877100000187, "cuda_event_ms": 772.8261108398438, "tokens_total": 128, "tokens_per_s": 165.61494705946006}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548021.808346, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.682500000351865, "prefill_cuda_event_ms": 6.618207931518555, "kv_decode_ms": 772.877100000187, "kv_decode_cuda_event_ms": 772.8261108398438, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 779.5596000005389, "cuda_event_ms": 779.4443187713623, "tokens_total": 137, "tokens_per_s": 175.740251290479}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548022.5886471, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.626500000493252, "prefill_cuda_event_ms": 6.569952011108398, "kv_decode_ms": 770.9236000000601, "kv_decode_cuda_event_ms": 770.8845825195312, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 6.626500000493252, "cuda_event_ms": 6.569952011108398, "tokens_total": 9, "tokens_per_s": 1358.1830527925865}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548022.5886471, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.626500000493252, "prefill_cuda_event_ms": 6.569952011108398, "kv_decode_ms": 770.9236000000601, "kv_decode_cuda_event_ms": 770.8845825195312, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 770.9236000000601, "cuda_event_ms": 770.8845825195312, "tokens_total": 128, "tokens_per_s": 166.0346109523564}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548022.5886471, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 6.626500000493252, "prefill_cuda_event_ms": 6.569952011108398, "kv_decode_ms": 770.9236000000601, "kv_decode_cuda_event_ms": 770.8845825195312, "gpu_peak_mb": 578.93505859375, "params_millions_measured": 5.03672, "latency_ms": 777.5501000005534, "cuda_event_ms": 777.4545345306396, "tokens_total": 137, "tokens_per_s": 176.19443428777453}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548023.366894, "prompt_tokens": 38, "prefill_ms": 21.9772, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.3113, "kv_decode_ms_equiv": 1427.424182857143, "kv_decode_ms_per_token": 11.15175142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 6763.652600000569, "ollama_total_duration_ms": 6749.2234, "ollama_load_ms": 6295.1336, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 21.9772, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1729.0646670185465}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548023.366894, "prompt_tokens": 38, "prefill_ms": 21.9772, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.3113, "kv_decode_ms_equiv": 1427.424182857143, "kv_decode_ms_per_token": 11.15175142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6763.652600000569, "ollama_total_duration_ms": 6749.2234, "ollama_load_ms": 6295.1336, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1427.424182857143, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.67201308289049}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548023.366894, "prompt_tokens": 38, "prefill_ms": 21.9772, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.3113, "kv_decode_ms_equiv": 1427.424182857143, "kv_decode_ms_per_token": 11.15175142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 6763.652600000569, "ollama_total_duration_ms": 6749.2234, "ollama_load_ms": 6295.1336, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1449.401382857143, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.5300411351694}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548030.1306555, "prompt_tokens": 38, "prefill_ms": 11.9485, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3208, "kv_decode_ms_equiv": 1434.7732114285716, "kv_decode_ms_per_token": 11.209165714285716, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 643.2527999995727, "ollama_total_duration_ms": 640.4549, "ollama_load_ms": 199.9505, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.9485, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3180.3155207766667}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548030.1306555, "prompt_tokens": 38, "prefill_ms": 11.9485, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3208, "kv_decode_ms_equiv": 1434.7732114285716, "kv_decode_ms_per_token": 11.209165714285716, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 643.2527999995727, "ollama_total_duration_ms": 640.4549, "ollama_load_ms": 199.9505, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1434.7732114285716, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 89.21270552058417}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548030.1306555, "prompt_tokens": 38, "prefill_ms": 11.9485, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.3208, "kv_decode_ms_equiv": 1434.7732114285716, "kv_decode_ms_per_token": 11.209165714285716, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 643.2527999995727, "ollama_total_duration_ms": 640.4549, "ollama_load_ms": 199.9505, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1446.7217114285716, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.7421779106934}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548030.7741392, "prompt_tokens": 38, "prefill_ms": 11.7516, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.7828, "kv_decode_ms_equiv": 1440.1199542857144, "kv_decode_ms_per_token": 11.250937142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 675.1973000000362, "ollama_total_duration_ms": 670.9611, "ollama_load_ms": 228.9615, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.7516, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3233.602232887437}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548030.7741392, "prompt_tokens": 38, "prefill_ms": 11.7516, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.7828, "kv_decode_ms_equiv": 1440.1199542857144, "kv_decode_ms_per_token": 11.250937142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 675.1973000000362, "ollama_total_duration_ms": 670.9611, "ollama_load_ms": 228.9615, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1440.1199542857144, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.88148492011332}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548030.7741392, "prompt_tokens": 38, "prefill_ms": 11.7516, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.7828, "kv_decode_ms_equiv": 1440.1199542857144, "kv_decode_ms_per_token": 11.250937142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 675.1973000000362, "ollama_total_duration_ms": 670.9611, "ollama_load_ms": 228.9615, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1451.8715542857144, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.33518310210849}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548031.4494505, "prompt_tokens": 38, "prefill_ms": 12.1219, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8175, "kv_decode_ms_equiv": 1440.2468571428572, "kv_decode_ms_per_token": 11.251928571428572, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.0554999998349, "ollama_total_duration_ms": 695.8288, "ollama_load_ms": 254.4466, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.1219, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3134.8220988458907}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548031.4494505, "prompt_tokens": 38, "prefill_ms": 12.1219, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8175, "kv_decode_ms_equiv": 1440.2468571428572, "kv_decode_ms_per_token": 11.251928571428572, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.0554999998349, "ollama_total_duration_ms": 695.8288, "ollama_load_ms": 254.4466, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1440.2468571428572, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 88.87365340544797}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548031.4494505, "prompt_tokens": 38, "prefill_ms": 12.1219, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.8175, "kv_decode_ms_equiv": 1440.2468571428572, "kv_decode_ms_per_token": 11.251928571428572, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 715.0554999998349, "ollama_total_duration_ms": 695.8288, "ollama_load_ms": 254.4466, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1452.3687571428572, "cuda_event_ms": null, "tokens_total": 166, "tokens_per_s": 114.2960416792221}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548032.1646738, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.615399999944202, "prefill_cuda_event_ms": 3.521536111831665, "kv_decode_ms": 248.06069999976899, "kv_decode_cuda_event_ms": 248.01280212402344, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 196.4336999999432, "params_millions_measured": 25.016064, "latency_ms": 3.615399999944202, "cuda_event_ms": 3.521536111831665, "tokens_total": 17, "tokens_per_s": 4702.107650678312}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548032.1646738, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.615399999944202, "prefill_cuda_event_ms": 3.521536111831665, "kv_decode_ms": 248.06069999976899, "kv_decode_cuda_event_ms": 248.01280212402344, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 196.4336999999432, "params_millions_measured": 25.016064, "latency_ms": 248.06069999976899, "cuda_event_ms": 248.01280212402344, "tokens_total": 128, "tokens_per_s": 516.0027364274922}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548032.1646738, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.615399999944202, "prefill_cuda_event_ms": 3.521536111831665, "kv_decode_ms": 248.06069999976899, "kv_decode_cuda_event_ms": 248.01280212402344, "gpu_peak_mb": 631.50439453125, "hf_load_ms": 196.4336999999432, "params_millions_measured": 25.016064, "latency_ms": 251.6760999997132, "cuda_event_ms": 251.5343382358551, "tokens_total": 145, "tokens_per_s": 576.1373447862759}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548032.613879, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.3014999999068095, "prefill_cuda_event_ms": 3.212383985519409, "kv_decode_ms": 231.9878999996945, "kv_decode_cuda_event_ms": 231.9226837158203, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 3.3014999999068095, "cuda_event_ms": 3.212383985519409, "tokens_total": 17, "tokens_per_s": 5149.174617743406}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548032.613879, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.3014999999068095, "prefill_cuda_event_ms": 3.212383985519409, "kv_decode_ms": 231.9878999996945, "kv_decode_cuda_event_ms": 231.9226837158203, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 231.9878999996945, "cuda_event_ms": 231.9226837158203, "tokens_total": 128, "tokens_per_s": 551.752914700157}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548032.613879, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.3014999999068095, "prefill_cuda_event_ms": 3.212383985519409, "kv_decode_ms": 231.9878999996945, "kv_decode_cuda_event_ms": 231.9226837158203, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 235.2893999996013, "cuda_event_ms": 235.13506770133972, "tokens_total": 145, "tokens_per_s": 616.2623560612833}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548032.8499293, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.4806000001262873, "prefill_cuda_event_ms": 2.4186880588531494, "kv_decode_ms": 226.1993000001894, "kv_decode_cuda_event_ms": 226.14437866210938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 2.4806000001262873, "cuda_event_ms": 2.4186880588531494, "tokens_total": 17, "tokens_per_s": 6853.180681744147}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548032.8499293, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 2.4806000001262873, "prefill_cuda_event_ms": 2.4186880588531494, "kv_decode_ms": 226.1993000001894, "kv_decode_cuda_event_ms": 226.14437866210938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 226.1993000001894, "cuda_event_ms": 226.14437866210938, "tokens_total": 128, "tokens_per_s": 565.8726618512649}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548032.8499293, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 2.4806000001262873, "prefill_cuda_event_ms": 2.4186880588531494, "kv_decode_ms": 226.1993000001894, "kv_decode_cuda_event_ms": 226.14437866210938, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 228.67990000031568, "cuda_event_ms": 228.56306672096252, "tokens_total": 145, "tokens_per_s": 634.0740922127386}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548033.0793421, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.377599999817903, "prefill_cuda_event_ms": 2.3176000118255615, "kv_decode_ms": 243.57380000037665, "kv_decode_cuda_event_ms": 243.5235595703125, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 2.377599999817903, "cuda_event_ms": 2.3176000118255615, "tokens_total": 17, "tokens_per_s": 7150.067295298622}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548033.0793421, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 2.377599999817903, "prefill_cuda_event_ms": 2.3176000118255615, "kv_decode_ms": 243.57380000037665, "kv_decode_cuda_event_ms": 243.5235595703125, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 243.57380000037665, "cuda_event_ms": 243.5235595703125, "tokens_total": 128, "tokens_per_s": 525.5080800964721}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548033.0793421, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 2.377599999817903, "prefill_cuda_event_ms": 2.3176000118255615, "kv_decode_ms": 243.57380000037665, "kv_decode_cuda_event_ms": 243.5235595703125, "gpu_peak_mb": 631.50439453125, "params_millions_measured": 25.016064, "latency_ms": 245.95140000019455, "cuda_event_ms": 245.84115958213806, "tokens_total": 145, "tokens_per_s": 589.5473658612445}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548033.325984, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.68269999939366, "prefill_cuda_event_ms": null, "kv_decode_ms": 1263.8444000003801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 131.3359000005221, "params_millions_measured": 25.016064, "latency_ms": 34.68269999939366, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 259.4953680122177}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548033.325984, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 34.68269999939366, "prefill_cuda_event_ms": null, "kv_decode_ms": 1263.8444000003801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 131.3359000005221, "params_millions_measured": 25.016064, "latency_ms": 1263.8444000003801, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 101.27829027051234}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548033.325984, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 34.68269999939366, "prefill_cuda_event_ms": null, "kv_decode_ms": 1263.8444000003801, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 131.3359000005221, "params_millions_measured": 25.016064, "latency_ms": 1298.5270999997738, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 105.50415158838338}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548034.7567108, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.020500000384345, "prefill_cuda_event_ms": null, "kv_decode_ms": 933.079400000679, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.020500000384345, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1122.124555771924}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548034.7567108, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.020500000384345, "prefill_cuda_event_ms": null, "kv_decode_ms": 933.079400000679, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 933.079400000679, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 137.1801799502881}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548034.7567108, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.020500000384345, "prefill_cuda_event_ms": null, "kv_decode_ms": 933.079400000679, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 941.0999000010634, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 145.57434338250934}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548035.6982234, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.049900000514754, "prefill_cuda_event_ms": null, "kv_decode_ms": 1168.4337999995478, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.049900000514754, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1118.026310814357}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548035.6982234, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.049900000514754, "prefill_cuda_event_ms": null, "kv_decode_ms": 1168.4337999995478, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1168.4337999995478, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 109.54835438691481}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548035.6982234, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.049900000514754, "prefill_cuda_event_ms": null, "kv_decode_ms": 1168.4337999995478, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1176.4837000000625, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 116.44870217920803}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548036.8757231, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.930700000040815, "prefill_cuda_event_ms": null, "kv_decode_ms": 1188.1299000006038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.930700000040815, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 906.2805240278137}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548036.8757231, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 9.930700000040815, "prefill_cuda_event_ms": null, "kv_decode_ms": 1188.1299000006038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1188.1299000006038, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 107.73232792132826}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548036.8757231, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 9.930700000040815, "prefill_cuda_event_ms": null, "kv_decode_ms": 1188.1299000006038, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1198.0606000006446, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 114.35147771316934}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548038.0747235, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.863499999482883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1076.7665000003035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.863499999482883, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1723.5261317880331}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548038.0747235, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.863499999482883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1076.7665000003035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1076.7665000003035, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 118.87442634959754}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548038.0747235, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.863499999482883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1076.7665000003035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1086.6299999997864, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 133.4400854016809}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548039.1618605, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.285199999794713, "prefill_cuda_event_ms": null, "kv_decode_ms": 1210.2156999999352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.285199999794713, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1830.8706328755281}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548039.1618605, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.285199999794713, "prefill_cuda_event_ms": null, "kv_decode_ms": 1210.2156999999352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1210.2156999999352, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 105.76626959971422}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548039.1618605, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.285199999794713, "prefill_cuda_event_ms": null, "kv_decode_ms": 1210.2156999999352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1219.5008999997299, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 118.90110126202623}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548040.3818395, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.877699999786273, "prefill_cuda_event_ms": null, "kv_decode_ms": 1234.059399999751, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.877699999786273, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1431.2535255399528}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548040.3818395, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.877699999786273, "prefill_cuda_event_ms": null, "kv_decode_ms": 1234.059399999751, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1234.059399999751, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 103.7227219370687}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548040.3818395, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.877699999786273, "prefill_cuda_event_ms": null, "kv_decode_ms": 1234.059399999751, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1245.9370999995372, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 116.37826660756299}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548041.628254, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.634899999378831, "prefill_cuda_event_ms": null, "kv_decode_ms": 1049.0205999994942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.634899999378831, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1968.7547048863253}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548041.628254, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.634899999378831, "prefill_cuda_event_ms": null, "kv_decode_ms": 1049.0205999994942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1049.0205999994942, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 122.01857618435875}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548041.628254, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.634899999378831, "prefill_cuda_event_ms": null, "kv_decode_ms": 1049.0205999994942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1057.655499998873, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 137.0956800207199}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548042.6865344, "prompt_tokens": 46, "prefill_ms": 379.3129, "prefill_cuda_event_ms": null, "kv_decode_ms": 1469.1953, "kv_decode_ms_equiv": 1469.1953, "kv_decode_ms_per_token": 11.47808828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 2199.7801999996227, "ollama_total_duration_ms": 2195.389, "ollama_load_ms": 240.8, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 379.3129, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 121.27191034104034}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548042.6865344, "prompt_tokens": 46, "prefill_ms": 379.3129, "prefill_cuda_event_ms": null, "kv_decode_ms": 1469.1953, "kv_decode_ms_equiv": 1469.1953, "kv_decode_ms_per_token": 11.47808828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2199.7801999996227, "ollama_total_duration_ms": 2195.389, "ollama_load_ms": 240.8, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1469.1953, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.12252210444724}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548042.6865344, "prompt_tokens": 46, "prefill_ms": 379.3129, "prefill_cuda_event_ms": null, "kv_decode_ms": 1469.1953, "kv_decode_ms_equiv": 1469.1953, "kv_decode_ms_per_token": 11.47808828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 2199.7801999996227, "ollama_total_duration_ms": 2195.389, "ollama_load_ms": 240.8, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1848.5082000000002, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 94.1299584172794}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548044.8864403, "prompt_tokens": 46, "prefill_ms": 12.167, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.9608, "kv_decode_ms_equiv": 1459.9608, "kv_decode_ms_per_token": 11.40594375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1736.2327000000732, "ollama_total_duration_ms": 1708.5984, "ollama_load_ms": 125.6286, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.167, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3780.718336483932}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548044.8864403, "prompt_tokens": 46, "prefill_ms": 12.167, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.9608, "kv_decode_ms_equiv": 1459.9608, "kv_decode_ms_per_token": 11.40594375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1736.2327000000732, "ollama_total_duration_ms": 1708.5984, "ollama_load_ms": 125.6286, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1459.9608, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.6735868524689}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548044.8864403, "prompt_tokens": 46, "prefill_ms": 12.167, "prefill_cuda_event_ms": null, "kv_decode_ms": 1459.9608, "kv_decode_ms_equiv": 1459.9608, "kv_decode_ms_per_token": 11.40594375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1736.2327000000732, "ollama_total_duration_ms": 1708.5984, "ollama_load_ms": 125.6286, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1472.1278, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.19625986276463}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548046.6227474, "prompt_tokens": 46, "prefill_ms": 12.422, "prefill_cuda_event_ms": null, "kv_decode_ms": 1457.9395, "kv_decode_ms_equiv": 1457.9395, "kv_decode_ms_per_token": 11.39015234375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1751.16159999925, "ollama_total_duration_ms": 1724.4536, "ollama_load_ms": 125.4646, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.422, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3703.107390114313}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548046.6227474, "prompt_tokens": 46, "prefill_ms": 12.422, "prefill_cuda_event_ms": null, "kv_decode_ms": 1457.9395, "kv_decode_ms_equiv": 1457.9395, "kv_decode_ms_per_token": 11.39015234375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1751.16159999925, "ollama_total_duration_ms": 1724.4536, "ollama_load_ms": 125.4646, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1457.9395, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.79513827562803}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548046.6227474, "prompt_tokens": 46, "prefill_ms": 12.422, "prefill_cuda_event_ms": null, "kv_decode_ms": 1457.9395, "kv_decode_ms_equiv": 1457.9395, "kv_decode_ms_per_token": 11.39015234375, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1751.16159999925, "ollama_total_duration_ms": 1724.4536, "ollama_load_ms": 125.4646, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1470.3615, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.33824539067435}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548048.3740592, "prompt_tokens": 46, "prefill_ms": 11.8489, "prefill_cuda_event_ms": null, "kv_decode_ms": 1456.4634, "kv_decode_ms_equiv": 1456.4634, "kv_decode_ms_per_token": 11.3786203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 128, "ollama_wall_ms": 1782.4479000000792, "ollama_total_duration_ms": 1765.8852, "ollama_load_ms": 164.8109, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.8489, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3882.2169146503047}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548048.3740592, "prompt_tokens": 46, "prefill_ms": 11.8489, "prefill_cuda_event_ms": null, "kv_decode_ms": 1456.4634, "kv_decode_ms_equiv": 1456.4634, "kv_decode_ms_per_token": 11.3786203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1782.4479000000792, "ollama_total_duration_ms": 1765.8852, "ollama_load_ms": 164.8109, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1456.4634, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 87.8841171017411}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548048.3740592, "prompt_tokens": 46, "prefill_ms": 11.8489, "prefill_cuda_event_ms": null, "kv_decode_ms": 1456.4634, "kv_decode_ms_equiv": 1456.4634, "kv_decode_ms_per_token": 11.3786203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 128, "gen_tokens_equiv": 128, "ollama_wall_ms": 1782.4479000000792, "ollama_total_duration_ms": 1765.8852, "ollama_load_ms": 164.8109, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 1468.3123, "cuda_event_ms": null, "tokens_total": 174, "tokens_per_s": 118.50340012816075}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548050.1566749, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 66.45900000057736, "prefill_cuda_event_ms": null, "kv_decode_ms": 3997.5817999993524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 66.45900000057736, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 135.42183902739754}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548050.1566749, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 66.45900000057736, "prefill_cuda_event_ms": null, "kv_decode_ms": 3997.5817999993524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 3997.5817999993524, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 32.01935730246239}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548050.1566749, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 66.45900000057736, "prefill_cuda_event_ms": null, "kv_decode_ms": 3997.5817999993524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 4064.0407999999297, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 33.71029149116868}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548054.222773, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.074299999811046, "prefill_cuda_event_ms": null, "kv_decode_ms": 4111.644999999953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 39.074299999811046, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 230.33042178730065}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548054.222773, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 39.074299999811046, "prefill_cuda_event_ms": null, "kv_decode_ms": 4111.644999999953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 4111.644999999953, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 31.131092300040848}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548054.222773, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 39.074299999811046, "prefill_cuda_event_ms": null, "kv_decode_ms": 4111.644999999953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 4150.719299999764, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 33.00632736114142}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548058.3740988, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 43.837400000484195, "prefill_cuda_event_ms": null, "kv_decode_ms": 3563.229400000637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 43.837400000484195, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 205.30414668526402}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548058.3740988, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 43.837400000484195, "prefill_cuda_event_ms": null, "kv_decode_ms": 3563.229400000637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 3563.229400000637, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 35.92246965631153}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548058.3740988, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 43.837400000484195, "prefill_cuda_event_ms": null, "kv_decode_ms": 3563.229400000637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 3607.066800001121, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 37.98099885479177}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548061.9819055, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.54889999939769, "prefill_cuda_event_ms": null, "kv_decode_ms": 2396.8050999992556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 26.54889999939769, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 338.997095932569}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548061.9819055, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 26.54889999939769, "prefill_cuda_event_ms": null, "kv_decode_ms": 2396.8050999992556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2396.8050999992556, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 53.404425749945105}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548061.9819055, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 26.54889999939769, "prefill_cuda_event_ms": null, "kv_decode_ms": 2396.8050999992556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 2423.3539999986533, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 56.53321801110202}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548064.4060817, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.324899999934132, "prefill_cuda_event_ms": 8.160032272338867, "kv_decode_ms": 901.0222999995676, "kv_decode_cuda_event_ms": 900.9879150390625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 8.324899999934132, "cuda_event_ms": 8.160032272338867, "tokens_total": 17, "tokens_per_s": 2042.0665713863839}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548064.4060817, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.324899999934132, "prefill_cuda_event_ms": 8.160032272338867, "kv_decode_ms": 901.0222999995676, "kv_decode_cuda_event_ms": 900.9879150390625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 901.0222999995676, "cuda_event_ms": 900.9879150390625, "tokens_total": 128, "tokens_per_s": 142.06085687342193}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548064.4060817, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.324899999934132, "prefill_cuda_event_ms": 8.160032272338867, "kv_decode_ms": 901.0222999995676, "kv_decode_cuda_event_ms": 900.9879150390625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 909.3471999995018, "cuda_event_ms": 909.1479473114014, "tokens_total": 145, "tokens_per_s": 159.4550464333969}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548065.3168604, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.42350000039005, "prefill_cuda_event_ms": 8.358912467956543, "kv_decode_ms": 1137.630199999876, "kv_decode_cuda_event_ms": 1137.59033203125, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 8.42350000039005, "cuda_event_ms": 8.358912467956543, "tokens_total": 17, "tokens_per_s": 2018.1634711477195}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548065.3168604, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.42350000039005, "prefill_cuda_event_ms": 8.358912467956543, "kv_decode_ms": 1137.630199999876, "kv_decode_cuda_event_ms": 1137.59033203125, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1137.630199999876, "cuda_event_ms": 1137.59033203125, "tokens_total": 128, "tokens_per_s": 112.51459393396374}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548065.3168604, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 8.42350000039005, "prefill_cuda_event_ms": 8.358912467956543, "kv_decode_ms": 1137.630199999876, "kv_decode_cuda_event_ms": 1137.59033203125, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1146.053700000266, "cuda_event_ms": 1145.9492444992065, "tokens_total": 145, "tokens_per_s": 126.52112200324152}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548066.4637911, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.480800000754243, "prefill_cuda_event_ms": 11.378687858581543, "kv_decode_ms": 1693.9078000004884, "kv_decode_cuda_event_ms": 1693.8638916015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 11.480800000754243, "cuda_event_ms": 11.378687858581543, "tokens_total": 17, "tokens_per_s": 1480.733049864397}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548066.4637911, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.480800000754243, "prefill_cuda_event_ms": 11.378687858581543, "kv_decode_ms": 1693.9078000004884, "kv_decode_cuda_event_ms": 1693.8638916015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1693.9078000004884, "cuda_event_ms": 1693.8638916015625, "tokens_total": 128, "tokens_per_s": 75.56491563470166}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548066.4637911, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 11.480800000754243, "prefill_cuda_event_ms": 11.378687858581543, "kv_decode_ms": 1693.9078000004884, "kv_decode_cuda_event_ms": 1693.8638916015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1705.3886000012426, "cuda_event_ms": 1705.242579460144, "tokens_total": 145, "tokens_per_s": 85.02460964022765}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548068.1702247, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.057700000084878, "prefill_cuda_event_ms": 8.956928253173828, "kv_decode_ms": 1134.104099999604, "kv_decode_cuda_event_ms": 1133.982666015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 9.057700000084878, "cuda_event_ms": 8.956928253173828, "tokens_total": 17, "tokens_per_s": 1876.8561555185859}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548068.1702247, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.057700000084878, "prefill_cuda_event_ms": 8.956928253173828, "kv_decode_ms": 1134.104099999604, "kv_decode_cuda_event_ms": 1133.982666015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1134.104099999604, "cuda_event_ms": 1133.982666015625, "tokens_total": 128, "tokens_per_s": 112.86441870728154}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548068.1702247, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 9.057700000084878, "prefill_cuda_event_ms": 8.956928253173828, "kv_decode_ms": 1134.104099999604, "kv_decode_cuda_event_ms": 1133.982666015625, "gpu_peak_mb": 632.99609375, "params_millions_measured": 74.824704, "latency_ms": 1143.1617999996888, "cuda_event_ms": 1142.9395942687988, "tokens_total": 145, "tokens_per_s": 126.84118731052723}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548069.314472, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 14.913500000147906, "prefill_cuda_event_ms": 14.806015968322754, "kv_decode_ms": 1512.8096000007645, "kv_decode_cuda_event_ms": 1512.6671142578125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 14.913500000147906, "cuda_event_ms": 14.806015968322754, "tokens_total": 9, "tokens_per_s": 603.4800683884228}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548069.314472, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.913500000147906, "prefill_cuda_event_ms": 14.806015968322754, "kv_decode_ms": 1512.8096000007645, "kv_decode_cuda_event_ms": 1512.6671142578125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 1512.8096000007645, "cuda_event_ms": 1512.6671142578125, "tokens_total": 128, "tokens_per_s": 84.6107798363623}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548069.314472, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 14.913500000147906, "prefill_cuda_event_ms": 14.806015968322754, "kv_decode_ms": 1512.8096000007645, "kv_decode_cuda_event_ms": 1512.6671142578125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 1527.7231000009124, "cuda_event_ms": 1527.4731302261353, "tokens_total": 137, "tokens_per_s": 89.67593669292438}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548070.8433163, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.50490000013815, "prefill_cuda_event_ms": 9.432064056396484, "kv_decode_ms": 1293.288500000017, "kv_decode_cuda_event_ms": 1293.223876953125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 9.50490000013815, "cuda_event_ms": 9.432064056396484, "tokens_total": 9, "tokens_per_s": 946.8800302863984}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548070.8433163, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 9.50490000013815, "prefill_cuda_event_ms": 9.432064056396484, "kv_decode_ms": 1293.288500000017, "kv_decode_cuda_event_ms": 1293.223876953125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 1293.288500000017, "cuda_event_ms": 1293.223876953125, "tokens_total": 128, "tokens_per_s": 98.97250304166342}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548070.8433163, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 9.50490000013815, "prefill_cuda_event_ms": 9.432064056396484, "kv_decode_ms": 1293.288500000017, "kv_decode_cuda_event_ms": 1293.223876953125, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 1302.793400000155, "cuda_event_ms": 1302.6559410095215, "tokens_total": 137, "tokens_per_s": 105.15865370517204}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548072.1469898, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.615100000497478, "prefill_cuda_event_ms": 8.524800300598145, "kv_decode_ms": 777.7859999996508, "kv_decode_cuda_event_ms": 777.7167358398438, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 8.615100000497478, "cuda_event_ms": 8.524800300598145, "tokens_total": 9, "tokens_per_s": 1044.677368745609}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548072.1469898, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.615100000497478, "prefill_cuda_event_ms": 8.524800300598145, "kv_decode_ms": 777.7859999996508, "kv_decode_cuda_event_ms": 777.7167358398438, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 777.7859999996508, "cuda_event_ms": 777.7167358398438, "tokens_total": 128, "tokens_per_s": 164.56968883479192}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548072.1469898, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 8.615100000497478, "prefill_cuda_event_ms": 8.524800300598145, "kv_decode_ms": 777.7859999996508, "kv_decode_cuda_event_ms": 777.7167358398438, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 786.4011000001483, "cuda_event_ms": 786.2415361404419, "tokens_total": 137, "tokens_per_s": 174.2113534683181}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548072.9343035, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.667899999934889, "prefill_cuda_event_ms": 5.618688106536865, "kv_decode_ms": 618.4510000002774, "kv_decode_cuda_event_ms": 618.4181518554688, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 5.667899999934889, "cuda_event_ms": 5.618688106536865, "tokens_total": 9, "tokens_per_s": 1587.8896946141233}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548072.9343035, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.667899999934889, "prefill_cuda_event_ms": 5.618688106536865, "kv_decode_ms": 618.4510000002774, "kv_decode_cuda_event_ms": 618.4181518554688, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 618.4510000002774, "cuda_event_ms": 618.4181518554688, "tokens_total": 128, "tokens_per_s": 206.96870083473482}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548072.9343035, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 5.667899999934889, "prefill_cuda_event_ms": 5.618688106536865, "kv_decode_ms": 618.4510000002774, "kv_decode_cuda_event_ms": 618.4181518554688, "gpu_peak_mb": 632.240234375, "params_millions_measured": 51.475968, "latency_ms": 624.1189000002123, "cuda_event_ms": 624.0368399620056, "tokens_total": 137, "tokens_per_s": 219.50945565012276}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548073.5593944, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 484.8653999997623, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.867199999986, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 136.06849999996484, "params_millions_measured": 74.824704, "latency_ms": 484.8653999997623, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 18.56185242338268}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548073.5593944, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 484.8653999997623, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.867199999986, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 136.06849999996484, "params_millions_measured": 74.824704, "latency_ms": 2458.867199999986, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 52.05649170479834}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548073.5593944, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 484.8653999997623, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.867199999986, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 136.06849999996484, "params_millions_measured": 74.824704, "latency_ms": 2943.7325999997483, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 46.53955321893426}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548076.6400902, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 23.850300000049174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2160.6517000000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 23.850300000049174, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 377.35374397728515}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548076.6400902, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 23.850300000049174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2160.6517000000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2160.6517000000167, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 59.24138536535019}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548076.6400902, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 23.850300000049174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2160.6517000000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2184.502000000066, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 62.71452257768401}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548078.8252492, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.34239999991405, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3471000000063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.34239999991405, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 465.29903218008064}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548078.8252492, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 19.34239999991405, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3471000000063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2067.3471000000063, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 61.915098823995066}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548078.8252492, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 19.34239999991405, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3471000000063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2086.6894999999204, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 65.65423365575244}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548080.912599, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.52720000008412, "prefill_cuda_event_ms": null, "kv_decode_ms": 2040.5835000001389, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.52720000008412, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 460.8955713036804}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548080.912599, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 19.52720000008412, "prefill_cuda_event_ms": null, "kv_decode_ms": 2040.5835000001389, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2040.5835000001389, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 62.72715622761396}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548080.912599, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 19.52720000008412, "prefill_cuda_event_ms": null, "kv_decode_ms": 2040.5835000001389, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2060.110700000223, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 66.50128073213986}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548082.9731894, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.27990000048885, "prefill_cuda_event_ms": null, "kv_decode_ms": 2135.779000000184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 23.27990000048885, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 730.2436865984398}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548082.9731894, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 23.27990000048885, "prefill_cuda_event_ms": null, "kv_decode_ms": 2135.779000000184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2135.779000000184, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 59.93129438953607}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548082.9731894, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 23.27990000048885, "prefill_cuda_event_ms": null, "kv_decode_ms": 2135.779000000184, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2159.058900000673, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 67.15889038504451}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548085.1328485, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.19539999987319, "prefill_cuda_event_ms": null, "kv_decode_ms": 2019.811999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.19539999987319, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 765.9244708406754}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548085.1328485, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.19539999987319, "prefill_cuda_event_ms": null, "kv_decode_ms": 2019.811999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2019.811999999547, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 63.37223464363451}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548085.1328485, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.19539999987319, "prefill_cuda_event_ms": null, "kv_decode_ms": 2019.811999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2042.0073999994202, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 71.00855755960589}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548087.1754339, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.685300000171992, "prefill_cuda_event_ms": null, "kv_decode_ms": 2001.245800000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.685300000171992, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 783.9411951813057}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548087.1754339, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 21.685300000171992, "prefill_cuda_event_ms": null, "kv_decode_ms": 2001.245800000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2001.245800000106, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 63.96015921682045}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548087.1754339, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 21.685300000171992, "prefill_cuda_event_ms": null, "kv_decode_ms": 2001.245800000106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2022.931100000278, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 71.678170353889}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548089.1988087, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.272800000791904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2034.222299999783, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.272800000791904, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 763.2628138085723}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548089.1988087, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.272800000791904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2034.222299999783, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2034.222299999783, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 62.923309807396}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548089.1988087, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 22.272800000791904, "prefill_cuda_event_ms": null, "kv_decode_ms": 2034.222299999783, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2056.495100000575, "cuda_event_ms": null, "tokens_total": 145, "tokens_per_s": 70.50831290575867}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548091.2557745, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.70260000021517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8986000000004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.70260000021517, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 656.8096565512146}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548091.2557745, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 13.70260000021517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8986000000004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1155.8986000000004, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 110.73635697802554}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548091.2557745, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 13.70260000021517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8986000000004, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1169.6012000002156, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 117.13394274901115}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548092.4259229, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.142800000219722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.9312999996837, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.142800000219722, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 741.1799584805108}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548092.4259229, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.142800000219722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.9312999996837, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1150.9312999996837, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 111.21428359801769}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548092.4259229, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.142800000219722, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.9312999996837, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1163.0740999999034, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 117.79129119977082}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548093.5893936, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.989299999389914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1128.2412999998996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.989299999389914, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 692.877984219528}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548093.5893936, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.989299999389914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1128.2412999998996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1128.2412999998996, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 113.45090806373724}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548093.5893936, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.989299999389914, "prefill_cuda_event_ms": null, "kv_decode_ms": 1128.2412999998996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1141.2305999992896, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 120.04585225815474}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548094.731318, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.925900000482216, "prefill_cuda_event_ms": null, "kv_decode_ms": 1151.668299999983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.925900000482216, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 696.2764681503218}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548094.731318, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.925900000482216, "prefill_cuda_event_ms": null, "kv_decode_ms": 1151.668299999983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1151.668299999983, "cuda_event_ms": null, "tokens_total": 128, "tokens_per_s": 111.14311299529724}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548094.731318, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 12.925900000482216, "prefill_cuda_event_ms": null, "kv_decode_ms": 1151.668299999983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1164.5942000004652, "cuda_event_ms": null, "tokens_total": 137, "tokens_per_s": 117.63754275948247}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548095.8964398, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.120699999177305, "prefill_cuda_event_ms": 7.023615837097168, "kv_decode_ms": 332.7186000005895, "kv_decode_cuda_event_ms": 332.63104248046875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 7.120699999177305, "cuda_event_ms": 7.023615837097168, "tokens_total": 9, "tokens_per_s": 1263.9206821014538}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548095.8964398, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 7.120699999177305, "prefill_cuda_event_ms": 7.023615837097168, "kv_decode_ms": 332.7186000005895, "kv_decode_cuda_event_ms": 332.63104248046875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 332.7186000005895, "cuda_event_ms": 332.63104248046875, "tokens_total": 128, "tokens_per_s": 384.709481224594}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548095.8964398, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 7.120699999177305, "prefill_cuda_event_ms": 7.023615837097168, "kv_decode_ms": 332.7186000005895, "kv_decode_cuda_event_ms": 332.63104248046875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 339.8392999997668, "cuda_event_ms": 339.6546583175659, "tokens_total": 137, "tokens_per_s": 403.1317154905098}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548096.2384791, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.5001000003612717, "prefill_cuda_event_ms": 2.4299519062042236, "kv_decode_ms": 224.96270000010554, "kv_decode_cuda_event_ms": 224.91033935546875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.5001000003612717, "cuda_event_ms": 2.4299519062042236, "tokens_total": 9, "tokens_per_s": 3599.85600523958}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548096.2384791, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.5001000003612717, "prefill_cuda_event_ms": 2.4299519062042236, "kv_decode_ms": 224.96270000010554, "kv_decode_cuda_event_ms": 224.91033935546875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 224.96270000010554, "cuda_event_ms": 224.91033935546875, "tokens_total": 128, "tokens_per_s": 568.9832136613757}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548096.2384791, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.5001000003612717, "prefill_cuda_event_ms": 2.4299519062042236, "kv_decode_ms": 224.96270000010554, "kv_decode_cuda_event_ms": 224.91033935546875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 227.4628000004668, "cuda_event_ms": 227.34029126167297, "tokens_total": 137, "tokens_per_s": 602.2962875675444}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548096.4667032, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.4924999997892883, "prefill_cuda_event_ms": 2.417664051055908, "kv_decode_ms": 230.12010000002192, "kv_decode_cuda_event_ms": 230.04588317871094, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.4924999997892883, "cuda_event_ms": 2.417664051055908, "tokens_total": 9, "tokens_per_s": 3610.8324977977313}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548096.4667032, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.4924999997892883, "prefill_cuda_event_ms": 2.417664051055908, "kv_decode_ms": 230.12010000002192, "kv_decode_cuda_event_ms": 230.04588317871094, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 230.12010000002192, "cuda_event_ms": 230.04588317871094, "tokens_total": 128, "tokens_per_s": 556.2312896613021}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548096.4667032, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.4924999997892883, "prefill_cuda_event_ms": 2.417664051055908, "kv_decode_ms": 230.12010000002192, "kv_decode_cuda_event_ms": 230.04588317871094, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 232.6125999998112, "cuda_event_ms": 232.46354722976685, "tokens_total": 137, "tokens_per_s": 588.9620768613188}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548096.700019, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.5519999999232823, "prefill_cuda_event_ms": 2.471872091293335, "kv_decode_ms": 228.0243999994127, "kv_decode_cuda_event_ms": 227.957763671875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 2.5519999999232823, "cuda_event_ms": 2.471872091293335, "tokens_total": 9, "tokens_per_s": 3526.645768131096}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548096.700019, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.5519999999232823, "prefill_cuda_event_ms": 2.471872091293335, "kv_decode_ms": 228.0243999994127, "kv_decode_cuda_event_ms": 227.957763671875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 228.0243999994127, "cuda_event_ms": 227.957763671875, "tokens_total": 128, "tokens_per_s": 561.3434351776813}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 128, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548096.700019, "prompt_tokens": 9, "gen_tokens": 128, "prefill_ms": 2.5519999999232823, "prefill_cuda_event_ms": 2.471872091293335, "kv_decode_ms": 228.0243999994127, "kv_decode_cuda_event_ms": 227.957763671875, "gpu_peak_mb": 630.7021484375, "params_millions_measured": 25.016064, "latency_ms": 230.576399999336, "cuda_event_ms": 230.42963576316833, "tokens_total": 137, "tokens_per_s": 594.1631493960116}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548096.9314842, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.255200000443438, "prefill_cuda_event_ms": 4.164544105529785, "kv_decode_ms": 420.01059999984136, "kv_decode_cuda_event_ms": 419.98541259765625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.255200000443438, "cuda_event_ms": 4.164544105529785, "tokens_total": 17, "tokens_per_s": 3995.111862715834}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548096.9314842, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.255200000443438, "prefill_cuda_event_ms": 4.164544105529785, "kv_decode_ms": 420.01059999984136, "kv_decode_cuda_event_ms": 419.98541259765625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 420.01059999984136, "cuda_event_ms": 419.98541259765625, "tokens_total": 128, "tokens_per_s": 304.75421334615925}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548096.9314842, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.255200000443438, "prefill_cuda_event_ms": 4.164544105529785, "kv_decode_ms": 420.01059999984136, "kv_decode_cuda_event_ms": 419.98541259765625, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 424.2658000002848, "cuda_event_ms": 424.14995670318604, "tokens_total": 145, "tokens_per_s": 341.7668829302354}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548097.3732207, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.366699999991397, "prefill_cuda_event_ms": 4.285439968109131, "kv_decode_ms": 412.3591999996279, "kv_decode_cuda_event_ms": 412.31768798828125, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.366699999991397, "cuda_event_ms": 4.285439968109131, "tokens_total": 17, "tokens_per_s": 3893.1000526790235}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548097.3732207, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.366699999991397, "prefill_cuda_event_ms": 4.285439968109131, "kv_decode_ms": 412.3591999996279, "kv_decode_cuda_event_ms": 412.31768798828125, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 412.3591999996279, "cuda_event_ms": 412.31768798828125, "tokens_total": 128, "tokens_per_s": 310.408983236255}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548097.3732207, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.366699999991397, "prefill_cuda_event_ms": 4.285439968109131, "kv_decode_ms": 412.3591999996279, "kv_decode_cuda_event_ms": 412.31768798828125, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 416.7258999996193, "cuda_event_ms": 416.6031279563904, "tokens_total": 145, "tokens_per_s": 347.9505353522122}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548097.7911804, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.0350999997826875, "prefill_cuda_event_ms": 3.984384059906006, "kv_decode_ms": 405.56070000002364, "kv_decode_cuda_event_ms": 405.5357360839844, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 4.0350999997826875, "cuda_event_ms": 3.984384059906006, "tokens_total": 17, "tokens_per_s": 4213.030656220551}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548097.7911804, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.0350999997826875, "prefill_cuda_event_ms": 3.984384059906006, "kv_decode_ms": 405.56070000002364, "kv_decode_cuda_event_ms": 405.5357360839844, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 405.56070000002364, "cuda_event_ms": 405.5357360839844, "tokens_total": 128, "tokens_per_s": 315.6124348340274}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548097.7911804, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 4.0350999997826875, "prefill_cuda_event_ms": 3.984384059906006, "kv_decode_ms": 405.56070000002364, "kv_decode_cuda_event_ms": 405.5357360839844, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 409.59579999980633, "cuda_event_ms": 409.5201201438904, "tokens_total": 145, "tokens_per_s": 354.00753621025547}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548098.2014263, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.9286000001084176, "prefill_cuda_event_ms": 3.8850560188293457, "kv_decode_ms": 391.95509999990463, "kv_decode_cuda_event_ms": 391.91961669921875, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 3.9286000001084176, "cuda_event_ms": 3.8850560188293457, "tokens_total": 17, "tokens_per_s": 4327.241256307807}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548098.2014263, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.9286000001084176, "prefill_cuda_event_ms": 3.8850560188293457, "kv_decode_ms": 391.95509999990463, "kv_decode_cuda_event_ms": 391.91961669921875, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 391.95509999990463, "cuda_event_ms": 391.91961669921875, "tokens_total": 128, "tokens_per_s": 326.5680176123009}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 128, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548098.2014263, "prompt_tokens": 17, "gen_tokens": 128, "prefill_ms": 3.9286000001084176, "prefill_cuda_event_ms": 3.8850560188293457, "kv_decode_ms": 391.95509999990463, "kv_decode_cuda_event_ms": 391.91961669921875, "gpu_peak_mb": 632.56591796875, "params_millions_measured": 45.1712, "latency_ms": 395.88370000001305, "cuda_event_ms": 395.8046727180481, "tokens_total": 145, "tokens_per_s": 366.26918461152917}
