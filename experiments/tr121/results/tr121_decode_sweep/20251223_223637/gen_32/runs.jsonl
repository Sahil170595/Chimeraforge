{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547491.115399, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 110.28929999974935, "prefill_cuda_event_ms": null, "kv_decode_ms": 791.9636000001447, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7839.22369999982, "params_millions_measured": 45.1712, "latency_ms": 110.28929999974935, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 154.140066171774}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547491.115399, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 110.28929999974935, "prefill_cuda_event_ms": null, "kv_decode_ms": 791.9636000001447, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7839.22369999982, "params_millions_measured": 45.1712, "latency_ms": 791.9636000001447, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 40.405897442753876}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547491.115399, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 110.28929999974935, "prefill_cuda_event_ms": null, "kv_decode_ms": 791.9636000001447, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 7839.22369999982, "params_millions_measured": 45.1712, "latency_ms": 902.252899999894, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 54.30849820488885}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547499.8608057, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 46.55300000013085, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.8890999999057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 46.55300000013085, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 365.1751766793164}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547499.8608057, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.55300000013085, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.8890999999057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 773.8890999999057, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 41.349593888845185}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547499.8608057, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.55300000013085, "prefill_cuda_event_ms": null, "kv_decode_ms": 773.8890999999057, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 820.4421000000366, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 59.72389764006237}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547500.6818657, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.383100000341074, "prefill_cuda_event_ms": null, "kv_decode_ms": 786.566700000094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 49.383100000341074, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 344.24732347468233}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547500.6818657, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 49.383100000341074, "prefill_cuda_event_ms": null, "kv_decode_ms": 786.566700000094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 786.566700000094, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 40.683135962908395}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547500.6818657, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 49.383100000341074, "prefill_cuda_event_ms": null, "kv_decode_ms": 786.566700000094, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 835.9498000004351, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 58.615959953545655}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547501.5183563, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 46.39739999993253, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.476400000276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 46.39739999993253, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 366.3998413709544}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547501.5183563, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.39739999993253, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.476400000276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 749.476400000276, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 42.69647449871432}
{"task_idx": 0, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547501.5183563, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.39739999993253, "prefill_cuda_event_ms": null, "kv_decode_ms": 749.476400000276, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 795.8738000002086, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 61.56755003115715}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547502.3151743, "prompt_tokens": 92, "prefill_ms": 803.7963, "prefill_cuda_event_ms": null, "kv_decode_ms": 1335.7904, "kv_decode_ms_equiv": 1335.7904, "kv_decode_ms_per_token": 41.74345, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 11903.519199999664, "ollama_total_duration_ms": 11754.0641, "ollama_load_ms": 9534.2719, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 803.7963, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 114.45685928138758}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547502.3151743, "prompt_tokens": 92, "prefill_ms": 803.7963, "prefill_cuda_event_ms": null, "kv_decode_ms": 1335.7904, "kv_decode_ms_equiv": 1335.7904, "kv_decode_ms_per_token": 41.74345, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 11903.519199999664, "ollama_total_duration_ms": 11754.0641, "ollama_load_ms": 9534.2719, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1335.7904, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 23.95585415196875}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547502.3151743, "prompt_tokens": 92, "prefill_ms": 803.7963, "prefill_cuda_event_ms": null, "kv_decode_ms": 1335.7904, "kv_decode_ms_equiv": 1335.7904, "kv_decode_ms_per_token": 41.74345, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 11903.519199999664, "ollama_total_duration_ms": 11754.0641, "ollama_load_ms": 9534.2719, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2139.5867, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 57.955118154361315}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547514.218843, "prompt_tokens": 92, "prefill_ms": 55.4208, "prefill_cuda_event_ms": null, "kv_decode_ms": 1479.4225, "kv_decode_ms_equiv": 1479.4225, "kv_decode_ms_per_token": 46.231953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1799.1087000000334, "ollama_total_duration_ms": 1783.3984, "ollama_load_ms": 231.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.4208, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1660.0265604249669}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547514.218843, "prompt_tokens": 92, "prefill_ms": 55.4208, "prefill_cuda_event_ms": null, "kv_decode_ms": 1479.4225, "kv_decode_ms_equiv": 1479.4225, "kv_decode_ms_per_token": 46.231953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1799.1087000000334, "ollama_total_duration_ms": 1783.3984, "ollama_load_ms": 231.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1479.4225, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.63006173016836}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547514.218843, "prompt_tokens": 92, "prefill_ms": 55.4208, "prefill_cuda_event_ms": null, "kv_decode_ms": 1479.4225, "kv_decode_ms_equiv": 1479.4225, "kv_decode_ms_per_token": 46.231953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1799.1087000000334, "ollama_total_duration_ms": 1783.3984, "ollama_load_ms": 231.9808, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1534.8433, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 80.7900063804559}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547516.0180616, "prompt_tokens": 92, "prefill_ms": 60.6082, "prefill_cuda_event_ms": null, "kv_decode_ms": 1517.4975, "kv_decode_ms_equiv": 1517.4975, "kv_decode_ms_per_token": 47.421796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1860.454799999843, "ollama_total_duration_ms": 1832.0369, "ollama_load_ms": 245.7774, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 60.6082, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1517.946416491498}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547516.0180616, "prompt_tokens": 92, "prefill_ms": 60.6082, "prefill_cuda_event_ms": null, "kv_decode_ms": 1517.4975, "kv_decode_ms_equiv": 1517.4975, "kv_decode_ms_per_token": 47.421796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1860.454799999843, "ollama_total_duration_ms": 1832.0369, "ollama_load_ms": 245.7774, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1517.4975, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.087349402552558}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547516.0180616, "prompt_tokens": 92, "prefill_ms": 60.6082, "prefill_cuda_event_ms": null, "kv_decode_ms": 1517.4975, "kv_decode_ms_equiv": 1517.4975, "kv_decode_ms_per_token": 47.421796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1860.454799999843, "ollama_total_duration_ms": 1832.0369, "ollama_load_ms": 245.7774, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1578.1056999999998, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 78.57521837732416}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547517.8806953, "prompt_tokens": 92, "prefill_ms": 65.1548, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.8275, "kv_decode_ms_equiv": 1638.8275, "kv_decode_ms_per_token": 51.213359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1956.103400000302, "ollama_total_duration_ms": 1952.3229, "ollama_load_ms": 237.9143, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 65.1548, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1412.0218310853536}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547517.8806953, "prompt_tokens": 92, "prefill_ms": 65.1548, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.8275, "kv_decode_ms_equiv": 1638.8275, "kv_decode_ms_per_token": 51.213359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1956.103400000302, "ollama_total_duration_ms": 1952.3229, "ollama_load_ms": 237.9143, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1638.8275, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 19.526155132251564}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547517.8806953, "prompt_tokens": 92, "prefill_ms": 65.1548, "prefill_cuda_event_ms": null, "kv_decode_ms": 1638.8275, "kv_decode_ms_equiv": 1638.8275, "kv_decode_ms_per_token": 51.213359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1956.103400000302, "ollama_total_duration_ms": 1952.3229, "ollama_load_ms": 237.9143, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1703.9823000000001, "cuda_event_ms": null, "tokens_total": 124, "tokens_per_s": 72.7707089445706}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547519.8370373, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 875.1932000000124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1576.4792999998463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 317.0372999998108, "params_millions_measured": 96.08832, "latency_ms": 875.1932000000124, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 10.283443701344883}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547519.8370373, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 875.1932000000124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1576.4792999998463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 317.0372999998108, "params_millions_measured": 96.08832, "latency_ms": 1576.4792999998463, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.29839529133248}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547519.8370373, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 875.1932000000124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1576.4792999998463, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 317.0372999998108, "params_millions_measured": 96.08832, "latency_ms": 2451.6724999998587, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 16.723277680849446}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547522.6073146, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 64.98839999994743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1555.78370000012, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 64.98839999994743, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 138.48625293140438}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547522.6073146, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 64.98839999994743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1555.78370000012, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1555.78370000012, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 20.568411919984463}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547522.6073146, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 64.98839999994743, "prefill_cuda_event_ms": null, "kv_decode_ms": 1555.78370000012, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1620.7721000000674, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 25.296585497737958}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547524.2286327, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 51.99379999976372, "prefill_cuda_event_ms": null, "kv_decode_ms": 1323.7711000001582, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 51.99379999976372, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 173.0975616331351}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547524.2286327, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 51.99379999976372, "prefill_cuda_event_ms": null, "kv_decode_ms": 1323.7711000001582, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1323.7711000001582, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 24.17336350672422}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547524.2286327, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 51.99379999976372, "prefill_cuda_event_ms": null, "kv_decode_ms": 1323.7711000001582, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1375.764899999922, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 29.80160345710399}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547525.6050096, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 52.37729999998919, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.3159000002997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 52.37729999998919, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 171.83016306686022}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547525.6050096, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 52.37729999998919, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.3159000002997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1181.3159000002997, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 27.08843587053377}
{"task_idx": 2, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547525.6050096, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 52.37729999998919, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.3159000002997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1233.693200000289, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 33.23354623336693}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547526.8392096, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 855.6286000002729, "prefill_cuda_event_ms": 722.2056884765625, "kv_decode_ms": 282.2088999996595, "kv_decode_cuda_event_ms": 282.1191711425781, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 434.4274999998561, "params_millions_measured": 96.08832, "latency_ms": 855.6286000002729, "cuda_event_ms": 722.2056884765625, "tokens_total": 17, "tokens_per_s": 19.868433570353513}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547526.8392096, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 855.6286000002729, "prefill_cuda_event_ms": 722.2056884765625, "kv_decode_ms": 282.2088999996595, "kv_decode_cuda_event_ms": 282.1191711425781, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 434.4274999998561, "params_millions_measured": 96.08832, "latency_ms": 282.2088999996595, "cuda_event_ms": 282.1191711425781, "tokens_total": 32, "tokens_per_s": 113.39117937116303}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547526.8392096, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 855.6286000002729, "prefill_cuda_event_ms": 722.2056884765625, "kv_decode_ms": 282.2088999996595, "kv_decode_cuda_event_ms": 282.1191711425781, "gpu_peak_mb": 207.6767578125, "hf_load_ms": 434.4274999998561, "params_millions_measured": 96.08832, "latency_ms": 1137.8374999999323, "cuda_event_ms": 1004.3248596191406, "tokens_total": 49, "tokens_per_s": 43.064145802896206}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547528.4160416, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.9245000000155414, "prefill_cuda_event_ms": 4.855807781219482, "kv_decode_ms": 120.33010000004651, "kv_decode_cuda_event_ms": 120.30054473876953, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 4.9245000000155414, "cuda_event_ms": 4.855807781219482, "tokens_total": 17, "tokens_per_s": 3452.1271194936235}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547528.4160416, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.9245000000155414, "prefill_cuda_event_ms": 4.855807781219482, "kv_decode_ms": 120.33010000004651, "kv_decode_cuda_event_ms": 120.30054473876953, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 120.33010000004651, "cuda_event_ms": 120.30054473876953, "tokens_total": 32, "tokens_per_s": 265.9351234644335}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547528.4160416, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.9245000000155414, "prefill_cuda_event_ms": 4.855807781219482, "kv_decode_ms": 120.33010000004651, "kv_decode_cuda_event_ms": 120.30054473876953, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 125.25460000006206, "cuda_event_ms": 125.15635251998901, "tokens_total": 49, "tokens_per_s": 391.20319732748914}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547528.542033, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.557300000215037, "prefill_cuda_event_ms": 4.51859188079834, "kv_decode_ms": 121.20009999989634, "kv_decode_cuda_event_ms": 121.17180633544922, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 4.557300000215037, "cuda_event_ms": 4.51859188079834, "tokens_total": 17, "tokens_per_s": 3730.278893028296}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547528.542033, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.557300000215037, "prefill_cuda_event_ms": 4.51859188079834, "kv_decode_ms": 121.20009999989634, "kv_decode_cuda_event_ms": 121.17180633544922, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 121.20009999989634, "cuda_event_ms": 121.17180633544922, "tokens_total": 32, "tokens_per_s": 264.02618479710304}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547528.542033, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.557300000215037, "prefill_cuda_event_ms": 4.51859188079834, "kv_decode_ms": 121.20009999989634, "kv_decode_cuda_event_ms": 121.17180633544922, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 125.75740000011137, "cuda_event_ms": 125.69039821624756, "tokens_total": 49, "tokens_per_s": 389.63909877237126}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547528.6682856, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.730900000140537, "prefill_cuda_event_ms": 4.687871932983398, "kv_decode_ms": 117.55519999996977, "kv_decode_cuda_event_ms": 117.52105712890625, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 4.730900000140537, "cuda_event_ms": 4.687871932983398, "tokens_total": 17, "tokens_per_s": 3593.3966051903435}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547528.6682856, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.730900000140537, "prefill_cuda_event_ms": 4.687871932983398, "kv_decode_ms": 117.55519999996977, "kv_decode_cuda_event_ms": 117.52105712890625, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 117.55519999996977, "cuda_event_ms": 117.52105712890625, "tokens_total": 32, "tokens_per_s": 272.21254355407694}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547528.6682856, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.730900000140537, "prefill_cuda_event_ms": 4.687871932983398, "kv_decode_ms": 117.55519999996977, "kv_decode_cuda_event_ms": 117.52105712890625, "gpu_peak_mb": 207.6767578125, "params_millions_measured": 96.08832, "latency_ms": 122.28610000011031, "cuda_event_ms": 122.20892906188965, "tokens_total": 49, "tokens_per_s": 400.6996706899296}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547528.7912142, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 15.092700000423065, "prefill_cuda_event_ms": 15.005696296691895, "kv_decode_ms": 153.55589999990116, "kv_decode_cuda_event_ms": 153.48326110839844, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 331.54259999992064, "params_millions_measured": 51.475968, "latency_ms": 15.092700000423065, "cuda_event_ms": 15.005696296691895, "tokens_total": 17, "tokens_per_s": 1126.3723521651839}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547528.7912142, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 15.092700000423065, "prefill_cuda_event_ms": 15.005696296691895, "kv_decode_ms": 153.55589999990116, "kv_decode_cuda_event_ms": 153.48326110839844, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 331.54259999992064, "params_millions_measured": 51.475968, "latency_ms": 153.55589999990116, "cuda_event_ms": 153.48326110839844, "tokens_total": 32, "tokens_per_s": 208.39316496481476}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547528.7912142, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 15.092700000423065, "prefill_cuda_event_ms": 15.005696296691895, "kv_decode_ms": 153.55589999990116, "kv_decode_cuda_event_ms": 153.48326110839844, "gpu_peak_mb": 316.27099609375, "hf_load_ms": 331.54259999992064, "params_millions_measured": 51.475968, "latency_ms": 168.64860000032422, "cuda_event_ms": 168.48895740509033, "tokens_total": 49, "tokens_per_s": 290.5449556053581}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547529.2928224, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.890000000268628, "prefill_cuda_event_ms": 9.809696197509766, "kv_decode_ms": 147.8610000003755, "kv_decode_cuda_event_ms": 147.77719116210938, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 9.890000000268628, "cuda_event_ms": 9.809696197509766, "tokens_total": 17, "tokens_per_s": 1718.9079878198436}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547529.2928224, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 9.890000000268628, "prefill_cuda_event_ms": 9.809696197509766, "kv_decode_ms": 147.8610000003755, "kv_decode_cuda_event_ms": 147.77719116210938, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 147.8610000003755, "cuda_event_ms": 147.77719116210938, "tokens_total": 32, "tokens_per_s": 216.41947504696122}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547529.2928224, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 9.890000000268628, "prefill_cuda_event_ms": 9.809696197509766, "kv_decode_ms": 147.8610000003755, "kv_decode_cuda_event_ms": 147.77719116210938, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 157.75100000064413, "cuda_event_ms": 157.58688735961914, "tokens_total": 49, "tokens_per_s": 310.6160975195081}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547529.4513102, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.697100000157661, "prefill_cuda_event_ms": 4.647808074951172, "kv_decode_ms": 132.9182999998011, "kv_decode_cuda_event_ms": 132.87628173828125, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 4.697100000157661, "cuda_event_ms": 4.647808074951172, "tokens_total": 17, "tokens_per_s": 3619.2544334651984}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547529.4513102, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.697100000157661, "prefill_cuda_event_ms": 4.647808074951172, "kv_decode_ms": 132.9182999998011, "kv_decode_cuda_event_ms": 132.87628173828125, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 132.9182999998011, "cuda_event_ms": 132.87628173828125, "tokens_total": 32, "tokens_per_s": 240.74939267240018}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547529.4513102, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.697100000157661, "prefill_cuda_event_ms": 4.647808074951172, "kv_decode_ms": 132.9182999998011, "kv_decode_cuda_event_ms": 132.87628173828125, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 137.61539999995875, "cuda_event_ms": 137.52408981323242, "tokens_total": 49, "tokens_per_s": 356.06480088721673}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547529.5896146, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.429700000149751, "prefill_cuda_event_ms": 5.380095958709717, "kv_decode_ms": 134.94450000007419, "kv_decode_cuda_event_ms": 134.90789794921875, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 5.429700000149751, "cuda_event_ms": 5.380095958709717, "tokens_total": 17, "tokens_per_s": 3130.9280438203104}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547529.5896146, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.429700000149751, "prefill_cuda_event_ms": 5.380095958709717, "kv_decode_ms": 134.94450000007419, "kv_decode_cuda_event_ms": 134.90789794921875, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 134.94450000007419, "cuda_event_ms": 134.90789794921875, "tokens_total": 32, "tokens_per_s": 237.13452567523987}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547529.5896146, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 5.429700000149751, "prefill_cuda_event_ms": 5.380095958709717, "kv_decode_ms": 134.94450000007419, "kv_decode_cuda_event_ms": 134.90789794921875, "gpu_peak_mb": 316.27099609375, "params_millions_measured": 51.475968, "latency_ms": 140.37420000022394, "cuda_event_ms": 140.28799390792847, "tokens_total": 49, "tokens_per_s": 349.0669937917497}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547529.7306254, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 102.56470000012996, "prefill_cuda_event_ms": null, "kv_decode_ms": 651.5628000001925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.35720000017682, "params_millions_measured": 51.475968, "latency_ms": 102.56470000012996, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 87.74948885911621}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547529.7306254, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 102.56470000012996, "prefill_cuda_event_ms": null, "kv_decode_ms": 651.5628000001925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.35720000017682, "params_millions_measured": 51.475968, "latency_ms": 651.5628000001925, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 49.11268721908394}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547529.7306254, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 102.56470000012996, "prefill_cuda_event_ms": null, "kv_decode_ms": 651.5628000001925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 139.35720000017682, "params_millions_measured": 51.475968, "latency_ms": 754.1275000003225, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 54.36746438763004}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547530.624795, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.876799999972718, "prefill_cuda_event_ms": null, "kv_decode_ms": 825.8495999998559, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 21.876799999972718, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 411.39471952073535}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547530.624795, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.876799999972718, "prefill_cuda_event_ms": null, "kv_decode_ms": 825.8495999998559, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 825.8495999998559, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 38.747975418291155}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547530.624795, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.876799999972718, "prefill_cuda_event_ms": null, "kv_decode_ms": 825.8495999998559, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 847.7263999998286, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 48.36466105102813}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547531.4732924, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.15719999992507, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.2342999999128, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 46.15719999992507, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 194.9858310299284}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547531.4732924, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 46.15719999992507, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.2342999999128, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 883.2342999999128, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 36.23047700933168}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547531.4732924, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 46.15719999992507, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.2342999999128, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 929.3914999998378, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 44.1148859226786}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547532.4033802, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 41.26209999958519, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.2151000000486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 41.26209999958519, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 218.11783695183905}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547532.4033802, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 41.26209999958519, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.2151000000486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 887.2151000000486, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 36.06791633731014}
{"task_idx": 5, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547532.4033802, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 41.26209999958519, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.2151000000486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 928.4771999996337, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 44.158327205036564}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547533.33273, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 49.09459999998944, "prefill_cuda_event_ms": 49.00678253173828, "kv_decode_ms": 101.23010000006616, "kv_decode_cuda_event_ms": 101.19475555419922, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 235.47050000024683, "params_millions_measured": 45.1712, "latency_ms": 49.09459999998944, "cuda_event_ms": 49.00678253173828, "tokens_total": 9, "tokens_per_s": 183.31955041902646}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547533.33273, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 49.09459999998944, "prefill_cuda_event_ms": 49.00678253173828, "kv_decode_ms": 101.23010000006616, "kv_decode_cuda_event_ms": 101.19475555419922, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 235.47050000024683, "params_millions_measured": 45.1712, "latency_ms": 101.23010000006616, "cuda_event_ms": 101.19475555419922, "tokens_total": 32, "tokens_per_s": 316.1115122871467}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547533.33273, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 49.09459999998944, "prefill_cuda_event_ms": 49.00678253173828, "kv_decode_ms": 101.23010000006616, "kv_decode_cuda_event_ms": 101.19475555419922, "gpu_peak_mb": 408.2998046875, "hf_load_ms": 235.47050000024683, "params_millions_measured": 45.1712, "latency_ms": 150.3247000000556, "cuda_event_ms": 150.2015380859375, "tokens_total": 41, "tokens_per_s": 272.74293579155545}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547533.719576, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.9824999998927524, "prefill_cuda_event_ms": 3.932159900665283, "kv_decode_ms": 103.26609999992797, "kv_decode_cuda_event_ms": 103.2323226928711, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 3.9824999998927524, "cuda_event_ms": 3.932159900665283, "tokens_total": 9, "tokens_per_s": 2259.8870057105755}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547533.719576, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.9824999998927524, "prefill_cuda_event_ms": 3.932159900665283, "kv_decode_ms": 103.26609999992797, "kv_decode_cuda_event_ms": 103.2323226928711, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 103.26609999992797, "cuda_event_ms": 103.2323226928711, "tokens_total": 32, "tokens_per_s": 309.87904065344117}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547533.719576, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.9824999998927524, "prefill_cuda_event_ms": 3.932159900665283, "kv_decode_ms": 103.26609999992797, "kv_decode_cuda_event_ms": 103.2323226928711, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 107.24859999982073, "cuda_event_ms": 107.16448259353638, "tokens_total": 41, "tokens_per_s": 382.289372542565}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547533.8276253, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.052799999954004, "prefill_cuda_event_ms": 3.999743938446045, "kv_decode_ms": 102.73229999984324, "kv_decode_cuda_event_ms": 102.70207977294922, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 4.052799999954004, "cuda_event_ms": 3.999743938446045, "tokens_total": 9, "tokens_per_s": 2220.68693251632}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547533.8276253, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.052799999954004, "prefill_cuda_event_ms": 3.999743938446045, "kv_decode_ms": 102.73229999984324, "kv_decode_cuda_event_ms": 102.70207977294922, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 102.73229999984324, "cuda_event_ms": 102.70207977294922, "tokens_total": 32, "tokens_per_s": 311.4891811051522}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547533.8276253, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.052799999954004, "prefill_cuda_event_ms": 3.999743938446045, "kv_decode_ms": 102.73229999984324, "kv_decode_cuda_event_ms": 102.70207977294922, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 106.78509999979724, "cuda_event_ms": 106.70182371139526, "tokens_total": 41, "tokens_per_s": 383.948696963133}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547533.9349968, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.1544999999132415, "prefill_cuda_event_ms": 4.111264228820801, "kv_decode_ms": 102.66450000017358, "kv_decode_cuda_event_ms": 102.6079330444336, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 4.1544999999132415, "cuda_event_ms": 4.111264228820801, "tokens_total": 9, "tokens_per_s": 2166.32567100444}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547533.9349968, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.1544999999132415, "prefill_cuda_event_ms": 4.111264228820801, "kv_decode_ms": 102.66450000017358, "kv_decode_cuda_event_ms": 102.6079330444336, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 102.66450000017358, "cuda_event_ms": 102.6079330444336, "tokens_total": 32, "tokens_per_s": 311.69488966435233}
{"task_idx": 6, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547533.9349968, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.1544999999132415, "prefill_cuda_event_ms": 4.111264228820801, "kv_decode_ms": 102.66450000017358, "kv_decode_cuda_event_ms": 102.6079330444336, "gpu_peak_mb": 408.2998046875, "params_millions_measured": 45.1712, "latency_ms": 106.81900000008682, "cuda_event_ms": 106.7191972732544, "tokens_total": 41, "tokens_per_s": 383.826847283411}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547534.0424616, "prompt_tokens": 17, "prefill_ms": 8.0185, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2396, "kv_decode_ms_equiv": 69.2396, "kv_decode_ms_per_token": 2.1637375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 2794.691999999941, "ollama_total_duration_ms": 2736.8505, "ollama_load_ms": 2609.253, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.0185, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2120.097275051444}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547534.0424616, "prompt_tokens": 17, "prefill_ms": 8.0185, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2396, "kv_decode_ms_equiv": 69.2396, "kv_decode_ms_per_token": 2.1637375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2794.691999999941, "ollama_total_duration_ms": 2736.8505, "ollama_load_ms": 2609.253, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 69.2396, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 462.16327072946694}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547534.0424616, "prompt_tokens": 17, "prefill_ms": 8.0185, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2396, "kv_decode_ms_equiv": 69.2396, "kv_decode_ms_per_token": 2.1637375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2794.691999999941, "ollama_total_duration_ms": 2736.8505, "ollama_load_ms": 2609.253, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 77.2581, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 634.2377045254802}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547536.8375447, "prompt_tokens": 17, "prefill_ms": 2.6901, "prefill_cuda_event_ms": null, "kv_decode_ms": 64.5283, "kv_decode_ms_equiv": 64.5283, "kv_decode_ms_per_token": 2.016509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 263.10609999973167, "ollama_total_duration_ms": 249.9647, "ollama_load_ms": 158.8761, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6901, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6319.467677781495}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547536.8375447, "prompt_tokens": 17, "prefill_ms": 2.6901, "prefill_cuda_event_ms": null, "kv_decode_ms": 64.5283, "kv_decode_ms_equiv": 64.5283, "kv_decode_ms_per_token": 2.016509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 263.10609999973167, "ollama_total_duration_ms": 249.9647, "ollama_load_ms": 158.8761, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 64.5283, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 495.9064472487266}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547536.8375447, "prompt_tokens": 17, "prefill_ms": 2.6901, "prefill_cuda_event_ms": null, "kv_decode_ms": 64.5283, "kv_decode_ms_equiv": 64.5283, "kv_decode_ms_per_token": 2.016509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 263.10609999973167, "ollama_total_duration_ms": 249.9647, "ollama_load_ms": 158.8761, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 67.2184, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 728.9670685407567}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547537.1007762, "prompt_tokens": 17, "prefill_ms": 2.396, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.9755, "kv_decode_ms_equiv": 65.9755, "kv_decode_ms_per_token": 2.061734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 268.9308999997593, "ollama_total_duration_ms": 241.1214, "ollama_load_ms": 149.4779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.396, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7095.158597662772}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547537.1007762, "prompt_tokens": 17, "prefill_ms": 2.396, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.9755, "kv_decode_ms_equiv": 65.9755, "kv_decode_ms_per_token": 2.061734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 268.9308999997593, "ollama_total_duration_ms": 241.1214, "ollama_load_ms": 149.4779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 65.9755, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 485.02853331918675}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547537.1007762, "prompt_tokens": 17, "prefill_ms": 2.396, "prefill_cuda_event_ms": null, "kv_decode_ms": 65.9755, "kv_decode_ms_equiv": 65.9755, "kv_decode_ms_per_token": 2.061734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 268.9308999997593, "ollama_total_duration_ms": 241.1214, "ollama_load_ms": 149.4779, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 68.3715, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 716.6728827069759}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547537.3698416, "prompt_tokens": 17, "prefill_ms": 2.6636, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.4907, "kv_decode_ms_equiv": 63.4907, "kv_decode_ms_per_token": 1.984084375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 249.32780000017374, "ollama_total_duration_ms": 238.0424, "ollama_load_ms": 149.8343, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6636, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6382.33969064424}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547537.3698416, "prompt_tokens": 17, "prefill_ms": 2.6636, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.4907, "kv_decode_ms_equiv": 63.4907, "kv_decode_ms_per_token": 1.984084375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 249.32780000017374, "ollama_total_duration_ms": 238.0424, "ollama_load_ms": 149.8343, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.4907, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 504.0108236324375}
{"task_idx": 7, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547537.3698416, "prompt_tokens": 17, "prefill_ms": 2.6636, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.4907, "kv_decode_ms_equiv": 63.4907, "kv_decode_ms_per_token": 1.984084375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 249.32780000017374, "ollama_total_duration_ms": 238.0424, "ollama_load_ms": 149.8343, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 66.15429999999999, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 740.6925929229092}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547537.620994, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.976640224456787, "kv_decode_ms": 217.19560000019555, "kv_decode_cuda_event_ms": 217.1439971923828, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 5.046700000093551, "cuda_event_ms": 4.976640224456787, "tokens_total": 9, "tokens_per_s": 1783.3435710133683}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547537.620994, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.976640224456787, "kv_decode_ms": 217.19560000019555, "kv_decode_cuda_event_ms": 217.1439971923828, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 217.19560000019555, "cuda_event_ms": 217.1439971923828, "tokens_total": 32, "tokens_per_s": 147.33263473095766}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547537.620994, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.976640224456787, "kv_decode_ms": 217.19560000019555, "kv_decode_cuda_event_ms": 217.1439971923828, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 222.2423000002891, "cuda_event_ms": 222.1206374168396, "tokens_total": 41, "tokens_per_s": 184.48333193072006}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547537.8444974, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.221700000078272, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 155.90629999996963, "kv_decode_cuda_event_ms": 155.8138885498047, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 11.221700000078272, "cuda_event_ms": 11.151359558105469, "tokens_total": 9, "tokens_per_s": 802.0175196215569}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547537.8444974, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 11.221700000078272, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 155.90629999996963, "kv_decode_cuda_event_ms": 155.8138885498047, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 155.90629999996963, "cuda_event_ms": 155.8138885498047, "tokens_total": 32, "tokens_per_s": 205.25148759226684}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547537.8444974, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 11.221700000078272, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 155.90629999996963, "kv_decode_cuda_event_ms": 155.8138885498047, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 167.1280000000479, "cuda_event_ms": 166.96524810791016, "tokens_total": 41, "tokens_per_s": 245.3209516058844}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547538.0124526, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.957300000114628, "prefill_cuda_event_ms": 4.909056186676025, "kv_decode_ms": 129.51650000013615, "kv_decode_cuda_event_ms": 129.4866180419922, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 4.957300000114628, "cuda_event_ms": 4.909056186676025, "tokens_total": 9, "tokens_per_s": 1815.5044075992762}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547538.0124526, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.957300000114628, "prefill_cuda_event_ms": 4.909056186676025, "kv_decode_ms": 129.51650000013615, "kv_decode_cuda_event_ms": 129.4866180419922, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 129.51650000013615, "cuda_event_ms": 129.4866180419922, "tokens_total": 32, "tokens_per_s": 247.07276679007202}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547538.0124526, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.957300000114628, "prefill_cuda_event_ms": 4.909056186676025, "kv_decode_ms": 129.51650000013615, "kv_decode_cuda_event_ms": 129.4866180419922, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 134.47380000025078, "cuda_event_ms": 134.3956742286682, "tokens_total": 41, "tokens_per_s": 304.89210537609216}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547538.147626, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.21649999973306, "prefill_cuda_event_ms": 5.157887935638428, "kv_decode_ms": 129.77439999986018, "kv_decode_cuda_event_ms": 129.73158264160156, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 5.21649999973306, "cuda_event_ms": 5.157887935638428, "tokens_total": 9, "tokens_per_s": 1725.2947379393368}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547538.147626, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.21649999973306, "prefill_cuda_event_ms": 5.157887935638428, "kv_decode_ms": 129.77439999986018, "kv_decode_cuda_event_ms": 129.73158264160156, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 129.77439999986018, "cuda_event_ms": 129.73158264160156, "tokens_total": 32, "tokens_per_s": 246.58176034745279}
{"task_idx": 8, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547538.147626, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.21649999973306, "prefill_cuda_event_ms": 5.157887935638428, "kv_decode_ms": 129.77439999986018, "kv_decode_cuda_event_ms": 129.73158264160156, "gpu_peak_mb": 408.78564453125, "params_millions_measured": 96.08832, "latency_ms": 134.99089999959324, "cuda_event_ms": 134.88947057724, "tokens_total": 41, "tokens_per_s": 303.7241769639549}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547538.2833495, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.36560000007739, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.28900000021895, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 179.4502000002467, "params_millions_measured": 5.03672, "latency_ms": 50.36560000007739, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 337.53196626216857}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547538.2833495, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.36560000007739, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.28900000021895, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 179.4502000002467, "params_millions_measured": 5.03672, "latency_ms": 407.28900000021895, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 78.56828934732536}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547538.2833495, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 50.36560000007739, "prefill_cuda_event_ms": null, "kv_decode_ms": 407.28900000021895, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 179.4502000002467, "params_millions_measured": 5.03672, "latency_ms": 457.65460000029634, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 107.06764446368128}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547538.9220989, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.418499999981577, "prefill_cuda_event_ms": null, "kv_decode_ms": 380.2413000003071, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.418499999981577, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 793.706375330421}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547538.9220989, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.418499999981577, "prefill_cuda_event_ms": null, "kv_decode_ms": 380.2413000003071, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 380.2413000003071, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 84.15708656575221}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547538.9220989, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.418499999981577, "prefill_cuda_event_ms": null, "kv_decode_ms": 380.2413000003071, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 401.65980000028867, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 121.99378678166146}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547539.324298, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 32.72870000000694, "prefill_cuda_event_ms": null, "kv_decode_ms": 459.46460000004663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 32.72870000000694, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 519.421791882855}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547539.324298, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 32.72870000000694, "prefill_cuda_event_ms": null, "kv_decode_ms": 459.46460000004663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 459.46460000004663, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 69.64627960455877}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547539.324298, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 32.72870000000694, "prefill_cuda_event_ms": null, "kv_decode_ms": 459.46460000004663, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 492.19330000005357, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 99.55438239406077}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547539.8177586, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.22030000000086, "prefill_cuda_event_ms": null, "kv_decode_ms": 448.72080000004644, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 25.22030000000086, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 674.0601816790212}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547539.8177586, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 25.22030000000086, "prefill_cuda_event_ms": null, "kv_decode_ms": 448.72080000004644, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 448.72080000004644, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 71.31383256581083}
{"task_idx": 9, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547539.8177586, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 25.22030000000086, "prefill_cuda_event_ms": null, "kv_decode_ms": 448.72080000004644, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 473.9411000000473, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 103.38837463135211}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547540.2927442, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.3026000000973, "prefill_cuda_event_ms": 3.2327680587768555, "kv_decode_ms": 87.10530000007566, "kv_decode_cuda_event_ms": 87.07481384277344, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 364.94260000017675, "params_millions_measured": 74.824704, "latency_ms": 3.3026000000973, "cuda_event_ms": 3.2327680587768555, "tokens_total": 9, "tokens_per_s": 2725.1256584917473}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547540.2927442, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.3026000000973, "prefill_cuda_event_ms": 3.2327680587768555, "kv_decode_ms": 87.10530000007566, "kv_decode_cuda_event_ms": 87.07481384277344, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 364.94260000017675, "params_millions_measured": 74.824704, "latency_ms": 87.10530000007566, "cuda_event_ms": 87.07481384277344, "tokens_total": 32, "tokens_per_s": 367.37144582444705}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547540.2927442, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.3026000000973, "prefill_cuda_event_ms": 3.2327680587768555, "kv_decode_ms": 87.10530000007566, "kv_decode_cuda_event_ms": 87.07481384277344, "gpu_peak_mb": 557.40087890625, "hf_load_ms": 364.94260000017675, "params_millions_measured": 74.824704, "latency_ms": 90.40790000017296, "cuda_event_ms": 90.3075819015503, "tokens_total": 41, "tokens_per_s": 453.50019190714045}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547540.7488303, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.3332000002701534, "prefill_cuda_event_ms": 3.2880640029907227, "kv_decode_ms": 85.42649999981222, "kv_decode_cuda_event_ms": 85.39033508300781, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 3.3332000002701534, "cuda_event_ms": 3.2880640029907227, "tokens_total": 9, "tokens_per_s": 2700.108004101331}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547540.7488303, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.3332000002701534, "prefill_cuda_event_ms": 3.2880640029907227, "kv_decode_ms": 85.42649999981222, "kv_decode_cuda_event_ms": 85.39033508300781, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 85.42649999981222, "cuda_event_ms": 85.39033508300781, "tokens_total": 32, "tokens_per_s": 374.5910226928452}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547540.7488303, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.3332000002701534, "prefill_cuda_event_ms": 3.2880640029907227, "kv_decode_ms": 85.42649999981222, "kv_decode_cuda_event_ms": 85.39033508300781, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 88.75970000008238, "cuda_event_ms": 88.67839908599854, "tokens_total": 41, "tokens_per_s": 461.9213449342658}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547540.8382738, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.481400000055146, "prefill_cuda_event_ms": 3.4303998947143555, "kv_decode_ms": 84.60169999989375, "kv_decode_cuda_event_ms": 84.57625579833984, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 3.481400000055146, "cuda_event_ms": 3.4303998947143555, "tokens_total": 9, "tokens_per_s": 2585.1668868436373}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547540.8382738, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.481400000055146, "prefill_cuda_event_ms": 3.4303998947143555, "kv_decode_ms": 84.60169999989375, "kv_decode_cuda_event_ms": 84.57625579833984, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 84.60169999989375, "cuda_event_ms": 84.57625579833984, "tokens_total": 32, "tokens_per_s": 378.242990389557}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547540.8382738, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.481400000055146, "prefill_cuda_event_ms": 3.4303998947143555, "kv_decode_ms": 84.60169999989375, "kv_decode_cuda_event_ms": 84.57625579833984, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 88.0830999999489, "cuda_event_ms": 88.0066556930542, "tokens_total": 41, "tokens_per_s": 465.46953956007206}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547540.9269137, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.4458999998605577, "prefill_cuda_event_ms": 3.3966081142425537, "kv_decode_ms": 82.00689999966926, "kv_decode_cuda_event_ms": 81.97708892822266, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 3.4458999998605577, "cuda_event_ms": 3.3966081142425537, "tokens_total": 9, "tokens_per_s": 2611.799529981774}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547540.9269137, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.4458999998605577, "prefill_cuda_event_ms": 3.3966081142425537, "kv_decode_ms": 82.00689999966926, "kv_decode_cuda_event_ms": 81.97708892822266, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 82.00689999966926, "cuda_event_ms": 81.97708892822266, "tokens_total": 32, "tokens_per_s": 390.21106760686064}
{"task_idx": 10, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547540.9269137, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.4458999998605577, "prefill_cuda_event_ms": 3.3966081142425537, "kv_decode_ms": 82.00689999966926, "kv_decode_cuda_event_ms": 81.97708892822266, "gpu_peak_mb": 557.40087890625, "params_millions_measured": 74.824704, "latency_ms": 85.45279999952982, "cuda_event_ms": 85.37369704246521, "tokens_total": 41, "tokens_per_s": 479.7970341548269}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547541.0129414, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 57.164100000136386, "prefill_cuda_event_ms": null, "kv_decode_ms": 1439.1207999997278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 57.164100000136386, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 297.3894454729356}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547541.0129414, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 57.164100000136386, "prefill_cuda_event_ms": null, "kv_decode_ms": 1439.1207999997278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1439.1207999997278, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 22.23579841247938}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547541.0129414, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 57.164100000136386, "prefill_cuda_event_ms": null, "kv_decode_ms": 1439.1207999997278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1496.2848999998641, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 32.74777417055031}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547542.509682, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 61.17790000007517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.296499999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 61.17790000007517, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 277.87812265506193}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547542.509682, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 61.17790000007517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.296499999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1279.296499999873, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 25.013747790291912}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547542.509682, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 61.17790000007517, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.296499999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1340.4743999999482, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 36.554222893030925}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547543.8507776, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 51.07590000034179, "prefill_cuda_event_ms": null, "kv_decode_ms": 1411.1917000000176, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 51.07590000034179, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 332.83799208406}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547543.8507776, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.07590000034179, "prefill_cuda_event_ms": null, "kv_decode_ms": 1411.1917000000176, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1411.1917000000176, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 22.675870330019375}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547543.8507776, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.07590000034179, "prefill_cuda_event_ms": null, "kv_decode_ms": 1411.1917000000176, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1462.2676000003594, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 33.50959837993262}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547545.313625, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 84.91649999996298, "prefill_cuda_event_ms": null, "kv_decode_ms": 1470.9391000001233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 84.91649999996298, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 200.19666378156674}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547545.313625, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 84.91649999996298, "prefill_cuda_event_ms": null, "kv_decode_ms": 1470.9391000001233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1470.9391000001233, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.754809563494042}
{"task_idx": 11, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547545.313625, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 84.91649999996298, "prefill_cuda_event_ms": null, "kv_decode_ms": 1470.9391000001233, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1555.8556000000863, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 31.493925271726557}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547546.8698485, "prompt_tokens": 99, "prefill_ms": 1026.537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1500.4033, "kv_decode_ms_equiv": 1500.4033, "kv_decode_ms_per_token": 46.887603125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 12593.073000000004, "ollama_total_duration_ms": 12448.0843, "ollama_load_ms": 9845.7265, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1026.537, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 96.44075177027229}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547546.8698485, "prompt_tokens": 99, "prefill_ms": 1026.537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1500.4033, "kv_decode_ms_equiv": 1500.4033, "kv_decode_ms_per_token": 46.887603125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12593.073000000004, "ollama_total_duration_ms": 12448.0843, "ollama_load_ms": 9845.7265, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1500.4033, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 21.32759905286799}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547546.8698485, "prompt_tokens": 99, "prefill_ms": 1026.537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1500.4033, "kv_decode_ms_equiv": 1500.4033, "kv_decode_ms_per_token": 46.887603125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 12593.073000000004, "ollama_total_duration_ms": 12448.0843, "ollama_load_ms": 9845.7265, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2526.9403, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 51.841351376603555}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547559.463831, "prompt_tokens": 99, "prefill_ms": 62.3782, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.1182, "kv_decode_ms_equiv": 1601.1182, "kv_decode_ms_per_token": 50.03494375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1955.5773000001864, "ollama_total_duration_ms": 1927.908, "ollama_load_ms": 254.2871, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 62.3782, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1587.0929266955443}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547559.463831, "prompt_tokens": 99, "prefill_ms": 62.3782, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.1182, "kv_decode_ms_equiv": 1601.1182, "kv_decode_ms_per_token": 50.03494375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1955.5773000001864, "ollama_total_duration_ms": 1927.908, "ollama_load_ms": 254.2871, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1601.1182, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 19.9860322617031}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547559.463831, "prompt_tokens": 99, "prefill_ms": 62.3782, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.1182, "kv_decode_ms_equiv": 1601.1182, "kv_decode_ms_per_token": 50.03494375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1955.5773000001864, "ollama_total_duration_ms": 1927.908, "ollama_load_ms": 254.2871, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1663.4964, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 78.74979470950463}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547561.419596, "prompt_tokens": 99, "prefill_ms": 63.6764, "prefill_cuda_event_ms": null, "kv_decode_ms": 1711.1947, "kv_decode_ms_equiv": 1711.1947, "kv_decode_ms_per_token": 53.474834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 2063.332199999877, "ollama_total_duration_ms": 2059.4362, "ollama_load_ms": 271.1522, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 63.6764, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1554.7361345804727}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547561.419596, "prompt_tokens": 99, "prefill_ms": 63.6764, "prefill_cuda_event_ms": null, "kv_decode_ms": 1711.1947, "kv_decode_ms_equiv": 1711.1947, "kv_decode_ms_per_token": 53.474834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2063.332199999877, "ollama_total_duration_ms": 2059.4362, "ollama_load_ms": 271.1522, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1711.1947, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 18.70038517533978}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547561.419596, "prompt_tokens": 99, "prefill_ms": 63.6764, "prefill_cuda_event_ms": null, "kv_decode_ms": 1711.1947, "kv_decode_ms_equiv": 1711.1947, "kv_decode_ms_per_token": 53.474834375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2063.332199999877, "ollama_total_duration_ms": 2059.4362, "ollama_load_ms": 271.1522, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1774.8711, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 73.80817683041883}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547563.4831517, "prompt_tokens": 99, "prefill_ms": 67.1164, "prefill_cuda_event_ms": null, "kv_decode_ms": 1875.3443, "kv_decode_ms_equiv": 1875.3443, "kv_decode_ms_per_token": 58.604509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 2219.868900000165, "ollama_total_duration_ms": 2215.9639, "ollama_load_ms": 253.3972, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 67.1164, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1475.0493173054576}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547563.4831517, "prompt_tokens": 99, "prefill_ms": 67.1164, "prefill_cuda_event_ms": null, "kv_decode_ms": 1875.3443, "kv_decode_ms_equiv": 1875.3443, "kv_decode_ms_per_token": 58.604509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2219.868900000165, "ollama_total_duration_ms": 2215.9639, "ollama_load_ms": 253.3972, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1875.3443, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 17.063533346916618}
{"task_idx": 12, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547563.4831517, "prompt_tokens": 99, "prefill_ms": 67.1164, "prefill_cuda_event_ms": null, "kv_decode_ms": 1875.3443, "kv_decode_ms_equiv": 1875.3443, "kv_decode_ms_per_token": 58.604509375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 2219.868900000165, "ollama_total_duration_ms": 2215.9639, "ollama_load_ms": 253.3972, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1942.4607, "cuda_event_ms": null, "tokens_total": 131, "tokens_per_s": 67.4402318667245}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547565.7032647, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 66.92479999992429, "prefill_cuda_event_ms": null, "kv_decode_ms": 879.5050999997329, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 66.92479999992429, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 254.01644831242277}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547565.7032647, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 66.92479999992429, "prefill_cuda_event_ms": null, "kv_decode_ms": 879.5050999997329, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 879.5050999997329, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 36.384098284375746}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547565.7032647, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 66.92479999992429, "prefill_cuda_event_ms": null, "kv_decode_ms": 879.5050999997329, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 946.4298999996572, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 51.77351222739026}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547566.6508982, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 46.91289999982473, "prefill_cuda_event_ms": null, "kv_decode_ms": 901.8458999998984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 46.91289999982473, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 362.37367547227973}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547566.6508982, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.91289999982473, "prefill_cuda_event_ms": null, "kv_decode_ms": 901.8458999998984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 901.8458999998984, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 35.482780373014506}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547566.6508982, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 46.91289999982473, "prefill_cuda_event_ms": null, "kv_decode_ms": 901.8458999998984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 948.7587999997231, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 51.64642478152962}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547567.6009145, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 51.40780000010636, "prefill_cuda_event_ms": null, "kv_decode_ms": 939.3054000001939, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 51.40780000010636, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 330.68911721499126}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547567.6009145, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.40780000010636, "prefill_cuda_event_ms": null, "kv_decode_ms": 939.3054000001939, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 939.3054000001939, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 34.06772706724926}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547567.6009145, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.40780000010636, "prefill_cuda_event_ms": null, "kv_decode_ms": 939.3054000001939, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 990.7132000003003, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 49.45931880183402}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547568.5923135, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 51.53100000006816, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.1433000002908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 51.53100000006816, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 329.89850769396116}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547568.5923135, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.53100000006816, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.1433000002908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 920.1433000002908, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 34.77719177001005}
{"task_idx": 13, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547568.5923135, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 51.53100000006816, "prefill_cuda_event_ms": null, "kv_decode_ms": 920.1433000002908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 971.6743000003589, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 50.428420305015685}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547569.5650365, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.005800000020827, "prefill_cuda_event_ms": 10.893312454223633, "kv_decode_ms": 179.78339999945092, "kv_decode_cuda_event_ms": 179.6822967529297, "gpu_peak_mb": 579.755859375, "hf_load_ms": 215.58999999979278, "params_millions_measured": 5.03672, "latency_ms": 11.005800000020827, "cuda_event_ms": 10.893312454223633, "tokens_total": 17, "tokens_per_s": 1544.6400988540433}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547569.5650365, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.005800000020827, "prefill_cuda_event_ms": 10.893312454223633, "kv_decode_ms": 179.78339999945092, "kv_decode_cuda_event_ms": 179.6822967529297, "gpu_peak_mb": 579.755859375, "hf_load_ms": 215.58999999979278, "params_millions_measured": 5.03672, "latency_ms": 179.78339999945092, "cuda_event_ms": 179.6822967529297, "tokens_total": 32, "tokens_per_s": 177.99196143858515}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547569.5650365, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 11.005800000020827, "prefill_cuda_event_ms": 10.893312454223633, "kv_decode_ms": 179.78339999945092, "kv_decode_cuda_event_ms": 179.6822967529297, "gpu_peak_mb": 579.755859375, "hf_load_ms": 215.58999999979278, "params_millions_measured": 5.03672, "latency_ms": 190.78919999947175, "cuda_event_ms": 190.57560920715332, "tokens_total": 49, "tokens_per_s": 256.8279546228805}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547569.9730074, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.692099999781931, "prefill_cuda_event_ms": 6.63756799697876, "kv_decode_ms": 167.93580000012298, "kv_decode_cuda_event_ms": 167.90118408203125, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 6.692099999781931, "cuda_event_ms": 6.63756799697876, "tokens_total": 17, "tokens_per_s": 2540.3087223074913}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547569.9730074, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.692099999781931, "prefill_cuda_event_ms": 6.63756799697876, "kv_decode_ms": 167.93580000012298, "kv_decode_cuda_event_ms": 167.90118408203125, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 167.93580000012298, "cuda_event_ms": 167.90118408203125, "tokens_total": 32, "tokens_per_s": 190.5490074181715}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547569.9730074, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.692099999781931, "prefill_cuda_event_ms": 6.63756799697876, "kv_decode_ms": 167.93580000012298, "kv_decode_cuda_event_ms": 167.90118408203125, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 174.6278999999049, "cuda_event_ms": 174.53875207901, "tokens_total": 49, "tokens_per_s": 280.5966286030278}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547570.1488519, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.100599999626866, "prefill_cuda_event_ms": 6.043647766113281, "kv_decode_ms": 168.83140000027197, "kv_decode_cuda_event_ms": 168.7889862060547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 6.100599999626866, "cuda_event_ms": 6.043647766113281, "tokens_total": 17, "tokens_per_s": 2786.6111531717834}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547570.1488519, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.100599999626866, "prefill_cuda_event_ms": 6.043647766113281, "kv_decode_ms": 168.83140000027197, "kv_decode_cuda_event_ms": 168.7889862060547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 168.83140000027197, "cuda_event_ms": 168.7889862060547, "tokens_total": 32, "tokens_per_s": 189.53820201661807}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547570.1488519, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.100599999626866, "prefill_cuda_event_ms": 6.043647766113281, "kv_decode_ms": 168.83140000027197, "kv_decode_cuda_event_ms": 168.7889862060547, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 174.93199999989884, "cuda_event_ms": 174.83263397216797, "tokens_total": 49, "tokens_per_s": 280.1088422931673}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547570.3243976, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.124000000454544, "prefill_cuda_event_ms": 6.07539176940918, "kv_decode_ms": 170.85649999989982, "kv_decode_cuda_event_ms": 170.81446838378906, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 6.124000000454544, "cuda_event_ms": 6.07539176940918, "tokens_total": 17, "tokens_per_s": 2775.963422393567}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547570.3243976, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.124000000454544, "prefill_cuda_event_ms": 6.07539176940918, "kv_decode_ms": 170.85649999989982, "kv_decode_cuda_event_ms": 170.81446838378906, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 170.85649999989982, "cuda_event_ms": 170.81446838378906, "tokens_total": 32, "tokens_per_s": 187.29167459253094}
{"task_idx": 14, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547570.3243976, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.124000000454544, "prefill_cuda_event_ms": 6.07539176940918, "kv_decode_ms": 170.85649999989982, "kv_decode_cuda_event_ms": 170.81446838378906, "gpu_peak_mb": 579.755859375, "params_millions_measured": 5.03672, "latency_ms": 176.98050000035437, "cuda_event_ms": 176.88986015319824, "tokens_total": 49, "tokens_per_s": 276.86666045073827}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547570.5019588, "prompt_tokens": 25, "prefill_ms": 9.9332, "prefill_cuda_event_ms": null, "kv_decode_ms": 73.1988, "kv_decode_ms_equiv": 73.1988, "kv_decode_ms_per_token": 2.2874625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 1716.182299999673, "ollama_total_duration_ms": 1646.523, "ollama_load_ms": 1516.1938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.9332, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2516.8123062054524}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547570.5019588, "prompt_tokens": 25, "prefill_ms": 9.9332, "prefill_cuda_event_ms": null, "kv_decode_ms": 73.1988, "kv_decode_ms_equiv": 73.1988, "kv_decode_ms_per_token": 2.2874625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1716.182299999673, "ollama_total_duration_ms": 1646.523, "ollama_load_ms": 1516.1938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 73.1988, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 437.16563659513537}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547570.5019588, "prompt_tokens": 25, "prefill_ms": 9.9332, "prefill_cuda_event_ms": null, "kv_decode_ms": 73.1988, "kv_decode_ms_equiv": 73.1988, "kv_decode_ms_per_token": 2.2874625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 1716.182299999673, "ollama_total_duration_ms": 1646.523, "ollama_load_ms": 1516.1938, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 83.132, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 685.6565462156569}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547572.2183268, "prompt_tokens": 25, "prefill_ms": 3.8851, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.3978, "kv_decode_ms_equiv": 66.3978, "kv_decode_ms_per_token": 2.07493125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.8541000001642, "ollama_total_duration_ms": 235.6212, "ollama_load_ms": 146.311, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.8851, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 6434.840802038558}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547572.2183268, "prompt_tokens": 25, "prefill_ms": 3.8851, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.3978, "kv_decode_ms_equiv": 66.3978, "kv_decode_ms_per_token": 2.07493125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.8541000001642, "ollama_total_duration_ms": 235.6212, "ollama_load_ms": 146.311, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 66.3978, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 481.94367885682954}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547572.2183268, "prompt_tokens": 25, "prefill_ms": 3.8851, "prefill_cuda_event_ms": null, "kv_decode_ms": 66.3978, "kv_decode_ms_equiv": 66.3978, "kv_decode_ms_per_token": 2.07493125, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 261.8541000001642, "ollama_total_duration_ms": 235.6212, "ollama_load_ms": 146.311, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 70.2829, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 811.0080830472277}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547572.4802973, "prompt_tokens": 25, "prefill_ms": 2.997, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.174, "kv_decode_ms_equiv": 63.174, "kv_decode_ms_per_token": 1.9741875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 265.653100000236, "ollama_total_duration_ms": 242.675, "ollama_load_ms": 153.95, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.997, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8341.675008341675}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547572.4802973, "prompt_tokens": 25, "prefill_ms": 2.997, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.174, "kv_decode_ms_equiv": 63.174, "kv_decode_ms_per_token": 1.9741875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 265.653100000236, "ollama_total_duration_ms": 242.675, "ollama_load_ms": 153.95, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 63.174, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 506.5374996042676}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547572.4802973, "prompt_tokens": 25, "prefill_ms": 2.997, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.174, "kv_decode_ms_equiv": 63.174, "kv_decode_ms_per_token": 1.9741875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 265.653100000236, "ollama_total_duration_ms": 242.675, "ollama_load_ms": 153.95, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 66.17099999999999, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 861.4045427755362}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547572.746088, "prompt_tokens": 25, "prefill_ms": 2.4691, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.1215, "kv_decode_ms_equiv": 67.1215, "kv_decode_ms_per_token": 2.097546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 253.2378999994762, "ollama_total_duration_ms": 242.8034, "ollama_load_ms": 154.4249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4691, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10125.146814628812}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547572.746088, "prompt_tokens": 25, "prefill_ms": 2.4691, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.1215, "kv_decode_ms_equiv": 67.1215, "kv_decode_ms_per_token": 2.097546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 253.2378999994762, "ollama_total_duration_ms": 242.8034, "ollama_load_ms": 154.4249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 67.1215, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 476.74739092541137}
{"task_idx": 15, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547572.746088, "prompt_tokens": 25, "prefill_ms": 2.4691, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.1215, "kv_decode_ms_equiv": 67.1215, "kv_decode_ms_per_token": 2.097546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 253.2378999994762, "ollama_total_duration_ms": 242.8034, "ollama_load_ms": 154.4249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 69.5906, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 819.0761395935659}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547572.9995718, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.96200000060344, "prefill_cuda_event_ms": 16.79360008239746, "kv_decode_ms": 189.64169999981095, "kv_decode_cuda_event_ms": 189.5692138671875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 16.96200000060344, "cuda_event_ms": 16.79360008239746, "tokens_total": 9, "tokens_per_s": 530.5978068435218}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547572.9995718, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 16.96200000060344, "prefill_cuda_event_ms": 16.79360008239746, "kv_decode_ms": 189.64169999981095, "kv_decode_cuda_event_ms": 189.5692138671875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 189.64169999981095, "cuda_event_ms": 189.5692138671875, "tokens_total": 32, "tokens_per_s": 168.7392593508279}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547572.9995718, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 16.96200000060344, "prefill_cuda_event_ms": 16.79360008239746, "kv_decode_ms": 189.64169999981095, "kv_decode_cuda_event_ms": 189.5692138671875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 206.6037000004144, "cuda_event_ms": 206.36281394958496, "tokens_total": 41, "tokens_per_s": 198.44755926402948}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547573.207426, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.324900000436173, "prefill_cuda_event_ms": 6.260735988616943, "kv_decode_ms": 175.56920000060927, "kv_decode_cuda_event_ms": 175.5278778076172, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 6.324900000436173, "cuda_event_ms": 6.260735988616943, "tokens_total": 9, "tokens_per_s": 1422.9473982797117}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547573.207426, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.324900000436173, "prefill_cuda_event_ms": 6.260735988616943, "kv_decode_ms": 175.56920000060927, "kv_decode_cuda_event_ms": 175.5278778076172, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 175.56920000060927, "cuda_event_ms": 175.5278778076172, "tokens_total": 32, "tokens_per_s": 182.26431515259483}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547573.207426, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.324900000436173, "prefill_cuda_event_ms": 6.260735988616943, "kv_decode_ms": 175.56920000060927, "kv_decode_cuda_event_ms": 175.5278778076172, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 181.89410000104544, "cuda_event_ms": 181.78861379623413, "tokens_total": 41, "tokens_per_s": 225.4058817727697}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547573.3899236, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.486199999926612, "prefill_cuda_event_ms": 6.430816173553467, "kv_decode_ms": 178.57470000035391, "kv_decode_cuda_event_ms": 178.48828125, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 6.486199999926612, "cuda_event_ms": 6.430816173553467, "tokens_total": 9, "tokens_per_s": 1387.5612839724076}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547573.3899236, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.486199999926612, "prefill_cuda_event_ms": 6.430816173553467, "kv_decode_ms": 178.57470000035391, "kv_decode_cuda_event_ms": 178.48828125, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 178.57470000035391, "cuda_event_ms": 178.48828125, "tokens_total": 32, "tokens_per_s": 179.1967171157873}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547573.3899236, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.486199999926612, "prefill_cuda_event_ms": 6.430816173553467, "kv_decode_ms": 178.57470000035391, "kv_decode_cuda_event_ms": 178.48828125, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 185.06090000028053, "cuda_event_ms": 184.91909742355347, "tokens_total": 41, "tokens_per_s": 221.5486901875969}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547573.5757637, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.658600000671868, "prefill_cuda_event_ms": 8.550399780273438, "kv_decode_ms": 183.78459999985353, "kv_decode_cuda_event_ms": 183.74041748046875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 8.658600000671868, "cuda_event_ms": 8.550399780273438, "tokens_total": 9, "tokens_per_s": 1039.429006918167}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547573.5757637, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 8.658600000671868, "prefill_cuda_event_ms": 8.550399780273438, "kv_decode_ms": 183.78459999985353, "kv_decode_cuda_event_ms": 183.74041748046875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 183.78459999985353, "cuda_event_ms": 183.74041748046875, "tokens_total": 32, "tokens_per_s": 174.11687377519937}
{"task_idx": 16, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547573.5757637, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 8.658600000671868, "prefill_cuda_event_ms": 8.550399780273438, "kv_decode_ms": 183.78459999985353, "kv_decode_cuda_event_ms": 183.74041748046875, "gpu_peak_mb": 578.58349609375, "params_millions_measured": 5.03672, "latency_ms": 192.4432000005254, "cuda_event_ms": 192.2908172607422, "tokens_total": 41, "tokens_per_s": 213.04987653441674}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547573.7691433, "prompt_tokens": 38, "prefill_ms": 23.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.9252, "kv_decode_ms_equiv": 359.9252, "kv_decode_ms_per_token": 11.2476625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 6574.488100000053, "ollama_total_duration_ms": 6561.287, "ollama_load_ms": 6142.3756, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 23.4804, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1618.3710669324203}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547573.7691433, "prompt_tokens": 38, "prefill_ms": 23.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.9252, "kv_decode_ms_equiv": 359.9252, "kv_decode_ms_per_token": 11.2476625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6574.488100000053, "ollama_total_duration_ms": 6561.287, "ollama_load_ms": 6142.3756, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 359.9252, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 88.90736186296486}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547573.7691433, "prompt_tokens": 38, "prefill_ms": 23.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 359.9252, "kv_decode_ms_equiv": 359.9252, "kv_decode_ms_per_token": 11.2476625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 6574.488100000053, "ollama_total_duration_ms": 6561.287, "ollama_load_ms": 6142.3756, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 383.4056, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 182.57427643206046}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547580.3437827, "prompt_tokens": 38, "prefill_ms": 11.6009, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.5534, "kv_decode_ms_equiv": 356.5534, "kv_decode_ms_per_token": 11.14229375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.2128999997585, "ollama_total_duration_ms": 516.4281, "ollama_load_ms": 113.4132, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6009, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3275.6079269711836}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547580.3437827, "prompt_tokens": 38, "prefill_ms": 11.6009, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.5534, "kv_decode_ms_equiv": 356.5534, "kv_decode_ms_per_token": 11.14229375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.2128999997585, "ollama_total_duration_ms": 516.4281, "ollama_load_ms": 113.4132, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 356.5534, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.7481274894588}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547580.3437827, "prompt_tokens": 38, "prefill_ms": 11.6009, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.5534, "kv_decode_ms_equiv": 356.5534, "kv_decode_ms_per_token": 11.14229375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 537.2128999997585, "ollama_total_duration_ms": 516.4281, "ollama_load_ms": 113.4132, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 368.15430000000003, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 190.13766782025903}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547580.88121, "prompt_tokens": 38, "prefill_ms": 11.5063, "prefill_cuda_event_ms": null, "kv_decode_ms": 357.5831, "kv_decode_ms_equiv": 357.5831, "kv_decode_ms_per_token": 11.174471875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.4051999996445, "ollama_total_duration_ms": 537.5895, "ollama_load_ms": 136.863, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5063, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3302.53860928361}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547580.88121, "prompt_tokens": 38, "prefill_ms": 11.5063, "prefill_cuda_event_ms": null, "kv_decode_ms": 357.5831, "kv_decode_ms_equiv": 357.5831, "kv_decode_ms_per_token": 11.174471875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.4051999996445, "ollama_total_duration_ms": 537.5895, "ollama_load_ms": 136.863, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 357.5831, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.48968785157912}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547580.88121, "prompt_tokens": 38, "prefill_ms": 11.5063, "prefill_cuda_event_ms": null, "kv_decode_ms": 357.5831, "kv_decode_ms_equiv": 357.5831, "kv_decode_ms_per_token": 11.174471875, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 541.4051999996445, "ollama_total_duration_ms": 537.5895, "ollama_load_ms": 136.863, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 369.0894, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 189.65594785436807}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547581.422877, "prompt_tokens": 38, "prefill_ms": 11.3646, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.8045, "kv_decode_ms_equiv": 354.8045, "kv_decode_ms_per_token": 11.087640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 548.9812000005259, "ollama_total_duration_ms": 528.9143, "ollama_load_ms": 129.5326, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.3646, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3343.7164528447993}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547581.422877, "prompt_tokens": 38, "prefill_ms": 11.3646, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.8045, "kv_decode_ms_equiv": 354.8045, "kv_decode_ms_per_token": 11.087640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 548.9812000005259, "ollama_total_duration_ms": 528.9143, "ollama_load_ms": 129.5326, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 354.8045, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.19051336722053}
{"task_idx": 17, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547581.422877, "prompt_tokens": 38, "prefill_ms": 11.3646, "prefill_cuda_event_ms": null, "kv_decode_ms": 354.8045, "kv_decode_ms_equiv": 354.8045, "kv_decode_ms_per_token": 11.087640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 548.9812000005259, "ollama_total_duration_ms": 528.9143, "ollama_load_ms": 129.5326, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 366.1691, "cuda_event_ms": null, "tokens_total": 70, "tokens_per_s": 191.16850657250978}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547581.9720247, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.5669121742248535, "kv_decode_ms": 73.10049999978219, "kv_decode_cuda_event_ms": 73.00198364257812, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 230.49970000010944, "params_millions_measured": 25.016064, "latency_ms": 6.702200000290759, "cuda_event_ms": 6.5669121742248535, "tokens_total": 17, "tokens_per_s": 2536.4805585125027}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547581.9720247, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.5669121742248535, "kv_decode_ms": 73.10049999978219, "kv_decode_cuda_event_ms": 73.00198364257812, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 230.49970000010944, "params_millions_measured": 25.016064, "latency_ms": 73.10049999978219, "cuda_event_ms": 73.00198364257812, "tokens_total": 32, "tokens_per_s": 437.7535037393089}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547581.9720247, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.5669121742248535, "kv_decode_ms": 73.10049999978219, "kv_decode_cuda_event_ms": 73.00198364257812, "gpu_peak_mb": 631.08251953125, "hf_load_ms": 230.49970000010944, "params_millions_measured": 25.016064, "latency_ms": 79.80270000007295, "cuda_event_ms": 79.56889581680298, "tokens_total": 49, "tokens_per_s": 614.0143127983791}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547582.2837322, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.3410000003423193, "prefill_cuda_event_ms": 2.2826240062713623, "kv_decode_ms": 52.81049999939569, "kv_decode_cuda_event_ms": 52.76262283325195, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.3410000003423193, "cuda_event_ms": 2.2826240062713623, "tokens_total": 17, "tokens_per_s": 7261.85390752419}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547582.2837322, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.3410000003423193, "prefill_cuda_event_ms": 2.2826240062713623, "kv_decode_ms": 52.81049999939569, "kv_decode_cuda_event_ms": 52.76262283325195, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 52.81049999939569, "cuda_event_ms": 52.76262283325195, "tokens_total": 32, "tokens_per_s": 605.9401066145213}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547582.2837322, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.3410000003423193, "prefill_cuda_event_ms": 2.2826240062713623, "kv_decode_ms": 52.81049999939569, "kv_decode_cuda_event_ms": 52.76262283325195, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 55.15149999973801, "cuda_event_ms": 55.045246839523315, "tokens_total": 49, "tokens_per_s": 888.4617825486663}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547582.3396864, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.2768999997424544, "prefill_cuda_event_ms": 2.225152015686035, "kv_decode_ms": 53.64570000074309, "kv_decode_cuda_event_ms": 53.5838737487793, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.2768999997424544, "cuda_event_ms": 2.225152015686035, "tokens_total": 17, "tokens_per_s": 7466.291888937991}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547582.3396864, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.2768999997424544, "prefill_cuda_event_ms": 2.225152015686035, "kv_decode_ms": 53.64570000074309, "kv_decode_cuda_event_ms": 53.5838737487793, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 53.64570000074309, "cuda_event_ms": 53.5838737487793, "tokens_total": 32, "tokens_per_s": 596.5063369395262}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547582.3396864, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.2768999997424544, "prefill_cuda_event_ms": 2.225152015686035, "kv_decode_ms": 53.64570000074309, "kv_decode_cuda_event_ms": 53.5838737487793, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 55.922600000485545, "cuda_event_ms": 55.80902576446533, "tokens_total": 49, "tokens_per_s": 876.2110488348997}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547582.396268, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.3307999999815365, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 53.59219999991183, "kv_decode_cuda_event_ms": 53.51935958862305, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 2.3307999999815365, "cuda_event_ms": 2.269184112548828, "tokens_total": 17, "tokens_per_s": 7293.633087409758}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547582.396268, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.3307999999815365, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 53.59219999991183, "kv_decode_cuda_event_ms": 53.51935958862305, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 53.59219999991183, "cuda_event_ms": 53.51935958862305, "tokens_total": 32, "tokens_per_s": 597.1018170564494}
{"task_idx": 18, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547582.396268, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 2.3307999999815365, "prefill_cuda_event_ms": 2.269184112548828, "kv_decode_ms": 53.59219999991183, "kv_decode_cuda_event_ms": 53.51935958862305, "gpu_peak_mb": 631.08251953125, "params_millions_measured": 25.016064, "latency_ms": 55.922999999893364, "cuda_event_ms": 55.788543701171875, "tokens_total": 49, "tokens_per_s": 876.2047815763359}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547582.4530017, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.54299999986688, "prefill_cuda_event_ms": null, "kv_decode_ms": 455.346699999609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 105.05529999954888, "params_millions_measured": 25.016064, "latency_ms": 39.54299999986688, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 227.60033381458913}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547582.4530017, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 39.54299999986688, "prefill_cuda_event_ms": null, "kv_decode_ms": 455.346699999609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 105.05529999954888, "params_millions_measured": 25.016064, "latency_ms": 455.346699999609, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 70.27612146970095}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547582.4530017, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 39.54299999986688, "prefill_cuda_event_ms": null, "kv_decode_ms": 455.346699999609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 105.05529999954888, "params_millions_measured": 25.016064, "latency_ms": 494.88969999947585, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 82.84674342594607}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547583.053499, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.66820000052394, "prefill_cuda_event_ms": null, "kv_decode_ms": 445.3822000004948, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.66820000052394, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 457.59144201097456}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547583.053499, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 19.66820000052394, "prefill_cuda_event_ms": null, "kv_decode_ms": 445.3822000004948, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 445.3822000004948, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 71.84840346103739}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547583.053499, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 19.66820000052394, "prefill_cuda_event_ms": null, "kv_decode_ms": 445.3822000004948, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 465.0504000010187, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 88.16248733451296}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547583.5190008, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.492899999524525, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.1086999996114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.492899999524525, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 514.4944520488101}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547583.5190008, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.492899999524525, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.1086999996114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 413.1086999996114, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 77.46145263953555}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547583.5190008, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 17.492899999524525, "prefill_cuda_event_ms": null, "kv_decode_ms": 413.1086999996114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 430.60159999913594, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 95.21562390869488}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547583.950064, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.454200000225683, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.54610000054527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.454200000225683, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 546.9728093663963}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547583.950064, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 16.454200000225683, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.54610000054527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 422.54610000054527, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 75.73138173552827}
{"task_idx": 19, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547583.950064, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 16.454200000225683, "prefill_cuda_event_ms": null, "kv_decode_ms": 422.54610000054527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 439.00030000077095, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 93.39401362579478}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547584.3894484, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.53120000002673, "prefill_cuda_event_ms": null, "kv_decode_ms": 457.428799999434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.53120000002673, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 789.5519060702096}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547584.3894484, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.53120000002673, "prefill_cuda_event_ms": null, "kv_decode_ms": 457.428799999434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 457.428799999434, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 69.95624237048388}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547584.3894484, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.53120000002673, "prefill_cuda_event_ms": null, "kv_decode_ms": 457.428799999434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 478.9599999994607, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 102.30499415411552}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547584.8691485, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.50750000055268, "prefill_cuda_event_ms": null, "kv_decode_ms": 495.5065999993167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.50750000055268, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 790.4219458125374}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547584.8691485, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.50750000055268, "prefill_cuda_event_ms": null, "kv_decode_ms": 495.5065999993167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 495.5065999993167, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 64.58037087708647}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547584.8691485, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 21.50750000055268, "prefill_cuda_event_ms": null, "kv_decode_ms": 495.5065999993167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 517.0140999998694, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 94.77497809056345}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547585.3865955, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.709200000368583, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.4648999997953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 26.709200000368583, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 636.484806724477}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547585.3865955, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 26.709200000368583, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.4648999997953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 547.4648999997953, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 58.45123586920726}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547585.3865955, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 26.709200000368583, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.4648999997953, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 574.1741000001639, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 85.3399691835386}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547585.9613445, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.550299999849813, "prefill_cuda_event_ms": null, "kv_decode_ms": 535.4522999996334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 25.550299999849813, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 665.3542228506095}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547585.9613445, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 25.550299999849813, "prefill_cuda_event_ms": null, "kv_decode_ms": 535.4522999996334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 535.4522999996334, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 59.76255961552861}
{"task_idx": 20, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547585.9613445, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 25.550299999849813, "prefill_cuda_event_ms": null, "kv_decode_ms": 535.4522999996334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 561.0025999994832, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 87.34362371947142}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547586.5231025, "prompt_tokens": 46, "prefill_ms": 365.7989, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.2952, "kv_decode_ms_equiv": 355.2952, "kv_decode_ms_per_token": 11.102975, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 898.8379000002169, "ollama_total_duration_ms": 879.3544, "ollama_load_ms": 123.4256, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 365.7989, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 125.75215507755765}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547586.5231025, "prompt_tokens": 46, "prefill_ms": 365.7989, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.2952, "kv_decode_ms_equiv": 355.2952, "kv_decode_ms_per_token": 11.102975, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 898.8379000002169, "ollama_total_duration_ms": 879.3544, "ollama_load_ms": 123.4256, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 355.2952, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.06595079246777}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547586.5231025, "prompt_tokens": 46, "prefill_ms": 365.7989, "prefill_cuda_event_ms": null, "kv_decode_ms": 355.2952, "kv_decode_ms_equiv": 355.2952, "kv_decode_ms_per_token": 11.102975, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 898.8379000002169, "ollama_total_duration_ms": 879.3544, "ollama_load_ms": 123.4256, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 721.0941, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 108.16896158212914}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547587.4220746, "prompt_tokens": 46, "prefill_ms": 11.9262, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.1138, "kv_decode_ms_equiv": 353.1138, "kv_decode_ms_per_token": 11.03480625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 553.0443000006926, "ollama_total_duration_ms": 527.5925, "ollama_load_ms": 127.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9262, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3857.054216766447}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547587.4220746, "prompt_tokens": 46, "prefill_ms": 11.9262, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.1138, "kv_decode_ms_equiv": 353.1138, "kv_decode_ms_per_token": 11.03480625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 553.0443000006926, "ollama_total_duration_ms": 527.5925, "ollama_load_ms": 127.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 353.1138, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.6223432785691}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547587.4220746, "prompt_tokens": 46, "prefill_ms": 11.9262, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.1138, "kv_decode_ms_equiv": 353.1138, "kv_decode_ms_per_token": 11.03480625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 553.0443000006926, "ollama_total_duration_ms": 527.5925, "ollama_load_ms": 127.4636, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 365.04, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 213.67521367521366}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547587.975196, "prompt_tokens": 46, "prefill_ms": 12.2883, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.4891, "kv_decode_ms_equiv": 356.4891, "kv_decode_ms_per_token": 11.140284375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.6876999998931, "ollama_total_duration_ms": 532.7913, "ollama_load_ms": 129.1532, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.2883, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3743.398191775917}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547587.975196, "prompt_tokens": 46, "prefill_ms": 12.2883, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.4891, "kv_decode_ms_equiv": 356.4891, "kv_decode_ms_per_token": 11.140284375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.6876999998931, "ollama_total_duration_ms": 532.7913, "ollama_load_ms": 129.1532, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 356.4891, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 89.76431537457947}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547587.975196, "prompt_tokens": 46, "prefill_ms": 12.2883, "prefill_cuda_event_ms": null, "kv_decode_ms": 356.4891, "kv_decode_ms_equiv": 356.4891, "kv_decode_ms_per_token": 11.140284375, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 546.6876999998931, "ollama_total_duration_ms": 532.7913, "ollama_load_ms": 129.1532, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 368.7774, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 211.50970748207456}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547588.5220742, "prompt_tokens": 46, "prefill_ms": 11.9493, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.4708, "kv_decode_ms_equiv": 353.4708, "kv_decode_ms_per_token": 11.0459625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 32, "ollama_wall_ms": 558.0643000002965, "ollama_total_duration_ms": 536.9644, "ollama_load_ms": 135.5295, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9493, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3849.597884394902}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547588.5220742, "prompt_tokens": 46, "prefill_ms": 11.9493, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.4708, "kv_decode_ms_equiv": 353.4708, "kv_decode_ms_per_token": 11.0459625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 558.0643000002965, "ollama_total_duration_ms": 536.9644, "ollama_load_ms": 135.5295, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 353.4708, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 90.53081612399102}
{"task_idx": 21, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547588.5220742, "prompt_tokens": 46, "prefill_ms": 11.9493, "prefill_cuda_event_ms": null, "kv_decode_ms": 353.4708, "kv_decode_ms_equiv": 353.4708, "kv_decode_ms_per_token": 11.0459625, "kv_decode_cuda_event_ms": null, "gen_tokens": 32, "gen_tokens_equiv": 32, "ollama_wall_ms": 558.0643000002965, "ollama_total_duration_ms": 536.9644, "ollama_load_ms": 135.5295, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 365.4201, "cuda_event_ms": null, "tokens_total": 78, "tokens_per_s": 213.4529545583289}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547589.0802677, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 90.00190000006114, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.7355999994106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 90.00190000006114, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 99.99788893338791}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547589.0802677, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 90.00190000006114, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.7355999994106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 981.7355999994106, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 32.59533422239064}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547589.0802677, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 90.00190000006114, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.7355999994106, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1071.7374999994718, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 38.255636291554794}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547590.1538427, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 51.226600000518374, "prefill_cuda_event_ms": null, "kv_decode_ms": 917.4822000004497, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 51.226600000518374, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 175.6899735666417}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547590.1538427, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 51.226600000518374, "prefill_cuda_event_ms": null, "kv_decode_ms": 917.4822000004497, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 917.4822000004497, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 34.878060849555794}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547590.1538427, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 51.226600000518374, "prefill_cuda_event_ms": null, "kv_decode_ms": 917.4822000004497, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 968.708800000968, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 42.324380660069394}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547591.1231709, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.33249999954569, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.6738999996669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 39.33249999954569, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 228.81840717228638}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547591.1231709, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 39.33249999954569, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.6738999996669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 958.6738999996669, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 33.37944216486036}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547591.1231709, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 39.33249999954569, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.6738999996669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 998.0063999992126, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 41.081900877621976}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547592.121976, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.55699999976059, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.0523000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 42.55699999976059, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 211.48107244520597}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547592.121976, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.55699999976059, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.0523000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 985.0523000004614, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 32.485584775534264}
{"task_idx": 22, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547592.121976, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.55699999976059, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.0523000004614, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1027.609300000222, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 39.898432215425785}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.1504717, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.094400000118185, "prefill_cuda_event_ms": 8.012800216674805, "kv_decode_ms": 138.0561999994825, "kv_decode_cuda_event_ms": 137.98399353027344, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 8.094400000118185, "cuda_event_ms": 8.012800216674805, "tokens_total": 17, "tokens_per_s": 2100.217434244884}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.1504717, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 8.094400000118185, "prefill_cuda_event_ms": 8.012800216674805, "kv_decode_ms": 138.0561999994825, "kv_decode_cuda_event_ms": 137.98399353027344, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 138.0561999994825, "cuda_event_ms": 137.98399353027344, "tokens_total": 32, "tokens_per_s": 231.78966247165977}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.1504717, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 8.094400000118185, "prefill_cuda_event_ms": 8.012800216674805, "kv_decode_ms": 138.0561999994825, "kv_decode_cuda_event_ms": 137.98399353027344, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 146.15059999960067, "cuda_event_ms": 145.99679374694824, "tokens_total": 49, "tokens_per_s": 335.27060443223553}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.2984366, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.93940000069415, "prefill_cuda_event_ms": 4.8803839683532715, "kv_decode_ms": 85.73720000003959, "kv_decode_cuda_event_ms": 85.6842269897461, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 4.93940000069415, "cuda_event_ms": 4.8803839683532715, "tokens_total": 17, "tokens_per_s": 3441.713567965934}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.2984366, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.93940000069415, "prefill_cuda_event_ms": 4.8803839683532715, "kv_decode_ms": 85.73720000003959, "kv_decode_cuda_event_ms": 85.6842269897461, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 85.73720000003959, "cuda_event_ms": 85.6842269897461, "tokens_total": 32, "tokens_per_s": 373.2335555626405}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.2984366, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.93940000069415, "prefill_cuda_event_ms": 4.8803839683532715, "kv_decode_ms": 85.73720000003959, "kv_decode_cuda_event_ms": 85.6842269897461, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 90.67660000073374, "cuda_event_ms": 90.56461095809937, "tokens_total": 49, "tokens_per_s": 540.3819728530128}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.390441, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.230199999961769, "prefill_cuda_event_ms": 3.1814401149749756, "kv_decode_ms": 75.16169999962585, "kv_decode_cuda_event_ms": 75.12166595458984, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 3.230199999961769, "cuda_event_ms": 3.1814401149749756, "tokens_total": 17, "tokens_per_s": 5262.832022847255}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.390441, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.230199999961769, "prefill_cuda_event_ms": 3.1814401149749756, "kv_decode_ms": 75.16169999962585, "kv_decode_cuda_event_ms": 75.12166595458984, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 75.16169999962585, "cuda_event_ms": 75.12166595458984, "tokens_total": 32, "tokens_per_s": 425.7487523587052}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.390441, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.230199999961769, "prefill_cuda_event_ms": 3.1814401149749756, "kv_decode_ms": 75.16169999962585, "kv_decode_cuda_event_ms": 75.12166595458984, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 78.39189999958762, "cuda_event_ms": 78.30310606956482, "tokens_total": 49, "tokens_per_s": 625.0645793794737}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.4694977, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.227899999728834, "prefill_cuda_event_ms": 3.1836159229278564, "kv_decode_ms": 79.42389999971056, "kv_decode_cuda_event_ms": 79.3855972290039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 3.227899999728834, "cuda_event_ms": 3.1836159229278564, "tokens_total": 17, "tokens_per_s": 5266.5819887320295}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.4694977, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.227899999728834, "prefill_cuda_event_ms": 3.1836159229278564, "kv_decode_ms": 79.42389999971056, "kv_decode_cuda_event_ms": 79.3855972290039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 79.42389999971056, "cuda_event_ms": 79.3855972290039, "tokens_total": 32, "tokens_per_s": 402.9013936625703}
{"task_idx": 23, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.4694977, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.227899999728834, "prefill_cuda_event_ms": 3.1836159229278564, "kv_decode_ms": 79.42389999971056, "kv_decode_cuda_event_ms": 79.3855972290039, "gpu_peak_mb": 631.58984375, "params_millions_measured": 74.824704, "latency_ms": 82.65179999943939, "cuda_event_ms": 82.56921315193176, "tokens_total": 49, "tokens_per_s": 592.8485526066263}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.5529265, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.79780000023311, "prefill_cuda_event_ms": 4.71449613571167, "kv_decode_ms": 129.64969999939058, "kv_decode_cuda_event_ms": 129.6189422607422, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 4.79780000023311, "cuda_event_ms": 4.71449613571167, "tokens_total": 9, "tokens_per_s": 1875.8597689696774}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.5529265, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.79780000023311, "prefill_cuda_event_ms": 4.71449613571167, "kv_decode_ms": 129.64969999939058, "kv_decode_cuda_event_ms": 129.6189422607422, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 129.64969999939058, "cuda_event_ms": 129.6189422607422, "tokens_total": 32, "tokens_per_s": 246.81892823624287}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.5529265, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.79780000023311, "prefill_cuda_event_ms": 4.71449613571167, "kv_decode_ms": 129.64969999939058, "kv_decode_cuda_event_ms": 129.6189422607422, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 134.4474999996237, "cuda_event_ms": 134.33343839645386, "tokens_total": 41, "tokens_per_s": 304.95174696528204}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.6881144, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.483799999434268, "prefill_cuda_event_ms": 4.4410881996154785, "kv_decode_ms": 129.61170000016864, "kv_decode_cuda_event_ms": 129.58502197265625, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 4.483799999434268, "cuda_event_ms": 4.4410881996154785, "tokens_total": 9, "tokens_per_s": 2007.2260139023936}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.6881144, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.483799999434268, "prefill_cuda_event_ms": 4.4410881996154785, "kv_decode_ms": 129.61170000016864, "kv_decode_cuda_event_ms": 129.58502197265625, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 129.61170000016864, "cuda_event_ms": 129.58502197265625, "tokens_total": 32, "tokens_per_s": 246.89129144944758}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.6881144, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.483799999434268, "prefill_cuda_event_ms": 4.4410881996154785, "kv_decode_ms": 129.61170000016864, "kv_decode_cuda_event_ms": 129.58502197265625, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 134.0954999996029, "cuda_event_ms": 134.02611017227173, "tokens_total": 41, "tokens_per_s": 305.7522437376453}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.8226924, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.136599999786995, "prefill_cuda_event_ms": 5.032959938049316, "kv_decode_ms": 129.38729999950738, "kv_decode_cuda_event_ms": 129.32199096679688, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 5.136599999786995, "cuda_event_ms": 5.032959938049316, "tokens_total": 9, "tokens_per_s": 1752.1317603810328}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.8226924, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.136599999786995, "prefill_cuda_event_ms": 5.032959938049316, "kv_decode_ms": 129.38729999950738, "kv_decode_cuda_event_ms": 129.32199096679688, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 129.38729999950738, "cuda_event_ms": 129.32199096679688, "tokens_total": 32, "tokens_per_s": 247.319481897542}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.8226924, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 5.136599999786995, "prefill_cuda_event_ms": 5.032959938049316, "kv_decode_ms": 129.38729999950738, "kv_decode_cuda_event_ms": 129.32199096679688, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 134.52389999929437, "cuda_event_ms": 134.3549509048462, "tokens_total": 41, "tokens_per_s": 304.77855607973794}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547593.9579077, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.973899999640707, "prefill_cuda_event_ms": 4.9027838706970215, "kv_decode_ms": 129.0724999998929, "kv_decode_cuda_event_ms": 129.0424346923828, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 4.973899999640707, "cuda_event_ms": 4.9027838706970215, "tokens_total": 9, "tokens_per_s": 1809.4453046201413}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547593.9579077, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.973899999640707, "prefill_cuda_event_ms": 4.9027838706970215, "kv_decode_ms": 129.0724999998929, "kv_decode_cuda_event_ms": 129.0424346923828, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 129.0724999998929, "cuda_event_ms": 129.0424346923828, "tokens_total": 32, "tokens_per_s": 247.92267911465692}
{"task_idx": 24, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547593.9579077, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 4.973899999640707, "prefill_cuda_event_ms": 4.9027838706970215, "kv_decode_ms": 129.0724999998929, "kv_decode_cuda_event_ms": 129.0424346923828, "gpu_peak_mb": 630.740234375, "params_millions_measured": 51.475968, "latency_ms": 134.0463999995336, "cuda_event_ms": 133.94521856307983, "tokens_total": 41, "tokens_per_s": 305.8642380559467}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547594.0931203, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 514.786799999456, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.7747999994608, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.42600000066886, "params_millions_measured": 74.824704, "latency_ms": 514.786799999456, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 17.482965763709387}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547594.0931203, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 514.786799999456, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.7747999994608, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.42600000066886, "params_millions_measured": 74.824704, "latency_ms": 1217.7747999994608, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 26.277436517830857}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547594.0931203, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 514.786799999456, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.7747999994608, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 157.42600000066886, "params_millions_measured": 74.824704, "latency_ms": 1732.5615999989168, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 23.66438226498015}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547595.9842694, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 43.05629999998928, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.9415999993798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 43.05629999998928, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 209.0286438918867}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547595.9842694, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 43.05629999998928, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.9415999993798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1268.9415999993798, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 25.21786660632423}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547595.9842694, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 43.05629999998928, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.9415999993798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1311.997899999369, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 31.250050019149967}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547597.2967625, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.441100000360166, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.5821000000142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 42.441100000360166, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 212.0585941439695}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547597.2967625, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.441100000360166, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.5821000000142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1181.5821000000142, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 27.08233308544503}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547597.2967625, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.441100000360166, "prefill_cuda_event_ms": null, "kv_decode_ms": 1181.5821000000142, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1224.0232000003743, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 33.4960971327892}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547598.5214918, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.172600000412785, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.3466999999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 42.172600000412785, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 213.40870612463797}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547598.5214918, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.172600000412785, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.3466999999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1064.3466999999873, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 30.06539128650503}
{"task_idx": 25, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547598.5214918, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 42.172600000412785, "prefill_cuda_event_ms": null, "kv_decode_ms": 1064.3466999999873, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1106.5193000004, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 37.05312686365721}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547599.6287, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 58.539099999507016, "prefill_cuda_event_ms": null, "kv_decode_ms": 1052.0987999998397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 58.539099999507016, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 290.4041913890573}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547599.6287, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 58.539099999507016, "prefill_cuda_event_ms": null, "kv_decode_ms": 1052.0987999998397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1052.0987999998397, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 30.415394447750415}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547599.6287, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 58.539099999507016, "prefill_cuda_event_ms": null, "kv_decode_ms": 1052.0987999998397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1110.6378999993467, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 44.11878975139316}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547600.7399895, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 45.04509999969741, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3675999995976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 45.04509999969741, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 377.39953957509687}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547600.7399895, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 45.04509999969741, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3675999995976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 897.3675999995976, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 35.65985667413705}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547600.7399895, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 45.04509999969741, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3675999995976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 942.412699999295, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 51.99420593550644}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547601.6833098, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 42.093900000509166, "prefill_cuda_event_ms": null, "kv_decode_ms": 815.4782999999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 42.093900000509166, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 403.8589914404312}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547601.6833098, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 42.093900000509166, "prefill_cuda_event_ms": null, "kv_decode_ms": 815.4782999999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 815.4782999999952, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 39.24077440196776}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547601.6833098, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 42.093900000509166, "prefill_cuda_event_ms": null, "kv_decode_ms": 815.4782999999952, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 857.5722000005044, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 57.138046219281804}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547602.5416234, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 32.17480000057549, "prefill_cuda_event_ms": null, "kv_decode_ms": 848.6370999999053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 32.17480000057549, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 528.3638126638218}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547602.5416234, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 32.17480000057549, "prefill_cuda_event_ms": null, "kv_decode_ms": 848.6370999999053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 848.6370999999053, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 37.70751950392408}
{"task_idx": 26, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547602.5416234, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 32.17480000057549, "prefill_cuda_event_ms": null, "kv_decode_ms": 848.6370999999053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 880.8119000004808, "cuda_event_ms": null, "tokens_total": 49, "tokens_per_s": 55.630492730596906}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547603.423038, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.88390000065556, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.9538999996439, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 38.88390000065556, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 231.45826421342164}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547603.423038, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 38.88390000065556, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.9538999996439, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 506.9538999996439, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 63.12211031421689}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547603.423038, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 38.88390000065556, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.9538999996439, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 545.8378000002995, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 75.11388914431633}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547603.9703214, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.028900000601425, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.5505999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.028900000601425, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 427.9824431968672}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547603.9703214, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.028900000601425, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.5505999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 451.5505999997913, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 70.8669194549067}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547603.9703214, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.028900000601425, "prefill_cuda_event_ms": null, "kv_decode_ms": 451.5505999997913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 472.5795000003927, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 86.75788941324355}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547604.4435225, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.793400000409747, "prefill_cuda_event_ms": null, "kv_decode_ms": 442.27609999961714, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 22.793400000409747, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 394.8511411126998}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547604.4435225, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 22.793400000409747, "prefill_cuda_event_ms": null, "kv_decode_ms": 442.27609999961714, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 442.27609999961714, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 72.35299397825861}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547604.4435225, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 22.793400000409747, "prefill_cuda_event_ms": null, "kv_decode_ms": 442.27609999961714, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 465.0695000000269, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 88.15886657800098}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547604.909451, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.81020000080025, "prefill_cuda_event_ms": null, "kv_decode_ms": 425.67549999967014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.81020000080025, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 412.6509614615994}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547604.909451, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.81020000080025, "prefill_cuda_event_ms": null, "kv_decode_ms": 425.67549999967014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 425.67549999967014, "cuda_event_ms": null, "tokens_total": 32, "tokens_per_s": 75.17463419911364}
{"task_idx": 27, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547604.909451, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 21.81020000080025, "prefill_cuda_event_ms": null, "kv_decode_ms": 425.67549999967014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 447.4857000004704, "cuda_event_ms": null, "tokens_total": 41, "tokens_per_s": 91.6230395741292}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.3575468, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.368899999870337, "prefill_cuda_event_ms": 5.846015930175781, "kv_decode_ms": 97.41759999997157, "kv_decode_cuda_event_ms": 97.35782623291016, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 6.368899999870337, "cuda_event_ms": 5.846015930175781, "tokens_total": 9, "tokens_per_s": 1413.1168647934855}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.3575468, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.368899999870337, "prefill_cuda_event_ms": 5.846015930175781, "kv_decode_ms": 97.41759999997157, "kv_decode_cuda_event_ms": 97.35782623291016, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 97.41759999997157, "cuda_event_ms": 97.35782623291016, "tokens_total": 32, "tokens_per_s": 328.48273823220177}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.3575468, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 6.368899999870337, "prefill_cuda_event_ms": 5.846015930175781, "kv_decode_ms": 97.41759999997157, "kv_decode_cuda_event_ms": 97.35782623291016, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 103.78649999984191, "cuda_event_ms": 103.20384216308594, "tokens_total": 41, "tokens_per_s": 395.04174435078215}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.4635937, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.611500000261003, "prefill_cuda_event_ms": 3.556351900100708, "kv_decode_ms": 89.71769999971002, "kv_decode_cuda_event_ms": 89.66041564941406, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 3.611500000261003, "cuda_event_ms": 3.556351900100708, "tokens_total": 9, "tokens_per_s": 2492.0393186624865}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.4635937, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.611500000261003, "prefill_cuda_event_ms": 3.556351900100708, "kv_decode_ms": 89.71769999971002, "kv_decode_cuda_event_ms": 89.66041564941406, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 89.71769999971002, "cuda_event_ms": 89.66041564941406, "tokens_total": 32, "tokens_per_s": 356.67432401971325}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.4635937, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.611500000261003, "prefill_cuda_event_ms": 3.556351900100708, "kv_decode_ms": 89.71769999971002, "kv_decode_cuda_event_ms": 89.66041564941406, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 93.32919999997102, "cuda_event_ms": 93.21676754951477, "tokens_total": 41, "tokens_per_s": 439.3051692290594}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.5577135, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.608899999562709, "prefill_cuda_event_ms": 3.540992021560669, "kv_decode_ms": 54.03870000009192, "kv_decode_cuda_event_ms": 53.96076965332031, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 3.608899999562709, "cuda_event_ms": 3.540992021560669, "tokens_total": 9, "tokens_per_s": 2493.8346867717396}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.5577135, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.608899999562709, "prefill_cuda_event_ms": 3.540992021560669, "kv_decode_ms": 54.03870000009192, "kv_decode_cuda_event_ms": 53.96076965332031, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 54.03870000009192, "cuda_event_ms": 53.96076965332031, "tokens_total": 32, "tokens_per_s": 592.1682053777305}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.5577135, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 3.608899999562709, "prefill_cuda_event_ms": 3.540992021560669, "kv_decode_ms": 54.03870000009192, "kv_decode_cuda_event_ms": 53.96076965332031, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 57.64759999965463, "cuda_event_ms": 57.50176167488098, "tokens_total": 41, "tokens_per_s": 711.2178130615262}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.616207, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.3099999998521525, "prefill_cuda_event_ms": 2.2466559410095215, "kv_decode_ms": 53.9070000004358, "kv_decode_cuda_event_ms": 53.85935974121094, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 2.3099999998521525, "cuda_event_ms": 2.2466559410095215, "tokens_total": 9, "tokens_per_s": 3896.103896353259}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.616207, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 2.3099999998521525, "prefill_cuda_event_ms": 2.2466559410095215, "kv_decode_ms": 53.9070000004358, "kv_decode_cuda_event_ms": 53.85935974121094, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 53.9070000004358, "cuda_event_ms": 53.85935974121094, "tokens_total": 32, "tokens_per_s": 593.6149294106758}
{"task_idx": 28, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 32, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.616207, "prompt_tokens": 9, "gen_tokens": 32, "prefill_ms": 2.3099999998521525, "prefill_cuda_event_ms": 2.2466559410095215, "kv_decode_ms": 53.9070000004358, "kv_decode_cuda_event_ms": 53.85935974121094, "gpu_peak_mb": 630.2802734375, "params_millions_measured": 25.016064, "latency_ms": 56.21700000028795, "cuda_event_ms": 56.10601568222046, "tokens_total": 41, "tokens_per_s": 729.316754714588}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.67313, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.995099999883678, "prefill_cuda_event_ms": 3.9504640102386475, "kv_decode_ms": 96.89939999952912, "kv_decode_cuda_event_ms": 96.85401916503906, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 3.995099999883678, "cuda_event_ms": 3.9504640102386475, "tokens_total": 17, "tokens_per_s": 4255.212635602356}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.67313, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.995099999883678, "prefill_cuda_event_ms": 3.9504640102386475, "kv_decode_ms": 96.89939999952912, "kv_decode_cuda_event_ms": 96.85401916503906, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 96.89939999952912, "cuda_event_ms": 96.85401916503906, "tokens_total": 32, "tokens_per_s": 330.23940292876426}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.67313, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.995099999883678, "prefill_cuda_event_ms": 3.9504640102386475, "kv_decode_ms": 96.89939999952912, "kv_decode_cuda_event_ms": 96.85401916503906, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 100.8944999994128, "cuda_event_ms": 100.80448317527771, "tokens_total": 49, "tokens_per_s": 485.6558087931966}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.7747803, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.8055000004533213, "prefill_cuda_event_ms": 3.734208106994629, "kv_decode_ms": 98.51210000033461, "kv_decode_cuda_event_ms": 98.47090911865234, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 3.8055000004533213, "cuda_event_ms": 3.734208106994629, "tokens_total": 17, "tokens_per_s": 4467.218499007992}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.7747803, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.8055000004533213, "prefill_cuda_event_ms": 3.734208106994629, "kv_decode_ms": 98.51210000033461, "kv_decode_cuda_event_ms": 98.47090911865234, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 98.51210000033461, "cuda_event_ms": 98.47090911865234, "tokens_total": 32, "tokens_per_s": 324.83319307873154}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.7747803, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.8055000004533213, "prefill_cuda_event_ms": 3.734208106994629, "kv_decode_ms": 98.51210000033461, "kv_decode_cuda_event_ms": 98.47090911865234, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 102.31760000078793, "cuda_event_ms": 102.20511722564697, "tokens_total": 49, "tokens_per_s": 478.9009906372184}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.8778253, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.050699999424978, "prefill_cuda_event_ms": 3.9933440685272217, "kv_decode_ms": 99.41799999978684, "kv_decode_cuda_event_ms": 99.346435546875, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 4.050699999424978, "cuda_event_ms": 3.9933440685272217, "tokens_total": 17, "tokens_per_s": 4196.805491004828}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.8778253, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.050699999424978, "prefill_cuda_event_ms": 3.9933440685272217, "kv_decode_ms": 99.41799999978684, "kv_decode_cuda_event_ms": 99.346435546875, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 99.41799999978684, "cuda_event_ms": 99.346435546875, "tokens_total": 32, "tokens_per_s": 321.8733026219458}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.8778253, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 4.050699999424978, "prefill_cuda_event_ms": 3.9933440685272217, "kv_decode_ms": 99.41799999978684, "kv_decode_cuda_event_ms": 99.346435546875, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 103.46869999921182, "cuda_event_ms": 103.33977961540222, "tokens_total": 49, "tokens_per_s": 473.57316754122996}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766547605.9820657, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.890300000421121, "prefill_cuda_event_ms": 3.8451199531555176, "kv_decode_ms": 98.84489999967627, "kv_decode_cuda_event_ms": 98.81394958496094, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 3.890300000421121, "cuda_event_ms": 3.8451199531555176, "tokens_total": 17, "tokens_per_s": 4369.842942230617}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766547605.9820657, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.890300000421121, "prefill_cuda_event_ms": 3.8451199531555176, "kv_decode_ms": 98.84489999967627, "kv_decode_cuda_event_ms": 98.81394958496094, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 98.84489999967627, "cuda_event_ms": 98.81394958496094, "tokens_total": 32, "tokens_per_s": 323.7395151404352}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 32, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766547605.9820657, "prompt_tokens": 17, "gen_tokens": 32, "prefill_ms": 3.890300000421121, "prefill_cuda_event_ms": 3.8451199531555176, "kv_decode_ms": 98.84489999967627, "kv_decode_cuda_event_ms": 98.81394958496094, "gpu_peak_mb": 631.44091796875, "params_millions_measured": 45.1712, "latency_ms": 102.7352000000974, "cuda_event_ms": 102.65906953811646, "tokens_total": 49, "tokens_per_s": 476.95434476161574}
