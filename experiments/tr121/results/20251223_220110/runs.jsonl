{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545278.4277427, "prompt_tokens": 19, "prefill_ms": 17.8528, "prefill_cuda_event_ms": null, "kv_decode_ms": 661.4707, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4718.093599999975, "ollama_total_duration_ms": 4696.2359, "ollama_load_ms": 3984.7871, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 17.8528, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1064.258827746908}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545278.4277427, "prompt_tokens": 19, "prefill_ms": 17.8528, "prefill_cuda_event_ms": null, "kv_decode_ms": 661.4707, "kv_decode_cuda_event_ms": null, "gen_tokens": 58, "ollama_wall_ms": 4718.093599999975, "ollama_total_duration_ms": 4696.2359, "ollama_load_ms": 3984.7871, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 661.4707, "cuda_event_ms": null, "tokens_total": 58, "tokens_per_s": 87.68340003570832}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545278.4277427, "prompt_tokens": 19, "prefill_ms": 17.8528, "prefill_cuda_event_ms": null, "kv_decode_ms": 661.4707, "kv_decode_cuda_event_ms": null, "gen_tokens": 58, "ollama_wall_ms": 4718.093599999975, "ollama_total_duration_ms": 4696.2359, "ollama_load_ms": 3984.7871, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 679.3235, "cuda_event_ms": null, "tokens_total": 77, "tokens_per_s": 113.34805876728835}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545283.14596, "prompt_tokens": 19, "prefill_ms": 11.7456, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.8362, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 822.9908999999225, "ollama_total_duration_ms": 821.4332, "ollama_load_ms": 134.1012, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.7456, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1617.627026290696}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545283.14596, "prompt_tokens": 19, "prefill_ms": 11.7456, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.8362, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 822.9908999999225, "ollama_total_duration_ms": 821.4332, "ollama_load_ms": 134.1012, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 646.8362, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 88.12122759981584}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545283.14596, "prompt_tokens": 19, "prefill_ms": 11.7456, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.8362, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 822.9908999999225, "ollama_total_duration_ms": 821.4332, "ollama_load_ms": 134.1012, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 658.5817999999999, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 115.3994841643058}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545283.9690688, "prompt_tokens": 19, "prefill_ms": 12.3021, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.3893, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 866.8061999999281, "ollama_total_duration_ms": 849.5826, "ollama_load_ms": 162.5018, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 12.3021, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1544.4517602685721}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545283.9690688, "prompt_tokens": 19, "prefill_ms": 12.3021, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.3893, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 866.8061999999281, "ollama_total_duration_ms": 849.5826, "ollama_load_ms": 162.5018, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 642.3893, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 88.73124132048899}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545283.9690688, "prompt_tokens": 19, "prefill_ms": 12.3021, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.3893, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 866.8061999999281, "ollama_total_duration_ms": 849.5826, "ollama_load_ms": 162.5018, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 654.6914, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 116.08522733000616}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545284.8359847, "prompt_tokens": 19, "prefill_ms": 11.5595, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.574, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 822.4892999999156, "ollama_total_duration_ms": 820.6005, "ollama_load_ms": 134.281, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.5595, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1643.6697088974436}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545284.8359847, "prompt_tokens": 19, "prefill_ms": 11.5595, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.574, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 822.4892999999156, "ollama_total_duration_ms": 820.6005, "ollama_load_ms": 134.281, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 646.574, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 88.15696269877849}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545284.8359847, "prompt_tokens": 19, "prefill_ms": 11.5595, "prefill_cuda_event_ms": null, "kv_decode_ms": 646.574, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 822.4892999999156, "ollama_total_duration_ms": 820.6005, "ollama_load_ms": 134.281, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 658.1334999999999, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 115.47809069132632}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545285.6587107, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 153.11380000002828, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.8571999999476, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5869.586099999879, "params_millions_measured": 45.1712, "latency_ms": 153.11380000002828, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 58.77980952728192}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545285.6587107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 153.11380000002828, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.8571999999476, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5869.586099999879, "params_millions_measured": 45.1712, "latency_ms": 1153.8571999999476, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 55.46613567086369}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545285.6587107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 153.11380000002828, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.8571999999476, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 5869.586099999879, "params_millions_measured": 45.1712, "latency_ms": 1306.970999999976, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 55.854338007500814}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545292.8495276, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.56059999995341, "prefill_cuda_event_ms": null, "kv_decode_ms": 953.9457000000766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.56059999995341, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 437.73041642852803}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545292.8495276, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.56059999995341, "prefill_cuda_event_ms": null, "kv_decode_ms": 953.9457000000766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 953.9457000000766, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.08977251010708}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545292.8495276, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.56059999995341, "prefill_cuda_event_ms": null, "kv_decode_ms": 953.9457000000766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 974.50630000003, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 74.9097260838619}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545293.8245158, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.26410000002943, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.147099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 22.26410000002943, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 404.23821308690236}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545293.8245158, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.26410000002943, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.147099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1033.147099999951, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.94664825560952}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545293.8245158, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.26410000002943, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.147099999951, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1055.4111999999805, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 69.16735391854981}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545294.8804765, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.471300000077463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1169.4199000000935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 19.471300000077463, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 462.21875272653574}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545294.8804765, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.471300000077463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1169.4199000000935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1169.4199000000935, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.72798949290574}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545294.8804765, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.471300000077463, "prefill_cuda_event_ms": null, "kv_decode_ms": 1169.4199000000935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1188.891200000171, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 61.40174979845885}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545296.0698304, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 578.645300000062, "prefill_cuda_event_ms": 518.8648681640625, "kv_decode_ms": 266.32789999985107, "kv_decode_cuda_event_ms": 266.2872009277344, "gpu_peak_mb": 13.068359375, "hf_load_ms": 290.5974000000242, "params_millions_measured": 0.102714, "latency_ms": 578.645300000062, "cuda_event_ms": 518.8648681640625, "tokens_total": 17, "tokens_per_s": 29.378964972148186}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545296.0698304, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 578.645300000062, "prefill_cuda_event_ms": 518.8648681640625, "kv_decode_ms": 266.32789999985107, "kv_decode_cuda_event_ms": 266.2872009277344, "gpu_peak_mb": 13.068359375, "hf_load_ms": 290.5974000000242, "params_millions_measured": 0.102714, "latency_ms": 266.32789999985107, "cuda_event_ms": 266.2872009277344, "tokens_total": 64, "tokens_per_s": 240.30527781744155}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545296.0698304, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 578.645300000062, "prefill_cuda_event_ms": 518.8648681640625, "kv_decode_ms": 266.32789999985107, "kv_decode_cuda_event_ms": 266.2872009277344, "gpu_peak_mb": 13.068359375, "hf_load_ms": 290.5974000000242, "params_millions_measured": 0.102714, "latency_ms": 844.973199999913, "cuda_event_ms": 785.1520690917969, "tokens_total": 81, "tokens_per_s": 95.86102849180108}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545297.206147, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.305900000010297, "prefill_cuda_event_ms": 2.258847951889038, "kv_decode_ms": 140.85960000011255, "kv_decode_cuda_event_ms": 140.8152618408203, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 2.305900000010297, "cuda_event_ms": 2.258847951889038, "tokens_total": 17, "tokens_per_s": 7372.392558187296}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545297.206147, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.305900000010297, "prefill_cuda_event_ms": 2.258847951889038, "kv_decode_ms": 140.85960000011255, "kv_decode_cuda_event_ms": 140.8152618408203, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 140.85960000011255, "cuda_event_ms": 140.8152618408203, "tokens_total": 64, "tokens_per_s": 454.3531289308564}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545297.206147, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.305900000010297, "prefill_cuda_event_ms": 2.258847951889038, "kv_decode_ms": 140.85960000011255, "kv_decode_cuda_event_ms": 140.8152618408203, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 143.16550000012285, "cuda_event_ms": 143.07410979270935, "tokens_total": 81, "tokens_per_s": 565.7787665319543}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545297.350074, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.6877000000240514, "prefill_cuda_event_ms": 2.637824058532715, "kv_decode_ms": 135.7249000000138, "kv_decode_cuda_event_ms": 135.6820526123047, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 2.6877000000240514, "cuda_event_ms": 2.637824058532715, "tokens_total": 17, "tokens_per_s": 6325.110689380464}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545297.350074, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.6877000000240514, "prefill_cuda_event_ms": 2.637824058532715, "kv_decode_ms": 135.7249000000138, "kv_decode_cuda_event_ms": 135.6820526123047, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 135.7249000000138, "cuda_event_ms": 135.6820526123047, "tokens_total": 64, "tokens_per_s": 471.54206781506923}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545297.350074, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.6877000000240514, "prefill_cuda_event_ms": 2.637824058532715, "kv_decode_ms": 135.7249000000138, "kv_decode_cuda_event_ms": 135.6820526123047, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 138.41260000003786, "cuda_event_ms": 138.3198766708374, "tokens_total": 81, "tokens_per_s": 585.2068381056193}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545297.489027, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.284700000018347, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 115.49809999996796, "kv_decode_cuda_event_ms": 115.45394897460938, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 2.284700000018347, "cuda_event_ms": 2.2415359020233154, "tokens_total": 17, "tokens_per_s": 7440.801855763769}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545297.489027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.284700000018347, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 115.49809999996796, "kv_decode_cuda_event_ms": 115.45394897460938, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 115.49809999996796, "cuda_event_ms": 115.45394897460938, "tokens_total": 64, "tokens_per_s": 554.1216695341114}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545297.489027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.284700000018347, "prefill_cuda_event_ms": 2.2415359020233154, "kv_decode_ms": 115.49809999996796, "kv_decode_cuda_event_ms": 115.45394897460938, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 117.78279999998631, "cuda_event_ms": 117.69548487663269, "tokens_total": 81, "tokens_per_s": 687.7065242124437}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545297.6073053, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.928099999977348, "prefill_cuda_event_ms": 11.850751876831055, "kv_decode_ms": 362.54059999987476, "kv_decode_cuda_event_ms": 362.4989318847656, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 374.2968000001383, "params_millions_measured": 96.08832, "latency_ms": 11.928099999977348, "cuda_event_ms": 11.850751876831055, "tokens_total": 1, "tokens_per_s": 83.83564859465456}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545297.6073053, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.928099999977348, "prefill_cuda_event_ms": 11.850751876831055, "kv_decode_ms": 362.54059999987476, "kv_decode_cuda_event_ms": 362.4989318847656, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 374.2968000001383, "params_millions_measured": 96.08832, "latency_ms": 362.54059999987476, "cuda_event_ms": 362.4989318847656, "tokens_total": 64, "tokens_per_s": 176.53195255930538}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545297.6073053, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.928099999977348, "prefill_cuda_event_ms": 11.850751876831055, "kv_decode_ms": 362.54059999987476, "kv_decode_cuda_event_ms": 362.4989318847656, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 374.2968000001383, "params_millions_measured": 96.08832, "latency_ms": 374.4686999998521, "cuda_event_ms": 374.3496837615967, "tokens_total": 65, "tokens_per_s": 173.57926042957843}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545298.356979, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.663999999953376, "prefill_cuda_event_ms": 8.616607666015625, "kv_decode_ms": 459.80250000002343, "kv_decode_cuda_event_ms": 459.75830078125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 8.663999999953376, "cuda_event_ms": 8.616607666015625, "tokens_total": 1, "tokens_per_s": 115.4201292711659}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545298.356979, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.663999999953376, "prefill_cuda_event_ms": 8.616607666015625, "kv_decode_ms": 459.80250000002343, "kv_decode_cuda_event_ms": 459.75830078125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 459.80250000002343, "cuda_event_ms": 459.75830078125, "tokens_total": 64, "tokens_per_s": 139.19019579057692}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545298.356979, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.663999999953376, "prefill_cuda_event_ms": 8.616607666015625, "kv_decode_ms": 459.80250000002343, "kv_decode_cuda_event_ms": 459.75830078125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 468.4664999999768, "cuda_event_ms": 468.3749084472656, "tokens_total": 65, "tokens_per_s": 138.7505830192836}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545298.8261034, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.628600000041843, "prefill_cuda_event_ms": 9.569279670715332, "kv_decode_ms": 438.64320000011503, "kv_decode_cuda_event_ms": 438.466552734375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 9.628600000041843, "cuda_event_ms": 9.569279670715332, "tokens_total": 1, "tokens_per_s": 103.8572585833511}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545298.8261034, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.628600000041843, "prefill_cuda_event_ms": 9.569279670715332, "kv_decode_ms": 438.64320000011503, "kv_decode_cuda_event_ms": 438.466552734375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 438.64320000011503, "cuda_event_ms": 438.466552734375, "tokens_total": 64, "tokens_per_s": 145.9044617584023}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545298.8261034, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.628600000041843, "prefill_cuda_event_ms": 9.569279670715332, "kv_decode_ms": 438.64320000011503, "kv_decode_cuda_event_ms": 438.466552734375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 448.2718000001569, "cuda_event_ms": 448.03583240509033, "tokens_total": 65, "tokens_per_s": 145.00131393493245}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545299.2750216, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.8102000000890257, "prefill_cuda_event_ms": 3.773375988006592, "kv_decode_ms": 398.9885000000868, "kv_decode_cuda_event_ms": 398.9605712890625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 3.8102000000890257, "cuda_event_ms": 3.773375988006592, "tokens_total": 1, "tokens_per_s": 262.45341451279063}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545299.2750216, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.8102000000890257, "prefill_cuda_event_ms": 3.773375988006592, "kv_decode_ms": 398.9885000000868, "kv_decode_cuda_event_ms": 398.9605712890625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 398.9885000000868, "cuda_event_ms": 398.9605712890625, "tokens_total": 64, "tokens_per_s": 160.40562572601988}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545299.2750216, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.8102000000890257, "prefill_cuda_event_ms": 3.773375988006592, "kv_decode_ms": 398.9885000000868, "kv_decode_cuda_event_ms": 398.9605712890625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 402.7987000001758, "cuda_event_ms": 402.7339472770691, "tokens_total": 65, "tokens_per_s": 161.37092795972686}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545299.6786342, "prompt_tokens": 30, "prefill_ms": 70.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.3041, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 5229.83639999984, "ollama_total_duration_ms": 5208.9921, "ollama_load_ms": 4787.0752, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 70.8188, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 423.61632786774135}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545299.6786342, "prompt_tokens": 30, "prefill_ms": 70.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.3041, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 5229.83639999984, "ollama_total_duration_ms": 5208.9921, "ollama_load_ms": 4787.0752, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 322.3041, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.97713649934953}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545299.6786342, "prompt_tokens": 30, "prefill_ms": 70.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.3041, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 5229.83639999984, "ollama_total_duration_ms": 5208.9921, "ollama_load_ms": 4787.0752, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 393.1229, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 150.08029295673185}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545304.9086795, "prompt_tokens": 30, "prefill_ms": 11.4179, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3293, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 494.69460000000254, "ollama_total_duration_ms": 477.3625, "ollama_load_ms": 121.9604, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4179, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2627.453384597868}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545304.9086795, "prompt_tokens": 30, "prefill_ms": 11.4179, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3293, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 494.69460000000254, "ollama_total_duration_ms": 477.3625, "ollama_load_ms": 121.9604, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 319.3293, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 90.8153432835634}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545304.9086795, "prompt_tokens": 30, "prefill_ms": 11.4179, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.3293, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 494.69460000000254, "ollama_total_duration_ms": 477.3625, "ollama_load_ms": 121.9604, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 330.74719999999996, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 178.38397422563216}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545305.4034953, "prompt_tokens": 30, "prefill_ms": 13.002, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.7097, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 483.7437000001046, "ollama_total_duration_ms": 480.8347, "ollama_load_ms": 124.1028, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 13.002, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2307.337332718043}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545305.4034953, "prompt_tokens": 30, "prefill_ms": 13.002, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.7097, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 483.7437000001046, "ollama_total_duration_ms": 480.8347, "ollama_load_ms": 124.1028, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 317.7097, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 91.27829587828134}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545305.4034953, "prompt_tokens": 30, "prefill_ms": 13.002, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.7097, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 483.7437000001046, "ollama_total_duration_ms": 480.8347, "ollama_load_ms": 124.1028, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 330.7117, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 178.40312271987958}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545305.887327, "prompt_tokens": 30, "prefill_ms": 11.416, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4479, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 494.9301999999989, "ollama_total_duration_ms": 476.8213, "ollama_load_ms": 120.4202, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.416, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2627.8906797477225}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545305.887327, "prompt_tokens": 30, "prefill_ms": 11.416, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4479, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 494.9301999999989, "ollama_total_duration_ms": 476.8213, "ollama_load_ms": 120.4202, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 322.4479, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.93700997897645}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545305.887327, "prompt_tokens": 30, "prefill_ms": 11.416, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4479, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 494.9301999999989, "ollama_total_duration_ms": 476.8213, "ollama_load_ms": 120.4202, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 333.8639, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 176.7187168184401}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545306.3823867, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 18.710999999939304, "prefill_cuda_event_ms": 18.65011215209961, "kv_decode_ms": 384.0139999999792, "kv_decode_cuda_event_ms": 383.97747802734375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 244.4796000002043, "params_millions_measured": 5.03672, "latency_ms": 18.710999999939304, "cuda_event_ms": 18.65011215209961, "tokens_total": 9, "tokens_per_s": 481.0004810020413}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545306.3823867, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.710999999939304, "prefill_cuda_event_ms": 18.65011215209961, "kv_decode_ms": 384.0139999999792, "kv_decode_cuda_event_ms": 383.97747802734375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 244.4796000002043, "params_millions_measured": 5.03672, "latency_ms": 384.0139999999792, "cuda_event_ms": 383.97747802734375, "tokens_total": 64, "tokens_per_s": 166.66059049931374}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545306.3823867, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.710999999939304, "prefill_cuda_event_ms": 18.65011215209961, "kv_decode_ms": 384.0139999999792, "kv_decode_cuda_event_ms": 383.97747802734375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 244.4796000002043, "params_millions_measured": 5.03672, "latency_ms": 402.7249999999185, "cuda_event_ms": 402.62759017944336, "tokens_total": 73, "tokens_per_s": 181.26513129310266}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545307.030251, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.128500000158965, "prefill_cuda_event_ms": 6.085631847381592, "kv_decode_ms": 369.08969999990404, "kv_decode_cuda_event_ms": 369.0557556152344, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.128500000158965, "cuda_event_ms": 6.085631847381592, "tokens_total": 9, "tokens_per_s": 1468.5485844442444}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545307.030251, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.128500000158965, "prefill_cuda_event_ms": 6.085631847381592, "kv_decode_ms": 369.08969999990404, "kv_decode_cuda_event_ms": 369.0557556152344, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 369.08969999990404, "cuda_event_ms": 369.0557556152344, "tokens_total": 64, "tokens_per_s": 173.3995828114863}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545307.030251, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.128500000158965, "prefill_cuda_event_ms": 6.085631847381592, "kv_decode_ms": 369.08969999990404, "kv_decode_cuda_event_ms": 369.0557556152344, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 375.218200000063, "cuda_event_ms": 375.14138746261597, "tokens_total": 73, "tokens_per_s": 194.55346249192533}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545307.4059503, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.414399999812304, "prefill_cuda_event_ms": 6.370304107666016, "kv_decode_ms": 371.25600000013037, "kv_decode_cuda_event_ms": 371.20306396484375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.414399999812304, "cuda_event_ms": 6.370304107666016, "tokens_total": 9, "tokens_per_s": 1403.0930406995753}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545307.4059503, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.414399999812304, "prefill_cuda_event_ms": 6.370304107666016, "kv_decode_ms": 371.25600000013037, "kv_decode_cuda_event_ms": 371.20306396484375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 371.25600000013037, "cuda_event_ms": 371.20306396484375, "tokens_total": 64, "tokens_per_s": 172.3877863252783}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545307.4059503, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.414399999812304, "prefill_cuda_event_ms": 6.370304107666016, "kv_decode_ms": 371.25600000013037, "kv_decode_cuda_event_ms": 371.20306396484375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 377.6703999999427, "cuda_event_ms": 377.57336807250977, "tokens_total": 73, "tokens_per_s": 193.29023402419432}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545307.7841809, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.048500000133572, "prefill_cuda_event_ms": 5.996511936187744, "kv_decode_ms": 371.0790000000088, "kv_decode_cuda_event_ms": 371.04241943359375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.048500000133572, "cuda_event_ms": 5.996511936187744, "tokens_total": 9, "tokens_per_s": 1487.972224485616}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545307.7841809, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.048500000133572, "prefill_cuda_event_ms": 5.996511936187744, "kv_decode_ms": 371.0790000000088, "kv_decode_cuda_event_ms": 371.04241943359375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 371.0790000000088, "cuda_event_ms": 371.04241943359375, "tokens_total": 64, "tokens_per_s": 172.47001312388596}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545307.7841809, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.048500000133572, "prefill_cuda_event_ms": 5.996511936187744, "kv_decode_ms": 371.0790000000088, "kv_decode_cuda_event_ms": 371.04241943359375, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 377.1275000001424, "cuda_event_ms": 377.0389313697815, "tokens_total": 73, "tokens_per_s": 193.56848811071174}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545308.1619844, "prompt_tokens": 25, "prefill_ms": 9.4897, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3973, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1683.1356999998661, "ollama_total_duration_ms": 1632.4021, "ollama_load_ms": 1438.2175, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 9.4897, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2634.4352297754413}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545308.1619844, "prompt_tokens": 25, "prefill_ms": 9.4897, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 1683.1356999998661, "ollama_total_duration_ms": 1632.4021, "ollama_load_ms": 1438.2175, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 125.3973, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 510.3778151523199}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545308.1619844, "prompt_tokens": 25, "prefill_ms": 9.4897, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 1683.1356999998661, "ollama_total_duration_ms": 1632.4021, "ollama_load_ms": 1438.2175, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.887, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 659.811545960693}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545309.8452358, "prompt_tokens": 25, "prefill_ms": 2.6858, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7724, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 316.03960000006737, "ollama_total_duration_ms": 285.6808, "ollama_load_ms": 127.5935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6858, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9308.213567652096}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545309.8452358, "prompt_tokens": 25, "prefill_ms": 2.6858, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7724, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 316.03960000006737, "ollama_total_duration_ms": 285.6808, "ollama_load_ms": 127.5935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 123.7724, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 517.0781208088395}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545309.8452358, "prompt_tokens": 25, "prefill_ms": 2.6858, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.7724, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 316.03960000006737, "ollama_total_duration_ms": 285.6808, "ollama_load_ms": 127.5935, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 126.4582, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 703.7898689052984}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545310.1613827, "prompt_tokens": 25, "prefill_ms": 1.7712, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.2215, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 302.3657999999614, "ollama_total_duration_ms": 285.9607, "ollama_load_ms": 123.7092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.7712, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 14114.724480578137}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545310.1613827, "prompt_tokens": 25, "prefill_ms": 1.7712, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.2215, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 302.3657999999614, "ollama_total_duration_ms": 285.9607, "ollama_load_ms": 123.7092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 119.2215, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 536.815926657524}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545310.1613827, "prompt_tokens": 25, "prefill_ms": 1.7712, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.2215, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 302.3657999999614, "ollama_total_duration_ms": 285.9607, "ollama_load_ms": 123.7092, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.9927, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 735.5815681441939}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545310.463999, "prompt_tokens": 25, "prefill_ms": 2.3102, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.1393, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 309.0079999999489, "ollama_total_duration_ms": 279.9974, "ollama_load_ms": 124.7634, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3102, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10821.573889706518}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545310.463999, "prompt_tokens": 25, "prefill_ms": 2.3102, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.1393, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 309.0079999999489, "ollama_total_duration_ms": 279.9974, "ollama_load_ms": 124.7634, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 116.1393, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 551.0623880116377}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545310.463999, "prompt_tokens": 25, "prefill_ms": 2.3102, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.1393, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 309.0079999999489, "ollama_total_duration_ms": 279.9974, "ollama_load_ms": 124.7634, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 118.4495, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 751.3750585692637}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545310.773128, "prompt_tokens": 27, "prefill_ms": 17.6833, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.1027, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 933.9676000001873, "ollama_total_duration_ms": 917.7575, "ollama_load_ms": 130.065, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 17.6833, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 1526.8643296217335}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545310.773128, "prompt_tokens": 27, "prefill_ms": 17.6833, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.1027, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 933.9676000001873, "ollama_total_duration_ms": 917.7575, "ollama_load_ms": 130.065, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 729.1027, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.77912905822458}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545310.773128, "prompt_tokens": 27, "prefill_ms": 17.6833, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.1027, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 933.9676000001873, "ollama_total_duration_ms": 917.7575, "ollama_load_ms": 130.065, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 746.7860000000001, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 121.855524875935}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545311.7072334, "prompt_tokens": 27, "prefill_ms": 11.9878, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.4606, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 925.8735000000797, "ollama_total_duration_ms": 900.4202, "ollama_load_ms": 122.3893, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.9878, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2252.2898279917918}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545311.7072334, "prompt_tokens": 27, "prefill_ms": 11.9878, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.4606, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 925.8735000000797, "ollama_total_duration_ms": 900.4202, "ollama_load_ms": 122.3893, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 726.4606, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.0983772554217}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545311.7072334, "prompt_tokens": 27, "prefill_ms": 11.9878, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.4606, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 925.8735000000797, "ollama_total_duration_ms": 900.4202, "ollama_load_ms": 122.3893, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 738.4484, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.23135915793168}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545312.6332264, "prompt_tokens": 27, "prefill_ms": 11.3313, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.9781, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 972.6413000000775, "ollama_total_duration_ms": 959.6265, "ollama_load_ms": 185.1384, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.3313, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2382.780440020121}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545312.6332264, "prompt_tokens": 27, "prefill_ms": 11.3313, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.9781, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 972.6413000000775, "ollama_total_duration_ms": 959.6265, "ollama_load_ms": 185.1384, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 726.9781, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.0356643480732}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545312.6332264, "prompt_tokens": 27, "prefill_ms": 11.3313, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.9781, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 972.6413000000775, "ollama_total_duration_ms": 959.6265, "ollama_load_ms": 185.1384, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 738.3094000000001, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.25455967376277}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545313.6059523, "prompt_tokens": 27, "prefill_ms": 11.9939, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.5164, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 955.44649999988, "ollama_total_duration_ms": 937.226, "ollama_load_ms": 160.3388, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.9939, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2251.1443317019484}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545313.6059523, "prompt_tokens": 27, "prefill_ms": 11.9939, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.5164, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 955.44649999988, "ollama_total_duration_ms": 937.226, "ollama_load_ms": 160.3388, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 726.5164, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.09161087072502}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545313.6059523, "prompt_tokens": 27, "prefill_ms": 11.9939, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.5164, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 955.44649999988, "ollama_total_duration_ms": 937.226, "ollama_load_ms": 160.3388, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 738.5103, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.22103022801441}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545314.5615118, "prompt_tokens": 17, "prefill_ms": 6.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.5428, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 293.24049999991075, "ollama_total_duration_ms": 258.479, "ollama_load_ms": 160.1273, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 6.7503, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2518.40658933677}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545314.5615118, "prompt_tokens": 17, "prefill_ms": 6.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.5428, "kv_decode_cuda_event_ms": null, "gen_tokens": 37, "ollama_wall_ms": 293.24049999991075, "ollama_total_duration_ms": 258.479, "ollama_load_ms": 160.1273, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 67.5428, "cuda_event_ms": null, "tokens_total": 37, "tokens_per_s": 547.8008018619305}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545314.5615118, "prompt_tokens": 17, "prefill_ms": 6.7503, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.5428, "kv_decode_cuda_event_ms": null, "gen_tokens": 37, "ollama_wall_ms": 293.24049999991075, "ollama_total_duration_ms": 258.479, "ollama_load_ms": 160.1273, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 74.2931, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 726.8508111789655}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545314.854918, "prompt_tokens": 17, "prefill_ms": 2.4836, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.6763, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 308.8227000000643, "ollama_total_duration_ms": 280.9739, "ollama_load_ms": 160.4782, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4836, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6844.902560798841}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545314.854918, "prompt_tokens": 17, "prefill_ms": 2.4836, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.6763, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 308.8227000000643, "ollama_total_duration_ms": 280.9739, "ollama_load_ms": 160.4782, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 91.6763, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 523.5813399973603}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545314.854918, "prompt_tokens": 17, "prefill_ms": 2.4836, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.6763, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 308.8227000000643, "ollama_total_duration_ms": 280.9739, "ollama_load_ms": 160.4782, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 94.1599, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 690.3150916685341}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545315.1638834, "prompt_tokens": 17, "prefill_ms": 2.8906, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.7868, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 318.4227000001556, "ollama_total_duration_ms": 281.6301, "ollama_load_ms": 157.6303, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.8906, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5881.131944924929}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545315.1638834, "prompt_tokens": 17, "prefill_ms": 2.8906, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.7868, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 318.4227000001556, "ollama_total_duration_ms": 281.6301, "ollama_load_ms": 157.6303, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 92.7868, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 517.3149629042061}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545315.1638834, "prompt_tokens": 17, "prefill_ms": 2.8906, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.7868, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 318.4227000001556, "ollama_total_duration_ms": 281.6301, "ollama_load_ms": 157.6303, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 95.6774, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 679.3662871273675}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545315.482427, "prompt_tokens": 17, "prefill_ms": 2.4608, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 288.9941999999337, "ollama_total_duration_ms": 265.0124, "ollama_load_ms": 148.4507, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4608, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6908.322496749025}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545315.482427, "prompt_tokens": 17, "prefill_ms": 2.4608, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 288.9941999999337, "ollama_total_duration_ms": 265.0124, "ollama_load_ms": 148.4507, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 84.8872, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 565.456276093451}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545315.482427, "prompt_tokens": 17, "prefill_ms": 2.4608, "prefill_cuda_event_ms": null, "kv_decode_ms": 84.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 288.9941999999337, "ollama_total_duration_ms": 265.0124, "ollama_load_ms": 148.4507, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 87.34800000000001, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 744.1498374318816}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545315.7716947, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 37.29339999995318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.042899999913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.89170000008926, "params_millions_measured": 5.03672, "latency_ms": 37.29339999995318, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 241.32956501716922}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545315.7716947, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.29339999995318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.042899999913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.89170000008926, "params_millions_measured": 5.03672, "latency_ms": 1039.042899999913, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.59514684139159}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545315.7716947, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.29339999995318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.042899999913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.89170000008926, "params_millions_measured": 5.03672, "latency_ms": 1076.3362999998662, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 67.82266843551507}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545317.0386288, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.763500000062777, "prefill_cuda_event_ms": null, "kv_decode_ms": 964.072299999998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 24.763500000062777, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 363.43812465835543}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545317.0386288, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.763500000062777, "prefill_cuda_event_ms": null, "kv_decode_ms": 964.072299999998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 964.072299999998, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 66.38506261408001}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545317.0386288, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.763500000062777, "prefill_cuda_event_ms": null, "kv_decode_ms": 964.072299999998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 988.8358000000608, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.82418799966133}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545318.0279276, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.34899999998197, "prefill_cuda_event_ms": null, "kv_decode_ms": 993.6806999999135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 26.34899999998197, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 341.5689399979566}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545318.0279276, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.34899999998197, "prefill_cuda_event_ms": null, "kv_decode_ms": 993.6806999999135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 993.6806999999135, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.40700720060838}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545318.0279276, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.34899999998197, "prefill_cuda_event_ms": null, "kv_decode_ms": 993.6806999999135, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1020.0296999998955, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 71.56654360162992}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545319.0484169, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 23.964900000009948, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.0025999999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 23.964900000009948, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 375.54924076446235}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545319.0484169, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 23.964900000009948, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.0025999999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 972.0025999999962, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.84344527473512}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545319.0484169, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 23.964900000009948, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.0025999999962, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 995.9675000000061, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.29556436329453}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545320.0452662, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.429199999909542, "prefill_cuda_event_ms": 8.346624374389648, "kv_decode_ms": 328.0265999999301, "kv_decode_cuda_event_ms": 327.9708251953125, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 551.4570000000276, "params_millions_measured": 74.824704, "latency_ms": 8.429199999909542, "cuda_event_ms": 8.346624374389648, "tokens_total": 1, "tokens_per_s": 118.63522042551267}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545320.0452662, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.429199999909542, "prefill_cuda_event_ms": 8.346624374389648, "kv_decode_ms": 328.0265999999301, "kv_decode_cuda_event_ms": 327.9708251953125, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 551.4570000000276, "params_millions_measured": 74.824704, "latency_ms": 328.0265999999301, "cuda_event_ms": 327.9708251953125, "tokens_total": 64, "tokens_per_s": 195.10612858839386}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545320.0452662, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.429199999909542, "prefill_cuda_event_ms": 8.346624374389648, "kv_decode_ms": 328.0265999999301, "kv_decode_cuda_event_ms": 327.9708251953125, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 551.4570000000276, "params_millions_measured": 74.824704, "latency_ms": 336.45579999983966, "cuda_event_ms": 336.31744956970215, "tokens_total": 65, "tokens_per_s": 193.1903090986423}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545320.9347374, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.996699999968769, "prefill_cuda_event_ms": 5.907455921173096, "kv_decode_ms": 274.191700000074, "kv_decode_cuda_event_ms": 274.1667175292969, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.996699999968769, "cuda_event_ms": 5.907455921173096, "tokens_total": 1, "tokens_per_s": 166.7583837786129}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545320.9347374, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.996699999968769, "prefill_cuda_event_ms": 5.907455921173096, "kv_decode_ms": 274.191700000074, "kv_decode_cuda_event_ms": 274.1667175292969, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 274.191700000074, "cuda_event_ms": 274.1667175292969, "tokens_total": 64, "tokens_per_s": 233.41333818632268}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545320.9347374, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.996699999968769, "prefill_cuda_event_ms": 5.907455921173096, "kv_decode_ms": 274.191700000074, "kv_decode_cuda_event_ms": 274.1667175292969, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 280.18840000004275, "cuda_event_ms": 280.07417345046997, "tokens_total": 65, "tokens_per_s": 231.98676319215957}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545321.215738, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.883900000142603, "prefill_cuda_event_ms": 2.844575881958008, "kv_decode_ms": 340.6413000000157, "kv_decode_cuda_event_ms": 340.60491943359375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 2.883900000142603, "cuda_event_ms": 2.844575881958008, "tokens_total": 1, "tokens_per_s": 346.75266130952946}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545321.215738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.883900000142603, "prefill_cuda_event_ms": 2.844575881958008, "kv_decode_ms": 340.6413000000157, "kv_decode_cuda_event_ms": 340.60491943359375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 340.6413000000157, "cuda_event_ms": 340.60491943359375, "tokens_total": 64, "tokens_per_s": 187.8809175516799}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545321.215738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.883900000142603, "prefill_cuda_event_ms": 2.844575881958008, "kv_decode_ms": 340.6413000000157, "kv_decode_cuda_event_ms": 340.60491943359375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 343.5252000001583, "cuda_event_ms": 343.44949531555176, "tokens_total": 65, "tokens_per_s": 189.21464859046745}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545321.5597372, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.699399999983143, "prefill_cuda_event_ms": 2.663424015045166, "kv_decode_ms": 297.34579999990274, "kv_decode_cuda_event_ms": 297.2989501953125, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 2.699399999983143, "cuda_event_ms": 2.663424015045166, "tokens_total": 1, "tokens_per_s": 370.45269319339286}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545321.5597372, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.699399999983143, "prefill_cuda_event_ms": 2.663424015045166, "kv_decode_ms": 297.34579999990274, "kv_decode_cuda_event_ms": 297.2989501953125, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 297.34579999990274, "cuda_event_ms": 297.2989501953125, "tokens_total": 64, "tokens_per_s": 215.2376122347144}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545321.5597372, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.699399999983143, "prefill_cuda_event_ms": 2.663424015045166, "kv_decode_ms": 297.34579999990274, "kv_decode_cuda_event_ms": 297.2989501953125, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 300.0451999998859, "cuda_event_ms": 299.96237421035767, "tokens_total": 65, "tokens_per_s": 216.63402713999332}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545321.8603027, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 38.765499999954045, "prefill_cuda_event_ms": 38.6693115234375, "kv_decode_ms": 466.3581999998314, "kv_decode_cuda_event_ms": 466.335693359375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 379.2038000001412, "params_millions_measured": 51.475968, "latency_ms": 38.765499999954045, "cuda_event_ms": 38.6693115234375, "tokens_total": 17, "tokens_per_s": 438.53426371438917}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545321.8603027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.765499999954045, "prefill_cuda_event_ms": 38.6693115234375, "kv_decode_ms": 466.3581999998314, "kv_decode_cuda_event_ms": 466.335693359375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 379.2038000001412, "params_millions_measured": 51.475968, "latency_ms": 466.3581999998314, "cuda_event_ms": 466.335693359375, "tokens_total": 64, "tokens_per_s": 137.23356853170617}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545321.8603027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.765499999954045, "prefill_cuda_event_ms": 38.6693115234375, "kv_decode_ms": 466.3581999998314, "kv_decode_cuda_event_ms": 466.335693359375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 379.2038000001412, "params_millions_measured": 51.475968, "latency_ms": 505.12369999978546, "cuda_event_ms": 505.0050048828125, "tokens_total": 81, "tokens_per_s": 160.35676013624862}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545322.7455554, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.026399999929708, "prefill_cuda_event_ms": 3.994623899459839, "kv_decode_ms": 402.7719000000616, "kv_decode_cuda_event_ms": 402.74945068359375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 4.026399999929708, "cuda_event_ms": 3.994623899459839, "tokens_total": 17, "tokens_per_s": 4222.133916227096}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545322.7455554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.026399999929708, "prefill_cuda_event_ms": 3.994623899459839, "kv_decode_ms": 402.7719000000616, "kv_decode_cuda_event_ms": 402.74945068359375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 402.7719000000616, "cuda_event_ms": 402.74945068359375, "tokens_total": 64, "tokens_per_s": 158.89887055176942}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545322.7455554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.026399999929708, "prefill_cuda_event_ms": 3.994623899459839, "kv_decode_ms": 402.7719000000616, "kv_decode_cuda_event_ms": 402.74945068359375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 406.7982999999913, "cuda_event_ms": 406.7440745830536, "tokens_total": 81, "tokens_per_s": 199.11587634462026}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545323.152902, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.8339000001506065, "prefill_cuda_event_ms": 7.782400131225586, "kv_decode_ms": 437.4569000001429, "kv_decode_cuda_event_ms": 437.4343566894531, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 7.8339000001506065, "cuda_event_ms": 7.782400131225586, "tokens_total": 17, "tokens_per_s": 2170.055783156943}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545323.152902, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.8339000001506065, "prefill_cuda_event_ms": 7.782400131225586, "kv_decode_ms": 437.4569000001429, "kv_decode_cuda_event_ms": 437.4343566894531, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 437.4569000001429, "cuda_event_ms": 437.4343566894531, "tokens_total": 64, "tokens_per_s": 146.30012693817173}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545323.152902, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.8339000001506065, "prefill_cuda_event_ms": 7.782400131225586, "kv_decode_ms": 437.4569000001429, "kv_decode_cuda_event_ms": 437.4343566894531, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 445.29080000029353, "cuda_event_ms": 445.2167568206787, "tokens_total": 81, "tokens_per_s": 181.90360097254785}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545323.5988562, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.540200000017649, "prefill_cuda_event_ms": 6.497280120849609, "kv_decode_ms": 498.63849999997, "kv_decode_cuda_event_ms": 498.5896911621094, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 6.540200000017649, "cuda_event_ms": 6.497280120849609, "tokens_total": 17, "tokens_per_s": 2599.3088896293884}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545323.5988562, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.540200000017649, "prefill_cuda_event_ms": 6.497280120849609, "kv_decode_ms": 498.63849999997, "kv_decode_cuda_event_ms": 498.5896911621094, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 498.63849999997, "cuda_event_ms": 498.5896911621094, "tokens_total": 64, "tokens_per_s": 128.34949567673544}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545323.5988562, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.540200000017649, "prefill_cuda_event_ms": 6.497280120849609, "kv_decode_ms": 498.63849999997, "kv_decode_cuda_event_ms": 498.5896911621094, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 505.17869999998766, "cuda_event_ms": 505.086971282959, "tokens_total": 81, "tokens_per_s": 160.33930171640645}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545324.1045964, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.116500000047381, "prefill_cuda_event_ms": null, "kv_decode_ms": 169.08039999998437, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 141.84750000003987, "params_millions_measured": 0.102714, "latency_ms": 12.116500000047381, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 742.7887591272072}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545324.1045964, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.116500000047381, "prefill_cuda_event_ms": null, "kv_decode_ms": 169.08039999998437, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 141.84750000003987, "params_millions_measured": 0.102714, "latency_ms": 169.08039999998437, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 378.51814876239894}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545324.1045964, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.116500000047381, "prefill_cuda_event_ms": null, "kv_decode_ms": 169.08039999998437, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 141.84750000003987, "params_millions_measured": 0.102714, "latency_ms": 181.19690000003175, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 402.87664965563545}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545324.4298477, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.304999999931169, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.09700000005068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.304999999931169, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2090.592334528199}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545324.4298477, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.304999999931169, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.09700000005068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 175.09700000005068, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 365.51168780722384}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545324.4298477, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.304999999931169, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.09700000005068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 179.40199999998185, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 406.9073923368044}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545324.6097536, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.286399999955393, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.34550000004856, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.286399999955393, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2099.6640537732505}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545324.6097536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.286399999955393, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.34550000004856, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 175.34550000004856, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 364.9936838982596}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545324.6097536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.286399999955393, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.34550000004856, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 179.63190000000395, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 406.3866161856463}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545324.7899992, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7055999998756306, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.13120000014715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.7055999998756306, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2428.756476765453}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545324.7899992, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7055999998756306, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.13120000014715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 174.13120000014715, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 367.5389591293572}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545324.7899992, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7055999998756306, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.13120000014715, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 177.83680000002278, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 410.48871774565583}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545324.968267, "prompt_tokens": 38, "prefill_ms": 78.5074, "prefill_cuda_event_ms": null, "kv_decode_ms": 525.6452, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 774.6291999999357, "ollama_total_duration_ms": 748.1382, "ollama_load_ms": 108.6787, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 78.5074, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 484.03080473942583}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545324.968267, "prompt_tokens": 38, "prefill_ms": 78.5074, "prefill_cuda_event_ms": null, "kv_decode_ms": 525.6452, "kv_decode_cuda_event_ms": null, "gen_tokens": 36, "ollama_wall_ms": 774.6291999999357, "ollama_total_duration_ms": 748.1382, "ollama_load_ms": 108.6787, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 525.6452, "cuda_event_ms": null, "tokens_total": 36, "tokens_per_s": 68.48726098897126}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545324.968267, "prompt_tokens": 38, "prefill_ms": 78.5074, "prefill_cuda_event_ms": null, "kv_decode_ms": 525.6452, "kv_decode_cuda_event_ms": null, "gen_tokens": 36, "ollama_wall_ms": 774.6291999999357, "ollama_total_duration_ms": 748.1382, "ollama_load_ms": 108.6787, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 604.1526, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 122.48561042359165}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545325.7431035, "prompt_tokens": 38, "prefill_ms": 11.8802, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.4662, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 551.9216999998662, "ollama_total_duration_ms": 535.7767, "ollama_load_ms": 107.2842, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.8802, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3198.5993501792896}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545325.7431035, "prompt_tokens": 38, "prefill_ms": 11.8802, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.4662, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 551.9216999998662, "ollama_total_duration_ms": 535.7767, "ollama_load_ms": 107.2842, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 387.4662, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 90.33045979236381}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545325.7431035, "prompt_tokens": 38, "prefill_ms": 11.8802, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.4662, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 551.9216999998662, "ollama_total_duration_ms": 535.7767, "ollama_load_ms": 107.2842, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 399.3464, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 182.79869306446736}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545326.295145, "prompt_tokens": 38, "prefill_ms": 12.2179, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.6734, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 561.99730000003, "ollama_total_duration_ms": 541.5462, "ollama_load_ms": 115.3136, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.2179, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3110.190785650562}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545326.295145, "prompt_tokens": 38, "prefill_ms": 12.2179, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.6734, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 561.99730000003, "ollama_total_duration_ms": 541.5462, "ollama_load_ms": 115.3136, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 388.6734, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 90.04989793487283}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545326.295145, "prompt_tokens": 38, "prefill_ms": 12.2179, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.6734, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 561.99730000003, "ollama_total_duration_ms": 541.5462, "ollama_load_ms": 115.3136, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 400.8913, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 182.09424849080037}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545326.857286, "prompt_tokens": 38, "prefill_ms": 12.3716, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 535.831799999869, "ollama_total_duration_ms": 523.4342, "ollama_load_ms": 101.1346, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.3716, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3071.5509715800704}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545326.857286, "prompt_tokens": 38, "prefill_ms": 12.3716, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 535.831799999869, "ollama_total_duration_ms": 523.4342, "ollama_load_ms": 101.1346, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 386.3584, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 90.58946304778155}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545326.857286, "prompt_tokens": 38, "prefill_ms": 12.3716, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 535.831799999869, "ollama_total_duration_ms": 523.4342, "ollama_load_ms": 101.1346, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 398.73, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 183.08128307375918}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545327.393233, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 69.50389999997242, "prefill_cuda_event_ms": 69.39100646972656, "kv_decode_ms": 456.8479000001844, "kv_decode_cuda_event_ms": 456.81048583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 69.50389999997242, "cuda_event_ms": 69.39100646972656, "tokens_total": 9, "tokens_per_s": 129.48913658087633}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545327.393233, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 69.50389999997242, "prefill_cuda_event_ms": 69.39100646972656, "kv_decode_ms": 456.8479000001844, "kv_decode_cuda_event_ms": 456.81048583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 456.8479000001844, "cuda_event_ms": 456.81048583984375, "tokens_total": 64, "tokens_per_s": 140.09038894558597}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545327.393233, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 69.50389999997242, "prefill_cuda_event_ms": 69.39100646972656, "kv_decode_ms": 456.8479000001844, "kv_decode_cuda_event_ms": 456.81048583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 526.3518000001568, "cuda_event_ms": 526.2014923095703, "tokens_total": 73, "tokens_per_s": 138.69051079521006}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545327.920446, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.488400000127513, "prefill_cuda_event_ms": 4.451327800750732, "kv_decode_ms": 234.91360000002715, "kv_decode_cuda_event_ms": 234.8574676513672, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 4.488400000127513, "cuda_event_ms": 4.451327800750732, "tokens_total": 9, "tokens_per_s": 2005.1688797220202}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545327.920446, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.488400000127513, "prefill_cuda_event_ms": 4.451327800750732, "kv_decode_ms": 234.91360000002715, "kv_decode_cuda_event_ms": 234.8574676513672, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 234.91360000002715, "cuda_event_ms": 234.8574676513672, "tokens_total": 64, "tokens_per_s": 272.4405909236102}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545327.920446, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.488400000127513, "prefill_cuda_event_ms": 4.451327800750732, "kv_decode_ms": 234.91360000002715, "kv_decode_cuda_event_ms": 234.8574676513672, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 239.40200000015466, "cuda_event_ms": 239.30879545211792, "tokens_total": 73, "tokens_per_s": 304.9264417170819}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545328.1604517, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.440499999873282, "prefill_cuda_event_ms": 4.396031856536865, "kv_decode_ms": 260.8656999998402, "kv_decode_cuda_event_ms": 260.8431701660156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 4.440499999873282, "cuda_event_ms": 4.396031856536865, "tokens_total": 9, "tokens_per_s": 2026.7987839785683}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545328.1604517, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.440499999873282, "prefill_cuda_event_ms": 4.396031856536865, "kv_decode_ms": 260.8656999998402, "kv_decode_cuda_event_ms": 260.8431701660156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 260.8656999998402, "cuda_event_ms": 260.8431701660156, "tokens_total": 64, "tokens_per_s": 245.33696840956554}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545328.1604517, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.440499999873282, "prefill_cuda_event_ms": 4.396031856536865, "kv_decode_ms": 260.8656999998402, "kv_decode_cuda_event_ms": 260.8431701660156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 265.3061999997135, "cuda_event_ms": 265.2392020225525, "tokens_total": 73, "tokens_per_s": 275.153765724581}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545328.4263735, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.939299999956347, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 260.15879999999925, "kv_decode_cuda_event_ms": 260.136962890625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 3.939299999956347, "cuda_event_ms": 3.9075839519500732, "tokens_total": 9, "tokens_per_s": 2284.669865229795}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545328.4263735, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.939299999956347, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 260.15879999999925, "kv_decode_cuda_event_ms": 260.136962890625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 260.15879999999925, "cuda_event_ms": 260.136962890625, "tokens_total": 64, "tokens_per_s": 246.00359472752868}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545328.4263735, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.939299999956347, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 260.15879999999925, "kv_decode_cuda_event_ms": 260.136962890625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 264.0980999999556, "cuda_event_ms": 264.0445468425751, "tokens_total": 73, "tokens_per_s": 276.4124391656444}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545328.6910045, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.7408000000832544, "prefill_cuda_event_ms": 2.6922879219055176, "kv_decode_ms": 170.21289999979672, "kv_decode_cuda_event_ms": 170.17625427246094, "gpu_peak_mb": 539.3515625, "hf_load_ms": 263.9495999999326, "params_millions_measured": 25.016064, "latency_ms": 2.7408000000832544, "cuda_event_ms": 2.6922879219055176, "tokens_total": 17, "tokens_per_s": 6202.568592923091}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545328.6910045, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7408000000832544, "prefill_cuda_event_ms": 2.6922879219055176, "kv_decode_ms": 170.21289999979672, "kv_decode_cuda_event_ms": 170.17625427246094, "gpu_peak_mb": 539.3515625, "hf_load_ms": 263.9495999999326, "params_millions_measured": 25.016064, "latency_ms": 170.21289999979672, "cuda_event_ms": 170.17625427246094, "tokens_total": 64, "tokens_per_s": 375.9997039006822}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545328.6910045, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7408000000832544, "prefill_cuda_event_ms": 2.6922879219055176, "kv_decode_ms": 170.21289999979672, "kv_decode_cuda_event_ms": 170.17625427246094, "gpu_peak_mb": 539.3515625, "hf_load_ms": 263.9495999999326, "params_millions_measured": 25.016064, "latency_ms": 172.95369999987997, "cuda_event_ms": 172.86854219436646, "tokens_total": 81, "tokens_per_s": 468.3334325895093}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545329.128564, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.701999999952022, "prefill_cuda_event_ms": 4.624159812927246, "kv_decode_ms": 199.25790000002053, "kv_decode_cuda_event_ms": 199.23455810546875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 4.701999999952022, "cuda_event_ms": 4.624159812927246, "tokens_total": 17, "tokens_per_s": 3615.482773324854}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545329.128564, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.701999999952022, "prefill_cuda_event_ms": 4.624159812927246, "kv_decode_ms": 199.25790000002053, "kv_decode_cuda_event_ms": 199.23455810546875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 199.25790000002053, "cuda_event_ms": 199.23455810546875, "tokens_total": 64, "tokens_per_s": 321.19178210747685}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545329.128564, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.701999999952022, "prefill_cuda_event_ms": 4.624159812927246, "kv_decode_ms": 199.25790000002053, "kv_decode_cuda_event_ms": 199.23455810546875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 203.95989999997255, "cuda_event_ms": 203.858717918396, "tokens_total": 81, "tokens_per_s": 397.1368881824854}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545329.3330958, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.1317000000635744, "prefill_cuda_event_ms": 2.100032091140747, "kv_decode_ms": 190.65099999988888, "kv_decode_cuda_event_ms": 190.5848388671875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 2.1317000000635744, "cuda_event_ms": 2.100032091140747, "tokens_total": 17, "tokens_per_s": 7974.855748694939}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545329.3330958, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.1317000000635744, "prefill_cuda_event_ms": 2.100032091140747, "kv_decode_ms": 190.65099999988888, "kv_decode_cuda_event_ms": 190.5848388671875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 190.65099999988888, "cuda_event_ms": 190.5848388671875, "tokens_total": 64, "tokens_per_s": 335.69191874177056}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545329.3330958, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.1317000000635744, "prefill_cuda_event_ms": 2.100032091140747, "kv_decode_ms": 190.65099999988888, "kv_decode_cuda_event_ms": 190.5848388671875, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 192.78269999995246, "cuda_event_ms": 192.68487095832825, "tokens_total": 81, "tokens_per_s": 420.1621826025882}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545329.5267136, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.946799999994255, "prefill_cuda_event_ms": 3.866624116897583, "kv_decode_ms": 207.2089999999207, "kv_decode_cuda_event_ms": 207.0108184814453, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 3.946799999994255, "cuda_event_ms": 3.866624116897583, "tokens_total": 17, "tokens_per_s": 4307.286915988838}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545329.5267136, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.946799999994255, "prefill_cuda_event_ms": 3.866624116897583, "kv_decode_ms": 207.2089999999207, "kv_decode_cuda_event_ms": 207.0108184814453, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 207.2089999999207, "cuda_event_ms": 207.0108184814453, "tokens_total": 64, "tokens_per_s": 308.8668928474366}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545329.5267136, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.946799999994255, "prefill_cuda_event_ms": 3.866624116897583, "kv_decode_ms": 207.2089999999207, "kv_decode_cuda_event_ms": 207.0108184814453, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 211.15579999991496, "cuda_event_ms": 210.8774425983429, "tokens_total": 81, "tokens_per_s": 383.603007826603}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545329.738769, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.7506999999550317, "prefill_cuda_event_ms": 3.699712038040161, "kv_decode_ms": 190.9550999998828, "kv_decode_cuda_event_ms": 190.89013671875, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.7506999999550317, "cuda_event_ms": 3.699712038040161, "tokens_total": 1, "tokens_per_s": 266.61689818220316}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545329.738769, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7506999999550317, "prefill_cuda_event_ms": 3.699712038040161, "kv_decode_ms": 190.9550999998828, "kv_decode_cuda_event_ms": 190.89013671875, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 190.9550999998828, "cuda_event_ms": 190.89013671875, "tokens_total": 64, "tokens_per_s": 335.157322323621}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545329.738769, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7506999999550317, "prefill_cuda_event_ms": 3.699712038040161, "kv_decode_ms": 190.9550999998828, "kv_decode_cuda_event_ms": 190.89013671875, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 194.70579999983784, "cuda_event_ms": 194.58984875679016, "tokens_total": 65, "tokens_per_s": 333.8369992062596}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545329.93407, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.9157999999588355, "prefill_cuda_event_ms": 3.853408098220825, "kv_decode_ms": 218.79500000000007, "kv_decode_cuda_event_ms": 218.7509765625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.9157999999588355, "cuda_event_ms": 3.853408098220825, "tokens_total": 1, "tokens_per_s": 255.3756575950029}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545329.93407, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.9157999999588355, "prefill_cuda_event_ms": 3.853408098220825, "kv_decode_ms": 218.79500000000007, "kv_decode_cuda_event_ms": 218.7509765625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 218.79500000000007, "cuda_event_ms": 218.7509765625, "tokens_total": 64, "tokens_per_s": 292.5112548275782}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545329.93407, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.9157999999588355, "prefill_cuda_event_ms": 3.853408098220825, "kv_decode_ms": 218.79500000000007, "kv_decode_cuda_event_ms": 218.7509765625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 222.7107999999589, "cuda_event_ms": 222.60438466072083, "tokens_total": 65, "tokens_per_s": 291.85832029704886}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545330.157248, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.8082999999933236, "prefill_cuda_event_ms": 1.7756160497665405, "kv_decode_ms": 213.19489999996222, "kv_decode_cuda_event_ms": 213.14968872070312, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 1.8082999999933236, "cuda_event_ms": 1.7756160497665405, "tokens_total": 1, "tokens_per_s": 553.0055853584538}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545330.157248, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.8082999999933236, "prefill_cuda_event_ms": 1.7756160497665405, "kv_decode_ms": 213.19489999996222, "kv_decode_cuda_event_ms": 213.14968872070312, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 213.19489999996222, "cuda_event_ms": 213.14968872070312, "tokens_total": 64, "tokens_per_s": 300.1947982808751}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545330.157248, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.8082999999933236, "prefill_cuda_event_ms": 1.7756160497665405, "kv_decode_ms": 213.19489999996222, "kv_decode_cuda_event_ms": 213.14968872070312, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 215.00319999995554, "cuda_event_ms": 214.92530477046967, "tokens_total": 65, "tokens_per_s": 302.32108173279954}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545330.3728125, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.7615000001096632, "prefill_cuda_event_ms": 3.708928108215332, "kv_decode_ms": 176.68670000011844, "kv_decode_cuda_event_ms": 176.6440887451172, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.7615000001096632, "cuda_event_ms": 3.708928108215332, "tokens_total": 1, "tokens_per_s": 265.85138906575725}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545330.3728125, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7615000001096632, "prefill_cuda_event_ms": 3.708928108215332, "kv_decode_ms": 176.68670000011844, "kv_decode_cuda_event_ms": 176.6440887451172, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 176.68670000011844, "cuda_event_ms": 176.6440887451172, "tokens_total": 64, "tokens_per_s": 362.2230762131904}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545330.3728125, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7615000001096632, "prefill_cuda_event_ms": 3.708928108215332, "kv_decode_ms": 176.68670000011844, "kv_decode_cuda_event_ms": 176.6440887451172, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 180.4482000002281, "cuda_event_ms": 180.35301685333252, "tokens_total": 65, "tokens_per_s": 360.21417780791296}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545330.5537758, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.149200000001656, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.126200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 124.22310000010839, "params_millions_measured": 25.016064, "latency_ms": 34.149200000001656, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 263.54936572451373}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545330.5537758, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.149200000001656, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.126200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 124.22310000010839, "params_millions_measured": 25.016064, "latency_ms": 643.126200000097, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.51390566888792}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545330.5537758, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.149200000001656, "prefill_cuda_event_ms": null, "kv_decode_ms": 643.126200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 124.22310000010839, "params_millions_measured": 25.016064, "latency_ms": 677.2754000000987, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 107.78480954717884}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545331.3556905, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.391799999908471, "prefill_cuda_event_ms": null, "kv_decode_ms": 699.2640999999367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.391799999908471, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 672.0530473918004}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545331.3556905, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.391799999908471, "prefill_cuda_event_ms": null, "kv_decode_ms": 699.2640999999367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 699.2640999999367, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 91.52479013294948}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545331.3556905, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.391799999908471, "prefill_cuda_event_ms": null, "kv_decode_ms": 699.2640999999367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 712.6558999998451, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 102.43372713256967}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545332.0691836, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.71780000012768, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.319900000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.71780000012768, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 507.96374267319544}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545332.0691836, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.71780000012768, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.319900000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 711.319900000035, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.97358291255011}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545332.0691836, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.71780000012768, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.319900000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 729.0377000001627, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 100.13199591733557}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545332.79862, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.785399999984293, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.1927000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.785399999984293, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 703.927917782084}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545332.79862, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.785399999984293, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.1927000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 711.1927000000833, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.98967509086145}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545332.79862, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.785399999984293, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.1927000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 723.9781000000676, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 100.83177930381207}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545333.5231392, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 701.614999999947, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.1579999998885, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 371.69010000002345, "params_millions_measured": 96.08832, "latency_ms": 701.614999999947, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 24.229812646538747}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545333.5231392, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 701.614999999947, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.1579999998885, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 371.69010000002345, "params_millions_measured": 96.08832, "latency_ms": 1834.1579999998885, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 34.89339522549523}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545333.5231392, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 701.614999999947, "prefill_cuda_event_ms": null, "kv_decode_ms": 1834.1579999998885, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 371.69010000002345, "params_millions_measured": 96.08832, "latency_ms": 2535.7729999998355, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 31.942922335715878}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545336.431769, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 30.708699999877354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1492.401799999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 30.708699999877354, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 553.5890480569967}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545336.431769, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.708699999877354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1492.401799999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1492.401799999925, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 42.88389360023769}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545336.431769, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.708699999877354, "prefill_cuda_event_ms": null, "kv_decode_ms": 1492.401799999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1523.1104999998024, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 53.180645790315616}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545337.955335, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 34.03510000021015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1398.3111999998528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.03510000021015, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 499.4843558530762}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545337.955335, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.03510000021015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1398.3111999998528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1398.3111999998528, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 45.76949680443577}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545337.955335, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.03510000021015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1398.3111999998528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1432.346300000063, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 56.55057020777478}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545339.3881423, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 30.690099999901577, "prefill_cuda_event_ms": null, "kv_decode_ms": 1302.7455000001282, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 30.690099999901577, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 553.9245554773207}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545339.3881423, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.690099999901577, "prefill_cuda_event_ms": null, "kv_decode_ms": 1302.7455000001282, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1302.7455000001282, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.127016750388854}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545339.3881423, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.690099999901577, "prefill_cuda_event_ms": null, "kv_decode_ms": 1302.7455000001282, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1333.4356000000298, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 60.7453408323568}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545340.7222335, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.162200000006123, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.0951999999652, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 18.162200000006123, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 55.05940910240295}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545340.7222335, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.162200000006123, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.0951999999652, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 724.0951999999652, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.38616800664205}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545340.7222335, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.162200000006123, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.0951999999652, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 742.2573999999713, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 87.57069986773121}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545341.4649332, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.116899999980888, "prefill_cuda_event_ms": null, "kv_decode_ms": 680.6209999999737, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 9.116899999980888, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 109.68640656386451}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545341.4649332, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.116899999980888, "prefill_cuda_event_ms": null, "kv_decode_ms": 680.6209999999737, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 680.6209999999737, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.03177392411118}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545341.4649332, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.116899999980888, "prefill_cuda_event_ms": null, "kv_decode_ms": 680.6209999999737, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 689.7378999999546, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 94.23869559727584}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545342.1550226, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.133700000096724, "prefill_cuda_event_ms": null, "kv_decode_ms": 684.8895999999058, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 10.133700000096724, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 98.68063984432688}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545342.1550226, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.133700000096724, "prefill_cuda_event_ms": null, "kv_decode_ms": 684.8895999999058, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 684.8895999999058, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 93.44571738278519}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545342.1550226, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.133700000096724, "prefill_cuda_event_ms": null, "kv_decode_ms": 684.8895999999058, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 695.0233000000026, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 93.52204451275196}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545342.850445, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.764600000129576, "prefill_cuda_event_ms": null, "kv_decode_ms": 694.9850000000879, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 10.764600000129576, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 92.89708860412489}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545342.850445, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.764600000129576, "prefill_cuda_event_ms": null, "kv_decode_ms": 694.9850000000879, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 694.9850000000879, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 92.0883184529046}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545342.850445, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.764600000129576, "prefill_cuda_event_ms": null, "kv_decode_ms": 694.9850000000879, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 705.7496000002175, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 92.1006543963751}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545343.5566916, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.922800000013012, "prefill_cuda_event_ms": null, "kv_decode_ms": 1375.2246999999898, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 24.922800000013012, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 361.1151235011837}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545343.5566916, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.922800000013012, "prefill_cuda_event_ms": null, "kv_decode_ms": 1375.2246999999898, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1375.2246999999898, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.53784941471781}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545343.5566916, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.922800000013012, "prefill_cuda_event_ms": null, "kv_decode_ms": 1375.2246999999898, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1400.1475000000028, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.137364099139454}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545344.9573429, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 32.07050000014533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1612.1146000000408, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 32.07050000014533, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 280.6317332114939}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545344.9573429, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 32.07050000014533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1612.1146000000408, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1612.1146000000408, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.699410947583}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545344.9573429, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 32.07050000014533, "prefill_cuda_event_ms": null, "kv_decode_ms": 1612.1146000000408, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1644.1851000001861, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 44.39889401746296}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545346.601885, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.909700000080193, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.856399999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 24.909700000080193, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 361.3050337808575}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545346.601885, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.909700000080193, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.856399999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1250.856399999975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.164945872284996}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545346.601885, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.909700000080193, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.856399999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1275.7661000000553, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.22052028188932}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545347.8780167, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.797900000161462, "prefill_cuda_event_ms": null, "kv_decode_ms": 1252.334000000019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 20.797900000161462, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 432.7359973809918}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545347.8780167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.797900000161462, "prefill_cuda_event_ms": null, "kv_decode_ms": 1252.334000000019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1252.334000000019, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.10457753282993}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545347.8780167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.797900000161462, "prefill_cuda_event_ms": null, "kv_decode_ms": 1252.334000000019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1273.1319000001804, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.33891358781416}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545349.151774, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.15169999988575, "prefill_cuda_event_ms": null, "kv_decode_ms": 1248.5681000000568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 17.15169999988575, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.303258569509794}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545349.151774, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.15169999988575, "prefill_cuda_event_ms": null, "kv_decode_ms": 1248.5681000000568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1248.5681000000568, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.25871788651103}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545349.151774, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.15169999988575, "prefill_cuda_event_ms": null, "kv_decode_ms": 1248.5681000000568, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1265.7197999999426, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 51.35417807322201}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545350.4180713, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 19.61699999992561, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.524900000073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 19.61699999992561, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 50.97619411754051}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545350.4180713, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.61699999992561, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.524900000073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1250.524900000073, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.178509120447146}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545350.4180713, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.61699999992561, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.524900000073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1270.1418999999987, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 51.17538441964639}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545351.6886208, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.213299999843002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.02070000015, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.213299999843002, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.904932110524726}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545351.6886208, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.213299999843002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.02070000015, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1265.02070000015, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.59205750545616}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545351.6886208, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.213299999843002, "prefill_cuda_event_ms": null, "kv_decode_ms": 1265.02070000015, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1283.233999999993, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 50.653271344119894}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545352.9723911, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.711499999904845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1215.8014999999978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 17.711499999904845, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 56.46049177118666}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545352.9723911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.711499999904845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1215.8014999999978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1215.8014999999978, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.64017193596168}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545352.9723911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.711499999904845, "prefill_cuda_event_ms": null, "kv_decode_ms": 1215.8014999999978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1233.5129999999026, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 52.695026319143075}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545354.2064452, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 12.831900000037422, "prefill_cuda_event_ms": null, "kv_decode_ms": 778.2933000000867, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.831900000037422, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1324.8232919482246}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545354.2064452, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 12.831900000037422, "prefill_cuda_event_ms": null, "kv_decode_ms": 778.2933000000867, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 778.2933000000867, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.23121026481003}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545354.2064452, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 12.831900000037422, "prefill_cuda_event_ms": null, "kv_decode_ms": 778.2933000000867, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 791.1252000001241, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 102.3858170615565}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545354.998507, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 18.690900000137844, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.041000000072, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.690900000137844, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 909.5335163033683}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545354.998507, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.690900000137844, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.041000000072, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 757.041000000072, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 84.53967486568615}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545354.998507, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.690900000137844, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.041000000072, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 775.7319000002099, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 104.41751847510471}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545355.7747223, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.56479999989824, "prefill_cuda_event_ms": null, "kv_decode_ms": 770.8953000001202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 19.56479999989824, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 868.9074255851539}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545355.7747223, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.56479999989824, "prefill_cuda_event_ms": null, "kv_decode_ms": 770.8953000001202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 770.8953000001202, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 83.02035308814312}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545355.7747223, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.56479999989824, "prefill_cuda_event_ms": null, "kv_decode_ms": 770.8953000001202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 790.4601000000184, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 102.47196537813625}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545356.5658047, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 17.6613000000998, "prefill_cuda_event_ms": null, "kv_decode_ms": 801.8019999999524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 17.6613000000998, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 962.5565501918849}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545356.5658047, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.6613000000998, "prefill_cuda_event_ms": null, "kv_decode_ms": 801.8019999999524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 801.8019999999524, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.82020498826867}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545356.5658047, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.6613000000998, "prefill_cuda_event_ms": null, "kv_decode_ms": 801.8019999999524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 819.4633000000522, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 98.8451831826939}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545357.3857212, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.548700000214012, "prefill_cuda_event_ms": 4.4912638664245605, "kv_decode_ms": 175.56039999999484, "kv_decode_cuda_event_ms": 175.52178955078125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 4.548700000214012, "cuda_event_ms": 4.4912638664245605, "tokens_total": 9, "tokens_per_s": 1978.5872885827948}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545357.3857212, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.548700000214012, "prefill_cuda_event_ms": 4.4912638664245605, "kv_decode_ms": 175.56039999999484, "kv_decode_cuda_event_ms": 175.52178955078125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 175.56039999999484, "cuda_event_ms": 175.52178955078125, "tokens_total": 64, "tokens_per_s": 364.5469023766287}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545357.3857212, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.548700000214012, "prefill_cuda_event_ms": 4.4912638664245605, "kv_decode_ms": 175.56039999999484, "kv_decode_cuda_event_ms": 175.52178955078125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 180.10910000020885, "cuda_event_ms": 180.0130534172058, "tokens_total": 73, "tokens_per_s": 405.309892725661}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545357.5677161, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7273000000368484, "prefill_cuda_event_ms": 3.6823039054870605, "kv_decode_ms": 121.98770000009063, "kv_decode_cuda_event_ms": 121.93718719482422, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 3.7273000000368484, "cuda_event_ms": 3.6823039054870605, "tokens_total": 9, "tokens_per_s": 2414.6164783921404}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545357.5677161, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7273000000368484, "prefill_cuda_event_ms": 3.6823039054870605, "kv_decode_ms": 121.98770000009063, "kv_decode_cuda_event_ms": 121.93718719482422, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 121.98770000009063, "cuda_event_ms": 121.93718719482422, "tokens_total": 64, "tokens_per_s": 524.6430582751577}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545357.5677161, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7273000000368484, "prefill_cuda_event_ms": 3.6823039054870605, "kv_decode_ms": 121.98770000009063, "kv_decode_cuda_event_ms": 121.93718719482422, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 125.71500000012747, "cuda_event_ms": 125.61949110031128, "tokens_total": 73, "tokens_per_s": 580.6785188714631}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545357.6945946, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.1719000001212407, "prefill_cuda_event_ms": 2.1267199516296387, "kv_decode_ms": 116.52660000004289, "kv_decode_cuda_event_ms": 116.48204803466797, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 2.1719000001212407, "cuda_event_ms": 2.1267199516296387, "tokens_total": 9, "tokens_per_s": 4143.837193009622}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545357.6945946, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.1719000001212407, "prefill_cuda_event_ms": 2.1267199516296387, "kv_decode_ms": 116.52660000004289, "kv_decode_cuda_event_ms": 116.48204803466797, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 116.52660000004289, "cuda_event_ms": 116.48204803466797, "tokens_total": 64, "tokens_per_s": 549.23081940069}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545357.6945946, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.1719000001212407, "prefill_cuda_event_ms": 2.1267199516296387, "kv_decode_ms": 116.52660000004289, "kv_decode_cuda_event_ms": 116.48204803466797, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 118.69850000016413, "cuda_event_ms": 118.60876798629761, "tokens_total": 73, "tokens_per_s": 615.0035594375587}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545357.8138094, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.5481000000127096, "prefill_cuda_event_ms": 2.4863359928131104, "kv_decode_ms": 117.40120000013121, "kv_decode_cuda_event_ms": 117.3565444946289, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 2.5481000000127096, "cuda_event_ms": 2.4863359928131104, "tokens_total": 9, "tokens_per_s": 3532.0434833621557}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545357.8138094, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.5481000000127096, "prefill_cuda_event_ms": 2.4863359928131104, "kv_decode_ms": 117.40120000013121, "kv_decode_cuda_event_ms": 117.3565444946289, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 117.40120000013121, "cuda_event_ms": 117.3565444946289, "tokens_total": 64, "tokens_per_s": 545.1392319663553}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545357.8138094, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.5481000000127096, "prefill_cuda_event_ms": 2.4863359928131104, "kv_decode_ms": 117.40120000013121, "kv_decode_cuda_event_ms": 117.3565444946289, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 119.94930000014392, "cuda_event_ms": 119.84288048744202, "tokens_total": 73, "tokens_per_s": 608.5904628031377}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545357.9342763, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.918799999923067, "prefill_cuda_event_ms": 9.873408317565918, "kv_decode_ms": 418.0830000000242, "kv_decode_cuda_event_ms": 418.0439758300781, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 9.918799999923067, "cuda_event_ms": 9.873408317565918, "tokens_total": 17, "tokens_per_s": 1713.9170061027398}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545357.9342763, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.918799999923067, "prefill_cuda_event_ms": 9.873408317565918, "kv_decode_ms": 418.0830000000242, "kv_decode_cuda_event_ms": 418.0439758300781, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 418.0830000000242, "cuda_event_ms": 418.0439758300781, "tokens_total": 64, "tokens_per_s": 153.07965164810886}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545357.9342763, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.918799999923067, "prefill_cuda_event_ms": 9.873408317565918, "kv_decode_ms": 418.0830000000242, "kv_decode_cuda_event_ms": 418.0439758300781, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 428.00179999994725, "cuda_event_ms": 427.91738414764404, "tokens_total": 81, "tokens_per_s": 189.25154053092763}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545358.362912, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.126599999992322, "prefill_cuda_event_ms": 7.070720195770264, "kv_decode_ms": 392.55199999979595, "kv_decode_cuda_event_ms": 392.5104675292969, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 7.126599999992322, "cuda_event_ms": 7.070720195770264, "tokens_total": 17, "tokens_per_s": 2385.4292369458526}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545358.362912, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.126599999992322, "prefill_cuda_event_ms": 7.070720195770264, "kv_decode_ms": 392.55199999979595, "kv_decode_cuda_event_ms": 392.5104675292969, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 392.55199999979595, "cuda_event_ms": 392.5104675292969, "tokens_total": 64, "tokens_per_s": 163.0357252033699}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545358.362912, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.126599999992322, "prefill_cuda_event_ms": 7.070720195770264, "kv_decode_ms": 392.55199999979595, "kv_decode_cuda_event_ms": 392.5104675292969, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 399.6785999997883, "cuda_event_ms": 399.58118772506714, "tokens_total": 81, "tokens_per_s": 202.6628395917192}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545358.763177, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.233399999928224, "prefill_cuda_event_ms": 6.18393611907959, "kv_decode_ms": 411.0250000001088, "kv_decode_cuda_event_ms": 410.98162841796875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 6.233399999928224, "cuda_event_ms": 6.18393611907959, "tokens_total": 17, "tokens_per_s": 2727.2435589238216}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545358.763177, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.233399999928224, "prefill_cuda_event_ms": 6.18393611907959, "kv_decode_ms": 411.0250000001088, "kv_decode_cuda_event_ms": 410.98162841796875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 411.0250000001088, "cuda_event_ms": 410.98162841796875, "tokens_total": 64, "tokens_per_s": 155.7082902499436}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545358.763177, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.233399999928224, "prefill_cuda_event_ms": 6.18393611907959, "kv_decode_ms": 411.0250000001088, "kv_decode_cuda_event_ms": 410.98162841796875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 417.258400000037, "cuda_event_ms": 417.16556453704834, "tokens_total": 81, "tokens_per_s": 194.1243124164614}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545359.1810336, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.852799999862327, "prefill_cuda_event_ms": 9.794783592224121, "kv_decode_ms": 429.83970000000227, "kv_decode_cuda_event_ms": 429.8045349121094, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 9.852799999862327, "cuda_event_ms": 9.794783592224121, "tokens_total": 17, "tokens_per_s": 1725.3978564710073}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545359.1810336, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.852799999862327, "prefill_cuda_event_ms": 9.794783592224121, "kv_decode_ms": 429.83970000000227, "kv_decode_cuda_event_ms": 429.8045349121094, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 429.83970000000227, "cuda_event_ms": 429.8045349121094, "tokens_total": 64, "tokens_per_s": 148.89271512147357}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545359.1810336, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.852799999862327, "prefill_cuda_event_ms": 9.794783592224121, "kv_decode_ms": 429.83970000000227, "kv_decode_cuda_event_ms": 429.8045349121094, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 439.6924999998646, "cuda_event_ms": 439.5993185043335, "tokens_total": 81, "tokens_per_s": 184.21965350790597}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545359.6212723, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.141100000173537, "prefill_cuda_event_ms": 5.094399929046631, "kv_decode_ms": 198.4356000000389, "kv_decode_cuda_event_ms": 198.3989715576172, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 211.56989999985854, "params_millions_measured": 45.1712, "latency_ms": 5.141100000173537, "cuda_event_ms": 5.094399929046631, "tokens_total": 9, "tokens_per_s": 1750.5981209655922}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545359.6212723, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.141100000173537, "prefill_cuda_event_ms": 5.094399929046631, "kv_decode_ms": 198.4356000000389, "kv_decode_cuda_event_ms": 198.3989715576172, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 211.56989999985854, "params_millions_measured": 45.1712, "latency_ms": 198.4356000000389, "cuda_event_ms": 198.3989715576172, "tokens_total": 64, "tokens_per_s": 322.52277313137085}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545359.6212723, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.141100000173537, "prefill_cuda_event_ms": 5.094399929046631, "kv_decode_ms": 198.4356000000389, "kv_decode_cuda_event_ms": 198.3989715576172, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 211.56989999985854, "params_millions_measured": 45.1712, "latency_ms": 203.57670000021244, "cuda_event_ms": 203.49337148666382, "tokens_total": 73, "tokens_per_s": 358.5872057063693}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545360.0370972, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.876299999978073, "prefill_cuda_event_ms": 4.828159809112549, "kv_decode_ms": 199.82920000006743, "kv_decode_cuda_event_ms": 199.7557830810547, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 4.876299999978073, "cuda_event_ms": 4.828159809112549, "tokens_total": 9, "tokens_per_s": 1845.6616697168897}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545360.0370972, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.876299999978073, "prefill_cuda_event_ms": 4.828159809112549, "kv_decode_ms": 199.82920000006743, "kv_decode_cuda_event_ms": 199.7557830810547, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 199.82920000006743, "cuda_event_ms": 199.7557830810547, "tokens_total": 64, "tokens_per_s": 320.27351358048975}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545360.0370972, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.876299999978073, "prefill_cuda_event_ms": 4.828159809112549, "kv_decode_ms": 199.82920000006743, "kv_decode_cuda_event_ms": 199.7557830810547, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 204.7055000000455, "cuda_event_ms": 204.58394289016724, "tokens_total": 73, "tokens_per_s": 356.6098614838574}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545360.2424335, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7320000001273, "prefill_cuda_event_ms": 3.688447952270508, "kv_decode_ms": 231.24359999997068, "kv_decode_cuda_event_ms": 231.21597290039062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.7320000001273, "cuda_event_ms": 3.688447952270508, "tokens_total": 9, "tokens_per_s": 2411.5755626187047}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545360.2424335, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7320000001273, "prefill_cuda_event_ms": 3.688447952270508, "kv_decode_ms": 231.24359999997068, "kv_decode_cuda_event_ms": 231.21597290039062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 231.24359999997068, "cuda_event_ms": 231.21597290039062, "tokens_total": 64, "tokens_per_s": 276.76441639901867}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545360.2424335, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7320000001273, "prefill_cuda_event_ms": 3.688447952270508, "kv_decode_ms": 231.24359999997068, "kv_decode_cuda_event_ms": 231.21597290039062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 234.97560000009798, "cuda_event_ms": 234.90442085266113, "tokens_total": 73, "tokens_per_s": 310.6705547298084}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545360.47808, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.630000000133805, "prefill_cuda_event_ms": 4.580287933349609, "kv_decode_ms": 217.4944000000778, "kv_decode_cuda_event_ms": 217.46893310546875, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 4.630000000133805, "cuda_event_ms": 4.580287933349609, "tokens_total": 9, "tokens_per_s": 1943.8444923844286}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545360.47808, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.630000000133805, "prefill_cuda_event_ms": 4.580287933349609, "kv_decode_ms": 217.4944000000778, "kv_decode_cuda_event_ms": 217.46893310546875, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 217.4944000000778, "cuda_event_ms": 217.46893310546875, "tokens_total": 64, "tokens_per_s": 294.26044992412267}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545360.47808, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.630000000133805, "prefill_cuda_event_ms": 4.580287933349609, "kv_decode_ms": 217.4944000000778, "kv_decode_cuda_event_ms": 217.46893310546875, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 222.1244000002116, "cuda_event_ms": 222.04922103881836, "tokens_total": 73, "tokens_per_s": 328.6446693831495}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545360.700932, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.72159999993528, "prefill_cuda_event_ms": null, "kv_decode_ms": 894.5513000001029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.72159999993528, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 53.41423809949241}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545360.700932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.72159999993528, "prefill_cuda_event_ms": null, "kv_decode_ms": 894.5513000001029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 894.5513000001029, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.54424793747731}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545360.700932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.72159999993528, "prefill_cuda_event_ms": null, "kv_decode_ms": 894.5513000001029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 913.2729000000381, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 71.17259255146767}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545361.6145477, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.26429999994616, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.0972999999904, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.26429999994616, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.39033345175086}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545361.6145477, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.26429999994616, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.0972999999904, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 928.0972999999904, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.95828702443231}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545361.6145477, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.26429999994616, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.0972999999904, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 941.3615999999365, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 69.04891807781875}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545362.5563533, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.399200000094424, "prefill_cuda_event_ms": null, "kv_decode_ms": 945.6010000001243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.399200000094424, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 74.63132127238589}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545362.5563533, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.399200000094424, "prefill_cuda_event_ms": null, "kv_decode_ms": 945.6010000001243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 945.6010000001243, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.68182351752122}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545362.5563533, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.399200000094424, "prefill_cuda_event_ms": null, "kv_decode_ms": 945.6010000001243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 959.0002000002187, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 67.77892225672652}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545363.5157795, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.026300000068659, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.5062000000798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.026300000068659, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 71.29463935571783}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545363.5157795, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.026300000068659, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.5062000000798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 882.5062000000798, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.52073696478757}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545363.5157795, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.026300000068659, "prefill_cuda_event_ms": null, "kv_decode_ms": 882.5062000000798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 896.5325000001485, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 72.5015546006299}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545364.4126523, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 30.311600000004546, "prefill_cuda_event_ms": null, "kv_decode_ms": 796.6493999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 30.311600000004546, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 560.8413940536775}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545364.4126523, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.311600000004546, "prefill_cuda_event_ms": null, "kv_decode_ms": 796.6493999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 796.6493999999784, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 80.33646921720111}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545364.4126523, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.311600000004546, "prefill_cuda_event_ms": null, "kv_decode_ms": 796.6493999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 826.960999999983, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 97.94899638556312}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545365.240107, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.54080000011527, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.674300000077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 22.54080000011527, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 754.1879613817197}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545365.240107, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.54080000011527, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.674300000077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 781.674300000077, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 81.87553307047922}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545365.240107, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.54080000011527, "prefill_cuda_event_ms": null, "kv_decode_ms": 781.674300000077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 804.2151000001923, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 100.71932247974533}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545366.044803, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.6465000001117, "prefill_cuda_event_ms": null, "kv_decode_ms": 794.1021999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.6465000001117, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 823.3841086822478}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545366.044803, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.6465000001117, "prefill_cuda_event_ms": null, "kv_decode_ms": 794.1021999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 794.1021999999975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 80.59416029825908}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545366.044803, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.6465000001117, "prefill_cuda_event_ms": null, "kv_decode_ms": 794.1021999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 814.7487000001092, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 99.41715770763321}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545366.860135, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.70230000003903, "prefill_cuda_event_ms": null, "kv_decode_ms": 785.8134000000518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.70230000003903, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 821.164798112671}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545366.860135, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.70230000003903, "prefill_cuda_event_ms": null, "kv_decode_ms": 785.8134000000518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 785.8134000000518, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 81.4442716298752}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545366.860135, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.70230000003903, "prefill_cuda_event_ms": null, "kv_decode_ms": 785.8134000000518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 806.5157000000909, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 100.43201886831325}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545367.6672025, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 94.5252999999866, "prefill_cuda_event_ms": null, "kv_decode_ms": 1080.2649999998266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 147.79590000011922, "params_millions_measured": 51.475968, "latency_ms": 94.5252999999866, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 95.21260445617497}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545367.6672025, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 94.5252999999866, "prefill_cuda_event_ms": null, "kv_decode_ms": 1080.2649999998266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 147.79590000011922, "params_millions_measured": 51.475968, "latency_ms": 1080.2649999998266, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.24472235980085}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545367.6672025, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 94.5252999999866, "prefill_cuda_event_ms": null, "kv_decode_ms": 1080.2649999998266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 147.79590000011922, "params_millions_measured": 51.475968, "latency_ms": 1174.7902999998132, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 62.138749358086805}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545368.9921227, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.226700000052006, "prefill_cuda_event_ms": null, "kv_decode_ms": 1017.3142999999527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 26.226700000052006, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 343.1617397530819}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545368.9921227, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.226700000052006, "prefill_cuda_event_ms": null, "kv_decode_ms": 1017.3142999999527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1017.3142999999527, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.91074449656608}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545368.9921227, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.226700000052006, "prefill_cuda_event_ms": null, "kv_decode_ms": 1017.3142999999527, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1043.5410000000047, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 69.95412734142661}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545370.036042, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.063000000140164, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.1554000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 21.063000000140164, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 427.28955988890993}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545370.036042, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.063000000140164, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.1554000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 902.1554000000833, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.94121478405393}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545370.036042, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.063000000140164, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.1554000000833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 923.2184000002235, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 79.0712143518612}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545370.9600897, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 15.443100000084087, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.0465000000513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 15.443100000084087, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 582.7845445507052}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545370.9600897, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 15.443100000084087, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.0465000000513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 827.0465000000513, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.38379885531954}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545370.9600897, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 15.443100000084087, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.0465000000513, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 842.4896000001354, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 86.64795387383805}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545371.8030696, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 48.69920000010097, "prefill_cuda_event_ms": 48.63385772705078, "kv_decode_ms": 426.28800000011324, "kv_decode_cuda_event_ms": 426.2471618652344, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 48.69920000010097, "cuda_event_ms": 48.63385772705078, "tokens_total": 17, "tokens_per_s": 349.0817097604222}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545371.8030696, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 48.69920000010097, "prefill_cuda_event_ms": 48.63385772705078, "kv_decode_ms": 426.28800000011324, "kv_decode_cuda_event_ms": 426.2471618652344, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 426.28800000011324, "cuda_event_ms": 426.2471618652344, "tokens_total": 64, "tokens_per_s": 150.1332432533475}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545371.8030696, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 48.69920000010097, "prefill_cuda_event_ms": 48.63385772705078, "kv_decode_ms": 426.28800000011324, "kv_decode_cuda_event_ms": 426.2471618652344, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 474.9872000002142, "cuda_event_ms": 474.88101959228516, "tokens_total": 81, "tokens_per_s": 170.53091114868667}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545372.2797925, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.049199999803022, "prefill_cuda_event_ms": 5.01043176651001, "kv_decode_ms": 255.78020000011747, "kv_decode_cuda_event_ms": 255.7532196044922, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 5.049199999803022, "cuda_event_ms": 5.01043176651001, "tokens_total": 17, "tokens_per_s": 3366.8699993391424}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545372.2797925, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.049199999803022, "prefill_cuda_event_ms": 5.01043176651001, "kv_decode_ms": 255.78020000011747, "kv_decode_cuda_event_ms": 255.7532196044922, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 255.78020000011747, "cuda_event_ms": 255.7532196044922, "tokens_total": 64, "tokens_per_s": 250.21483289156316}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545372.2797925, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.049199999803022, "prefill_cuda_event_ms": 5.01043176651001, "kv_decode_ms": 255.78020000011747, "kv_decode_cuda_event_ms": 255.7532196044922, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 260.8293999999205, "cuda_event_ms": 260.7636513710022, "tokens_total": 81, "tokens_per_s": 310.5478140118587}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545372.5412097, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.989300000033836, "prefill_cuda_event_ms": 4.942848205566406, "kv_decode_ms": 257.90299999994204, "kv_decode_cuda_event_ms": 257.87493896484375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.989300000033836, "cuda_event_ms": 4.942848205566406, "tokens_total": 17, "tokens_per_s": 3407.2916040095224}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545372.5412097, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.989300000033836, "prefill_cuda_event_ms": 4.942848205566406, "kv_decode_ms": 257.90299999994204, "kv_decode_cuda_event_ms": 257.87493896484375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 257.90299999994204, "cuda_event_ms": 257.87493896484375, "tokens_total": 64, "tokens_per_s": 248.15531420733524}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545372.5412097, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.989300000033836, "prefill_cuda_event_ms": 4.942848205566406, "kv_decode_ms": 257.90299999994204, "kv_decode_cuda_event_ms": 257.87493896484375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 262.8922999999759, "cuda_event_ms": 262.81778717041016, "tokens_total": 81, "tokens_per_s": 308.11096407162717}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545372.8046396, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.930800000011004, "prefill_cuda_event_ms": 4.891647815704346, "kv_decode_ms": 260.3426999999101, "kv_decode_cuda_event_ms": 260.31585693359375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.930800000011004, "cuda_event_ms": 4.891647815704346, "tokens_total": 17, "tokens_per_s": 3447.7163948977977}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545372.8046396, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.930800000011004, "prefill_cuda_event_ms": 4.891647815704346, "kv_decode_ms": 260.3426999999101, "kv_decode_cuda_event_ms": 260.31585693359375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 260.3426999999101, "cuda_event_ms": 260.31585693359375, "tokens_total": 64, "tokens_per_s": 245.82982353652358}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545372.8046396, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.930800000011004, "prefill_cuda_event_ms": 4.891647815704346, "kv_decode_ms": 260.3426999999101, "kv_decode_cuda_event_ms": 260.31585693359375, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 265.2734999999211, "cuda_event_ms": 265.2075047492981, "tokens_total": 81, "tokens_per_s": 305.34523802801294}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 360, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545373.0704293, "prompt_tokens": 84, "prefill_ms": 710.6401, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.471, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 12955.60670000009, "ollama_total_duration_ms": 12846.947, "ollama_load_ms": 10844.6386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 710.6401, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 118.20329305931371}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 361, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545373.0704293, "prompt_tokens": 84, "prefill_ms": 710.6401, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.471, "kv_decode_cuda_event_ms": null, "gen_tokens": 38, "ollama_wall_ms": 12955.60670000009, "ollama_total_duration_ms": 12846.947, "ollama_load_ms": 10844.6386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1233.471, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 30.807372041985584}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 362, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545373.0704293, "prompt_tokens": 84, "prefill_ms": 710.6401, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.471, "kv_decode_cuda_event_ms": null, "gen_tokens": 38, "ollama_wall_ms": 12955.60670000009, "ollama_total_duration_ms": 12846.947, "ollama_load_ms": 10844.6386, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1944.1111, "cuda_event_ms": null, "tokens_total": 122, "tokens_per_s": 62.75361526406593}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 363, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545386.0266554, "prompt_tokens": 84, "prefill_ms": 41.7404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1286.4886, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1597.2446999999192, "ollama_total_duration_ms": 1571.146, "ollama_load_ms": 225.9729, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 41.7404, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 2012.4387883201885}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 364, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545386.0266554, "prompt_tokens": 84, "prefill_ms": 41.7404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1286.4886, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1597.2446999999192, "ollama_total_duration_ms": 1571.146, "ollama_load_ms": 225.9729, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1286.4886, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 26.42852801027541}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 365, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545386.0266554, "prompt_tokens": 84, "prefill_ms": 41.7404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1286.4886, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1597.2446999999192, "ollama_total_duration_ms": 1571.146, "ollama_load_ms": 225.9729, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1328.2289999999998, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 88.84010212094451}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 366, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545387.6240234, "prompt_tokens": 84, "prefill_ms": 44.8639, "prefill_cuda_event_ms": null, "kv_decode_ms": 1321.925, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1623.3073999999306, "ollama_total_duration_ms": 1605.5271, "ollama_load_ms": 233.1053, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 44.8639, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1872.3294229881933}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 367, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545387.6240234, "prompt_tokens": 84, "prefill_ms": 44.8639, "prefill_cuda_event_ms": null, "kv_decode_ms": 1321.925, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1623.3073999999306, "ollama_total_duration_ms": 1605.5271, "ollama_load_ms": 233.1053, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1321.925, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 25.72006732605859}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 368, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545387.6240234, "prompt_tokens": 84, "prefill_ms": 44.8639, "prefill_cuda_event_ms": null, "kv_decode_ms": 1321.925, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1623.3073999999306, "ollama_total_duration_ms": 1605.5271, "ollama_load_ms": 233.1053, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1366.7889, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 86.33374180899479}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 369, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545389.2474556, "prompt_tokens": 84, "prefill_ms": 49.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1311.2958, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1604.4120000001385, "ollama_total_duration_ms": 1589.6133, "ollama_load_ms": 215.5225, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 49.9739, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1680.8774180122023}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 370, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545389.2474556, "prompt_tokens": 84, "prefill_ms": 49.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1311.2958, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1604.4120000001385, "ollama_total_duration_ms": 1589.6133, "ollama_load_ms": 215.5225, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1311.2958, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 25.928550979878068}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 371, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545389.2474556, "prompt_tokens": 84, "prefill_ms": 49.9739, "prefill_cuda_event_ms": null, "kv_decode_ms": 1311.2958, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 1604.4120000001385, "ollama_total_duration_ms": 1589.6133, "ollama_load_ms": 215.5225, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 1361.2697, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 86.6837776525842}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 372, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545390.851994, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.668999999936204, "prefill_cuda_event_ms": 12.804096221923828, "kv_decode_ms": 186.40479999999116, "kv_decode_cuda_event_ms": 186.34649658203125, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 13.668999999936204, "cuda_event_ms": 12.804096221923828, "tokens_total": 9, "tokens_per_s": 658.4241714859905}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 373, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545390.851994, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.668999999936204, "prefill_cuda_event_ms": 12.804096221923828, "kv_decode_ms": 186.40479999999116, "kv_decode_cuda_event_ms": 186.34649658203125, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 186.40479999999116, "cuda_event_ms": 186.34649658203125, "tokens_total": 64, "tokens_per_s": 343.3387981425534}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 374, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545390.851994, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.668999999936204, "prefill_cuda_event_ms": 12.804096221923828, "kv_decode_ms": 186.40479999999116, "kv_decode_cuda_event_ms": 186.34649658203125, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 200.07379999992736, "cuda_event_ms": 199.15059280395508, "tokens_total": 73, "tokens_per_s": 364.8653646805654}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 375, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545391.0560727, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.8283000001229084, "prefill_cuda_event_ms": 3.7795839309692383, "kv_decode_ms": 144.74119999999857, "kv_decode_cuda_event_ms": 144.716796875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 3.8283000001229084, "cuda_event_ms": 3.7795839309692383, "tokens_total": 9, "tokens_per_s": 2350.912937782058}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 376, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545391.0560727, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8283000001229084, "prefill_cuda_event_ms": 3.7795839309692383, "kv_decode_ms": 144.74119999999857, "kv_decode_cuda_event_ms": 144.716796875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 144.74119999999857, "cuda_event_ms": 144.716796875, "tokens_total": 64, "tokens_per_s": 442.16850489011165}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 377, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545391.0560727, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8283000001229084, "prefill_cuda_event_ms": 3.7795839309692383, "kv_decode_ms": 144.74119999999857, "kv_decode_cuda_event_ms": 144.716796875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 148.56950000012148, "cuda_event_ms": 148.49638080596924, "tokens_total": 73, "tokens_per_s": 491.35253197958065}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 378, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545391.2055628, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.9852000000119006, "prefill_cuda_event_ms": 2.9519360065460205, "kv_decode_ms": 147.8255000001809, "kv_decode_cuda_event_ms": 147.80210876464844, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 2.9852000000119006, "cuda_event_ms": 2.9519360065460205, "tokens_total": 9, "tokens_per_s": 3014.873375306218}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 379, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545391.2055628, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.9852000000119006, "prefill_cuda_event_ms": 2.9519360065460205, "kv_decode_ms": 147.8255000001809, "kv_decode_cuda_event_ms": 147.80210876464844, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 147.8255000001809, "cuda_event_ms": 147.80210876464844, "tokens_total": 64, "tokens_per_s": 432.94289550802586}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 380, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545391.2055628, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.9852000000119006, "prefill_cuda_event_ms": 2.9519360065460205, "kv_decode_ms": 147.8255000001809, "kv_decode_cuda_event_ms": 147.80210876464844, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 150.8107000001928, "cuda_event_ms": 150.75404477119446, "tokens_total": 73, "tokens_per_s": 484.0505348752222}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 381, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545391.3569062, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.9682999997930892, "prefill_cuda_event_ms": 2.935807943344116, "kv_decode_ms": 146.43870000008974, "kv_decode_cuda_event_ms": 146.4094696044922, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 2.9682999997930892, "cuda_event_ms": 2.935807943344116, "tokens_total": 9, "tokens_per_s": 3032.0385407901363}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 382, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545391.3569062, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.9682999997930892, "prefill_cuda_event_ms": 2.935807943344116, "kv_decode_ms": 146.43870000008974, "kv_decode_cuda_event_ms": 146.4094696044922, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 146.43870000008974, "cuda_event_ms": 146.4094696044922, "tokens_total": 64, "tokens_per_s": 437.0429401514817}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 383, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545391.3569062, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.9682999997930892, "prefill_cuda_event_ms": 2.935807943344116, "kv_decode_ms": 146.43870000008974, "kv_decode_cuda_event_ms": 146.4094696044922, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 149.40699999988283, "cuda_event_ms": 149.3452775478363, "tokens_total": 73, "tokens_per_s": 488.59825844878253}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 384, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545391.506859, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 517.7977000000737, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.6689999998125, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 376.19999999992615, "params_millions_measured": 74.824704, "latency_ms": 517.7977000000737, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 32.831354793575905}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 385, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545391.506859, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 517.7977000000737, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.6689999998125, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 376.19999999992615, "params_millions_measured": 74.824704, "latency_ms": 1374.6689999998125, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.556662003732335}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 386, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545391.506859, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 517.7977000000737, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.6689999998125, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 376.19999999992615, "params_millions_measured": 74.824704, "latency_ms": 1892.4666999998863, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 42.80128152321247}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 387, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545393.7762501, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.75429999996959, "prefill_cuda_event_ms": null, "kv_decode_ms": 1187.9040999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 27.75429999996959, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 612.5176999606773}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 388, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545393.7762501, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.75429999996959, "prefill_cuda_event_ms": null, "kv_decode_ms": 1187.9040999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1187.9040999999688, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 53.87640298573065}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 389, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545393.7762501, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.75429999996959, "prefill_cuda_event_ms": null, "kv_decode_ms": 1187.9040999999688, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1215.6583999999384, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 66.63056003232825}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 390, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545394.9924848, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.31300000000192, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.7849000001024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 29.31300000000192, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 579.9474635826728}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 391, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545394.9924848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.31300000000192, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.7849000001024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1346.7849000001024, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.52058030944298}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 392, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545394.9924848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.31300000000192, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.7849000001024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1376.0979000001043, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 58.86209113464518}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 393, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545396.36956, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.751400000122885, "prefill_cuda_event_ms": null, "kv_decode_ms": 1028.242999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 29.751400000122885, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 571.4016819352966}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 394, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545396.36956, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.751400000122885, "prefill_cuda_event_ms": null, "kv_decode_ms": 1028.242999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1028.242999999975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.24209646941585}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 395, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545396.36956, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.751400000122885, "prefill_cuda_event_ms": null, "kv_decode_ms": 1028.242999999975, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1057.9944000000978, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 76.55995154605026}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 396, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545397.4280589, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.021499999946172, "prefill_cuda_event_ms": 11.369471549987793, "kv_decode_ms": 370.14720000001944, "kv_decode_cuda_event_ms": 369.5226745605469, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 12.021499999946172, "cuda_event_ms": 11.369471549987793, "tokens_total": 9, "tokens_per_s": 748.6586532496193}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 397, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545397.4280589, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.021499999946172, "prefill_cuda_event_ms": 11.369471549987793, "kv_decode_ms": 370.14720000001944, "kv_decode_cuda_event_ms": 369.5226745605469, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 370.14720000001944, "cuda_event_ms": 369.5226745605469, "tokens_total": 64, "tokens_per_s": 172.90418514579238}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 398, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545397.4280589, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.021499999946172, "prefill_cuda_event_ms": 11.369471549987793, "kv_decode_ms": 370.14720000001944, "kv_decode_cuda_event_ms": 369.5226745605469, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 382.1686999999656, "cuda_event_ms": 380.89214611053467, "tokens_total": 73, "tokens_per_s": 191.0151197625723}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 399, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545397.8122096, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.022000000053595, "prefill_cuda_event_ms": 4.981503963470459, "kv_decode_ms": 281.527399999959, "kv_decode_cuda_event_ms": 281.50177001953125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 5.022000000053595, "cuda_event_ms": 4.981503963470459, "tokens_total": 9, "tokens_per_s": 1792.1146953213763}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 400, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545397.8122096, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.022000000053595, "prefill_cuda_event_ms": 4.981503963470459, "kv_decode_ms": 281.527399999959, "kv_decode_cuda_event_ms": 281.50177001953125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 281.527399999959, "cuda_event_ms": 281.50177001953125, "tokens_total": 64, "tokens_per_s": 227.3313361328571}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 401, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545397.8122096, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.022000000053595, "prefill_cuda_event_ms": 4.981503963470459, "kv_decode_ms": 281.527399999959, "kv_decode_cuda_event_ms": 281.50177001953125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 286.5494000000126, "cuda_event_ms": 286.4832739830017, "tokens_total": 73, "tokens_per_s": 254.7553755129021}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 402, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545398.0992439, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.5186000002104265, "prefill_cuda_event_ms": 4.48086404800415, "kv_decode_ms": 273.15669999984493, "kv_decode_cuda_event_ms": 273.1325378417969, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 4.5186000002104265, "cuda_event_ms": 4.48086404800415, "tokens_total": 9, "tokens_per_s": 1991.767361479414}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 403, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545398.0992439, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.5186000002104265, "prefill_cuda_event_ms": 4.48086404800415, "kv_decode_ms": 273.15669999984493, "kv_decode_cuda_event_ms": 273.1325378417969, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 273.15669999984493, "cuda_event_ms": 273.1325378417969, "tokens_total": 64, "tokens_per_s": 234.29774924077034}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 404, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545398.0992439, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.5186000002104265, "prefill_cuda_event_ms": 4.48086404800415, "kv_decode_ms": 273.15669999984493, "kv_decode_cuda_event_ms": 273.1325378417969, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 277.67530000005536, "cuda_event_ms": 277.613401889801, "tokens_total": 73, "tokens_per_s": 262.8969879567446}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 405, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545398.3774242, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.1258000001107575, "prefill_cuda_event_ms": 4.089856147766113, "kv_decode_ms": 246.5540000000601, "kv_decode_cuda_event_ms": 246.53106689453125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 4.1258000001107575, "cuda_event_ms": 4.089856147766113, "tokens_total": 9, "tokens_per_s": 2181.3951233114535}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 406, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545398.3774242, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.1258000001107575, "prefill_cuda_event_ms": 4.089856147766113, "kv_decode_ms": 246.5540000000601, "kv_decode_cuda_event_ms": 246.53106689453125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 246.5540000000601, "cuda_event_ms": 246.53106689453125, "tokens_total": 64, "tokens_per_s": 259.5780234755242}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 407, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545398.3774242, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.1258000001107575, "prefill_cuda_event_ms": 4.089856147766113, "kv_decode_ms": 246.5540000000601, "kv_decode_cuda_event_ms": 246.53106689453125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 250.67980000017087, "cuda_event_ms": 250.62092304229736, "tokens_total": 73, "tokens_per_s": 291.20814680700335}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 408, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545398.6285534, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.680999999938649, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.18390000001637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.680999999938649, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 372.9951510715716}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 409, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545398.6285534, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.680999999938649, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.18390000001637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 159.18390000001637, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 402.0507099021535}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 410, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545398.6285534, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.680999999938649, "prefill_cuda_event_ms": null, "kv_decode_ms": 159.18390000001637, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 161.864899999955, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 401.56945699789185}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 411, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545398.790976, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.3856000000250788, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.54679999998189, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.3856000000250788, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 419.18175720551955}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 412, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545398.790976, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.3856000000250788, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.54679999998189, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 152.54679999998189, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 419.5433794744144}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 413, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545398.790976, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.3856000000250788, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.54679999998189, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 154.93240000000696, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 419.5378113293093}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 414, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545398.9462771, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.1666000000095664, "prefill_cuda_event_ms": null, "kv_decode_ms": 158.71980000019903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.1666000000095664, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 461.5526631568285}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 415, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545398.9462771, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.1666000000095664, "prefill_cuda_event_ms": null, "kv_decode_ms": 158.71980000019903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 158.71980000019903, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 403.22631454878183}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 416, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545398.9462771, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.1666000000095664, "prefill_cuda_event_ms": null, "kv_decode_ms": 158.71980000019903, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 160.8864000002086, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 404.01177476726264}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 417, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545399.1074386, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.7244999998856656, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.63480000002528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.7244999998856656, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 367.0398238362875}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 418, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545399.1074386, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.7244999998856656, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.63480000002528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 160.63480000002528, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 398.4192715401017}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 419, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545399.1074386, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.7244999998856656, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.63480000002528, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 163.35929999991095, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 397.89592634172305}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 420, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545399.2711413, "prompt_tokens": 99, "prefill_ms": 224.6339, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.8083, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2845.03380000001, "ollama_total_duration_ms": 2822.4122, "ollama_load_ms": 202.6897, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 224.6339, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 440.7170956832428}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 421, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545399.2711413, "prompt_tokens": 99, "prefill_ms": 224.6339, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.8083, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2845.03380000001, "ollama_total_duration_ms": 2822.4122, "ollama_load_ms": 202.6897, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2389.8083, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.780390711673398}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 422, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545399.2711413, "prompt_tokens": 99, "prefill_ms": 224.6339, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.8083, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2845.03380000001, "ollama_total_duration_ms": 2822.4122, "ollama_load_ms": 202.6897, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2614.4422, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 62.345994874164745}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 423, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545402.1162946, "prompt_tokens": 99, "prefill_ms": 41.9383, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.4655, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2845.7564000000275, "ollama_total_duration_ms": 2843.3256, "ollama_load_ms": 214.803, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 41.9383, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2360.6107066810055}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 424, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545402.1162946, "prompt_tokens": 99, "prefill_ms": 41.9383, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.4655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2845.7564000000275, "ollama_total_duration_ms": 2843.3256, "ollama_load_ms": 214.803, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2570.4655, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.89821396163458}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 425, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545402.1162946, "prompt_tokens": 99, "prefill_ms": 41.9383, "prefill_cuda_event_ms": null, "kv_decode_ms": 2570.4655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2845.7564000000275, "ollama_total_duration_ms": 2843.3256, "ollama_load_ms": 214.803, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2612.4037999999996, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 62.394642053422224}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 426, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545404.962213, "prompt_tokens": 99, "prefill_ms": 48.1333, "prefill_cuda_event_ms": null, "kv_decode_ms": 2792.2103, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3071.168299999954, "ollama_total_duration_ms": 3067.7764, "ollama_load_ms": 213.058, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 48.1333, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2056.788127969618}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 427, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545404.962213, "prompt_tokens": 99, "prefill_ms": 48.1333, "prefill_cuda_event_ms": null, "kv_decode_ms": 2792.2103, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3071.168299999954, "ollama_total_duration_ms": 3067.7764, "ollama_load_ms": 213.058, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2792.2103, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 22.92090964638301}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 428, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545404.962213, "prompt_tokens": 99, "prefill_ms": 48.1333, "prefill_cuda_event_ms": null, "kv_decode_ms": 2792.2103, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3071.168299999954, "ollama_total_duration_ms": 3067.7764, "ollama_load_ms": 213.058, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2840.3436, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 57.38742312725826}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 429, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545408.033512, "prompt_tokens": 99, "prefill_ms": 53.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 2964.5127, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3275.7517000000007, "ollama_total_duration_ms": 3252.8458, "ollama_load_ms": 222.9813, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.4683, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1851.5643848785169}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 430, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545408.033512, "prompt_tokens": 99, "prefill_ms": 53.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 2964.5127, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3275.7517000000007, "ollama_total_duration_ms": 3252.8458, "ollama_load_ms": 222.9813, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2964.5127, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.588708322956414}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 431, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545408.033512, "prompt_tokens": 99, "prefill_ms": 53.4683, "prefill_cuda_event_ms": null, "kv_decode_ms": 2964.5127, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3275.7517000000007, "ollama_total_duration_ms": 3252.8458, "ollama_load_ms": 222.9813, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3017.981, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 54.009617688116656}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 432, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.309401, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.157899999943766, "prefill_cuda_event_ms": 10.667008399963379, "kv_decode_ms": 168.6299999998937, "kv_decode_cuda_event_ms": 168.55142211914062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 11.157899999943766, "cuda_event_ms": 10.667008399963379, "tokens_total": 9, "tokens_per_s": 806.6033931156722}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 433, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.309401, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.157899999943766, "prefill_cuda_event_ms": 10.667008399963379, "kv_decode_ms": 168.6299999998937, "kv_decode_cuda_event_ms": 168.55142211914062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 168.6299999998937, "cuda_event_ms": 168.55142211914062, "tokens_total": 64, "tokens_per_s": 379.52914665267355}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 434, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.309401, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.157899999943766, "prefill_cuda_event_ms": 10.667008399963379, "kv_decode_ms": 168.6299999998937, "kv_decode_cuda_event_ms": 168.55142211914062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 179.78789999983746, "cuda_event_ms": 179.218430519104, "tokens_total": 73, "tokens_per_s": 406.0339989513532}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 435, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.4907324, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.1899999999277497, "prefill_cuda_event_ms": 2.1483519077301025, "kv_decode_ms": 95.7791000000725, "kv_decode_cuda_event_ms": 95.74400329589844, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 2.1899999999277497, "cuda_event_ms": 2.1483519077301025, "tokens_total": 9, "tokens_per_s": 4109.58904123147}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 436, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.4907324, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.1899999999277497, "prefill_cuda_event_ms": 2.1483519077301025, "kv_decode_ms": 95.7791000000725, "kv_decode_cuda_event_ms": 95.74400329589844, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 95.7791000000725, "cuda_event_ms": 95.74400329589844, "tokens_total": 64, "tokens_per_s": 668.2042324468653}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 437, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.4907324, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.1899999999277497, "prefill_cuda_event_ms": 2.1483519077301025, "kv_decode_ms": 95.7791000000725, "kv_decode_cuda_event_ms": 95.74400329589844, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 97.96910000000025, "cuda_event_ms": 97.89235520362854, "tokens_total": 73, "tokens_per_s": 745.1329041503883}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 438, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.58957, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 1.9955999998728657, "prefill_cuda_event_ms": 1.9599039554595947, "kv_decode_ms": 98.79399999999805, "kv_decode_cuda_event_ms": 98.75660705566406, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 1.9955999998728657, "cuda_event_ms": 1.9599039554595947, "tokens_total": 9, "tokens_per_s": 4509.921828308962}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 439, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.58957, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 1.9955999998728657, "prefill_cuda_event_ms": 1.9599039554595947, "kv_decode_ms": 98.79399999999805, "kv_decode_cuda_event_ms": 98.75660705566406, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 98.79399999999805, "cuda_event_ms": 98.75660705566406, "tokens_total": 64, "tokens_per_s": 647.81262019962}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 440, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.58957, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 1.9955999998728657, "prefill_cuda_event_ms": 1.9599039554595947, "kv_decode_ms": 98.79399999999805, "kv_decode_cuda_event_ms": 98.75660705566406, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 100.78959999987092, "cuda_event_ms": 100.71651101112366, "tokens_total": 73, "tokens_per_s": 724.2810766199439}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 441, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.6912417, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.227599999969243, "prefill_cuda_event_ms": 2.1810240745544434, "kv_decode_ms": 100.45079999986228, "kv_decode_cuda_event_ms": 100.4052505493164, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 2.227599999969243, "cuda_event_ms": 2.1810240745544434, "tokens_total": 9, "tokens_per_s": 4040.222661215777}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 442, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.6912417, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.227599999969243, "prefill_cuda_event_ms": 2.1810240745544434, "kv_decode_ms": 100.45079999986228, "kv_decode_cuda_event_ms": 100.4052505493164, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 100.45079999986228, "cuda_event_ms": 100.4052505493164, "tokens_total": 64, "tokens_per_s": 637.1278277533653}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 443, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.6912417, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.227599999969243, "prefill_cuda_event_ms": 2.1810240745544434, "kv_decode_ms": 100.45079999986228, "kv_decode_cuda_event_ms": 100.4052505493164, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 102.67839999983153, "cuda_event_ms": 102.58627462387085, "tokens_total": 73, "tokens_per_s": 710.9577087305585}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 444, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.7944338, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.662399999891932, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.16330000010203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 5.662399999891932, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3002.2605256294946}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 445, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.7944338, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.662399999891932, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.16330000010203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 147.16330000010203, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 434.89103601207387}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 446, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.7944338, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.662399999891932, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.16330000010203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 152.82569999999396, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 530.0155667535186}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 447, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545411.9484677, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.184299999906216, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.6590000000706, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.184299999906216, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4062.8062042351235}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 448, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545411.9484677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.184299999906216, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.6590000000706, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 143.6590000000706, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 445.49941180133897}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 449, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545411.9484677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.184299999906216, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.6590000000706, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 147.8432999999768, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 547.8773809838708}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 450, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.0967953, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.505700000005163, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2838000001102, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.505700000005163, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4849.245514440758}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 451, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.0967953, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.505700000005163, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2838000001102, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 141.2838000001102, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 452.9889484848941}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 452, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.0967953, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.505700000005163, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2838000001102, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 144.78950000011537, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 559.4328318002027}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 453, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.2422962, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.4698000001753826, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.58950000018922, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.4698000001753826, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4899.4178336332725}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 454, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.2422962, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.4698000001753826, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.58950000018922, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 143.58950000018922, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 445.7150418374301}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 455, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.2422962, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.4698000001753826, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.58950000018922, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 147.0593000003646, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 550.7982154124164}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 456, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.3897176, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.675499999848398, "prefill_cuda_event_ms": 4.61516809463501, "kv_decode_ms": 127.76569999982712, "kv_decode_cuda_event_ms": 127.72662353515625, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.675499999848398, "cuda_event_ms": 4.61516809463501, "tokens_total": 1, "tokens_per_s": 213.88086836326056}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 457, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.3897176, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.675499999848398, "prefill_cuda_event_ms": 4.61516809463501, "kv_decode_ms": 127.76569999982712, "kv_decode_cuda_event_ms": 127.72662353515625, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 127.76569999982712, "cuda_event_ms": 127.72662353515625, "tokens_total": 64, "tokens_per_s": 500.9169127558226}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 458, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.3897176, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.675499999848398, "prefill_cuda_event_ms": 4.61516809463501, "kv_decode_ms": 127.76569999982712, "kv_decode_cuda_event_ms": 127.72662353515625, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 132.44119999967552, "cuda_event_ms": 132.34179162979126, "tokens_total": 65, "tokens_per_s": 490.78383463876236}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 459, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.523328, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.9671000000016647, "prefill_cuda_event_ms": 1.9220479726791382, "kv_decode_ms": 128.59809999986282, "kv_decode_cuda_event_ms": 128.55807495117188, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 1.9671000000016647, "cuda_event_ms": 1.9220479726791382, "tokens_total": 1, "tokens_per_s": 508.3625641803435}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 460, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.523328, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9671000000016647, "prefill_cuda_event_ms": 1.9220479726791382, "kv_decode_ms": 128.59809999986282, "kv_decode_cuda_event_ms": 128.55807495117188, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 128.59809999986282, "cuda_event_ms": 128.55807495117188, "tokens_total": 64, "tokens_per_s": 497.67453796026746}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 461, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.523328, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9671000000016647, "prefill_cuda_event_ms": 1.9220479726791382, "kv_decode_ms": 128.59809999986282, "kv_decode_cuda_event_ms": 128.55807495117188, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 130.5651999998645, "cuda_event_ms": 130.480122923851, "tokens_total": 65, "tokens_per_s": 497.835564147778}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 462, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.654355, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.9933000000946777, "prefill_cuda_event_ms": 1.9466240406036377, "kv_decode_ms": 125.08739999998397, "kv_decode_cuda_event_ms": 125.04883575439453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 1.9933000000946777, "cuda_event_ms": 1.9466240406036377, "tokens_total": 1, "tokens_per_s": 501.68063008704263}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 463, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.654355, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9933000000946777, "prefill_cuda_event_ms": 1.9466240406036377, "kv_decode_ms": 125.08739999998397, "kv_decode_cuda_event_ms": 125.04883575439453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 125.08739999998397, "cuda_event_ms": 125.04883575439453, "tokens_total": 64, "tokens_per_s": 511.64225973206095}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 464, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.654355, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9933000000946777, "prefill_cuda_event_ms": 1.9466240406036377, "kv_decode_ms": 125.08739999998397, "kv_decode_cuda_event_ms": 125.04883575439453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 127.08070000007865, "cuda_event_ms": 126.99545979499817, "tokens_total": 65, "tokens_per_s": 511.4860084966464}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 465, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.7819574, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.229299999906289, "prefill_cuda_event_ms": 4.172800064086914, "kv_decode_ms": 122.68349999999373, "kv_decode_cuda_event_ms": 122.60761260986328, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.229299999906289, "cuda_event_ms": 4.172800064086914, "tokens_total": 1, "tokens_per_s": 236.4457475284699}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 466, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.7819574, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.229299999906289, "prefill_cuda_event_ms": 4.172800064086914, "kv_decode_ms": 122.68349999999373, "kv_decode_cuda_event_ms": 122.60761260986328, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 122.68349999999373, "cuda_event_ms": 122.60761260986328, "tokens_total": 64, "tokens_per_s": 521.6675429051443}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 467, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.7819574, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.229299999906289, "prefill_cuda_event_ms": 4.172800064086914, "kv_decode_ms": 122.68349999999373, "kv_decode_cuda_event_ms": 122.60761260986328, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 126.91279999990002, "cuda_event_ms": 126.7804126739502, "tokens_total": 65, "tokens_per_s": 512.1626817787584}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 468, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545412.9094336, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.478000000106476, "prefill_cuda_event_ms": null, "kv_decode_ms": 927.6259999999183, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 31.478000000106476, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 540.0597242500316}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 469, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545412.9094336, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.478000000106476, "prefill_cuda_event_ms": null, "kv_decode_ms": 927.6259999999183, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 927.6259999999183, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.99332273998965}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 470, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545412.9094336, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.478000000106476, "prefill_cuda_event_ms": null, "kv_decode_ms": 927.6259999999183, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 959.1040000000248, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 84.4538235686619}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 471, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545413.8690312, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 25.017599999955564, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.3587000001462, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 25.017599999955564, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 679.5216167829926}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 472, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545413.8690312, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.017599999955564, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.3587000001462, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1056.3587000001462, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.58548104918447}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 473, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545413.8690312, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 25.017599999955564, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.3587000001462, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1081.3763000001018, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 74.90454525403634}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 474, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545414.9509494, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 24.717799999962153, "prefill_cuda_event_ms": null, "kv_decode_ms": 868.751599999996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 24.717799999962153, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 687.7634740966441}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 475, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545414.9509494, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.717799999962153, "prefill_cuda_event_ms": null, "kv_decode_ms": 868.751599999996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 868.751599999996, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.66892907017414}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 476, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545414.9509494, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.717799999962153, "prefill_cuda_event_ms": null, "kv_decode_ms": 868.751599999996, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 893.4693999999581, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 90.65783338523266}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 477, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545415.844905, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.98470000002817, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.0499999999483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 23.98470000002817, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 708.7851838872296}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 478, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545415.844905, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.98470000002817, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.0499999999483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 843.0499999999483, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.91483304668041}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 479, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545415.844905, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.98470000002817, "prefill_cuda_event_ms": null, "kv_decode_ms": 843.0499999999483, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 867.0346999999765, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 93.4218665066141}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 480, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545416.7126658, "prompt_tokens": 46, "prefill_ms": 22.828, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.3503, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 5761.992700000064, "ollama_total_duration_ms": 5738.5457, "ollama_load_ms": 4948.5692, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.828, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2015.0692132468896}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 481, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545416.7126658, "prompt_tokens": 46, "prefill_ms": 22.828, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.3503, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 5761.992700000064, "ollama_total_duration_ms": 5738.5457, "ollama_load_ms": 4948.5692, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 721.3503, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.72249723885885}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 482, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545416.7126658, "prompt_tokens": 46, "prefill_ms": 22.828, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.3503, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 5761.992700000064, "ollama_total_duration_ms": 5738.5457, "ollama_load_ms": 4948.5692, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 744.1782999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.8140386517586}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 483, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545422.4748685, "prompt_tokens": 46, "prefill_ms": 11.6546, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7331, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 898.5889000000498, "ollama_total_duration_ms": 878.6812, "ollama_load_ms": 103.3831, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6546, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3946.939405899816}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 484, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545422.4748685, "prompt_tokens": 46, "prefill_ms": 11.6546, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7331, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 898.5889000000498, "ollama_total_duration_ms": 878.6812, "ollama_load_ms": 103.3831, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 722.7331, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.55274512818079}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 485, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545422.4748685, "prompt_tokens": 46, "prefill_ms": 11.6546, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7331, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 898.5889000000498, "ollama_total_duration_ms": 878.6812, "ollama_load_ms": 103.3831, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 734.3877, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.78464372428897}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 486, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545423.3735669, "prompt_tokens": 46, "prefill_ms": 11.8905, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.7982, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 891.768200000115, "ollama_total_duration_ms": 889.5557, "ollama_load_ms": 111.657, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.8905, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3868.6346242798872}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 487, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545423.3735669, "prompt_tokens": 46, "prefill_ms": 11.8905, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.7982, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 891.768200000115, "ollama_total_duration_ms": 889.5557, "ollama_load_ms": 111.657, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 711.7982, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.91312425347522}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 488, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545423.3735669, "prompt_tokens": 46, "prefill_ms": 11.8905, "prefill_cuda_event_ms": null, "kv_decode_ms": 711.7982, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 891.768200000115, "ollama_total_duration_ms": 889.5557, "ollama_load_ms": 111.657, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 723.6886999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 151.99905705312244}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 489, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545424.2654395, "prompt_tokens": 46, "prefill_ms": 11.9794, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.1338, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 914.04179999995, "ollama_total_duration_ms": 887.5827, "ollama_load_ms": 106.9334, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9794, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3839.925204935139}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 490, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545424.2654395, "prompt_tokens": 46, "prefill_ms": 11.9794, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.1338, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 914.04179999995, "ollama_total_duration_ms": 887.5827, "ollama_load_ms": 106.9334, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 717.1338, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.24415499590175}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 491, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545424.2654395, "prompt_tokens": 46, "prefill_ms": 11.9794, "prefill_cuda_event_ms": null, "kv_decode_ms": 717.1338, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 914.04179999995, "ollama_total_duration_ms": 887.5827, "ollama_load_ms": 106.9334, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 729.1132, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 150.86820537606505}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 492, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545425.179854, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.281600000027538, "prefill_cuda_event_ms": null, "kv_decode_ms": 627.7291000001242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.281600000027538, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.29213347773812}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 493, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545425.179854, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.281600000027538, "prefill_cuda_event_ms": null, "kv_decode_ms": 627.7291000001242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 627.7291000001242, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 101.9548082126308}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 494, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545425.179854, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.281600000027538, "prefill_cuda_event_ms": null, "kv_decode_ms": 627.7291000001242, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 641.0107000001517, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 101.40236348626414}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 495, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545425.8211684, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.170899999868197, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.925000000032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.170899999868197, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 122.38553892669482}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 496, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545425.8211684, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.170899999868197, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.925000000032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 597.925000000032, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 107.03683572353819}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 497, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545425.8211684, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.170899999868197, "prefill_cuda_event_ms": null, "kv_decode_ms": 597.925000000032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 606.0958999999002, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 107.24375465996503}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 498, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545426.4276054, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.078799999950206, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.6896999999153, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.078799999950206, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 90.26248330184627}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 499, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545426.4276054, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.078799999950206, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.6896999999153, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 641.6896999999153, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.73667958206038}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 500, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545426.4276054, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.078799999950206, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.6896999999153, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 652.7684999998655, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 99.57588333385172}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 501, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545427.0808349, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.597199999916484, "prefill_cuda_event_ms": null, "kv_decode_ms": 603.620200000023, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.597199999916484, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 104.19705747600364}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 502, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545427.0808349, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.597199999916484, "prefill_cuda_event_ms": null, "kv_decode_ms": 603.620200000023, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 603.620200000023, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 106.02693548028637}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 503, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545427.0808349, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.597199999916484, "prefill_cuda_event_ms": null, "kv_decode_ms": 603.620200000023, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 613.2173999999395, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 105.9982968519915}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 504, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545427.69454, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.054500000147527, "prefill_cuda_event_ms": null, "kv_decode_ms": 1183.307799999966, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 24.054500000147527, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 374.15036687292616}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 505, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545427.69454, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.054500000147527, "prefill_cuda_event_ms": null, "kv_decode_ms": 1183.307799999966, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1183.307799999966, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.085674073982986}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 506, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545427.69454, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.054500000147527, "prefill_cuda_event_ms": null, "kv_decode_ms": 1183.307799999966, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1207.3623000001135, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 60.462381507185654}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 507, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545428.9024107, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.085099999912927, "prefill_cuda_event_ms": null, "kv_decode_ms": 1098.960600000055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.085099999912927, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 407.51456864743574}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 508, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545428.9024107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.085099999912927, "prefill_cuda_event_ms": null, "kv_decode_ms": 1098.960600000055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1098.960600000055, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.23684670769526}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 509, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545428.9024107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.085099999912927, "prefill_cuda_event_ms": null, "kv_decode_ms": 1098.960600000055, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1121.0456999999678, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 65.11777352163439}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 510, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545430.0238543, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.917299999946408, "prefill_cuda_event_ms": null, "kv_decode_ms": 1293.567400000029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 25.917299999946408, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 347.2583949724165}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 511, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545430.0238543, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.917299999946408, "prefill_cuda_event_ms": null, "kv_decode_ms": 1293.567400000029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1293.567400000029, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.475582022242186}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 512, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545430.0238543, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.917299999946408, "prefill_cuda_event_ms": null, "kv_decode_ms": 1293.567400000029, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1319.4846999999754, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 55.324627864196806}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 513, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545431.3437188, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.357700000180557, "prefill_cuda_event_ms": null, "kv_decode_ms": 1486.772700000074, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.357700000180557, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 421.39368939183123}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 514, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545431.3437188, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.357700000180557, "prefill_cuda_event_ms": null, "kv_decode_ms": 1486.772700000074, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1486.772700000074, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 43.04625717165563}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 515, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545431.3437188, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.357700000180557, "prefill_cuda_event_ms": null, "kv_decode_ms": 1486.772700000074, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1508.1304000002547, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 48.404302439621716}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 516, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545432.8523064, "prompt_tokens": 92, "prefill_ms": 758.716, "prefill_cuda_event_ms": null, "kv_decode_ms": 2165.1059, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 11911.391600000115, "ollama_total_duration_ms": 11835.1815, "ollama_load_ms": 8854.7178, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 758.716, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 121.25749292225285}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 517, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545432.8523064, "prompt_tokens": 92, "prefill_ms": 758.716, "prefill_cuda_event_ms": null, "kv_decode_ms": 2165.1059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 11911.391600000115, "ollama_total_duration_ms": 11835.1815, "ollama_load_ms": 8854.7178, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2165.1059, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.559755021682776}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 518, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545432.8523064, "prompt_tokens": 92, "prefill_ms": 758.716, "prefill_cuda_event_ms": null, "kv_decode_ms": 2165.1059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 11911.391600000115, "ollama_total_duration_ms": 11835.1815, "ollama_load_ms": 8854.7178, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2923.8219, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 53.354823014356654}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 519, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545444.7638361, "prompt_tokens": 92, "prefill_ms": 42.5741, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3014, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2704.5217999998385, "ollama_total_duration_ms": 2689.2299, "ollama_load_ms": 245.1074, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 42.5741, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2160.938223004127}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 520, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545444.7638361, "prompt_tokens": 92, "prefill_ms": 42.5741, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3014, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2704.5217999998385, "ollama_total_duration_ms": 2689.2299, "ollama_load_ms": 245.1074, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2389.3014, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.78607228037451}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 521, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545444.7638361, "prompt_tokens": 92, "prefill_ms": 42.5741, "prefill_cuda_event_ms": null, "kv_decode_ms": 2389.3014, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2704.5217999998385, "ollama_total_duration_ms": 2689.2299, "ollama_load_ms": 245.1074, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2431.8754999999996, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 64.1480207354365}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 522, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545447.4685342, "prompt_tokens": 92, "prefill_ms": 49.4855, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.1512, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2742.297600000029, "ollama_total_duration_ms": 2739.9466, "ollama_load_ms": 206.5764, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 49.4855, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1859.13045235473}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 523, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545447.4685342, "prompt_tokens": 92, "prefill_ms": 49.4855, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.1512, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2742.297600000029, "ollama_total_duration_ms": 2739.9466, "ollama_load_ms": 206.5764, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2468.1512, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.93034008613411}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 524, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545447.4685342, "prompt_tokens": 92, "prefill_ms": 49.4855, "prefill_cuda_event_ms": null, "kv_decode_ms": 2468.1512, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 2742.297600000029, "ollama_total_duration_ms": 2739.9466, "ollama_load_ms": 206.5764, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2517.6367, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 61.962871767797154}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 525, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545450.211728, "prompt_tokens": 92, "prefill_ms": 53.3776, "prefill_cuda_event_ms": null, "kv_decode_ms": 2911.8098, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3208.953999999949, "ollama_total_duration_ms": 3185.7653, "ollama_load_ms": 208.8432, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.3776, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1723.5694373669853}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 526, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545450.211728, "prompt_tokens": 92, "prefill_ms": 53.3776, "prefill_cuda_event_ms": null, "kv_decode_ms": 2911.8098, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3208.953999999949, "ollama_total_duration_ms": 3185.7653, "ollama_load_ms": 208.8432, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2911.8098, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.979457586824523}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 527, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545450.211728, "prompt_tokens": 92, "prefill_ms": 53.3776, "prefill_cuda_event_ms": null, "kv_decode_ms": 2911.8098, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3208.953999999949, "ollama_total_duration_ms": 3185.7653, "ollama_load_ms": 208.8432, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2965.1874, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 52.610502796551756}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 528, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545453.420832, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.21699999998782, "prefill_cuda_event_ms": 10.796031951904297, "kv_decode_ms": 459.78439999998955, "kv_decode_cuda_event_ms": 459.7145690917969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 11.21699999998782, "cuda_event_ms": 10.796031951904297, "tokens_total": 1, "tokens_per_s": 89.1503967193622}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 529, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545453.420832, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.21699999998782, "prefill_cuda_event_ms": 10.796031951904297, "kv_decode_ms": 459.78439999998955, "kv_decode_cuda_event_ms": 459.7145690917969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 459.78439999998955, "cuda_event_ms": 459.7145690917969, "tokens_total": 64, "tokens_per_s": 139.195675190375}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 530, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545453.420832, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.21699999998782, "prefill_cuda_event_ms": 10.796031951904297, "kv_decode_ms": 459.78439999998955, "kv_decode_cuda_event_ms": 459.7145690917969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 471.00139999997737, "cuda_event_ms": 470.5106010437012, "tokens_total": 65, "tokens_per_s": 138.00383608202253}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 531, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545453.8934736, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.976200000053723, "prefill_cuda_event_ms": 6.923264026641846, "kv_decode_ms": 469.94569999992564, "kv_decode_cuda_event_ms": 469.88824462890625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 6.976200000053723, "cuda_event_ms": 6.923264026641846, "tokens_total": 1, "tokens_per_s": 143.3445142043375}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 532, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545453.8934736, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.976200000053723, "prefill_cuda_event_ms": 6.923264026641846, "kv_decode_ms": 469.94569999992564, "kv_decode_cuda_event_ms": 469.88824462890625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 469.94569999992564, "cuda_event_ms": 469.88824462890625, "tokens_total": 64, "tokens_per_s": 136.18594658917004}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 533, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545453.8934736, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.976200000053723, "prefill_cuda_event_ms": 6.923264026641846, "kv_decode_ms": 469.94569999992564, "kv_decode_cuda_event_ms": 469.88824462890625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 476.92189999997936, "cuda_event_ms": 476.8115086555481, "tokens_total": 65, "tokens_per_s": 136.29065891082547}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 534, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545454.3710911, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.819400000016685, "prefill_cuda_event_ms": 7.766016006469727, "kv_decode_ms": 446.26229999994393, "kv_decode_cuda_event_ms": 446.2060852050781, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 7.819400000016685, "cuda_event_ms": 7.766016006469727, "tokens_total": 1, "tokens_per_s": 127.88705015702818}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 535, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545454.3710911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.819400000016685, "prefill_cuda_event_ms": 7.766016006469727, "kv_decode_ms": 446.26229999994393, "kv_decode_cuda_event_ms": 446.2060852050781, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 446.26229999994393, "cuda_event_ms": 446.2060852050781, "tokens_total": 64, "tokens_per_s": 143.4134140392501}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 536, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545454.3710911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.819400000016685, "prefill_cuda_event_ms": 7.766016006469727, "kv_decode_ms": 446.26229999994393, "kv_decode_cuda_event_ms": 446.2060852050781, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 454.0816999999606, "cuda_event_ms": 453.97210121154785, "tokens_total": 65, "tokens_per_s": 143.1460461850932}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 537, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545454.8257632, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.324899999957779, "prefill_cuda_event_ms": 7.2775678634643555, "kv_decode_ms": 439.7756999999274, "kv_decode_cuda_event_ms": 439.7413024902344, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 7.324899999957779, "cuda_event_ms": 7.2775678634643555, "tokens_total": 1, "tokens_per_s": 136.52063509478137}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 538, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545454.8257632, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.324899999957779, "prefill_cuda_event_ms": 7.2775678634643555, "kv_decode_ms": 439.7756999999274, "kv_decode_cuda_event_ms": 439.7413024902344, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 439.7756999999274, "cuda_event_ms": 439.7413024902344, "tokens_total": 64, "tokens_per_s": 145.5287320331946}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 539, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545454.8257632, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.324899999957779, "prefill_cuda_event_ms": 7.2775678634643555, "kv_decode_ms": 439.7756999999274, "kv_decode_cuda_event_ms": 439.7413024902344, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 447.10059999988516, "cuda_event_ms": 447.01887035369873, "tokens_total": 65, "tokens_per_s": 145.38115135613035}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 540, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545455.2733338, "prompt_tokens": 18, "prefill_ms": 10.7355, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.8147, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2746.4800000000196, "ollama_total_duration_ms": 2725.6383, "ollama_load_ms": 2545.2712, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 10.7355, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1676.680173256951}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 541, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545455.2733338, "prompt_tokens": 18, "prefill_ms": 10.7355, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.8147, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2746.4800000000196, "ollama_total_duration_ms": 2725.6383, "ollama_load_ms": 2545.2712, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 138.8147, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 244.93083225335647}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 542, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545455.2733338, "prompt_tokens": 18, "prefill_ms": 10.7355, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.8147, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2746.4800000000196, "ollama_total_duration_ms": 2725.6383, "ollama_load_ms": 2545.2712, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 149.5502, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 347.7093310473674}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 543, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545458.0200117, "prompt_tokens": 18, "prefill_ms": 4.126, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6525, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 307.2032999998555, "ollama_total_duration_ms": 289.3495, "ollama_load_ms": 131.5933, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.126, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4362.578768783325}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 544, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545458.0200117, "prompt_tokens": 18, "prefill_ms": 4.126, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6525, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 307.2032999998555, "ollama_total_duration_ms": 289.3495, "ollama_load_ms": 131.5933, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 132.6525, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 256.3087766909783}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 545, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545458.0200117, "prompt_tokens": 18, "prefill_ms": 4.126, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.6525, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 307.2032999998555, "ollama_total_duration_ms": 289.3495, "ollama_load_ms": 131.5933, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 136.7785, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 380.17670905880675}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 546, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545458.3274329, "prompt_tokens": 18, "prefill_ms": 4.7035, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.1494, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 311.13530000016, "ollama_total_duration_ms": 289.913, "ollama_load_ms": 129.7194, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.7035, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3826.937387052195}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 547, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545458.3274329, "prompt_tokens": 18, "prefill_ms": 4.7035, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.1494, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 311.13530000016, "ollama_total_duration_ms": 289.913, "ollama_load_ms": 129.7194, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 125.1494, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 271.67529368898295}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 548, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545458.3274329, "prompt_tokens": 18, "prefill_ms": 4.7035, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.1494, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 311.13530000016, "ollama_total_duration_ms": 289.913, "ollama_load_ms": 129.7194, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 129.8529, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 400.45312811650723}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 549, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545458.6386793, "prompt_tokens": 18, "prefill_ms": 4.1729, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.2667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 305.82460000005085, "ollama_total_duration_ms": 285.6939, "ollama_load_ms": 131.7058, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.1729, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4313.5469337870545}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 550, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545458.6386793, "prompt_tokens": 18, "prefill_ms": 4.1729, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.2667, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 305.82460000005085, "ollama_total_duration_ms": 285.6939, "ollama_load_ms": 131.7058, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 127.2667, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 267.1555088644555}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 551, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545458.6386793, "prompt_tokens": 18, "prefill_ms": 4.1729, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.2667, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 305.82460000005085, "ollama_total_duration_ms": 285.6939, "ollama_load_ms": 131.7058, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 131.4396, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 395.6189763206826}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 552, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545458.944618, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.495899999999892, "prefill_cuda_event_ms": 11.287296295166016, "kv_decode_ms": 457.3998999999276, "kv_decode_cuda_event_ms": 457.34234619140625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 11.495899999999892, "cuda_event_ms": 11.287296295166016, "tokens_total": 17, "tokens_per_s": 1478.7880896667646}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 553, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545458.944618, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.495899999999892, "prefill_cuda_event_ms": 11.287296295166016, "kv_decode_ms": 457.3998999999276, "kv_decode_cuda_event_ms": 457.34234619140625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 457.3998999999276, "cuda_event_ms": 457.34234619140625, "tokens_total": 64, "tokens_per_s": 139.92132486257677}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 554, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545458.944618, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.495899999999892, "prefill_cuda_event_ms": 11.287296295166016, "kv_decode_ms": 457.3998999999276, "kv_decode_cuda_event_ms": 457.34234619140625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 468.8957999999275, "cuda_event_ms": 468.62964248657227, "tokens_total": 81, "tokens_per_s": 172.7462690005168}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 555, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545459.4145892, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.38199999995959, "prefill_cuda_event_ms": 9.32863998413086, "kv_decode_ms": 426.95129999992787, "kv_decode_cuda_event_ms": 426.8921203613281, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 9.38199999995959, "cuda_event_ms": 9.32863998413086, "tokens_total": 17, "tokens_per_s": 1811.9803879847818}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 556, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545459.4145892, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.38199999995959, "prefill_cuda_event_ms": 9.32863998413086, "kv_decode_ms": 426.95129999992787, "kv_decode_cuda_event_ms": 426.8921203613281, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 426.95129999992787, "cuda_event_ms": 426.8921203613281, "tokens_total": 64, "tokens_per_s": 149.9000003045097}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 557, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545459.4145892, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.38199999995959, "prefill_cuda_event_ms": 9.32863998413086, "kv_decode_ms": 426.95129999992787, "kv_decode_cuda_event_ms": 426.8921203613281, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 436.33329999988746, "cuda_event_ms": 436.220760345459, "tokens_total": 81, "tokens_per_s": 185.63790570195053}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 558, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545459.8516102, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.933899999921778, "prefill_cuda_event_ms": 6.884352207183838, "kv_decode_ms": 424.13489999989906, "kv_decode_cuda_event_ms": 424.0783386230469, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 6.933899999921778, "cuda_event_ms": 6.884352207183838, "tokens_total": 17, "tokens_per_s": 2451.7226957688717}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 559, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545459.8516102, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.933899999921778, "prefill_cuda_event_ms": 6.884352207183838, "kv_decode_ms": 424.13489999989906, "kv_decode_cuda_event_ms": 424.0783386230469, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 424.13489999989906, "cuda_event_ms": 424.0783386230469, "tokens_total": 64, "tokens_per_s": 150.8953872930882}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 560, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545459.8516102, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.933899999921778, "prefill_cuda_event_ms": 6.884352207183838, "kv_decode_ms": 424.13489999989906, "kv_decode_cuda_event_ms": 424.0783386230469, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 431.06879999982084, "cuda_event_ms": 430.9626908302307, "tokens_total": 81, "tokens_per_s": 187.90503975243317}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 561, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545460.2833312, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.672100000036153, "prefill_cuda_event_ms": 7.620512008666992, "kv_decode_ms": 426.2644000000364, "kv_decode_cuda_event_ms": 426.19903564453125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 7.672100000036153, "cuda_event_ms": 7.620512008666992, "tokens_total": 17, "tokens_per_s": 2215.820961655856}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 562, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545460.2833312, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.672100000036153, "prefill_cuda_event_ms": 7.620512008666992, "kv_decode_ms": 426.2644000000364, "kv_decode_cuda_event_ms": 426.19903564453125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 426.2644000000364, "cuda_event_ms": 426.19903564453125, "tokens_total": 64, "tokens_per_s": 150.14155533512658}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 563, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545460.2833312, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.672100000036153, "prefill_cuda_event_ms": 7.620512008666992, "kv_decode_ms": 426.2644000000364, "kv_decode_cuda_event_ms": 426.19903564453125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 433.93650000007256, "cuda_event_ms": 433.81954765319824, "tokens_total": 81, "tokens_per_s": 186.6632560293648}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 564, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545460.7180574, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 28.00999999999476, "prefill_cuda_event_ms": null, "kv_decode_ms": 1224.916099999973, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 28.00999999999476, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 35.701535166018814}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 565, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545460.7180574, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 28.00999999999476, "prefill_cuda_event_ms": null, "kv_decode_ms": 1224.916099999973, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1224.916099999973, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.2484764466737}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 566, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545460.7180574, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 28.00999999999476, "prefill_cuda_event_ms": null, "kv_decode_ms": 1224.916099999973, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1252.9260999999678, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 51.87855851993319}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 567, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545461.9713433, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 21.01859999993394, "prefill_cuda_event_ms": null, "kv_decode_ms": 1121.167400000104, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 21.01859999993394, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 47.57690807204776}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 568, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545461.9713433, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.01859999993394, "prefill_cuda_event_ms": null, "kv_decode_ms": 1121.167400000104, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1121.167400000104, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 57.08335793566069}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 569, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545461.9713433, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.01859999993394, "prefill_cuda_event_ms": null, "kv_decode_ms": 1121.167400000104, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1142.186000000038, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 56.90841946933148}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 570, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545463.1138816, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.101100000014412, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.4108999999262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.101100000014412, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 62.10755786866145}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 571, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545463.1138816, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.101100000014412, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.4108999999262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1153.4108999999262, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 55.48759769827396}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 572, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545463.1138816, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.101100000014412, "prefill_cuda_event_ms": null, "kv_decode_ms": 1153.4108999999262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1169.5119999999406, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 55.57873711428639}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 573, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545464.2838006, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 24.210999999922933, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.7495999999956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 24.210999999922933, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 41.30353971348491}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 574, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545464.2838006, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 24.210999999922933, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.7495999999956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1349.7495999999956, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.416202234844306}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 575, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545464.2838006, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 24.210999999922933, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.7495999999956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1373.9605999999185, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 47.308489049834364}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 576, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545465.658238, "prompt_tokens": 26, "prefill_ms": 75.7477, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.8066, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 948.8741000000118, "ollama_total_duration_ms": 923.7901, "ollama_load_ms": 189.2851, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 75.7477, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 343.2447453850084}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 577, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545465.658238, "prompt_tokens": 26, "prefill_ms": 75.7477, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.8066, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 948.8741000000118, "ollama_total_duration_ms": 923.7901, "ollama_load_ms": 189.2851, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 616.8066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 103.76023862260878}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 578, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545465.658238, "prompt_tokens": 26, "prefill_ms": 75.7477, "prefill_cuda_event_ms": null, "kv_decode_ms": 616.8066, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 948.8741000000118, "ollama_total_duration_ms": 923.7901, "ollama_load_ms": 189.2851, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 692.5543, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 129.9537090449081}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 579, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545466.6072223, "prompt_tokens": 26, "prefill_ms": 6.2871, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.8374, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 500.2346999999645, "ollama_total_duration_ms": 490.1289, "ollama_load_ms": 137.5758, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 6.2871, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4135.4519571821675}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 580, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545466.6072223, "prompt_tokens": 26, "prefill_ms": 6.2871, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.8374, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 500.2346999999645, "ollama_total_duration_ms": 490.1289, "ollama_load_ms": 137.5758, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 311.8374, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 205.23516422340617}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 581, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545466.6072223, "prompt_tokens": 26, "prefill_ms": 6.2871, "prefill_cuda_event_ms": null, "kv_decode_ms": 311.8374, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 500.2346999999645, "ollama_total_duration_ms": 490.1289, "ollama_load_ms": 137.5758, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 318.1245, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 282.9081067317984}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 582, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545467.107563, "prompt_tokens": 26, "prefill_ms": 5.5207, "prefill_cuda_event_ms": null, "kv_decode_ms": 288.7094, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 487.24319999996624, "ollama_total_duration_ms": 464.7254, "ollama_load_ms": 136.8833, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.5207, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4709.5477022841305}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 583, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545467.107563, "prompt_tokens": 26, "prefill_ms": 5.5207, "prefill_cuda_event_ms": null, "kv_decode_ms": 288.7094, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 487.24319999996624, "ollama_total_duration_ms": 464.7254, "ollama_load_ms": 136.8833, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 288.7094, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 221.67619066092064}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 584, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545467.107563, "prompt_tokens": 26, "prefill_ms": 5.5207, "prefill_cuda_event_ms": null, "kv_decode_ms": 288.7094, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 487.24319999996624, "ollama_total_duration_ms": 464.7254, "ollama_load_ms": 136.8833, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 294.2301, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 305.88304867516956}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 585, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545467.59496, "prompt_tokens": 26, "prefill_ms": 4.9246, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8301, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 478.07699999998476, "ollama_total_duration_ms": 436.496, "ollama_load_ms": 128.4956, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.9246, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5279.616618608618}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 586, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545467.59496, "prompt_tokens": 26, "prefill_ms": 4.9246, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8301, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 478.07699999998476, "ollama_total_duration_ms": 436.496, "ollama_load_ms": 128.4956, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 269.8301, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 237.1862887053742}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 587, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545467.59496, "prompt_tokens": 26, "prefill_ms": 4.9246, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8301, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 478.07699999998476, "ollama_total_duration_ms": 436.496, "ollama_load_ms": 128.4956, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.7547, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 327.5649151770652}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 588, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545468.07317, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 29.86319999990883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.713499999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 29.86319999990883, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 33.486029628541246}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 589, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545468.07317, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.86319999990883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.713499999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1313.713499999949, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 48.716862542710025}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 590, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545468.07317, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.86319999990883, "prefill_cuda_event_ms": null, "kv_decode_ms": 1313.713499999949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1343.5766999998577, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 48.378332253013085}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 591, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545469.4172435, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.954899999949703, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.82469999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 17.954899999949703, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 55.695102729773005}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 592, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545469.4172435, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.954899999949703, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.82469999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1275.82469999993, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.16363141425582}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 593, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545469.4172435, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.954899999949703, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.82469999993, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1293.7795999998798, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 50.24039643228726}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 594, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545470.711547, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.052599999942686, "prefill_cuda_event_ms": null, "kv_decode_ms": 1594.4498999999723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 17.052599999942686, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.64208390529075}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 595, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545470.711547, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.052599999942686, "prefill_cuda_event_ms": null, "kv_decode_ms": 1594.4498999999723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1594.4498999999723, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.13923548178034}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 596, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545470.711547, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.052599999942686, "prefill_cuda_event_ms": null, "kv_decode_ms": 1594.4498999999723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1611.502499999915, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 40.33502895589888}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 597, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545472.3234427, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 31.58419999999751, "prefill_cuda_event_ms": null, "kv_decode_ms": 2000.682000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 31.58419999999751, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 31.66140032041587}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 598, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545472.3234427, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 31.58419999999751, "prefill_cuda_event_ms": null, "kv_decode_ms": 2000.682000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2000.682000000097, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.989091719722023}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 599, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545472.3234427, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 31.58419999999751, "prefill_cuda_event_ms": null, "kv_decode_ms": 2000.682000000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2032.2662000000946, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.983998946593204}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 600, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545474.3561254, "prompt_tokens": 10, "prefill_ms": 10.5723, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.3579, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1311.8323999999575, "ollama_total_duration_ms": 1280.1758, "ollama_load_ms": 1226.8313, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 10.5723, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 945.8679757479451}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 601, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545474.3561254, "prompt_tokens": 10, "prefill_ms": 10.5723, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.3579, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 1311.8323999999575, "ollama_total_duration_ms": 1280.1758, "ollama_load_ms": 1226.8313, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 23.3579, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 470.93274652258975}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 602, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545474.3561254, "prompt_tokens": 10, "prefill_ms": 10.5723, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.3579, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 1311.8323999999575, "ollama_total_duration_ms": 1280.1758, "ollama_load_ms": 1226.8313, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 33.9302, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 618.9176603733547}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 603, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545475.66817, "prompt_tokens": 10, "prefill_ms": 2.748, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.3274, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 177.54000000013548, "ollama_total_duration_ms": 170.7339, "ollama_load_ms": 139.1154, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.748, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3639.0101892285297}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 604, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545475.66817, "prompt_tokens": 10, "prefill_ms": 2.748, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.3274, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 177.54000000013548, "ollama_total_duration_ms": 170.7339, "ollama_load_ms": 139.1154, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 19.3274, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 569.1401844014197}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 605, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545475.66817, "prompt_tokens": 10, "prefill_ms": 2.748, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.3274, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 177.54000000013548, "ollama_total_duration_ms": 170.7339, "ollama_load_ms": 139.1154, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 22.075400000000002, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 951.2851409260986}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 606, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545475.8459036, "prompt_tokens": 10, "prefill_ms": 2.3318, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.4312, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 182.30759999983093, "ollama_total_duration_ms": 160.7518, "ollama_load_ms": 130.5971, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.3318, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4288.532464190755}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 607, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545475.8459036, "prompt_tokens": 10, "prefill_ms": 2.3318, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.4312, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 182.30759999983093, "ollama_total_duration_ms": 160.7518, "ollama_load_ms": 130.5971, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 20.4312, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 538.3922628137359}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 608, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545475.8459036, "prompt_tokens": 10, "prefill_ms": 2.3318, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.4312, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 182.30759999983093, "ollama_total_duration_ms": 160.7518, "ollama_load_ms": 130.5971, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 22.763, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 922.5497517901857}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 609, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545476.0283554, "prompt_tokens": 10, "prefill_ms": 2.975, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1433, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 164.26209999985986, "ollama_total_duration_ms": 157.2814, "ollama_load_ms": 124.4499, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.975, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3361.344537815126}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 610, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545476.0283554, "prompt_tokens": 10, "prefill_ms": 2.975, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1433, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 164.26209999985986, "ollama_total_duration_ms": 157.2814, "ollama_load_ms": 124.4499, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 21.1433, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 520.2593729455667}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 611, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545476.0283554, "prompt_tokens": 10, "prefill_ms": 2.975, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.1433, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 164.26209999985986, "ollama_total_duration_ms": 157.2814, "ollama_load_ms": 124.4499, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 24.1183, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 870.7081344870907}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 612, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545476.1928375, "prompt_tokens": 11, "prefill_ms": 87.2154, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.679, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 559.049400000049, "ollama_total_duration_ms": 511.2575, "ollama_load_ms": 131.8221, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 87.2154, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 126.124514707265}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 613, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545476.1928375, "prompt_tokens": 11, "prefill_ms": 87.2154, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.679, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 559.049400000049, "ollama_total_duration_ms": 511.2575, "ollama_load_ms": 131.8221, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 254.679, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 251.2967303939469}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 614, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545476.1928375, "prompt_tokens": 11, "prefill_ms": 87.2154, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.679, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 559.049400000049, "ollama_total_duration_ms": 511.2575, "ollama_load_ms": 131.8221, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 341.8944, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 219.3659796709159}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 615, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545476.752094, "prompt_tokens": 11, "prefill_ms": 4.2525, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.3177, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 485.43289999997796, "ollama_total_duration_ms": 435.3141, "ollama_load_ms": 148.0362, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.2525, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2586.7136978248086}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 616, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545476.752094, "prompt_tokens": 11, "prefill_ms": 4.2525, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.3177, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 485.43289999997796, "ollama_total_duration_ms": 435.3141, "ollama_load_ms": 148.0362, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 249.3177, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 256.7005872427028}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 617, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545476.752094, "prompt_tokens": 11, "prefill_ms": 4.2525, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.3177, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 485.43289999997796, "ollama_total_duration_ms": 435.3141, "ollama_load_ms": 148.0362, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 253.5702, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 295.7760809432654}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 618, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545477.237637, "prompt_tokens": 11, "prefill_ms": 4.6647, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.9636, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 444.492099999934, "ollama_total_duration_ms": 420.2905, "ollama_load_ms": 128.3767, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6647, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2358.1366432996765}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 619, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545477.237637, "prompt_tokens": 11, "prefill_ms": 4.6647, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.9636, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 444.492099999934, "ollama_total_duration_ms": 420.2905, "ollama_load_ms": 128.3767, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 253.9636, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 252.00461798462456}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 620, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545477.237637, "prompt_tokens": 11, "prefill_ms": 4.6647, "prefill_cuda_event_ms": null, "kv_decode_ms": 253.9636, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 444.492099999934, "ollama_total_duration_ms": 420.2905, "ollama_load_ms": 128.3767, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 258.6283, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 289.9914665177786}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 621, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545477.6822727, "prompt_tokens": 11, "prefill_ms": 4.6516, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.0134, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 452.7491999999711, "ollama_total_duration_ms": 419.727, "ollama_load_ms": 131.6544, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6516, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2364.7777108951755}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 622, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545477.6822727, "prompt_tokens": 11, "prefill_ms": 4.6516, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.0134, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 452.7491999999711, "ollama_total_duration_ms": 419.727, "ollama_load_ms": 131.6544, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 252.0134, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 253.9547500251971}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 623, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545477.6822727, "prompt_tokens": 11, "prefill_ms": 4.6516, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.0134, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 452.7491999999711, "ollama_total_duration_ms": 419.727, "ollama_load_ms": 131.6544, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 256.66499999999996, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 292.2096896733096}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 624, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545478.1351833, "prompt_tokens": 11, "prefill_ms": 18.0457, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.9976, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4579.678000000058, "ollama_total_duration_ms": 4557.1695, "ollama_load_ms": 4419.9402, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 18.0457, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 609.5634971211978}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 625, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545478.1351833, "prompt_tokens": 11, "prefill_ms": 18.0457, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.9976, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 4579.678000000058, "ollama_total_duration_ms": 4557.1695, "ollama_load_ms": 4419.9402, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 106.9976, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 93.4600402252013}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 626, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545478.1351833, "prompt_tokens": 11, "prefill_ms": 18.0457, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.9976, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 4579.678000000058, "ollama_total_duration_ms": 4557.1695, "ollama_load_ms": 4419.9402, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 125.0433, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 167.94182495183668}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 627, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545482.7153704, "prompt_tokens": 11, "prefill_ms": 11.1738, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.1638, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 266.1467000000357, "ollama_total_duration_ms": 249.4761, "ollama_load_ms": 126.5144, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.1738, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 984.4457570387872}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 628, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545482.7153704, "prompt_tokens": 11, "prefill_ms": 11.1738, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.1638, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 266.1467000000357, "ollama_total_duration_ms": 249.4761, "ollama_load_ms": 126.5144, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 103.1638, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 96.9332265775398}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 629, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545482.7153704, "prompt_tokens": 11, "prefill_ms": 11.1738, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.1638, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 266.1467000000357, "ollama_total_duration_ms": 249.4761, "ollama_load_ms": 126.5144, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 114.3376, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 183.66661535662809}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 630, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545482.9816356, "prompt_tokens": 11, "prefill_ms": 12.3855, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.6753, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 260.1872000000185, "ollama_total_duration_ms": 244.7901, "ollama_load_ms": 120.7002, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 12.3855, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 888.135319526866}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 631, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545482.9816356, "prompt_tokens": 11, "prefill_ms": 12.3855, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.6753, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 260.1872000000185, "ollama_total_duration_ms": 244.7901, "ollama_load_ms": 120.7002, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 104.6753, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 95.53352127961419}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 632, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545482.9816356, "prompt_tokens": 11, "prefill_ms": 12.3855, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.6753, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 260.1872000000185, "ollama_total_duration_ms": 244.7901, "ollama_load_ms": 120.7002, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 117.0608, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 179.39395596134656}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 633, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545483.2419312, "prompt_tokens": 11, "prefill_ms": 11.5889, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2284, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 266.4984000000459, "ollama_total_duration_ms": 248.0952, "ollama_load_ms": 125.6532, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.5889, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 949.1841330928733}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 634, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545483.2419312, "prompt_tokens": 11, "prefill_ms": 11.5889, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2284, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 266.4984000000459, "ollama_total_duration_ms": 248.0952, "ollama_load_ms": 125.6532, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 104.2284, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 95.94314025735788}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 635, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545483.2419312, "prompt_tokens": 11, "prefill_ms": 11.5889, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.2284, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 266.4984000000459, "ollama_total_duration_ms": 248.0952, "ollama_load_ms": 125.6532, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 115.81729999999999, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 181.320061856044}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 636, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545483.5087318, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.055799999941883, "prefill_cuda_event_ms": 8.986495971679688, "kv_decode_ms": 275.7936000000427, "kv_decode_cuda_event_ms": 275.76727294921875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 9.055799999941883, "cuda_event_ms": 8.986495971679688, "tokens_total": 17, "tokens_per_s": 1877.2499392774907}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 637, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545483.5087318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.055799999941883, "prefill_cuda_event_ms": 8.986495971679688, "kv_decode_ms": 275.7936000000427, "kv_decode_cuda_event_ms": 275.76727294921875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 275.7936000000427, "cuda_event_ms": 275.76727294921875, "tokens_total": 64, "tokens_per_s": 232.05759669546387}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 638, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545483.5087318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.055799999941883, "prefill_cuda_event_ms": 8.986495971679688, "kv_decode_ms": 275.7936000000427, "kv_decode_cuda_event_ms": 275.76727294921875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 284.8493999999846, "cuda_event_ms": 284.75376892089844, "tokens_total": 81, "tokens_per_s": 284.3607885430139}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 639, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545483.794571, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.12860000001092, "prefill_cuda_event_ms": 6.074368000030518, "kv_decode_ms": 216.0497000002124, "kv_decode_cuda_event_ms": 216.0035858154297, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 6.12860000001092, "cuda_event_ms": 6.074368000030518, "tokens_total": 17, "tokens_per_s": 2773.879842047076}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 640, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545483.794571, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.12860000001092, "prefill_cuda_event_ms": 6.074368000030518, "kv_decode_ms": 216.0497000002124, "kv_decode_cuda_event_ms": 216.0035858154297, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 216.0497000002124, "cuda_event_ms": 216.0035858154297, "tokens_total": 64, "tokens_per_s": 296.22813639610274}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 641, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545483.794571, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.12860000001092, "prefill_cuda_event_ms": 6.074368000030518, "kv_decode_ms": 216.0497000002124, "kv_decode_cuda_event_ms": 216.0035858154297, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 222.17830000022332, "cuda_event_ms": 222.0779538154602, "tokens_total": 81, "tokens_per_s": 364.5720576668315}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 642, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545484.0173366, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.551599999809696, "prefill_cuda_event_ms": 3.5131518840789795, "kv_decode_ms": 199.65360000014698, "kv_decode_cuda_event_ms": 199.62777709960938, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 3.551599999809696, "cuda_event_ms": 3.5131518840789795, "tokens_total": 17, "tokens_per_s": 4786.575065016023}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 643, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545484.0173366, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.551599999809696, "prefill_cuda_event_ms": 3.5131518840789795, "kv_decode_ms": 199.65360000014698, "kv_decode_cuda_event_ms": 199.62777709960938, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 199.65360000014698, "cuda_event_ms": 199.62777709960938, "tokens_total": 64, "tokens_per_s": 320.5552016089511}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 644, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545484.0173366, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.551599999809696, "prefill_cuda_event_ms": 3.5131518840789795, "kv_decode_ms": 199.65360000014698, "kv_decode_cuda_event_ms": 199.62777709960938, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 203.20519999995668, "cuda_event_ms": 203.14092898368835, "tokens_total": 81, "tokens_per_s": 398.61184654731903}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 645, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545484.2219224, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.704200000081983, "prefill_cuda_event_ms": 5.664768218994141, "kv_decode_ms": 211.46180000005188, "kv_decode_cuda_event_ms": 211.36282348632812, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 5.704200000081983, "cuda_event_ms": 5.664768218994141, "tokens_total": 17, "tokens_per_s": 2980.2601591381213}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 646, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545484.2219224, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.704200000081983, "prefill_cuda_event_ms": 5.664768218994141, "kv_decode_ms": 211.46180000005188, "kv_decode_cuda_event_ms": 211.36282348632812, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 211.46180000005188, "cuda_event_ms": 211.36282348632812, "tokens_total": 64, "tokens_per_s": 302.6551367669447}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 647, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545484.2219224, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.704200000081983, "prefill_cuda_event_ms": 5.664768218994141, "kv_decode_ms": 211.46180000005188, "kv_decode_cuda_event_ms": 211.36282348632812, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 217.16600000013386, "cuda_event_ms": 217.02759170532227, "tokens_total": 81, "tokens_per_s": 372.98656327394747}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 648, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545484.439738, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.463100000151826, "prefill_cuda_event_ms": 10.394944190979004, "kv_decode_ms": 451.99909999996635, "kv_decode_cuda_event_ms": 451.9465026855469, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 10.463100000151826, "cuda_event_ms": 10.394944190979004, "tokens_total": 1, "tokens_per_s": 95.57396947228732}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 649, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545484.439738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.463100000151826, "prefill_cuda_event_ms": 10.394944190979004, "kv_decode_ms": 451.99909999996635, "kv_decode_cuda_event_ms": 451.9465026855469, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 451.99909999996635, "cuda_event_ms": 451.9465026855469, "tokens_total": 64, "tokens_per_s": 141.59320228736024}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 650, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545484.439738, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.463100000151826, "prefill_cuda_event_ms": 10.394944190979004, "kv_decode_ms": 451.99909999996635, "kv_decode_cuda_event_ms": 451.9465026855469, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 462.4622000001182, "cuda_event_ms": 462.3414468765259, "tokens_total": 65, "tokens_per_s": 140.5520278197513}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 651, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545484.902952, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.860199999891847, "prefill_cuda_event_ms": 6.787775993347168, "kv_decode_ms": 401.7389999999068, "kv_decode_cuda_event_ms": 401.7022399902344, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 6.860199999891847, "cuda_event_ms": 6.787775993347168, "tokens_total": 1, "tokens_per_s": 145.76834494850957}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 652, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545484.902952, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.860199999891847, "prefill_cuda_event_ms": 6.787775993347168, "kv_decode_ms": 401.7389999999068, "kv_decode_cuda_event_ms": 401.7022399902344, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 401.7389999999068, "cuda_event_ms": 401.7022399902344, "tokens_total": 64, "tokens_per_s": 159.30741103058165}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 653, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545484.902952, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.860199999891847, "prefill_cuda_event_ms": 6.787775993347168, "kv_decode_ms": 401.7389999999068, "kv_decode_cuda_event_ms": 401.7022399902344, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 408.59919999979866, "cuda_event_ms": 408.49001598358154, "tokens_total": 65, "tokens_per_s": 159.080096094246}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 654, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545485.3119867, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.109700000024532, "prefill_cuda_event_ms": 6.067264080047607, "kv_decode_ms": 390.84060000004683, "kv_decode_cuda_event_ms": 390.80242919921875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 6.109700000024532, "cuda_event_ms": 6.067264080047607, "tokens_total": 1, "tokens_per_s": 163.67415748661716}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 655, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545485.3119867, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.109700000024532, "prefill_cuda_event_ms": 6.067264080047607, "kv_decode_ms": 390.84060000004683, "kv_decode_cuda_event_ms": 390.80242919921875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 390.84060000004683, "cuda_event_ms": 390.80242919921875, "tokens_total": 64, "tokens_per_s": 163.7496206893356}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 656, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545485.3119867, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.109700000024532, "prefill_cuda_event_ms": 6.067264080047607, "kv_decode_ms": 390.84060000004683, "kv_decode_cuda_event_ms": 390.80242919921875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 396.95030000007137, "cuda_event_ms": 396.86969327926636, "tokens_total": 65, "tokens_per_s": 163.74845918994976}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 657, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545485.709439, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.06739999986894, "prefill_cuda_event_ms": 7.011328220367432, "kv_decode_ms": 387.67060000009224, "kv_decode_cuda_event_ms": 387.631103515625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 7.06739999986894, "cuda_event_ms": 7.011328220367432, "tokens_total": 1, "tokens_per_s": 141.49475054737871}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 658, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545485.709439, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.06739999986894, "prefill_cuda_event_ms": 7.011328220367432, "kv_decode_ms": 387.67060000009224, "kv_decode_cuda_event_ms": 387.631103515625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 387.67060000009224, "cuda_event_ms": 387.631103515625, "tokens_total": 64, "tokens_per_s": 165.0886087311877}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 659, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545485.709439, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.06739999986894, "prefill_cuda_event_ms": 7.011328220367432, "kv_decode_ms": 387.67060000009224, "kv_decode_cuda_event_ms": 387.631103515625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 394.7379999999612, "cuda_event_ms": 394.64243173599243, "tokens_total": 65, "tokens_per_s": 164.6661836458775}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 660, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545486.1046653, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.9192999999831954, "prefill_cuda_event_ms": 4.879360198974609, "kv_decode_ms": 310.78100000013364, "kv_decode_cuda_event_ms": 310.7204284667969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.9192999999831954, "cuda_event_ms": 4.879360198974609, "tokens_total": 1, "tokens_per_s": 203.28095460805724}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 661, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545486.1046653, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.9192999999831954, "prefill_cuda_event_ms": 4.879360198974609, "kv_decode_ms": 310.78100000013364, "kv_decode_cuda_event_ms": 310.7204284667969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 310.78100000013364, "cuda_event_ms": 310.7204284667969, "tokens_total": 64, "tokens_per_s": 205.9327951193042}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 662, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545486.1046653, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.9192999999831954, "prefill_cuda_event_ms": 4.879360198974609, "kv_decode_ms": 310.78100000013364, "kv_decode_cuda_event_ms": 310.7204284667969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 315.70030000011684, "cuda_event_ms": 315.5997886657715, "tokens_total": 65, "tokens_per_s": 205.89147365389246}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 663, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545486.4209878, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.778700000088065, "prefill_cuda_event_ms": 4.729856014251709, "kv_decode_ms": 317.76669999999285, "kv_decode_cuda_event_ms": 317.7389831542969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.778700000088065, "cuda_event_ms": 4.729856014251709, "tokens_total": 1, "tokens_per_s": 209.2619331578821}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 664, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545486.4209878, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.778700000088065, "prefill_cuda_event_ms": 4.729856014251709, "kv_decode_ms": 317.76669999999285, "kv_decode_cuda_event_ms": 317.7389831542969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 317.76669999999285, "cuda_event_ms": 317.7389831542969, "tokens_total": 64, "tokens_per_s": 201.40562242677234}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 665, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545486.4209878, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.778700000088065, "prefill_cuda_event_ms": 4.729856014251709, "kv_decode_ms": 317.76669999999285, "kv_decode_cuda_event_ms": 317.7389831542969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 322.5454000000809, "cuda_event_ms": 322.4688391685486, "tokens_total": 65, "tokens_per_s": 201.52201829566843}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 666, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545486.7443438, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.276299999901312, "prefill_cuda_event_ms": 4.235263824462891, "kv_decode_ms": 343.40640000004896, "kv_decode_cuda_event_ms": 343.3809814453125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.276299999901312, "cuda_event_ms": 4.235263824462891, "tokens_total": 1, "tokens_per_s": 233.84701728669128}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 667, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545486.7443438, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.276299999901312, "prefill_cuda_event_ms": 4.235263824462891, "kv_decode_ms": 343.40640000004896, "kv_decode_cuda_event_ms": 343.3809814453125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 343.40640000004896, "cuda_event_ms": 343.3809814453125, "tokens_total": 64, "tokens_per_s": 186.36810496249015}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 668, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545486.7443438, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.276299999901312, "prefill_cuda_event_ms": 4.235263824462891, "kv_decode_ms": 343.40640000004896, "kv_decode_cuda_event_ms": 343.3809814453125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 347.6826999999503, "cuda_event_ms": 347.6162452697754, "tokens_total": 65, "tokens_per_s": 186.95206865342826}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 669, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545487.0924885, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.347699999902034, "prefill_cuda_event_ms": 5.288959980010986, "kv_decode_ms": 246.74770000001445, "kv_decode_cuda_event_ms": 246.719482421875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 5.347699999902034, "cuda_event_ms": 5.288959980010986, "tokens_total": 1, "tokens_per_s": 186.99627877747804}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 670, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545487.0924885, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.347699999902034, "prefill_cuda_event_ms": 5.288959980010986, "kv_decode_ms": 246.74770000001445, "kv_decode_cuda_event_ms": 246.719482421875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 246.74770000001445, "cuda_event_ms": 246.719482421875, "tokens_total": 64, "tokens_per_s": 259.3742515127649}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 671, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545487.0924885, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.347699999902034, "prefill_cuda_event_ms": 5.288959980010986, "kv_decode_ms": 246.74770000001445, "kv_decode_cuda_event_ms": 246.719482421875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 252.09539999991648, "cuda_event_ms": 252.008442401886, "tokens_total": 65, "tokens_per_s": 257.83889749682675}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 672, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545487.3451004, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 16.33059999994657, "prefill_cuda_event_ms": null, "kv_decode_ms": 532.9114999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.33059999994657, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1040.9905331130283}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 673, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545487.3451004, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.33059999994657, "prefill_cuda_event_ms": null, "kv_decode_ms": 532.9114999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 532.9114999999547, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 120.09498762928824}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 674, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545487.3451004, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.33059999994657, "prefill_cuda_event_ms": null, "kv_decode_ms": 532.9114999999547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 549.2420999999013, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 147.4759491306558}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 675, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545487.8948727, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.739900000113266, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.8983999999746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 9.739900000113266, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1745.3977966716604}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 676, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545487.8948727, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.739900000113266, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.8983999999746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 534.8983999999746, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 119.64889033132842}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 677, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545487.8948727, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.739900000113266, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.8983999999746, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 544.6383000000878, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 148.72255586870577}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 678, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545488.439919, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.41179999997621, "prefill_cuda_event_ms": null, "kv_decode_ms": 487.9069000000982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.41179999997621, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2020.9705413880595}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 679, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545488.439919, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.41179999997621, "prefill_cuda_event_ms": null, "kv_decode_ms": 487.9069000000982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 487.9069000000982, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 131.17256591367558}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 680, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545488.439919, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.41179999997621, "prefill_cuda_event_ms": null, "kv_decode_ms": 487.9069000000982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 496.3187000000744, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 163.20158801187193}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 681, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545488.9365878, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.199600000078135, "prefill_cuda_event_ms": null, "kv_decode_ms": 521.6928999998345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.199600000078135, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1666.73202869424}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 682, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545488.9365878, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.199600000078135, "prefill_cuda_event_ms": null, "kv_decode_ms": 521.6928999998345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 521.6928999998345, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 122.6775369187894}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 683, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545488.9365878, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.199600000078135, "prefill_cuda_event_ms": null, "kv_decode_ms": 521.6928999998345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 531.8924999999126, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 152.28641125793897}
