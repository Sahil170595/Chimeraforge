{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544499.6813934, "prompt_tokens": 19, "prefill_ms": 241.3822, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.4971, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 45180.24779999996, "ollama_total_duration_ms": 45149.4251, "ollama_load_ms": 44181.0182, "ollama_done_reason": "stop", "latency_ms": 241.3822, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 78.71334340311753}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544499.6813934, "prompt_tokens": 19, "prefill_ms": 241.3822, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.4971, "kv_decode_cuda_event_ms": null, "gen_tokens": 58, "ollama_wall_ms": 45180.24779999996, "ollama_total_duration_ms": 45149.4251, "ollama_load_ms": 44181.0182, "ollama_done_reason": "stop", "latency_ms": 673.4971, "cuda_event_ms": null, "tokens_total": 58, "tokens_per_s": 86.11766850963426}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544499.6813934, "prompt_tokens": 19, "prefill_ms": 241.3822, "prefill_cuda_event_ms": null, "kv_decode_ms": 673.4971, "kv_decode_cuda_event_ms": null, "gen_tokens": 58, "ollama_wall_ms": 45180.24779999996, "ollama_total_duration_ms": 45149.4251, "ollama_load_ms": 44181.0182, "ollama_done_reason": "stop", "latency_ms": 914.8793000000001, "cuda_event_ms": null, "tokens_total": 77, "tokens_per_s": 84.16410776809575}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544544.8617482, "prompt_tokens": 19, "prefill_ms": 11.9177, "prefill_cuda_event_ms": null, "kv_decode_ms": 654.7349, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 924.917399999913, "ollama_total_duration_ms": 901.917, "ollama_load_ms": 184.1691, "ollama_done_reason": "stop", "latency_ms": 11.9177, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1594.2673502437551}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544544.8617482, "prompt_tokens": 19, "prefill_ms": 11.9177, "prefill_cuda_event_ms": null, "kv_decode_ms": 654.7349, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 924.917399999913, "ollama_total_duration_ms": 901.917, "ollama_load_ms": 184.1691, "ollama_done_reason": "stop", "latency_ms": 654.7349, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 87.05813604865114}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544544.8617482, "prompt_tokens": 19, "prefill_ms": 11.9177, "prefill_cuda_event_ms": null, "kv_decode_ms": 654.7349, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 924.917399999913, "ollama_total_duration_ms": 901.917, "ollama_load_ms": 184.1691, "ollama_done_reason": "stop", "latency_ms": 666.6526, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 114.002405450755}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544545.7868173, "prompt_tokens": 19, "prefill_ms": 11.2946, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.169, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 912.4343000000863, "ollama_total_duration_ms": 892.4925, "ollama_load_ms": 177.5681, "ollama_done_reason": "stop", "latency_ms": 11.2946, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1682.2198218617746}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544545.7868173, "prompt_tokens": 19, "prefill_ms": 11.2946, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.169, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 912.4343000000863, "ollama_total_duration_ms": 892.4925, "ollama_load_ms": 177.5681, "ollama_done_reason": "stop", "latency_ms": 656.169, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 86.86786483360231}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544545.7868173, "prompt_tokens": 19, "prefill_ms": 11.2946, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.169, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 912.4343000000863, "ollama_total_duration_ms": 892.4925, "ollama_load_ms": 177.5681, "ollama_done_reason": "stop", "latency_ms": 667.4635999999999, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 113.86388710934949}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544546.6993923, "prompt_tokens": 19, "prefill_ms": 12.0393, "prefill_cuda_event_ms": null, "kv_decode_ms": 652.193, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1949.3113999999423, "ollama_total_duration_ms": 1921.7693, "ollama_load_ms": 1202.7004, "ollama_done_reason": "stop", "latency_ms": 12.0393, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1578.1648434709657}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544546.6993923, "prompt_tokens": 19, "prefill_ms": 12.0393, "prefill_cuda_event_ms": null, "kv_decode_ms": 652.193, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 1949.3113999999423, "ollama_total_duration_ms": 1921.7693, "ollama_load_ms": 1202.7004, "ollama_done_reason": "stop", "latency_ms": 652.193, "cuda_event_ms": null, "tokens_total": 57, "tokens_per_s": 87.39744216819254}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544546.6993923, "prompt_tokens": 19, "prefill_ms": 12.0393, "prefill_cuda_event_ms": null, "kv_decode_ms": 652.193, "kv_decode_cuda_event_ms": null, "gen_tokens": 57, "ollama_wall_ms": 1949.3113999999423, "ollama_total_duration_ms": 1921.7693, "ollama_load_ms": 1202.7004, "ollama_done_reason": "stop", "latency_ms": 664.2323, "cuda_event_ms": null, "tokens_total": 76, "tokens_per_s": 114.41780232608382}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544548.6490982, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 361.89919999992526, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.3141999999916, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 45556.88680000003, "latency_ms": 361.89919999992526, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 24.868803246876087}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544548.6490982, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 361.89919999992526, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.3141999999916, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 45556.88680000003, "latency_ms": 1137.3141999999916, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.2729279208863}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544548.6490982, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 361.89919999992526, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.3141999999916, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 45556.88680000003, "latency_ms": 1499.2133999999169, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 48.69220085679867}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544595.7275136, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.400800000037634, "prefill_cuda_event_ms": null, "kv_decode_ms": 1266.6608999998061, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 21.400800000037634, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 420.54502635341544}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544595.7275136, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.400800000037634, "prefill_cuda_event_ms": null, "kv_decode_ms": 1266.6608999998061, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1266.6608999998061, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.526545818229486}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544595.7275136, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.400800000037634, "prefill_cuda_event_ms": null, "kv_decode_ms": 1266.6608999998061, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1288.0616999998438, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 56.67430372319032}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544597.0162802, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.521499999991647, "prefill_cuda_event_ms": null, "kv_decode_ms": 1317.0525000000453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 24.521499999991647, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 367.02485573896644}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544597.0162802, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.521499999991647, "prefill_cuda_event_ms": null, "kv_decode_ms": 1317.0525000000453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1317.0525000000453, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 48.59335523830508}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544597.0162802, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.521499999991647, "prefill_cuda_event_ms": null, "kv_decode_ms": 1317.0525000000453, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1341.574000000037, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 54.413696150937625}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544598.3587804, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.232299999946918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.109599999958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 26.232299999946918, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 343.0884825203361}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544598.3587804, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.232299999946918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.109599999958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1217.109599999958, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.58359641564097}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544598.3587804, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.232299999946918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1217.109599999958, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1243.3418999999049, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.71273219377999}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544599.6028128, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 1156.4487000000554, "prefill_cuda_event_ms": 960.1986694335938, "kv_decode_ms": 466.00680000005923, "kv_decode_cuda_event_ms": 465.9045715332031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 408.358600000156, "latency_ms": 1156.4487000000554, "cuda_event_ms": 960.1986694335938, "tokens_total": 17, "tokens_per_s": 14.700176497236052}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544599.6028128, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1156.4487000000554, "prefill_cuda_event_ms": 960.1986694335938, "kv_decode_ms": 466.00680000005923, "kv_decode_cuda_event_ms": 465.9045715332031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 408.358600000156, "latency_ms": 466.00680000005923, "cuda_event_ms": 465.9045715332031, "tokens_total": 64, "tokens_per_s": 137.3370517339916}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544599.6028128, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1156.4487000000554, "prefill_cuda_event_ms": 960.1986694335938, "kv_decode_ms": 466.00680000005923, "kv_decode_cuda_event_ms": 465.9045715332031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 408.358600000156, "latency_ms": 1622.4555000001146, "cuda_event_ms": 1426.1032409667969, "tokens_total": 81, "tokens_per_s": 49.92432766260417}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544601.6368184, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.951699999968696, "prefill_cuda_event_ms": 5.841631889343262, "kv_decode_ms": 275.3965000001699, "kv_decode_cuda_event_ms": 275.3210144042969, "gpu_peak_mb": 13.068359375, "latency_ms": 5.951699999968696, "cuda_event_ms": 5.841631889343262, "tokens_total": 17, "tokens_per_s": 2856.3267637968}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544601.6368184, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.951699999968696, "prefill_cuda_event_ms": 5.841631889343262, "kv_decode_ms": 275.3965000001699, "kv_decode_cuda_event_ms": 275.3210144042969, "gpu_peak_mb": 13.068359375, "latency_ms": 275.3965000001699, "cuda_event_ms": 275.3210144042969, "tokens_total": 64, "tokens_per_s": 232.3922054200417}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544601.6368184, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.951699999968696, "prefill_cuda_event_ms": 5.841631889343262, "kv_decode_ms": 275.3965000001699, "kv_decode_cuda_event_ms": 275.3210144042969, "gpu_peak_mb": 13.068359375, "latency_ms": 281.3482000001386, "cuda_event_ms": 281.16264629364014, "tokens_total": 81, "tokens_per_s": 287.899478297569}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544601.9193177, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.8880000001881854, "prefill_cuda_event_ms": 3.7867519855499268, "kv_decode_ms": 106.39749999995729, "kv_decode_cuda_event_ms": 106.33094024658203, "gpu_peak_mb": 13.068359375, "latency_ms": 3.8880000001881854, "cuda_event_ms": 3.7867519855499268, "tokens_total": 17, "tokens_per_s": 4372.427983327462}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544601.9193177, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.8880000001881854, "prefill_cuda_event_ms": 3.7867519855499268, "kv_decode_ms": 106.39749999995729, "kv_decode_cuda_event_ms": 106.33094024658203, "gpu_peak_mb": 13.068359375, "latency_ms": 106.39749999995729, "cuda_event_ms": 106.33094024658203, "tokens_total": 64, "tokens_per_s": 601.5178928078732}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544601.9193177, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.8880000001881854, "prefill_cuda_event_ms": 3.7867519855499268, "kv_decode_ms": 106.39749999995729, "kv_decode_cuda_event_ms": 106.33094024658203, "gpu_peak_mb": 13.068359375, "latency_ms": 110.28550000014548, "cuda_event_ms": 110.11769223213196, "tokens_total": 81, "tokens_per_s": 734.4573856027597}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544602.030509, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.5781999997889216, "prefill_cuda_event_ms": 2.520927906036377, "kv_decode_ms": 133.93020000012257, "kv_decode_cuda_event_ms": 133.8817596435547, "gpu_peak_mb": 13.068359375, "latency_ms": 2.5781999997889216, "cuda_event_ms": 2.520927906036377, "tokens_total": 17, "tokens_per_s": 6593.74757636793}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544602.030509, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.5781999997889216, "prefill_cuda_event_ms": 2.520927906036377, "kv_decode_ms": 133.93020000012257, "kv_decode_cuda_event_ms": 133.8817596435547, "gpu_peak_mb": 13.068359375, "latency_ms": 133.93020000012257, "cuda_event_ms": 133.8817596435547, "tokens_total": 64, "tokens_per_s": 477.86085587822186}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544602.030509, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.5781999997889216, "prefill_cuda_event_ms": 2.520927906036377, "kv_decode_ms": 133.93020000012257, "kv_decode_cuda_event_ms": 133.8817596435547, "gpu_peak_mb": 13.068359375, "latency_ms": 136.5083999999115, "cuda_event_ms": 136.40268754959106, "tokens_total": 81, "tokens_per_s": 593.3700783252351}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544602.1677046, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 31.50510000000395, "prefill_cuda_event_ms": 31.350784301757812, "kv_decode_ms": 656.6328999999769, "kv_decode_cuda_event_ms": 656.5355224609375, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 562.8830000000562, "latency_ms": 31.50510000000395, "cuda_event_ms": 31.350784301757812, "tokens_total": 1, "tokens_per_s": 31.74089274434535}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544602.1677046, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 31.50510000000395, "prefill_cuda_event_ms": 31.350784301757812, "kv_decode_ms": 656.6328999999769, "kv_decode_cuda_event_ms": 656.5355224609375, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 562.8830000000562, "latency_ms": 656.6328999999769, "cuda_event_ms": 656.5355224609375, "tokens_total": 64, "tokens_per_s": 97.46694081274674}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544602.1677046, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 31.50510000000395, "prefill_cuda_event_ms": 31.350784301757812, "kv_decode_ms": 656.6328999999769, "kv_decode_cuda_event_ms": 656.5355224609375, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 562.8830000000562, "latency_ms": 688.1379999999808, "cuda_event_ms": 687.8863067626953, "tokens_total": 65, "tokens_per_s": 94.45779770918305}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544603.4197037, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.81279999993967, "prefill_cuda_event_ms": 10.726400375366211, "kv_decode_ms": 603.2930999999735, "kv_decode_cuda_event_ms": 603.2537841796875, "gpu_peak_mb": 208.4775390625, "latency_ms": 10.81279999993967, "cuda_event_ms": 10.726400375366211, "tokens_total": 1, "tokens_per_s": 92.48298313161987}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544603.4197037, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.81279999993967, "prefill_cuda_event_ms": 10.726400375366211, "kv_decode_ms": 603.2930999999735, "kv_decode_cuda_event_ms": 603.2537841796875, "gpu_peak_mb": 208.4775390625, "latency_ms": 603.2930999999735, "cuda_event_ms": 603.2537841796875, "tokens_total": 64, "tokens_per_s": 106.0844223147966}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544603.4197037, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.81279999993967, "prefill_cuda_event_ms": 10.726400375366211, "kv_decode_ms": 603.2930999999735, "kv_decode_cuda_event_ms": 603.2537841796875, "gpu_peak_mb": 208.4775390625, "latency_ms": 614.1058999999132, "cuda_event_ms": 613.9801845550537, "tokens_total": 65, "tokens_per_s": 105.84493651666462}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544604.0345685, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.4250999999349006, "prefill_cuda_event_ms": 4.380671977996826, "kv_decode_ms": 434.54449999990175, "kv_decode_cuda_event_ms": 434.4893493652344, "gpu_peak_mb": 208.4775390625, "latency_ms": 4.4250999999349006, "cuda_event_ms": 4.380671977996826, "tokens_total": 1, "tokens_per_s": 225.98359359442983}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544604.0345685, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.4250999999349006, "prefill_cuda_event_ms": 4.380671977996826, "kv_decode_ms": 434.54449999990175, "kv_decode_cuda_event_ms": 434.4893493652344, "gpu_peak_mb": 208.4775390625, "latency_ms": 434.54449999990175, "cuda_event_ms": 434.4893493652344, "tokens_total": 64, "tokens_per_s": 147.28065825252528}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544604.0345685, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.4250999999349006, "prefill_cuda_event_ms": 4.380671977996826, "kv_decode_ms": 434.54449999990175, "kv_decode_cuda_event_ms": 434.4893493652344, "gpu_peak_mb": 208.4775390625, "latency_ms": 438.96959999983665, "cuda_event_ms": 438.8700213432312, "tokens_total": 65, "tokens_per_s": 148.074035195203}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544604.4742837, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.609800000040195, "prefill_cuda_event_ms": 13.528063774108887, "kv_decode_ms": 548.0210999999144, "kv_decode_cuda_event_ms": 547.9608154296875, "gpu_peak_mb": 208.4775390625, "latency_ms": 13.609800000040195, "cuda_event_ms": 13.528063774108887, "tokens_total": 1, "tokens_per_s": 73.47646548788715}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544604.4742837, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.609800000040195, "prefill_cuda_event_ms": 13.528063774108887, "kv_decode_ms": 548.0210999999144, "kv_decode_cuda_event_ms": 547.9608154296875, "gpu_peak_mb": 208.4775390625, "latency_ms": 548.0210999999144, "cuda_event_ms": 547.9608154296875, "tokens_total": 64, "tokens_per_s": 116.78382456443738}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544604.4742837, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.609800000040195, "prefill_cuda_event_ms": 13.528063774108887, "kv_decode_ms": 548.0210999999144, "kv_decode_cuda_event_ms": 547.9608154296875, "gpu_peak_mb": 208.4775390625, "latency_ms": 561.6308999999546, "cuda_event_ms": 561.4888792037964, "tokens_total": 65, "tokens_per_s": 115.73437287728515}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544605.0370438, "prompt_tokens": 30, "prefill_ms": 8525.4753, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.9607, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 37821.810299999925, "ollama_total_duration_ms": 37801.0684, "ollama_load_ms": 28920.922, "ollama_done_reason": "stop", "latency_ms": 8525.4753, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 3.518865393932934}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544605.0370438, "prompt_tokens": 30, "prefill_ms": 8525.4753, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.9607, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 37821.810299999925, "ollama_total_duration_ms": 37801.0684, "ollama_load_ms": 28920.922, "ollama_done_reason": "stop", "latency_ms": 324.9607, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.24156059486579}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544605.0370438, "prompt_tokens": 30, "prefill_ms": 8525.4753, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.9607, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 37821.810299999925, "ollama_total_duration_ms": 37801.0684, "ollama_load_ms": 28920.922, "ollama_done_reason": "stop", "latency_ms": 8850.436, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 6.666338245935003}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544642.8589704, "prompt_tokens": 30, "prefill_ms": 11.8675, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.675, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 517.2914999998284, "ollama_total_duration_ms": 492.9599, "ollama_load_ms": 127.1704, "ollama_done_reason": "stop", "latency_ms": 11.8675, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2527.9123657046557}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544642.8589704, "prompt_tokens": 30, "prefill_ms": 11.8675, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.675, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 517.2914999998284, "ollama_total_duration_ms": 492.9599, "ollama_load_ms": 127.1704, "ollama_done_reason": "stop", "latency_ms": 323.675, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.59604541592647}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544642.8589704, "prompt_tokens": 30, "prefill_ms": 11.8675, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.675, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 517.2914999998284, "ollama_total_duration_ms": 492.9599, "ollama_load_ms": 127.1704, "ollama_done_reason": "stop", "latency_ms": 335.5425, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 175.83465581856245}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544643.3770063, "prompt_tokens": 30, "prefill_ms": 12.1655, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.1912, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 510.4764999998679, "ollama_total_duration_ms": 494.9129, "ollama_load_ms": 129.7232, "ollama_done_reason": "stop", "latency_ms": 12.1655, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2465.9898894414537}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544643.3770063, "prompt_tokens": 30, "prefill_ms": 12.1655, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.1912, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 510.4764999998679, "ollama_total_duration_ms": 494.9129, "ollama_load_ms": 129.7232, "ollama_done_reason": "stop", "latency_ms": 323.1912, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.73016592035923}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544643.3770063, "prompt_tokens": 30, "prefill_ms": 12.1655, "prefill_cuda_event_ms": null, "kv_decode_ms": 323.1912, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 510.4764999998679, "ollama_total_duration_ms": 494.9129, "ollama_load_ms": 129.7232, "ollama_done_reason": "stop", "latency_ms": 335.3567, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 175.9320747132829}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544643.8875997, "prompt_tokens": 30, "prefill_ms": 11.7573, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7352, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 503.8331000000653, "ollama_total_duration_ms": 488.5082, "ollama_load_ms": 122.4961, "ollama_done_reason": "stop", "latency_ms": 11.7573, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2551.606236125641}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544643.8875997, "prompt_tokens": 30, "prefill_ms": 11.7573, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7352, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 503.8331000000653, "ollama_total_duration_ms": 488.5082, "ollama_load_ms": 122.4961, "ollama_done_reason": "stop", "latency_ms": 322.7352, "cuda_event_ms": null, "tokens_total": 29, "tokens_per_s": 89.8569477391992}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544643.8875997, "prompt_tokens": 30, "prefill_ms": 11.7573, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.7352, "kv_decode_cuda_event_ms": null, "gen_tokens": 29, "ollama_wall_ms": 503.8331000000653, "ollama_total_duration_ms": 488.5082, "ollama_load_ms": 122.4961, "ollama_done_reason": "stop", "latency_ms": 334.4925, "cuda_event_ms": null, "tokens_total": 59, "tokens_per_s": 176.38661554444417}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544644.391575, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 27.48810000002777, "prefill_cuda_event_ms": 27.35638427734375, "kv_decode_ms": 1052.725799999962, "kv_decode_cuda_event_ms": 1052.6617431640625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 313.03249999996297, "latency_ms": 27.48810000002777, "cuda_event_ms": 27.35638427734375, "tokens_total": 9, "tokens_per_s": 327.4144084164023}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544644.391575, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 27.48810000002777, "prefill_cuda_event_ms": 27.35638427734375, "kv_decode_ms": 1052.725799999962, "kv_decode_cuda_event_ms": 1052.6617431640625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 313.03249999996297, "latency_ms": 1052.725799999962, "cuda_event_ms": 1052.6617431640625, "tokens_total": 64, "tokens_per_s": 60.79455827909064}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544644.391575, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 27.48810000002777, "prefill_cuda_event_ms": 27.35638427734375, "kv_decode_ms": 1052.725799999962, "kv_decode_cuda_event_ms": 1052.6617431640625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 313.03249999996297, "latency_ms": 1080.2138999999897, "cuda_event_ms": 1080.0181274414062, "tokens_total": 73, "tokens_per_s": 67.57920815497809}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544645.7857308, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 17.333900000039648, "prefill_cuda_event_ms": 17.24518394470215, "kv_decode_ms": 1086.766099999977, "kv_decode_cuda_event_ms": 1086.7064208984375, "gpu_peak_mb": 229.62109375, "latency_ms": 17.333900000039648, "cuda_event_ms": 17.24518394470215, "tokens_total": 9, "tokens_per_s": 519.2137949324396}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544645.7857308, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.333900000039648, "prefill_cuda_event_ms": 17.24518394470215, "kv_decode_ms": 1086.766099999977, "kv_decode_cuda_event_ms": 1086.7064208984375, "gpu_peak_mb": 229.62109375, "latency_ms": 1086.766099999977, "cuda_event_ms": 1086.7064208984375, "tokens_total": 64, "tokens_per_s": 58.89031687683426}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544645.7857308, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 17.333900000039648, "prefill_cuda_event_ms": 17.24518394470215, "kv_decode_ms": 1086.766099999977, "kv_decode_cuda_event_ms": 1086.7064208984375, "gpu_peak_mb": 229.62109375, "latency_ms": 1104.1000000000167, "cuda_event_ms": 1103.9516048431396, "tokens_total": 73, "tokens_per_s": 66.11719952902716}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544646.8908806, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.53460000011364, "prefill_cuda_event_ms": 16.43008041381836, "kv_decode_ms": 947.8335000001152, "kv_decode_cuda_event_ms": 947.7867431640625, "gpu_peak_mb": 229.62109375, "latency_ms": 16.53460000011364, "cuda_event_ms": 16.43008041381836, "tokens_total": 9, "tokens_per_s": 544.3131372962239}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544646.8908806, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.53460000011364, "prefill_cuda_event_ms": 16.43008041381836, "kv_decode_ms": 947.8335000001152, "kv_decode_cuda_event_ms": 947.7867431640625, "gpu_peak_mb": 229.62109375, "latency_ms": 947.8335000001152, "cuda_event_ms": 947.7867431640625, "tokens_total": 64, "tokens_per_s": 67.52240768024365}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544646.8908806, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.53460000011364, "prefill_cuda_event_ms": 16.43008041381836, "kv_decode_ms": 947.8335000001152, "kv_decode_cuda_event_ms": 947.7867431640625, "gpu_peak_mb": 229.62109375, "latency_ms": 964.3681000002289, "cuda_event_ms": 964.2168235778809, "tokens_total": 73, "tokens_per_s": 75.69723635609958}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544647.8560095, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.589399999915258, "prefill_cuda_event_ms": 7.503871917724609, "kv_decode_ms": 445.7655999999588, "kv_decode_cuda_event_ms": 445.6908874511719, "gpu_peak_mb": 229.62109375, "latency_ms": 7.589399999915258, "cuda_event_ms": 7.503871917724609, "tokens_total": 9, "tokens_per_s": 1185.8644952302543}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544647.8560095, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.589399999915258, "prefill_cuda_event_ms": 7.503871917724609, "kv_decode_ms": 445.7655999999588, "kv_decode_cuda_event_ms": 445.6908874511719, "gpu_peak_mb": 229.62109375, "latency_ms": 445.7655999999588, "cuda_event_ms": 445.6908874511719, "tokens_total": 64, "tokens_per_s": 143.57321426329423}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544647.8560095, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.589399999915258, "prefill_cuda_event_ms": 7.503871917724609, "kv_decode_ms": 445.7655999999588, "kv_decode_cuda_event_ms": 445.6908874511719, "gpu_peak_mb": 229.62109375, "latency_ms": 453.35499999987405, "cuda_event_ms": 453.1947593688965, "tokens_total": 73, "tokens_per_s": 161.0217158739184}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544648.310111, "prompt_tokens": 25, "prefill_ms": 28.2545, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.417, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 9197.419399999944, "ollama_total_duration_ms": 9111.8688, "ollama_load_ms": 8607.1895, "ollama_done_reason": "length", "latency_ms": 28.2545, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 884.814808260631}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544648.310111, "prompt_tokens": 25, "prefill_ms": 28.2545, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.417, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 9197.419399999944, "ollama_total_duration_ms": 9111.8688, "ollama_load_ms": 8607.1895, "ollama_done_reason": "length", "latency_ms": 391.417, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 163.5084832799802}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544648.310111, "prompt_tokens": 25, "prefill_ms": 28.2545, "prefill_cuda_event_ms": null, "kv_decode_ms": 391.417, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 9197.419399999944, "ollama_total_duration_ms": 9111.8688, "ollama_load_ms": 8607.1895, "ollama_done_reason": "length", "latency_ms": 419.6715, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 212.07063143434806}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544657.5076735, "prompt_tokens": 25, "prefill_ms": 2.9065, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.2617, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 412.2884000000795, "ollama_total_duration_ms": 376.6281, "ollama_load_ms": 193.4732, "ollama_done_reason": "length", "latency_ms": 2.9065, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8601.41063134354}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544657.5076735, "prompt_tokens": 25, "prefill_ms": 2.9065, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.2617, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 412.2884000000795, "ollama_total_duration_ms": 376.6281, "ollama_load_ms": 193.4732, "ollama_done_reason": "length", "latency_ms": 135.2617, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 473.1568507567183}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544657.5076735, "prompt_tokens": 25, "prefill_ms": 2.9065, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.2617, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 412.2884000000795, "ollama_total_duration_ms": 376.6281, "ollama_load_ms": 193.4732, "ollama_done_reason": "length", "latency_ms": 138.16819999999998, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 644.1424292999402}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544657.9203334, "prompt_tokens": 25, "prefill_ms": 2.8119, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.9747, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 378.2664999998815, "ollama_total_duration_ms": 342.8233, "ollama_load_ms": 153.2375, "ollama_done_reason": "length", "latency_ms": 2.8119, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8890.785589814715}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544657.9203334, "prompt_tokens": 25, "prefill_ms": 2.8119, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.9747, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 378.2664999998815, "ollama_total_duration_ms": 342.8233, "ollama_load_ms": 153.2375, "ollama_done_reason": "length", "latency_ms": 129.9747, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 492.403521608436}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544657.9203334, "prompt_tokens": 25, "prefill_ms": 2.8119, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.9747, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 378.2664999998815, "ollama_total_duration_ms": 342.8233, "ollama_load_ms": 153.2375, "ollama_done_reason": "length", "latency_ms": 132.78660000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 670.2483533730059}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544658.2987456, "prompt_tokens": 25, "prefill_ms": 2.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1072, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 348.8909000000149, "ollama_total_duration_ms": 334.7766, "ollama_load_ms": 158.9647, "ollama_done_reason": "length", "latency_ms": 2.3721, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10539.184688672483}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544658.2987456, "prompt_tokens": 25, "prefill_ms": 2.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1072, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 348.8909000000149, "ollama_total_duration_ms": 334.7766, "ollama_load_ms": 158.9647, "ollama_done_reason": "length", "latency_ms": 134.1072, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 477.23015617356856}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544658.2987456, "prompt_tokens": 25, "prefill_ms": 2.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1072, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 348.8909000000149, "ollama_total_duration_ms": 334.7766, "ollama_load_ms": 158.9647, "ollama_done_reason": "length", "latency_ms": 136.4793, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 652.1135439586809}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544658.6477609, "prompt_tokens": 27, "prefill_ms": 19.1388, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.0797, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 979.9141999999392, "ollama_total_duration_ms": 965.089, "ollama_load_ms": 156.5388, "ollama_done_reason": "length", "latency_ms": 19.1388, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 1410.7467552824628}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544658.6477609, "prompt_tokens": 27, "prefill_ms": 19.1388, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.0797, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 979.9141999999392, "ollama_total_duration_ms": 965.089, "ollama_load_ms": 156.5388, "ollama_done_reason": "length", "latency_ms": 736.0797, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 86.94710640709151}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544658.6477609, "prompt_tokens": 27, "prefill_ms": 19.1388, "prefill_cuda_event_ms": null, "kv_decode_ms": 736.0797, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 979.9141999999392, "ollama_total_duration_ms": 965.089, "ollama_load_ms": 156.5388, "ollama_done_reason": "length", "latency_ms": 755.2185, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 120.4949296130855}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544659.627873, "prompt_tokens": 27, "prefill_ms": 11.7406, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.3007, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 958.0998999999792, "ollama_total_duration_ms": 939.4203, "ollama_load_ms": 141.2084, "ollama_done_reason": "length", "latency_ms": 11.7406, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2299.7121101136227}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544659.627873, "prompt_tokens": 27, "prefill_ms": 11.7406, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.3007, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 958.0998999999792, "ollama_total_duration_ms": 939.4203, "ollama_load_ms": 141.2084, "ollama_done_reason": "length", "latency_ms": 728.3007, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.87579086495454}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544659.627873, "prompt_tokens": 27, "prefill_ms": 11.7406, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.3007, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 958.0998999999792, "ollama_total_duration_ms": 939.4203, "ollama_load_ms": 141.2084, "ollama_done_reason": "length", "latency_ms": 740.0413, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.9661101346641}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544660.5860965, "prompt_tokens": 27, "prefill_ms": 12.0106, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.4438, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 960.0443000001633, "ollama_total_duration_ms": 957.5904, "ollama_load_ms": 159.7519, "ollama_done_reason": "length", "latency_ms": 12.0106, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2248.0142540755664}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544660.5860965, "prompt_tokens": 27, "prefill_ms": 12.0106, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.4438, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 960.0443000001633, "ollama_total_duration_ms": 957.5904, "ollama_load_ms": 159.7519, "ollama_done_reason": "length", "latency_ms": 734.4438, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.14077237768227}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544660.5860965, "prompt_tokens": 27, "prefill_ms": 12.0106, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.4438, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 960.0443000001633, "ollama_total_duration_ms": 957.5904, "ollama_load_ms": 159.7519, "ollama_done_reason": "length", "latency_ms": 746.4544, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 121.90965717396803}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544661.5462644, "prompt_tokens": 27, "prefill_ms": 11.8141, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.8793, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 975.0610000000961, "ollama_total_duration_ms": 957.7547, "ollama_load_ms": 158.6349, "ollama_done_reason": "length", "latency_ms": 11.8141, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2285.404728248449}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544661.5462644, "prompt_tokens": 27, "prefill_ms": 11.8141, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.8793, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 975.0610000000961, "ollama_total_duration_ms": 957.7547, "ollama_load_ms": 158.6349, "ollama_done_reason": "length", "latency_ms": 729.8793, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.68573105169583}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544661.5462644, "prompt_tokens": 27, "prefill_ms": 11.8141, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.8793, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 975.0610000000961, "ollama_total_duration_ms": 957.7547, "ollama_load_ms": 158.6349, "ollama_done_reason": "length", "latency_ms": 741.6934, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.6922067797826}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544662.5214846, "prompt_tokens": 17, "prefill_ms": 7.7501, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7571, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 298.3831999999893, "ollama_total_duration_ms": 275.0902, "ollama_load_ms": 151.8141, "ollama_done_reason": "stop", "latency_ms": 7.7501, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2193.5200836118247}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544662.5214846, "prompt_tokens": 17, "prefill_ms": 7.7501, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7571, "kv_decode_cuda_event_ms": null, "gen_tokens": 37, "ollama_wall_ms": 298.3831999999893, "ollama_total_duration_ms": 275.0902, "ollama_load_ms": 151.8141, "ollama_done_reason": "stop", "latency_ms": 82.7571, "cuda_event_ms": null, "tokens_total": 37, "tokens_per_s": 447.0915486405396}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544662.5214846, "prompt_tokens": 17, "prefill_ms": 7.7501, "prefill_cuda_event_ms": null, "kv_decode_ms": 82.7571, "kv_decode_cuda_event_ms": null, "gen_tokens": 37, "ollama_wall_ms": 298.3831999999893, "ollama_total_duration_ms": 275.0902, "ollama_load_ms": 151.8141, "ollama_done_reason": "stop", "latency_ms": 90.5072, "cuda_event_ms": null, "tokens_total": 54, "tokens_per_s": 596.637615570916}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544662.8199935, "prompt_tokens": 17, "prefill_ms": 2.0388, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.9096, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 328.4318999999414, "ollama_total_duration_ms": 286.8514, "ollama_load_ms": 146.5187, "ollama_done_reason": "stop", "latency_ms": 2.0388, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 8338.238179321168}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544662.8199935, "prompt_tokens": 17, "prefill_ms": 2.0388, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.9096, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 328.4318999999414, "ollama_total_duration_ms": 286.8514, "ollama_load_ms": 146.5187, "ollama_done_reason": "stop", "latency_ms": 101.9096, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 471.0056756183912}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544662.8199935, "prompt_tokens": 17, "prefill_ms": 2.0388, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.9096, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 328.4318999999414, "ollama_total_duration_ms": 286.8514, "ollama_load_ms": 146.5187, "ollama_done_reason": "stop", "latency_ms": 103.94839999999999, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 625.3102500856194}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544663.1485715, "prompt_tokens": 17, "prefill_ms": 2.773, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.4508, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 321.2068000000272, "ollama_total_duration_ms": 296.2595, "ollama_load_ms": 158.6201, "ollama_done_reason": "stop", "latency_ms": 2.773, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6130.544536602956}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544663.1485715, "prompt_tokens": 17, "prefill_ms": 2.773, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.4508, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 321.2068000000272, "ollama_total_duration_ms": 296.2595, "ollama_load_ms": 158.6201, "ollama_done_reason": "stop", "latency_ms": 103.4508, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 463.98867867624034}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544663.1485715, "prompt_tokens": 17, "prefill_ms": 2.773, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.4508, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 321.2068000000272, "ollama_total_duration_ms": 296.2595, "ollama_load_ms": 158.6201, "ollama_done_reason": "stop", "latency_ms": 106.2238, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 611.9155970695833}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544663.4699569, "prompt_tokens": 17, "prefill_ms": 2.2432, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.896, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 314.65330000014546, "ollama_total_duration_ms": 301.5032, "ollama_load_ms": 159.554, "ollama_done_reason": "stop", "latency_ms": 2.2432, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7578.45934379458}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544663.4699569, "prompt_tokens": 17, "prefill_ms": 2.2432, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.896, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 314.65330000014546, "ollama_total_duration_ms": 301.5032, "ollama_load_ms": 159.554, "ollama_done_reason": "stop", "latency_ms": 102.896, "cuda_event_ms": null, "tokens_total": 48, "tokens_per_s": 466.4904369460426}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544663.4699569, "prompt_tokens": 17, "prefill_ms": 2.2432, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.896, "kv_decode_cuda_event_ms": null, "gen_tokens": 48, "ollama_wall_ms": 314.65330000014546, "ollama_total_duration_ms": 301.5032, "ollama_load_ms": 159.554, "ollama_done_reason": "stop", "latency_ms": 105.1392, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 618.2280253226199}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544663.7847903, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 44.40049999993789, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.1759000001548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.56369999998606, "latency_ms": 44.40049999993789, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 202.7004200405984}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544663.7847903, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 44.40049999993789, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.1759000001548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.56369999998606, "latency_ms": 1279.1759000001548, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.032212145329076}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544663.7847903, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 44.40049999993789, "prefill_cuda_event_ms": null, "kv_decode_ms": 1279.1759000001548, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 238.56369999998606, "latency_ms": 1323.5764000000927, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 55.15359748027759}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544665.3476732, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.5727000000079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1239.9339999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 39.5727000000079, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 227.42951580251545}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544665.3476732, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.5727000000079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1239.9339999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1239.9339999999484, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.615650510432545}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544665.3476732, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.5727000000079, "prefill_cuda_event_ms": null, "kv_decode_ms": 1239.9339999999484, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1279.5066999999563, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.053237782969404}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544666.6277127, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.90980000005584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1226.9154000000526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 38.90980000005584, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 231.30419585778094}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544666.6277127, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.90980000005584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1226.9154000000526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1226.9154000000526, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.1633357931584}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544666.6277127, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.90980000005584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1226.9154000000526, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1265.8252000001085, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.669889965845}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544667.8941555, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.75290000005771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1220.1998999998978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.75290000005771, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 258.97119377044953}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544667.8941555, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.75290000005771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1220.1998999998978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1220.1998999998978, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.450422262782816}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544667.8941555, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.75290000005771, "prefill_cuda_event_ms": null, "kv_decode_ms": 1220.1998999998978, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1254.9527999999555, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.169518407387585}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544669.149812, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.606000000185304, "prefill_cuda_event_ms": 14.050304412841797, "kv_decode_ms": 248.13319999998384, "kv_decode_cuda_event_ms": 248.0517120361328, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 644.8665000000346, "latency_ms": 14.606000000185304, "cuda_event_ms": 14.050304412841797, "tokens_total": 1, "tokens_per_s": 68.46501437678441}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544669.149812, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.606000000185304, "prefill_cuda_event_ms": 14.050304412841797, "kv_decode_ms": 248.13319999998384, "kv_decode_cuda_event_ms": 248.0517120361328, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 644.8665000000346, "latency_ms": 248.13319999998384, "cuda_event_ms": 248.0517120361328, "tokens_total": 64, "tokens_per_s": 257.92598491457073}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544669.149812, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.606000000185304, "prefill_cuda_event_ms": 14.050304412841797, "kv_decode_ms": 248.13319999998384, "kv_decode_cuda_event_ms": 248.0517120361328, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 644.8665000000346, "latency_ms": 262.73920000016915, "cuda_event_ms": 262.1020164489746, "tokens_total": 65, "tokens_per_s": 247.39361313408185}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544670.05905, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.7668999998459185, "prefill_cuda_event_ms": 3.717087984085083, "kv_decode_ms": 189.994200000001, "kv_decode_cuda_event_ms": 189.94371032714844, "gpu_peak_mb": 379.8994140625, "latency_ms": 3.7668999998459185, "cuda_event_ms": 3.717087984085083, "tokens_total": 1, "tokens_per_s": 265.47028061294543}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544670.05905, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7668999998459185, "prefill_cuda_event_ms": 3.717087984085083, "kv_decode_ms": 189.994200000001, "kv_decode_cuda_event_ms": 189.94371032714844, "gpu_peak_mb": 379.8994140625, "latency_ms": 189.994200000001, "cuda_event_ms": 189.94371032714844, "tokens_total": 64, "tokens_per_s": 336.85238812553047}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544670.05905, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.7668999998459185, "prefill_cuda_event_ms": 3.717087984085083, "kv_decode_ms": 189.994200000001, "kv_decode_cuda_event_ms": 189.94371032714844, "gpu_peak_mb": 379.8994140625, "latency_ms": 193.76109999984692, "cuda_event_ms": 193.66079831123352, "tokens_total": 65, "tokens_per_s": 335.46465208987433}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544670.2534468, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.501200000073368, "prefill_cuda_event_ms": 3.4526400566101074, "kv_decode_ms": 249.6755999998186, "kv_decode_cuda_event_ms": 249.65017700195312, "gpu_peak_mb": 379.8994140625, "latency_ms": 3.501200000073368, "cuda_event_ms": 3.4526400566101074, "tokens_total": 1, "tokens_per_s": 285.6163600991217}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544670.2534468, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.501200000073368, "prefill_cuda_event_ms": 3.4526400566101074, "kv_decode_ms": 249.6755999998186, "kv_decode_cuda_event_ms": 249.65017700195312, "gpu_peak_mb": 379.8994140625, "latency_ms": 249.6755999998186, "cuda_event_ms": 249.65017700195312, "tokens_total": 64, "tokens_per_s": 256.33261720427026}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544670.2534468, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.501200000073368, "prefill_cuda_event_ms": 3.4526400566101074, "kv_decode_ms": 249.6755999998186, "kv_decode_cuda_event_ms": 249.65017700195312, "gpu_peak_mb": 379.8994140625, "latency_ms": 253.17679999989195, "cuda_event_ms": 253.10281705856323, "tokens_total": 65, "tokens_per_s": 256.7375841705391}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544670.507238, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.576199999997698, "prefill_cuda_event_ms": 3.5287039279937744, "kv_decode_ms": 223.89880000014273, "kv_decode_cuda_event_ms": 223.86688232421875, "gpu_peak_mb": 379.8994140625, "latency_ms": 3.576199999997698, "cuda_event_ms": 3.5287039279937744, "tokens_total": 1, "tokens_per_s": 279.62641910425697}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544670.507238, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.576199999997698, "prefill_cuda_event_ms": 3.5287039279937744, "kv_decode_ms": 223.89880000014273, "kv_decode_cuda_event_ms": 223.86688232421875, "gpu_peak_mb": 379.8994140625, "latency_ms": 223.89880000014273, "cuda_event_ms": 223.86688232421875, "tokens_total": 64, "tokens_per_s": 285.84342569035294}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544670.507238, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.576199999997698, "prefill_cuda_event_ms": 3.5287039279937744, "kv_decode_ms": 223.89880000014273, "kv_decode_cuda_event_ms": 223.86688232421875, "gpu_peak_mb": 379.8994140625, "latency_ms": 227.47500000014043, "cuda_event_ms": 227.39558625221252, "tokens_total": 65, "tokens_per_s": 285.7456863389817}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544670.7354214, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 60.92820000003485, "prefill_cuda_event_ms": 60.82236862182617, "kv_decode_ms": 772.2445000001699, "kv_decode_cuda_event_ms": 772.1922607421875, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 480.3844000000481, "latency_ms": 60.92820000003485, "cuda_event_ms": 60.82236862182617, "tokens_total": 17, "tokens_per_s": 279.01694125200277}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544670.7354214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.92820000003485, "prefill_cuda_event_ms": 60.82236862182617, "kv_decode_ms": 772.2445000001699, "kv_decode_cuda_event_ms": 772.1922607421875, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 480.3844000000481, "latency_ms": 772.2445000001699, "cuda_event_ms": 772.1922607421875, "tokens_total": 64, "tokens_per_s": 82.87530697853585}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544670.7354214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 60.92820000003485, "prefill_cuda_event_ms": 60.82236862182617, "kv_decode_ms": 772.2445000001699, "kv_decode_cuda_event_ms": 772.1922607421875, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 480.3844000000481, "latency_ms": 833.1727000002047, "cuda_event_ms": 833.0146293640137, "tokens_total": 81, "tokens_per_s": 97.2187398842762}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544672.0498505, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.315700000021025, "prefill_cuda_event_ms": 13.245439529418945, "kv_decode_ms": 549.5789999999943, "kv_decode_cuda_event_ms": 549.5469970703125, "gpu_peak_mb": 489.4736328125, "latency_ms": 13.315700000021025, "cuda_event_ms": 13.245439529418945, "tokens_total": 17, "tokens_per_s": 1276.6884204340108}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544672.0498505, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.315700000021025, "prefill_cuda_event_ms": 13.245439529418945, "kv_decode_ms": 549.5789999999943, "kv_decode_cuda_event_ms": 549.5469970703125, "gpu_peak_mb": 489.4736328125, "latency_ms": 549.5789999999943, "cuda_event_ms": 549.5469970703125, "tokens_total": 64, "tokens_per_s": 116.45277567010505}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544672.0498505, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.315700000021025, "prefill_cuda_event_ms": 13.245439529418945, "kv_decode_ms": 549.5789999999943, "kv_decode_cuda_event_ms": 549.5469970703125, "gpu_peak_mb": 489.4736328125, "latency_ms": 562.8947000000153, "cuda_event_ms": 562.7924365997314, "tokens_total": 81, "tokens_per_s": 143.89902765117134}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544672.6137793, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.116200000202298, "prefill_cuda_event_ms": 5.071872234344482, "kv_decode_ms": 451.54619999993884, "kv_decode_cuda_event_ms": 451.5041198730469, "gpu_peak_mb": 489.4736328125, "latency_ms": 5.116200000202298, "cuda_event_ms": 5.071872234344482, "tokens_total": 17, "tokens_per_s": 3322.77862462918}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544672.6137793, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.116200000202298, "prefill_cuda_event_ms": 5.071872234344482, "kv_decode_ms": 451.54619999993884, "kv_decode_cuda_event_ms": 451.5041198730469, "gpu_peak_mb": 489.4736328125, "latency_ms": 451.54619999993884, "cuda_event_ms": 451.5041198730469, "tokens_total": 64, "tokens_per_s": 141.73522000629984}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544672.6137793, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.116200000202298, "prefill_cuda_event_ms": 5.071872234344482, "kv_decode_ms": 451.54619999993884, "kv_decode_cuda_event_ms": 451.5041198730469, "gpu_peak_mb": 489.4736328125, "latency_ms": 456.66240000014113, "cuda_event_ms": 456.57599210739136, "tokens_total": 81, "tokens_per_s": 177.37391998985458}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544673.0711572, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.0722000000860135, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 279.4280999999046, "kv_decode_cuda_event_ms": 279.40142822265625, "gpu_peak_mb": 489.4736328125, "latency_ms": 5.0722000000860135, "cuda_event_ms": 5.01964807510376, "tokens_total": 17, "tokens_per_s": 3351.602854720184}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544673.0711572, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.0722000000860135, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 279.4280999999046, "kv_decode_cuda_event_ms": 279.40142822265625, "gpu_peak_mb": 489.4736328125, "latency_ms": 279.4280999999046, "cuda_event_ms": 279.40142822265625, "tokens_total": 64, "tokens_per_s": 229.03924122170196}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544673.0711572, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.0722000000860135, "prefill_cuda_event_ms": 5.01964807510376, "kv_decode_ms": 279.4280999999046, "kv_decode_cuda_event_ms": 279.40142822265625, "gpu_peak_mb": 489.4736328125, "latency_ms": 284.5002999999906, "cuda_event_ms": 284.42107629776, "tokens_total": 81, "tokens_per_s": 284.70971735355874}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544673.3564107, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 10.734700000057273, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.0295999998034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 183.5759999999027, "latency_ms": 10.734700000057273, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 838.4025636442548}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544673.3564107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.734700000057273, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.0295999998034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 183.5759999999027, "latency_ms": 184.0295999998034, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 347.7701413254627}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544673.3564107, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 10.734700000057273, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.0295999998034, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 183.5759999999027, "latency_ms": 194.76429999986067, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 374.8120163708248}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544673.7354379, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.403500000056738, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.99680000014087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 5.403500000056738, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1665.5871194421204}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544673.7354379, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.403500000056738, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.99680000014087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 203.99680000014087, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 313.730411457218}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544673.7354379, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.403500000056738, "prefill_cuda_event_ms": null, "kv_decode_ms": 203.99680000014087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 209.4003000001976, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 348.6145912872671}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544673.9455621, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.490299999815761, "prefill_cuda_event_ms": null, "kv_decode_ms": 185.17689999998765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 4.490299999815761, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2004.320424107359}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544673.9455621, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.490299999815761, "prefill_cuda_event_ms": null, "kv_decode_ms": 185.17689999998765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 185.17689999998765, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 345.61546283583033}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544673.9455621, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.490299999815761, "prefill_cuda_event_ms": null, "kv_decode_ms": 185.17689999998765, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 189.66719999980342, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 384.8846822227336}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544674.136199, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.547499999944193, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.54499999995278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 4.547499999944193, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1979.1094007939412}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544674.136199, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.547499999944193, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.54499999995278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 189.54499999995278, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 337.6506898098918}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544674.136199, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.547499999944193, "prefill_cuda_event_ms": null, "kv_decode_ms": 189.54499999995278, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 194.09249999989697, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 376.1093293148306}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544674.3307872, "prompt_tokens": 38, "prefill_ms": 8003.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 433.2732, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 8625.902499999938, "ollama_total_duration_ms": 8622.5807, "ollama_load_ms": 150.8658, "ollama_done_reason": "stop", "latency_ms": 8003.2721, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 4.748057984933437}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544674.3307872, "prompt_tokens": 38, "prefill_ms": 8003.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 433.2732, "kv_decode_cuda_event_ms": null, "gen_tokens": 36, "ollama_wall_ms": 8625.902499999938, "ollama_total_duration_ms": 8622.5807, "ollama_load_ms": 150.8658, "ollama_done_reason": "stop", "latency_ms": 433.2732, "cuda_event_ms": null, "tokens_total": 36, "tokens_per_s": 83.08845319765912}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544674.3307872, "prompt_tokens": 38, "prefill_ms": 8003.2721, "prefill_cuda_event_ms": null, "kv_decode_ms": 433.2732, "kv_decode_cuda_event_ms": null, "gen_tokens": 36, "ollama_wall_ms": 8625.902499999938, "ollama_total_duration_ms": 8622.5807, "ollama_load_ms": 150.8658, "ollama_done_reason": "stop", "latency_ms": 8436.5453, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 8.771362846827836}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544682.9568174, "prompt_tokens": 38, "prefill_ms": 11.9686, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.9953, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 586.6596999999274, "ollama_total_duration_ms": 571.5705, "ollama_load_ms": 130.2552, "ollama_done_reason": "stop", "latency_ms": 11.9686, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3174.9745166519056}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544682.9568174, "prompt_tokens": 38, "prefill_ms": 11.9686, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.9953, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 586.6596999999274, "ollama_total_duration_ms": 571.5705, "ollama_load_ms": 130.2552, "ollama_done_reason": "stop", "latency_ms": 393.9953, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 88.83354699916471}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544682.9568174, "prompt_tokens": 38, "prefill_ms": 11.9686, "prefill_cuda_event_ms": null, "kv_decode_ms": 393.9953, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 586.6596999999274, "ollama_total_duration_ms": 571.5705, "ollama_load_ms": 130.2552, "ollama_done_reason": "stop", "latency_ms": 405.96389999999997, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 179.81894449235512}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544683.5435867, "prompt_tokens": 38, "prefill_ms": 11.9035, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.1699, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 581.071300000076, "ollama_total_duration_ms": 566.9177, "ollama_load_ms": 127.57, "ollama_done_reason": "stop", "latency_ms": 11.9035, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3192.3383878691143}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544683.5435867, "prompt_tokens": 38, "prefill_ms": 11.9035, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.1699, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 581.071300000076, "ollama_total_duration_ms": 566.9177, "ollama_load_ms": 127.57, "ollama_done_reason": "stop", "latency_ms": 392.1699, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 89.2470329823885}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544683.5435867, "prompt_tokens": 38, "prefill_ms": 11.9035, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.1699, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 581.071300000076, "ollama_total_duration_ms": 566.9177, "ollama_load_ms": 127.57, "ollama_done_reason": "stop", "latency_ms": 404.0734, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 180.66024638097932}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544684.1247754, "prompt_tokens": 38, "prefill_ms": 12.1943, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.3284, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 576.4788999999837, "ollama_total_duration_ms": 572.5062, "ollama_load_ms": 129.7535, "ollama_done_reason": "stop", "latency_ms": 12.1943, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3116.210032556194}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544684.1247754, "prompt_tokens": 38, "prefill_ms": 12.1943, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.3284, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 576.4788999999837, "ollama_total_duration_ms": 572.5062, "ollama_load_ms": 129.7535, "ollama_done_reason": "stop", "latency_ms": 396.3284, "cuda_event_ms": null, "tokens_total": 35, "tokens_per_s": 88.31060302516802}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544684.1247754, "prompt_tokens": 38, "prefill_ms": 12.1943, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.3284, "kv_decode_cuda_event_ms": null, "gen_tokens": 35, "ollama_wall_ms": 576.4788999999837, "ollama_total_duration_ms": 572.5062, "ollama_load_ms": 129.7535, "ollama_done_reason": "stop", "latency_ms": 408.5227, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 178.69264058031538}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544684.7014077, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 78.46330000006674, "prefill_cuda_event_ms": 78.34111785888672, "kv_decode_ms": 677.6675999999497, "kv_decode_cuda_event_ms": 677.6361083984375, "gpu_peak_mb": 489.1572265625, "latency_ms": 78.46330000006674, "cuda_event_ms": 78.34111785888672, "tokens_total": 9, "tokens_per_s": 114.70330715114385}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544684.7014077, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 78.46330000006674, "prefill_cuda_event_ms": 78.34111785888672, "kv_decode_ms": 677.6675999999497, "kv_decode_cuda_event_ms": 677.6361083984375, "gpu_peak_mb": 489.1572265625, "latency_ms": 677.6675999999497, "cuda_event_ms": 677.6361083984375, "tokens_total": 64, "tokens_per_s": 94.44158168400665}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544684.7014077, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 78.46330000006674, "prefill_cuda_event_ms": 78.34111785888672, "kv_decode_ms": 677.6675999999497, "kv_decode_cuda_event_ms": 677.6361083984375, "gpu_peak_mb": 489.1572265625, "latency_ms": 756.1309000000165, "cuda_event_ms": 755.9772262573242, "tokens_total": 73, "tokens_per_s": 96.544130123499}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544685.4585385, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.35530000001927, "prefill_cuda_event_ms": 5.311327934265137, "kv_decode_ms": 418.3837999999014, "kv_decode_cuda_event_ms": 418.3561096191406, "gpu_peak_mb": 489.1572265625, "latency_ms": 5.35530000001927, "cuda_event_ms": 5.311327934265137, "tokens_total": 9, "tokens_per_s": 1680.5781188668452}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544685.4585385, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.35530000001927, "prefill_cuda_event_ms": 5.311327934265137, "kv_decode_ms": 418.3837999999014, "kv_decode_cuda_event_ms": 418.3561096191406, "gpu_peak_mb": 489.1572265625, "latency_ms": 418.3837999999014, "cuda_event_ms": 418.3561096191406, "tokens_total": 64, "tokens_per_s": 152.9695939470292}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544685.4585385, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.35530000001927, "prefill_cuda_event_ms": 5.311327934265137, "kv_decode_ms": 418.3837999999014, "kv_decode_cuda_event_ms": 418.3561096191406, "gpu_peak_mb": 489.1572265625, "latency_ms": 423.73909999992065, "cuda_event_ms": 423.66743755340576, "tokens_total": 73, "tokens_per_s": 172.27581783227856}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544685.8829377, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.689699999948061, "prefill_cuda_event_ms": 5.641215801239014, "kv_decode_ms": 425.65620000004856, "kv_decode_cuda_event_ms": 425.63055419921875, "gpu_peak_mb": 489.1572265625, "latency_ms": 5.689699999948061, "cuda_event_ms": 5.641215801239014, "tokens_total": 9, "tokens_per_s": 1581.8057191208952}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544685.8829377, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.689699999948061, "prefill_cuda_event_ms": 5.641215801239014, "kv_decode_ms": 425.65620000004856, "kv_decode_cuda_event_ms": 425.63055419921875, "gpu_peak_mb": 489.1572265625, "latency_ms": 425.65620000004856, "cuda_event_ms": 425.63055419921875, "tokens_total": 64, "tokens_per_s": 150.3560854980914}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544685.8829377, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.689699999948061, "prefill_cuda_event_ms": 5.641215801239014, "kv_decode_ms": 425.65620000004856, "kv_decode_cuda_event_ms": 425.63055419921875, "gpu_peak_mb": 489.1572265625, "latency_ms": 431.3458999999966, "cuda_event_ms": 431.27177000045776, "tokens_total": 73, "tokens_per_s": 169.2377277725384}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544686.3148992, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.148400000029142, "prefill_cuda_event_ms": 6.102816104888916, "kv_decode_ms": 490.40790000003653, "kv_decode_cuda_event_ms": 490.3648986816406, "gpu_peak_mb": 489.1572265625, "latency_ms": 6.148400000029142, "cuda_event_ms": 6.102816104888916, "tokens_total": 9, "tokens_per_s": 1463.7954589742603}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544686.3148992, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.148400000029142, "prefill_cuda_event_ms": 6.102816104888916, "kv_decode_ms": 490.40790000003653, "kv_decode_cuda_event_ms": 490.3648986816406, "gpu_peak_mb": 489.1572265625, "latency_ms": 490.40790000003653, "cuda_event_ms": 490.3648986816406, "tokens_total": 64, "tokens_per_s": 130.50360730321682}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544686.3148992, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.148400000029142, "prefill_cuda_event_ms": 6.102816104888916, "kv_decode_ms": 490.40790000003653, "kv_decode_cuda_event_ms": 490.3648986816406, "gpu_peak_mb": 489.1572265625, "latency_ms": 496.5563000000657, "cuda_event_ms": 496.46771478652954, "tokens_total": 73, "tokens_per_s": 147.01253412753064}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544686.8121297, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.134600000066712, "prefill_cuda_event_ms": 3.0556159019470215, "kv_decode_ms": 155.3518000000622, "kv_decode_cuda_event_ms": 155.30905151367188, "gpu_peak_mb": 539.3515625, "hf_load_ms": 274.3307000000641, "latency_ms": 3.134600000066712, "cuda_event_ms": 3.0556159019470215, "tokens_total": 17, "tokens_per_s": 5423.339500937344}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544686.8121297, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.134600000066712, "prefill_cuda_event_ms": 3.0556159019470215, "kv_decode_ms": 155.3518000000622, "kv_decode_cuda_event_ms": 155.30905151367188, "gpu_peak_mb": 539.3515625, "hf_load_ms": 274.3307000000641, "latency_ms": 155.3518000000622, "cuda_event_ms": 155.30905151367188, "tokens_total": 64, "tokens_per_s": 411.9681909058947}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544686.8121297, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.134600000066712, "prefill_cuda_event_ms": 3.0556159019470215, "kv_decode_ms": 155.3518000000622, "kv_decode_cuda_event_ms": 155.30905151367188, "gpu_peak_mb": 539.3515625, "hf_load_ms": 274.3307000000641, "latency_ms": 158.48640000012892, "cuda_event_ms": 158.3646674156189, "tokens_total": 81, "tokens_per_s": 511.08486280169217}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.2458441, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.53740000016478, "prefill_cuda_event_ms": 2.490367889404297, "kv_decode_ms": 197.7044999998725, "kv_decode_cuda_event_ms": 197.65554809570312, "gpu_peak_mb": 539.3515625, "latency_ms": 2.53740000016478, "cuda_event_ms": 2.490367889404297, "tokens_total": 17, "tokens_per_s": 6699.771419128246}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.2458441, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.53740000016478, "prefill_cuda_event_ms": 2.490367889404297, "kv_decode_ms": 197.7044999998725, "kv_decode_cuda_event_ms": 197.65554809570312, "gpu_peak_mb": 539.3515625, "latency_ms": 197.7044999998725, "cuda_event_ms": 197.65554809570312, "tokens_total": 64, "tokens_per_s": 323.7154440088176}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.2458441, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.53740000016478, "prefill_cuda_event_ms": 2.490367889404297, "kv_decode_ms": 197.7044999998725, "kv_decode_cuda_event_ms": 197.65554809570312, "gpu_peak_mb": 539.3515625, "latency_ms": 200.24190000003728, "cuda_event_ms": 200.14591598510742, "tokens_total": 81, "tokens_per_s": 404.5107442547485}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.4469762, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.1339000000753, "prefill_cuda_event_ms": 8.011775970458984, "kv_decode_ms": 141.4092999998502, "kv_decode_cuda_event_ms": 141.3570556640625, "gpu_peak_mb": 539.3515625, "latency_ms": 8.1339000000753, "cuda_event_ms": 8.011775970458984, "tokens_total": 17, "tokens_per_s": 2090.018318376501}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.4469762, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.1339000000753, "prefill_cuda_event_ms": 8.011775970458984, "kv_decode_ms": 141.4092999998502, "kv_decode_cuda_event_ms": 141.3570556640625, "gpu_peak_mb": 539.3515625, "latency_ms": 141.4092999998502, "cuda_event_ms": 141.3570556640625, "tokens_total": 64, "tokens_per_s": 452.5869232085004}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.4469762, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.1339000000753, "prefill_cuda_event_ms": 8.011775970458984, "kv_decode_ms": 141.4092999998502, "kv_decode_cuda_event_ms": 141.3570556640625, "gpu_peak_mb": 539.3515625, "latency_ms": 149.5431999999255, "cuda_event_ms": 149.36883163452148, "tokens_total": 81, "tokens_per_s": 541.6495032876143}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.5972512, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.7064000000791566, "prefill_cuda_event_ms": 2.649087905883789, "kv_decode_ms": 126.2877000001481, "kv_decode_cuda_event_ms": 126.24896240234375, "gpu_peak_mb": 539.3515625, "latency_ms": 2.7064000000791566, "cuda_event_ms": 2.649087905883789, "tokens_total": 17, "tokens_per_s": 6281.407034992161}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.5972512, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7064000000791566, "prefill_cuda_event_ms": 2.649087905883789, "kv_decode_ms": 126.2877000001481, "kv_decode_cuda_event_ms": 126.24896240234375, "gpu_peak_mb": 539.3515625, "latency_ms": 126.2877000001481, "cuda_event_ms": 126.24896240234375, "tokens_total": 64, "tokens_per_s": 506.7793617266364}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.5972512, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7064000000791566, "prefill_cuda_event_ms": 2.649087905883789, "kv_decode_ms": 126.2877000001481, "kv_decode_cuda_event_ms": 126.24896240234375, "gpu_peak_mb": 539.3515625, "latency_ms": 128.99410000022726, "cuda_event_ms": 128.89805030822754, "tokens_total": 81, "tokens_per_s": 627.9356962826772}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.726842, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.8822000001582637, "prefill_cuda_event_ms": 2.8056960105895996, "kv_decode_ms": 123.2611000000361, "kv_decode_cuda_event_ms": 123.1800308227539, "gpu_peak_mb": 537.74755859375, "latency_ms": 2.8822000001582637, "cuda_event_ms": 2.8056960105895996, "tokens_total": 1, "tokens_per_s": 346.9571854642597}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.726842, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.8822000001582637, "prefill_cuda_event_ms": 2.8056960105895996, "kv_decode_ms": 123.2611000000361, "kv_decode_cuda_event_ms": 123.1800308227539, "gpu_peak_mb": 537.74755859375, "latency_ms": 123.2611000000361, "cuda_event_ms": 123.1800308227539, "tokens_total": 64, "tokens_per_s": 519.2230152090259}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.726842, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.8822000001582637, "prefill_cuda_event_ms": 2.8056960105895996, "kv_decode_ms": 123.2611000000361, "kv_decode_cuda_event_ms": 123.1800308227539, "gpu_peak_mb": 537.74755859375, "latency_ms": 126.14330000019436, "cuda_event_ms": 125.9857268333435, "tokens_total": 65, "tokens_per_s": 515.2869791728918}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.8537948, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.614900000025955, "prefill_cuda_event_ms": 2.5384960174560547, "kv_decode_ms": 117.53679999992528, "kv_decode_cuda_event_ms": 117.47942352294922, "gpu_peak_mb": 537.74755859375, "latency_ms": 2.614900000025955, "cuda_event_ms": 2.5384960174560547, "tokens_total": 1, "tokens_per_s": 382.42380205364424}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.8537948, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.614900000025955, "prefill_cuda_event_ms": 2.5384960174560547, "kv_decode_ms": 117.53679999992528, "kv_decode_cuda_event_ms": 117.47942352294922, "gpu_peak_mb": 537.74755859375, "latency_ms": 117.53679999992528, "cuda_event_ms": 117.47942352294922, "tokens_total": 64, "tokens_per_s": 544.5103150676272}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.8537948, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.614900000025955, "prefill_cuda_event_ms": 2.5384960174560547, "kv_decode_ms": 117.53679999992528, "kv_decode_cuda_event_ms": 117.47942352294922, "gpu_peak_mb": 537.74755859375, "latency_ms": 120.15169999995123, "cuda_event_ms": 120.01791954040527, "tokens_total": 65, "tokens_per_s": 540.9827742764054}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544687.9747167, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.6636000000053173, "prefill_cuda_event_ms": 2.5917439460754395, "kv_decode_ms": 246.00680000003194, "kv_decode_cuda_event_ms": 245.93203735351562, "gpu_peak_mb": 537.74755859375, "latency_ms": 2.6636000000053173, "cuda_event_ms": 2.5917439460754395, "tokens_total": 1, "tokens_per_s": 375.4317465077353}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544687.9747167, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.6636000000053173, "prefill_cuda_event_ms": 2.5917439460754395, "kv_decode_ms": 246.00680000003194, "kv_decode_cuda_event_ms": 245.93203735351562, "gpu_peak_mb": 537.74755859375, "latency_ms": 246.00680000003194, "cuda_event_ms": 245.93203735351562, "tokens_total": 64, "tokens_per_s": 260.15541033821705}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544687.9747167, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.6636000000053173, "prefill_cuda_event_ms": 2.5917439460754395, "kv_decode_ms": 246.00680000003194, "kv_decode_cuda_event_ms": 245.93203735351562, "gpu_peak_mb": 537.74755859375, "latency_ms": 248.67040000003726, "cuda_event_ms": 248.52378129959106, "tokens_total": 65, "tokens_per_s": 261.39017752008385}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544688.224071, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.6170000001002336, "prefill_cuda_event_ms": 2.5630719661712646, "kv_decode_ms": 198.33149999999478, "kv_decode_cuda_event_ms": 198.28941345214844, "gpu_peak_mb": 537.74755859375, "latency_ms": 2.6170000001002336, "cuda_event_ms": 2.5630719661712646, "tokens_total": 1, "tokens_per_s": 382.1169277652652}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544688.224071, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.6170000001002336, "prefill_cuda_event_ms": 2.5630719661712646, "kv_decode_ms": 198.33149999999478, "kv_decode_cuda_event_ms": 198.28941345214844, "gpu_peak_mb": 537.74755859375, "latency_ms": 198.33149999999478, "cuda_event_ms": 198.28941345214844, "tokens_total": 64, "tokens_per_s": 322.69205849802825}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544688.224071, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.6170000001002336, "prefill_cuda_event_ms": 2.5630719661712646, "kv_decode_ms": 198.33149999999478, "kv_decode_cuda_event_ms": 198.28941345214844, "gpu_peak_mb": 537.74755859375, "latency_ms": 200.948500000095, "cuda_event_ms": 200.8524854183197, "tokens_total": 65, "tokens_per_s": 323.465962671875}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544688.4256935, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.374199999954726, "prefill_cuda_event_ms": null, "kv_decode_ms": 940.528200000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 165.3500999998414, "latency_ms": 36.374199999954726, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 247.42812213082905}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544688.4256935, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.374199999954726, "prefill_cuda_event_ms": null, "kv_decode_ms": 940.528200000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 165.3500999998414, "latency_ms": 940.528200000017, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.04686983335411}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544688.4256935, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.374199999954726, "prefill_cuda_event_ms": null, "kv_decode_ms": 940.528200000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 165.3500999998414, "latency_ms": 976.9023999999717, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 74.72599105089937}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544689.5686982, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 24.32819999989988, "prefill_cuda_event_ms": null, "kv_decode_ms": 970.9953999999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 24.32819999989988, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 369.9410560599238}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544689.5686982, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.32819999989988, "prefill_cuda_event_ms": null, "kv_decode_ms": 970.9953999999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 970.9953999999925, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.91174376315325}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544689.5686982, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 24.32819999989988, "prefill_cuda_event_ms": null, "kv_decode_ms": 970.9953999999925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 995.3235999998924, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.34298111690298}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544690.5650074, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.84860000008848, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.6216999999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 22.84860000008848, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 393.89721908410786}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544690.5650074, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.84860000008848, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.6216999999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 972.6216999999906, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.80153414220618}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544690.5650074, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.84860000008848, "prefill_cuda_event_ms": null, "kv_decode_ms": 972.6216999999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 995.4703000000791, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.33217274286757}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544691.5613196, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.14480000000185, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.4224999998423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 21.14480000000185, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 425.63656312659435}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544691.5613196, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.14480000000185, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.4224999998423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 988.4224999998423, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.74963894489473}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544691.5613196, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.14480000000185, "prefill_cuda_event_ms": null, "kv_decode_ms": 988.4224999998423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1009.5672999998442, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 72.30820570358337}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544692.5714715, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 869.0326000000823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2751.6657000001032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 365.5564000000595, "latency_ms": 869.0326000000823, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 19.561981909537558}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544692.5714715, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 869.0326000000823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2751.6657000001032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 365.5564000000595, "latency_ms": 2751.6657000001032, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.258639303458118}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544692.5714715, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 869.0326000000823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2751.6657000001032, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 365.5564000000595, "latency_ms": 3620.6983000001856, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 22.371375157105977}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544696.559027, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.57160000003569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2141.2536000000273, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 49.57160000003569, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 342.93829531400564}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544696.559027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.57160000003569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2141.2536000000273, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2141.2536000000273, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.88903322801147}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544696.559027, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.57160000003569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2141.2536000000273, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2190.825200000063, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 36.97237004576982}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544698.7504828, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 46.97250000003805, "prefill_cuda_event_ms": null, "kv_decode_ms": 2235.938699999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 46.97250000003805, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 361.91388578394225}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544698.7504828, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 46.97250000003805, "prefill_cuda_event_ms": null, "kv_decode_ms": 2235.938699999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2235.938699999906, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.623324959670267}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544698.7504828, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 46.97250000003805, "prefill_cuda_event_ms": null, "kv_decode_ms": 2235.938699999906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2282.911199999944, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 35.48101213923782}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544701.0339236, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.87370000003466, "prefill_cuda_event_ms": null, "kv_decode_ms": 2151.8621999998686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 49.87370000003466, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 340.8610149234604}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544701.0339236, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.87370000003466, "prefill_cuda_event_ms": null, "kv_decode_ms": 2151.8621999998686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2151.8621999998686, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.74168141436004}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544701.0339236, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.87370000003466, "prefill_cuda_event_ms": null, "kv_decode_ms": 2151.8621999998686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2201.7358999999033, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 36.78915350383466}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544703.236119, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 21.847700000080295, "prefill_cuda_event_ms": null, "kv_decode_ms": 1399.8904999998558, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 21.847700000080295, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 45.77140843184064}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544703.236119, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.847700000080295, "prefill_cuda_event_ms": null, "kv_decode_ms": 1399.8904999998558, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1399.8904999998558, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 45.717861504172355}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544703.236119, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.847700000080295, "prefill_cuda_event_ms": null, "kv_decode_ms": 1399.8904999998558, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1421.7381999999361, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 45.71868435412576}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544704.6583447, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 20.709499999838954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1431.4232000001539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 20.709499999838954, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 48.287018035576736}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544704.6583447, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.709499999838954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1431.4232000001539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1431.4232000001539, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 44.71074661916414}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544704.6583447, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.709499999838954, "prefill_cuda_event_ms": null, "kv_decode_ms": 1431.4232000001539, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1452.1326999999928, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 44.76174939108549}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544706.1108503, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 19.83329999984562, "prefill_cuda_event_ms": null, "kv_decode_ms": 1438.9846999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 19.83329999984562, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 50.42025280754004}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544706.1108503, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.83329999984562, "prefill_cuda_event_ms": null, "kv_decode_ms": 1438.9846999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1438.9846999999918, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 44.475802974138894}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544706.1108503, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.83329999984562, "prefill_cuda_event_ms": null, "kv_decode_ms": 1438.9846999999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1458.8179999998374, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 44.55662049687298}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544707.5700967, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 20.62120000005052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1423.3610999999655, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 20.62120000005052, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 48.49378309688816}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544707.5700967, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.62120000005052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1423.3610999999655, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1423.3610999999655, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 44.963994027939606}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544707.5700967, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.62120000005052, "prefill_cuda_event_ms": null, "kv_decode_ms": 1423.3610999999655, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1443.982300000016, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 45.014402184846226}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544709.0146835, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 49.17590000013661, "prefill_cuda_event_ms": null, "kv_decode_ms": 2664.1041999998833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 49.17590000013661, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 183.01647758302337}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544709.0146835, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 49.17590000013661, "prefill_cuda_event_ms": null, "kv_decode_ms": 2664.1041999998833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2664.1041999998833, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.023084382361173}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544709.0146835, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 49.17590000013661, "prefill_cuda_event_ms": null, "kv_decode_ms": 2664.1041999998833, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2713.28010000002, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 26.904704752008264}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544711.7287786, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.90060000007179, "prefill_cuda_event_ms": null, "kv_decode_ms": 2543.7142999999196, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 46.90060000007179, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 191.89519963467896}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544711.7287786, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 46.90060000007179, "prefill_cuda_event_ms": null, "kv_decode_ms": 2543.7142999999196, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2543.7142999999196, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.16005826597823}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544711.7287786, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 46.90060000007179, "prefill_cuda_event_ms": null, "kv_decode_ms": 2543.7142999999196, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2590.6148999999914, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 28.178638206705383}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544714.319962, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 46.87829999988935, "prefill_cuda_event_ms": null, "kv_decode_ms": 2382.9006999999365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 46.87829999988935, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 191.98648415196888}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544714.319962, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 46.87829999988935, "prefill_cuda_event_ms": null, "kv_decode_ms": 2382.9006999999365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2382.9006999999365, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.85802224154859}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544714.319962, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 46.87829999988935, "prefill_cuda_event_ms": null, "kv_decode_ms": 2382.9006999999365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2429.778999999826, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 30.043884649593743}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544716.750326, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 45.482300000003306, "prefill_cuda_event_ms": null, "kv_decode_ms": 2463.0408999998963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 45.482300000003306, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 197.87917497574543}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544716.750326, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 45.482300000003306, "prefill_cuda_event_ms": null, "kv_decode_ms": 2463.0408999998963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2463.0408999998963, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.984140174043677}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544716.750326, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 45.482300000003306, "prefill_cuda_event_ms": null, "kv_decode_ms": 2463.0408999998963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2508.5231999998996, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 29.100787267984177}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544719.259427, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 29.84040000001187, "prefill_cuda_event_ms": null, "kv_decode_ms": 2099.876600000016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 29.84040000001187, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 33.51161512578928}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544719.259427, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.84040000001187, "prefill_cuda_event_ms": null, "kv_decode_ms": 2099.876600000016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2099.876600000016, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.477981420431806}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544719.259427, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.84040000001187, "prefill_cuda_event_ms": null, "kv_decode_ms": 2099.876600000016, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2129.717000000028, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 30.520486994281}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544721.3899267, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 36.75780000003215, "prefill_cuda_event_ms": null, "kv_decode_ms": 2470.8184000000983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 36.75780000003215, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 27.20511020787766}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544721.3899267, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.75780000003215, "prefill_cuda_event_ms": null, "kv_decode_ms": 2470.8184000000983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2470.8184000000983, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.902348792609548}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544721.3899267, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.75780000003215, "prefill_cuda_event_ms": null, "kv_decode_ms": 2470.8184000000983, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2507.5762000001305, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 25.92144557760463}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544723.8981678, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 35.52839999997559, "prefill_cuda_event_ms": null, "kv_decode_ms": 2495.3656999998657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 35.52839999997559, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.14649688701678}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544723.8981678, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 35.52839999997559, "prefill_cuda_event_ms": null, "kv_decode_ms": 2495.3656999998657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2495.3656999998657, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.647543364086253}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544723.8981678, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 35.52839999997559, "prefill_cuda_event_ms": null, "kv_decode_ms": 2495.3656999998657, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2530.8940999998413, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 25.68262338594257}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544726.42966, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 34.85870000008617, "prefill_cuda_event_ms": null, "kv_decode_ms": 2608.2331999998587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.85870000008617, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.68724306980834}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544726.42966, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.85870000008617, "prefill_cuda_event_ms": null, "kv_decode_ms": 2608.2331999998587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2608.2331999998587, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.537683210229616}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544726.42966, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.85870000008617, "prefill_cuda_event_ms": null, "kv_decode_ms": 2608.2331999998587, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2643.091899999945, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 24.59241012391637}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544729.073318, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 37.33069999998406, "prefill_cuda_event_ms": null, "kv_decode_ms": 1022.7987999999186, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 37.33069999998406, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 455.3892640643561}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544729.073318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 37.33069999998406, "prefill_cuda_event_ms": null, "kv_decode_ms": 1022.7987999999186, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1022.7987999999186, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.57340153313153}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544729.073318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 37.33069999998406, "prefill_cuda_event_ms": null, "kv_decode_ms": 1022.7987999999186, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1060.1294999999027, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 76.40575986236345}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544730.1344345, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 30.789400000003297, "prefill_cuda_event_ms": null, "kv_decode_ms": 1004.1166999999405, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 30.789400000003297, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 552.1380734927664}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544730.1344345, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.789400000003297, "prefill_cuda_event_ms": null, "kv_decode_ms": 1004.1166999999405, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1004.1166999999405, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.737611375255284}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544730.1344345, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.789400000003297, "prefill_cuda_event_ms": null, "kv_decode_ms": 1004.1166999999405, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1034.9060999999438, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 78.26797039847808}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544731.1697989, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 30.15719999984867, "prefill_cuda_event_ms": null, "kv_decode_ms": 997.1277000001919, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 30.15719999984867, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 563.712811537056}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544731.1697989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.15719999984867, "prefill_cuda_event_ms": null, "kv_decode_ms": 997.1277000001919, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 997.1277000001919, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.18435672781699}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544731.1697989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 30.15719999984867, "prefill_cuda_event_ms": null, "kv_decode_ms": 997.1277000001919, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1027.2849000000406, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 78.84862320082462}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544732.1980329, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.771199999913733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.5154999998449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 29.771199999913733, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 571.021658517267}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544732.1980329, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.771199999913733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.5154999998449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1002.5154999998449, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.83941195922647}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544732.1980329, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.771199999913733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.5154999998449, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1032.2866999997586, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 78.46657328823372}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544733.231137, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.377299999916431, "prefill_cuda_event_ms": 6.82803201675415, "kv_decode_ms": 243.37340000010954, "kv_decode_cuda_event_ms": 243.2112579345703, "gpu_peak_mb": 538.54931640625, "latency_ms": 7.377299999916431, "cuda_event_ms": 6.82803201675415, "tokens_total": 9, "tokens_per_s": 1219.9585214240915}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544733.231137, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.377299999916431, "prefill_cuda_event_ms": 6.82803201675415, "kv_decode_ms": 243.37340000010954, "kv_decode_cuda_event_ms": 243.2112579345703, "gpu_peak_mb": 538.54931640625, "latency_ms": 243.37340000010954, "cuda_event_ms": 243.2112579345703, "tokens_total": 64, "tokens_per_s": 262.9703985726098}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544733.231137, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.377299999916431, "prefill_cuda_event_ms": 6.82803201675415, "kv_decode_ms": 243.37340000010954, "kv_decode_cuda_event_ms": 243.2112579345703, "gpu_peak_mb": 538.54931640625, "latency_ms": 250.75070000002597, "cuda_event_ms": 250.03928995132446, "tokens_total": 73, "tokens_per_s": 291.12580742543264}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544733.483591, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.862400000092748, "prefill_cuda_event_ms": 4.784128189086914, "kv_decode_ms": 277.06800000009935, "kv_decode_cuda_event_ms": 276.9725341796875, "gpu_peak_mb": 538.54931640625, "latency_ms": 4.862400000092748, "cuda_event_ms": 4.784128189086914, "tokens_total": 9, "tokens_per_s": 1850.937808454329}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544733.483591, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.862400000092748, "prefill_cuda_event_ms": 4.784128189086914, "kv_decode_ms": 277.06800000009935, "kv_decode_cuda_event_ms": 276.9725341796875, "gpu_peak_mb": 538.54931640625, "latency_ms": 277.06800000009935, "cuda_event_ms": 276.9725341796875, "tokens_total": 64, "tokens_per_s": 230.99022622597}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544733.483591, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.862400000092748, "prefill_cuda_event_ms": 4.784128189086914, "kv_decode_ms": 277.06800000009935, "kv_decode_cuda_event_ms": 276.9725341796875, "gpu_peak_mb": 538.54931640625, "latency_ms": 281.9304000001921, "cuda_event_ms": 281.7566623687744, "tokens_total": 73, "tokens_per_s": 258.9291541456695}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544733.766374, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.541400000107387, "prefill_cuda_event_ms": 4.472832202911377, "kv_decode_ms": 284.6334000000752, "kv_decode_cuda_event_ms": 284.5778503417969, "gpu_peak_mb": 538.54931640625, "latency_ms": 4.541400000107387, "cuda_event_ms": 4.472832202911377, "tokens_total": 9, "tokens_per_s": 1981.7677367743831}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544733.766374, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.541400000107387, "prefill_cuda_event_ms": 4.472832202911377, "kv_decode_ms": 284.6334000000752, "kv_decode_cuda_event_ms": 284.5778503417969, "gpu_peak_mb": 538.54931640625, "latency_ms": 284.6334000000752, "cuda_event_ms": 284.5778503417969, "tokens_total": 64, "tokens_per_s": 224.85063242747722}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544733.766374, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.541400000107387, "prefill_cuda_event_ms": 4.472832202911377, "kv_decode_ms": 284.6334000000752, "kv_decode_cuda_event_ms": 284.5778503417969, "gpu_peak_mb": 538.54931640625, "latency_ms": 289.1748000001826, "cuda_event_ms": 289.05068254470825, "tokens_total": 73, "tokens_per_s": 252.4424673241026}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544734.056363, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.7595999999903142, "prefill_cuda_event_ms": 2.6818559169769287, "kv_decode_ms": 290.1578000000882, "kv_decode_cuda_event_ms": 290.1114807128906, "gpu_peak_mb": 538.54931640625, "latency_ms": 2.7595999999903142, "cuda_event_ms": 2.6818559169769287, "tokens_total": 9, "tokens_per_s": 3261.3422235221005}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544734.056363, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.7595999999903142, "prefill_cuda_event_ms": 2.6818559169769287, "kv_decode_ms": 290.1578000000882, "kv_decode_cuda_event_ms": 290.1114807128906, "gpu_peak_mb": 538.54931640625, "latency_ms": 290.1578000000882, "cuda_event_ms": 290.1114807128906, "tokens_total": 64, "tokens_per_s": 220.56963486758085}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544734.056363, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.7595999999903142, "prefill_cuda_event_ms": 2.6818559169769287, "kv_decode_ms": 290.1578000000882, "kv_decode_cuda_event_ms": 290.1114807128906, "gpu_peak_mb": 538.54931640625, "latency_ms": 292.9174000000785, "cuda_event_ms": 292.79333662986755, "tokens_total": 73, "tokens_per_s": 249.21701476245673}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544734.3501806, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.340900000088368, "prefill_cuda_event_ms": 19.213119506835938, "kv_decode_ms": 965.3376000001117, "kv_decode_cuda_event_ms": 965.2725830078125, "gpu_peak_mb": 539.29345703125, "latency_ms": 19.340900000088368, "cuda_event_ms": 19.213119506835938, "tokens_total": 17, "tokens_per_s": 878.966335585331}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544734.3501806, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.340900000088368, "prefill_cuda_event_ms": 19.213119506835938, "kv_decode_ms": 965.3376000001117, "kv_decode_cuda_event_ms": 965.2725830078125, "gpu_peak_mb": 539.29345703125, "latency_ms": 965.3376000001117, "cuda_event_ms": 965.2725830078125, "tokens_total": 64, "tokens_per_s": 66.29804951137571}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544734.3501806, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.340900000088368, "prefill_cuda_event_ms": 19.213119506835938, "kv_decode_ms": 965.3376000001117, "kv_decode_cuda_event_ms": 965.2725830078125, "gpu_peak_mb": 539.29345703125, "latency_ms": 984.6785000002001, "cuda_event_ms": 984.4857025146484, "tokens_total": 81, "tokens_per_s": 82.26035198288938}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544735.3357372, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 14.046199999938835, "prefill_cuda_event_ms": 13.96019172668457, "kv_decode_ms": 639.1418999999132, "kv_decode_cuda_event_ms": 639.0814819335938, "gpu_peak_mb": 539.29345703125, "latency_ms": 14.046199999938835, "cuda_event_ms": 13.96019172668457, "tokens_total": 17, "tokens_per_s": 1210.2917515110155}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544735.3357372, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.046199999938835, "prefill_cuda_event_ms": 13.96019172668457, "kv_decode_ms": 639.1418999999132, "kv_decode_cuda_event_ms": 639.0814819335938, "gpu_peak_mb": 539.29345703125, "latency_ms": 639.1418999999132, "cuda_event_ms": 639.0814819335938, "tokens_total": 64, "tokens_per_s": 100.13425813580473}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544735.3357372, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.046199999938835, "prefill_cuda_event_ms": 13.96019172668457, "kv_decode_ms": 639.1418999999132, "kv_decode_cuda_event_ms": 639.0814819335938, "gpu_peak_mb": 539.29345703125, "latency_ms": 653.188099999852, "cuda_event_ms": 653.0416736602783, "tokens_total": 81, "tokens_per_s": 124.0071581218616}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544735.989818, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.274300000062794, "prefill_cuda_event_ms": 7.196671962738037, "kv_decode_ms": 445.84389999999985, "kv_decode_cuda_event_ms": 445.79022216796875, "gpu_peak_mb": 539.29345703125, "latency_ms": 7.274300000062794, "cuda_event_ms": 7.196671962738037, "tokens_total": 17, "tokens_per_s": 2336.9946248921888}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544735.989818, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.274300000062794, "prefill_cuda_event_ms": 7.196671962738037, "kv_decode_ms": 445.84389999999985, "kv_decode_cuda_event_ms": 445.79022216796875, "gpu_peak_mb": 539.29345703125, "latency_ms": 445.84389999999985, "cuda_event_ms": 445.79022216796875, "tokens_total": 64, "tokens_per_s": 143.54799964741028}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544735.989818, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.274300000062794, "prefill_cuda_event_ms": 7.196671962738037, "kv_decode_ms": 445.84389999999985, "kv_decode_cuda_event_ms": 445.79022216796875, "gpu_peak_mb": 539.29345703125, "latency_ms": 453.11820000006264, "cuda_event_ms": 452.9868941307068, "tokens_total": 81, "tokens_per_s": 178.76130334201716}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544736.443654, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.456500000193046, "prefill_cuda_event_ms": 8.350720405578613, "kv_decode_ms": 444.58540000005087, "kv_decode_cuda_event_ms": 444.5317077636719, "gpu_peak_mb": 539.29345703125, "latency_ms": 8.456500000193046, "cuda_event_ms": 8.350720405578613, "tokens_total": 17, "tokens_per_s": 2010.2879441390555}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544736.443654, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.456500000193046, "prefill_cuda_event_ms": 8.350720405578613, "kv_decode_ms": 444.58540000005087, "kv_decode_cuda_event_ms": 444.5317077636719, "gpu_peak_mb": 539.29345703125, "latency_ms": 444.58540000005087, "cuda_event_ms": 444.5317077636719, "tokens_total": 64, "tokens_per_s": 143.954344879505}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544736.443654, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.456500000193046, "prefill_cuda_event_ms": 8.350720405578613, "kv_decode_ms": 444.58540000005087, "kv_decode_cuda_event_ms": 444.5317077636719, "gpu_peak_mb": 539.29345703125, "latency_ms": 453.0419000002439, "cuda_event_ms": 452.8824281692505, "tokens_total": 81, "tokens_per_s": 178.79140980107223}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544736.8973622, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.682199999910154, "prefill_cuda_event_ms": 5.617663860321045, "kv_decode_ms": 238.1550000000061, "kv_decode_cuda_event_ms": 238.11276245117188, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 265.8209999999599, "latency_ms": 5.682199999910154, "cuda_event_ms": 5.617663860321045, "tokens_total": 9, "tokens_per_s": 1583.8935623776542}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544736.8973622, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.682199999910154, "prefill_cuda_event_ms": 5.617663860321045, "kv_decode_ms": 238.1550000000061, "kv_decode_cuda_event_ms": 238.11276245117188, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 265.8209999999599, "latency_ms": 238.1550000000061, "cuda_event_ms": 238.11276245117188, "tokens_total": 64, "tokens_per_s": 268.73254813041234}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544736.8973622, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.682199999910154, "prefill_cuda_event_ms": 5.617663860321045, "kv_decode_ms": 238.1550000000061, "kv_decode_cuda_event_ms": 238.11276245117188, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 265.8209999999599, "latency_ms": 243.83719999991627, "cuda_event_ms": 243.73042631149292, "tokens_total": 73, "tokens_per_s": 299.38007818341526}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544737.4076838, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.455700000084107, "prefill_cuda_event_ms": 5.390336036682129, "kv_decode_ms": 201.33239999995567, "kv_decode_cuda_event_ms": 201.29177856445312, "gpu_peak_mb": 632.16455078125, "latency_ms": 5.455700000084107, "cuda_event_ms": 5.390336036682129, "tokens_total": 9, "tokens_per_s": 1649.6508238835077}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544737.4076838, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.455700000084107, "prefill_cuda_event_ms": 5.390336036682129, "kv_decode_ms": 201.33239999995567, "kv_decode_cuda_event_ms": 201.29177856445312, "gpu_peak_mb": 632.16455078125, "latency_ms": 201.33239999995567, "cuda_event_ms": 201.29177856445312, "tokens_total": 64, "tokens_per_s": 317.88226832846624}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544737.4076838, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.455700000084107, "prefill_cuda_event_ms": 5.390336036682129, "kv_decode_ms": 201.33239999995567, "kv_decode_cuda_event_ms": 201.29177856445312, "gpu_peak_mb": 632.16455078125, "latency_ms": 206.78810000003978, "cuda_event_ms": 206.68211460113525, "tokens_total": 73, "tokens_per_s": 353.0183796842563}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544737.6151872, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.264299999931609, "prefill_cuda_event_ms": 4.207615852355957, "kv_decode_ms": 217.9602999999588, "kv_decode_cuda_event_ms": 217.93280029296875, "gpu_peak_mb": 632.16455078125, "latency_ms": 4.264299999931609, "cuda_event_ms": 4.207615852355957, "tokens_total": 9, "tokens_per_s": 2110.545693348109}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544737.6151872, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.264299999931609, "prefill_cuda_event_ms": 4.207615852355957, "kv_decode_ms": 217.9602999999588, "kv_decode_cuda_event_ms": 217.93280029296875, "gpu_peak_mb": 632.16455078125, "latency_ms": 217.9602999999588, "cuda_event_ms": 217.93280029296875, "tokens_total": 64, "tokens_per_s": 293.63145490262264}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544737.6151872, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.264299999931609, "prefill_cuda_event_ms": 4.207615852355957, "kv_decode_ms": 217.9602999999588, "kv_decode_cuda_event_ms": 217.93280029296875, "gpu_peak_mb": 632.16455078125, "latency_ms": 222.22459999989042, "cuda_event_ms": 222.1404161453247, "tokens_total": 73, "tokens_per_s": 328.49648508777153}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544737.8380868, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.0933999998742365, "prefill_cuda_event_ms": 4.05072021484375, "kv_decode_ms": 204.37079999987873, "kv_decode_cuda_event_ms": 204.34329223632812, "gpu_peak_mb": 632.16455078125, "latency_ms": 4.0933999998742365, "cuda_event_ms": 4.05072021484375, "tokens_total": 9, "tokens_per_s": 2198.6612596561567}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544737.8380868, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.0933999998742365, "prefill_cuda_event_ms": 4.05072021484375, "kv_decode_ms": 204.37079999987873, "kv_decode_cuda_event_ms": 204.34329223632812, "gpu_peak_mb": 632.16455078125, "latency_ms": 204.37079999987873, "cuda_event_ms": 204.34329223632812, "tokens_total": 64, "tokens_per_s": 313.15628260024414}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544737.8380868, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.0933999998742365, "prefill_cuda_event_ms": 4.05072021484375, "kv_decode_ms": 204.37079999987873, "kv_decode_cuda_event_ms": 204.34329223632812, "gpu_peak_mb": 632.16455078125, "latency_ms": 208.46419999975296, "cuda_event_ms": 208.39401245117188, "tokens_total": 73, "tokens_per_s": 350.1800309121974}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544738.0470557, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 19.917099999929633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1063.9699999999266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 19.917099999929633, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 50.20811262701563}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544738.0470557, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.917099999929633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1063.9699999999266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1063.9699999999266, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.15207195692023}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544738.0470557, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.917099999929633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1063.9699999999266, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1083.8870999998562, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 59.96934551579092}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544739.131353, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.95150000000467, "prefill_cuda_event_ms": null, "kv_decode_ms": 1065.8922000000075, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 16.95150000000467, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.99182963157977}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544739.131353, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.95150000000467, "prefill_cuda_event_ms": null, "kv_decode_ms": 1065.8922000000075, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1065.8922000000075, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.04359540298686}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544739.131353, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.95150000000467, "prefill_cuda_event_ms": null, "kv_decode_ms": 1065.8922000000075, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1082.8437000000122, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 60.02713041595871}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544740.2147677, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.027200000029552, "prefill_cuda_event_ms": null, "kv_decode_ms": 1101.7515000000913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 17.027200000029552, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.72956211228296}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544740.2147677, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.027200000029552, "prefill_cuda_event_ms": null, "kv_decode_ms": 1101.7515000000913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1101.7515000000913, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.089324135247104}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544740.2147677, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.027200000029552, "prefill_cuda_event_ms": null, "kv_decode_ms": 1101.7515000000913, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1118.7787000001208, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 58.09906820713782}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544741.3341272, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.969799999993484, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.7098999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 15.969799999993484, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 62.61819183711806}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544741.3341272, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.969799999993484, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.7098999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1071.7098999998598, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.71765307011569}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544741.3341272, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.969799999993484, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.7098999998598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1087.6796999998533, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 59.760240078038386}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544742.4224043, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 33.78300000008494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1230.6139000002077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 33.78300000008494, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 503.2116745095834}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544742.4224043, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 33.78300000008494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1230.6139000002077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1230.6139000002077, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.006563553352684}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544742.4224043, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 33.78300000008494, "prefill_cuda_event_ms": null, "kv_decode_ms": 1230.6139000002077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1264.3969000002926, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 64.06216275916309}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544743.6875844, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.552999999850726, "prefill_cuda_event_ms": null, "kv_decode_ms": 1299.103599999853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 29.552999999850726, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 575.2377085265749}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544743.6875844, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.552999999850726, "prefill_cuda_event_ms": null, "kv_decode_ms": 1299.103599999853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1299.103599999853, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.26473916322551}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544743.6875844, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.552999999850726, "prefill_cuda_event_ms": null, "kv_decode_ms": 1299.103599999853, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1328.6565999997038, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 60.963833694890056}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544745.01681, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 35.207899999932124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.393599999854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 35.207899999932124, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 482.8461794095295}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544745.01681, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.207899999932124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.393599999854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1268.393599999854, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.45752359520528}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544745.01681, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.207899999932124, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.393599999854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1303.601499999786, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 62.135552927802934}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544746.3210437, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 38.140899999916655, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.081200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 38.140899999916655, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 445.71575395539037}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544746.3210437, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.140899999916655, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.081200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1268.081200000097, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.46995413227095}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544746.3210437, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 38.140899999916655, "prefill_cuda_event_ms": null, "kv_decode_ms": 1268.081200000097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1306.2221000000136, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 62.01089385947394}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544747.62779, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 104.63009999989481, "prefill_cuda_event_ms": null, "kv_decode_ms": 1618.1031999999504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.57550000008268, "latency_ms": 104.63009999989481, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 86.01731241783243}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544747.62779, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 104.63009999989481, "prefill_cuda_event_ms": null, "kv_decode_ms": 1618.1031999999504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.57550000008268, "latency_ms": 1618.1031999999504, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.55248342627464}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544747.62779, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 104.63009999989481, "prefill_cuda_event_ms": null, "kv_decode_ms": 1618.1031999999504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.57550000008268, "latency_ms": 1722.7332999998453, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 42.374521929776684}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544749.5041947, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.2080000000351, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.295700000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.2080000000351, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 263.0963517303194}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544749.5041947, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.2080000000351, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.295700000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1573.295700000017, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.67893912123405}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544749.5041947, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.2080000000351, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.295700000017, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1607.503700000052, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 45.412026112286796}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544751.1124928, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 28.371499999821026, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5206000001108, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 28.371499999821026, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 317.2197451688058}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544751.1124928, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 28.371499999821026, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5206000001108, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1508.5206000001108, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 42.42567188011572}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544751.1124928, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 28.371499999821026, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5206000001108, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1536.8920999999318, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 47.49845483622646}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544752.6501052, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.66170000003876, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.883999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 33.66170000003876, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 267.3661758018649}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544752.6501052, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.66170000003876, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.883999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1705.883999999969, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 37.5172051558026}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544752.6501052, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.66170000003876, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.883999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1739.5457000000079, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 41.96497970705781}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544754.3901677, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 55.106000000023414, "prefill_cuda_event_ms": 54.97139358520508, "kv_decode_ms": 666.8824999999288, "kv_decode_cuda_event_ms": 666.734619140625, "gpu_peak_mb": 633.97998046875, "latency_ms": 55.106000000023414, "cuda_event_ms": 54.97139358520508, "tokens_total": 17, "tokens_per_s": 308.4963524841719}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544754.3901677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 55.106000000023414, "prefill_cuda_event_ms": 54.97139358520508, "kv_decode_ms": 666.8824999999288, "kv_decode_cuda_event_ms": 666.734619140625, "gpu_peak_mb": 633.97998046875, "latency_ms": 666.8824999999288, "cuda_event_ms": 666.734619140625, "tokens_total": 64, "tokens_per_s": 95.96893005890368}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544754.3901677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 55.106000000023414, "prefill_cuda_event_ms": 54.97139358520508, "kv_decode_ms": 666.8824999999288, "kv_decode_cuda_event_ms": 666.734619140625, "gpu_peak_mb": 633.97998046875, "latency_ms": 721.9884999999522, "cuda_event_ms": 721.7060127258301, "tokens_total": 81, "tokens_per_s": 112.19015261324158}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544755.114309, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 14.096899999913148, "prefill_cuda_event_ms": 14.036831855773926, "kv_decode_ms": 635.9694000000218, "kv_decode_cuda_event_ms": 635.8968505859375, "gpu_peak_mb": 633.97998046875, "latency_ms": 14.096899999913148, "cuda_event_ms": 14.036831855773926, "tokens_total": 17, "tokens_per_s": 1205.9388943742765}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544755.114309, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.096899999913148, "prefill_cuda_event_ms": 14.036831855773926, "kv_decode_ms": 635.9694000000218, "kv_decode_cuda_event_ms": 635.8968505859375, "gpu_peak_mb": 633.97998046875, "latency_ms": 635.9694000000218, "cuda_event_ms": 635.8968505859375, "tokens_total": 64, "tokens_per_s": 100.63377263119547}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544755.114309, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.096899999913148, "prefill_cuda_event_ms": 14.036831855773926, "kv_decode_ms": 635.9694000000218, "kv_decode_cuda_event_ms": 635.8968505859375, "gpu_peak_mb": 633.97998046875, "latency_ms": 650.0662999999349, "cuda_event_ms": 649.9336824417114, "tokens_total": 81, "tokens_per_s": 124.60267514253255}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544755.765306, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.705699999993158, "prefill_cuda_event_ms": 11.628543853759766, "kv_decode_ms": 623.6206000000948, "kv_decode_cuda_event_ms": 623.5579833984375, "gpu_peak_mb": 633.97998046875, "latency_ms": 11.705699999993158, "cuda_event_ms": 11.628543853759766, "tokens_total": 17, "tokens_per_s": 1452.2839300520207}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544755.765306, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.705699999993158, "prefill_cuda_event_ms": 11.628543853759766, "kv_decode_ms": 623.6206000000948, "kv_decode_cuda_event_ms": 623.5579833984375, "gpu_peak_mb": 633.97998046875, "latency_ms": 623.6206000000948, "cuda_event_ms": 623.5579833984375, "tokens_total": 64, "tokens_per_s": 102.62650079229306}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544755.765306, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.705699999993158, "prefill_cuda_event_ms": 11.628543853759766, "kv_decode_ms": 623.6206000000948, "kv_decode_cuda_event_ms": 623.5579833984375, "gpu_peak_mb": 633.97998046875, "latency_ms": 635.3263000000879, "cuda_event_ms": 635.1865272521973, "tokens_total": 81, "tokens_per_s": 127.49354150770839}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544756.4015715, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.312699999914912, "prefill_cuda_event_ms": 13.188096046447754, "kv_decode_ms": 651.4898000000358, "kv_decode_cuda_event_ms": 651.4503784179688, "gpu_peak_mb": 633.97998046875, "latency_ms": 13.312699999914912, "cuda_event_ms": 13.188096046447754, "tokens_total": 17, "tokens_per_s": 1276.9761205547074}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544756.4015715, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.312699999914912, "prefill_cuda_event_ms": 13.188096046447754, "kv_decode_ms": 651.4898000000358, "kv_decode_cuda_event_ms": 651.4503784179688, "gpu_peak_mb": 633.97998046875, "latency_ms": 651.4898000000358, "cuda_event_ms": 651.4503784179688, "tokens_total": 64, "tokens_per_s": 98.23638067702133}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 100.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544756.4015715, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.312699999914912, "prefill_cuda_event_ms": 13.188096046447754, "kv_decode_ms": 651.4898000000358, "kv_decode_cuda_event_ms": 651.4503784179688, "gpu_peak_mb": 633.97998046875, "latency_ms": 664.8024999999507, "cuda_event_ms": 664.6384744644165, "tokens_total": 81, "tokens_per_s": 121.84069704913266}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 360, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544757.067465, "prompt_tokens": 84, "prefill_ms": 7676.3631, "prefill_cuda_event_ms": null, "kv_decode_ms": 2109.7897, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 50095.22850000008, "ollama_total_duration_ms": 49939.3033, "ollama_load_ms": 39951.898, "ollama_done_reason": "stop", "latency_ms": 7676.3631, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 10.942681958335191}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 361, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544757.067465, "prompt_tokens": 84, "prefill_ms": 7676.3631, "prefill_cuda_event_ms": null, "kv_decode_ms": 2109.7897, "kv_decode_cuda_event_ms": null, "gen_tokens": 38, "ollama_wall_ms": 50095.22850000008, "ollama_total_duration_ms": 49939.3033, "ollama_load_ms": 39951.898, "ollama_done_reason": "stop", "latency_ms": 2109.7897, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 18.011273825064176}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 362, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544757.067465, "prompt_tokens": 84, "prefill_ms": 7676.3631, "prefill_cuda_event_ms": null, "kv_decode_ms": 2109.7897, "kv_decode_cuda_event_ms": null, "gen_tokens": 38, "ollama_wall_ms": 50095.22850000008, "ollama_total_duration_ms": 49939.3033, "ollama_load_ms": 39951.898, "ollama_done_reason": "stop", "latency_ms": 9786.1528, "cuda_event_ms": null, "tokens_total": 122, "tokens_per_s": 12.466594635636591}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 363, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544807.1636233, "prompt_tokens": 84, "prefill_ms": 59.2246, "prefill_cuda_event_ms": null, "kv_decode_ms": 1828.8694, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2206.203799999912, "ollama_total_duration_ms": 2179.2213, "ollama_load_ms": 280.3052, "ollama_done_reason": "stop", "latency_ms": 59.2246, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1418.32954549292}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 364, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544807.1636233, "prompt_tokens": 84, "prefill_ms": 59.2246, "prefill_cuda_event_ms": null, "kv_decode_ms": 1828.8694, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2206.203799999912, "ollama_total_duration_ms": 2179.2213, "ollama_load_ms": 280.3052, "ollama_done_reason": "stop", "latency_ms": 1828.8694, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 18.590720583984837}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 365, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544807.1636233, "prompt_tokens": 84, "prefill_ms": 59.2246, "prefill_cuda_event_ms": null, "kv_decode_ms": 1828.8694, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2206.203799999912, "ollama_total_duration_ms": 2179.2213, "ollama_load_ms": 280.3052, "ollama_done_reason": "stop", "latency_ms": 1888.094, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 62.49688839644636}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 366, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544809.370036, "prompt_tokens": 84, "prefill_ms": 64.0141, "prefill_cuda_event_ms": null, "kv_decode_ms": 2010.9368, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2341.2450000000717, "ollama_total_duration_ms": 2337.4409, "ollama_load_ms": 246.2951, "ollama_done_reason": "stop", "latency_ms": 64.0141, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1312.2109035353149}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 367, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544809.370036, "prompt_tokens": 84, "prefill_ms": 64.0141, "prefill_cuda_event_ms": null, "kv_decode_ms": 2010.9368, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2341.2450000000717, "ollama_total_duration_ms": 2337.4409, "ollama_load_ms": 246.2951, "ollama_done_reason": "stop", "latency_ms": 2010.9368, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 16.907542792990807}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 368, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544809.370036, "prompt_tokens": 84, "prefill_ms": 64.0141, "prefill_cuda_event_ms": null, "kv_decode_ms": 2010.9368, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2341.2450000000717, "ollama_total_duration_ms": 2337.4409, "ollama_load_ms": 246.2951, "ollama_done_reason": "stop", "latency_ms": 2074.9509, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 56.868815546430525}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 369, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544811.7113981, "prompt_tokens": 84, "prefill_ms": 70.8748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2173.2799, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 2524.92489999986, "ollama_total_duration_ms": 2521.9372, "ollama_load_ms": 267.6929, "ollama_done_reason": "stop", "latency_ms": 70.8748, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1185.1885296325354}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 370, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544811.7113981, "prompt_tokens": 84, "prefill_ms": 70.8748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2173.2799, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2524.92489999986, "ollama_total_duration_ms": 2521.9372, "ollama_load_ms": 267.6929, "ollama_done_reason": "stop", "latency_ms": 2173.2799, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 15.644556414477492}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 371, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544811.7113981, "prompt_tokens": 84, "prefill_ms": 70.8748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2173.2799, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 2524.92489999986, "ollama_total_duration_ms": 2521.9372, "ollama_load_ms": 267.6929, "ollama_done_reason": "stop", "latency_ms": 2244.1547, "cuda_event_ms": null, "tokens_total": 118, "tokens_per_s": 52.581045326331555}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 372, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544814.2365167, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.209900000039852, "prefill_cuda_event_ms": 10.669055938720703, "kv_decode_ms": 446.69920000001184, "kv_decode_cuda_event_ms": 446.603271484375, "gpu_peak_mb": 632.3837890625, "latency_ms": 11.209900000039852, "cuda_event_ms": 10.669055938720703, "tokens_total": 9, "tokens_per_s": 802.861756123427}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 373, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544814.2365167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.209900000039852, "prefill_cuda_event_ms": 10.669055938720703, "kv_decode_ms": 446.69920000001184, "kv_decode_cuda_event_ms": 446.603271484375, "gpu_peak_mb": 632.3837890625, "latency_ms": 446.69920000001184, "cuda_event_ms": 446.603271484375, "tokens_total": 64, "tokens_per_s": 143.27314667229828}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 374, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544814.2365167, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.209900000039852, "prefill_cuda_event_ms": 10.669055938720703, "kv_decode_ms": 446.69920000001184, "kv_decode_cuda_event_ms": 446.603271484375, "gpu_peak_mb": 632.3837890625, "latency_ms": 457.9091000000517, "cuda_event_ms": 457.2723274230957, "tokens_total": 73, "tokens_per_s": 159.42028669006962}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 375, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544814.6967714, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.84400000000096, "prefill_cuda_event_ms": 6.780928134918213, "kv_decode_ms": 447.1725000000788, "kv_decode_cuda_event_ms": 447.1212463378906, "gpu_peak_mb": 632.3837890625, "latency_ms": 6.84400000000096, "cuda_event_ms": 6.780928134918213, "tokens_total": 9, "tokens_per_s": 1315.0204558735736}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 376, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544814.6967714, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.84400000000096, "prefill_cuda_event_ms": 6.780928134918213, "kv_decode_ms": 447.1725000000788, "kv_decode_cuda_event_ms": 447.1212463378906, "gpu_peak_mb": 632.3837890625, "latency_ms": 447.1725000000788, "cuda_event_ms": 447.1212463378906, "tokens_total": 64, "tokens_per_s": 143.12150232849453}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 377, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544814.6967714, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.84400000000096, "prefill_cuda_event_ms": 6.780928134918213, "kv_decode_ms": 447.1725000000788, "kv_decode_cuda_event_ms": 447.1212463378906, "gpu_peak_mb": 632.3837890625, "latency_ms": 454.01650000007976, "cuda_event_ms": 453.90217447280884, "tokens_total": 73, "tokens_per_s": 160.78710795750195}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 378, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544815.151561, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.53200000005927, "prefill_cuda_event_ms": 8.446975708007812, "kv_decode_ms": 189.47909999997137, "kv_decode_cuda_event_ms": 189.4318084716797, "gpu_peak_mb": 632.3837890625, "latency_ms": 8.53200000005927, "cuda_event_ms": 8.446975708007812, "tokens_total": 9, "tokens_per_s": 1054.8523206677776}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 379, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544815.151561, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.53200000005927, "prefill_cuda_event_ms": 8.446975708007812, "kv_decode_ms": 189.47909999997137, "kv_decode_cuda_event_ms": 189.4318084716797, "gpu_peak_mb": 632.3837890625, "latency_ms": 189.47909999997137, "cuda_event_ms": 189.4318084716797, "tokens_total": 64, "tokens_per_s": 337.7681232389729}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 380, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544815.151561, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.53200000005927, "prefill_cuda_event_ms": 8.446975708007812, "kv_decode_ms": 189.47909999997137, "kv_decode_cuda_event_ms": 189.4318084716797, "gpu_peak_mb": 632.3837890625, "latency_ms": 198.01110000003064, "cuda_event_ms": 197.8787841796875, "tokens_total": 73, "tokens_per_s": 368.66620103614747}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 381, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544815.3502607, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.9593999999851803, "prefill_cuda_event_ms": 3.9096319675445557, "kv_decode_ms": 177.84419999998136, "kv_decode_cuda_event_ms": 177.78892517089844, "gpu_peak_mb": 632.3837890625, "latency_ms": 3.9593999999851803, "cuda_event_ms": 3.9096319675445557, "tokens_total": 9, "tokens_per_s": 2273.071677535406}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 382, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544815.3502607, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.9593999999851803, "prefill_cuda_event_ms": 3.9096319675445557, "kv_decode_ms": 177.84419999998136, "kv_decode_cuda_event_ms": 177.78892517089844, "gpu_peak_mb": 632.3837890625, "latency_ms": 177.84419999998136, "cuda_event_ms": 177.78892517089844, "tokens_total": 64, "tokens_per_s": 359.8655452356991}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 383, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544815.3502607, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.9593999999851803, "prefill_cuda_event_ms": 3.9096319675445557, "kv_decode_ms": 177.84419999998136, "kv_decode_cuda_event_ms": 177.78892517089844, "gpu_peak_mb": 632.3837890625, "latency_ms": 181.80359999996654, "cuda_event_ms": 181.698557138443, "tokens_total": 73, "tokens_per_s": 401.5322028827451}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 384, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544815.5327115, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 576.1027000000922, "prefill_cuda_event_ms": null, "kv_decode_ms": 1905.1883000001908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 233.76089999987926, "latency_ms": 576.1027000000922, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 29.508627541577706}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 385, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544815.5327115, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 576.1027000000922, "prefill_cuda_event_ms": null, "kv_decode_ms": 1905.1883000001908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 233.76089999987926, "latency_ms": 1905.1883000001908, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.592480071389055}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 386, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544815.5327115, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 576.1027000000922, "prefill_cuda_event_ms": null, "kv_decode_ms": 1905.1883000001908, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 233.76089999987926, "latency_ms": 2481.291000000283, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 32.644296859977636}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 387, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544818.2485392, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 41.78790000014487, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.314699999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 41.78790000014487, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 406.8163272129268}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 388, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544818.2485392, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.78790000014487, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.314699999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2067.314699999997, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.95803459434603}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 389, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544818.2485392, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 41.78790000014487, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.314699999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2109.102600000142, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 38.404959531126906}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 390, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544820.358178, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 47.00529999990977, "prefill_cuda_event_ms": null, "kv_decode_ms": 1893.4985999999299, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 47.00529999990977, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 361.6613445724766}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 391, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544820.358178, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.00529999990977, "prefill_cuda_event_ms": null, "kv_decode_ms": 1893.4985999999299, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1893.4985999999299, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.799866553903115}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 392, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544820.358178, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.00529999990977, "prefill_cuda_event_ms": null, "kv_decode_ms": 1893.4985999999299, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1940.5038999998396, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 41.74173522661134}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 393, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544822.2992764, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 36.56290000003537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1765.28919999987, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 36.56290000003537, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 464.9521783005056}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 394, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544822.2992764, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 36.56290000003537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1765.28919999987, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1765.28919999987, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 36.25468280211804}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 395, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544822.2992764, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 36.56290000003537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1765.28919999987, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1801.8520999999055, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 44.95374509373119}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 396, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544824.1018755, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.589300000148796, "prefill_cuda_event_ms": 13.083647727966309, "kv_decode_ms": 482.3437999998532, "kv_decode_cuda_event_ms": 482.2630310058594, "gpu_peak_mb": 632.44970703125, "latency_ms": 13.589300000148796, "cuda_event_ms": 13.083647727966309, "tokens_total": 9, "tokens_per_s": 662.2857689433197}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 397, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544824.1018755, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.589300000148796, "prefill_cuda_event_ms": 13.083647727966309, "kv_decode_ms": 482.3437999998532, "kv_decode_cuda_event_ms": 482.2630310058594, "gpu_peak_mb": 632.44970703125, "latency_ms": 482.3437999998532, "cuda_event_ms": 482.2630310058594, "tokens_total": 64, "tokens_per_s": 132.68544138023435}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 398, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544824.1018755, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.589300000148796, "prefill_cuda_event_ms": 13.083647727966309, "kv_decode_ms": 482.3437999998532, "kv_decode_cuda_event_ms": 482.2630310058594, "gpu_peak_mb": 632.44970703125, "latency_ms": 495.933100000002, "cuda_event_ms": 495.3466787338257, "tokens_total": 73, "tokens_per_s": 147.19727318059574}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 399, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544824.6000035, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.198199999995268, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 361.07940000010785, "kv_decode_cuda_event_ms": 361.0213928222656, "gpu_peak_mb": 632.44970703125, "latency_ms": 5.198199999995268, "cuda_event_ms": 5.155839920043945, "tokens_total": 9, "tokens_per_s": 1731.368550653725}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 400, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544824.6000035, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.198199999995268, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 361.07940000010785, "kv_decode_cuda_event_ms": 361.0213928222656, "gpu_peak_mb": 632.44970703125, "latency_ms": 361.07940000010785, "cuda_event_ms": 361.0213928222656, "tokens_total": 64, "tokens_per_s": 177.2463341857245}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 401, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544824.6000035, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.198199999995268, "prefill_cuda_event_ms": 5.155839920043945, "kv_decode_ms": 361.07940000010785, "kv_decode_cuda_event_ms": 361.0213928222656, "gpu_peak_mb": 632.44970703125, "latency_ms": 366.2776000001031, "cuda_event_ms": 366.17723274230957, "tokens_total": 73, "tokens_per_s": 199.30238704190333}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 402, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544824.9669664, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.54010000004746, "prefill_cuda_event_ms": 11.431936264038086, "kv_decode_ms": 649.6737999998459, "kv_decode_cuda_event_ms": 649.5887451171875, "gpu_peak_mb": 632.44970703125, "latency_ms": 11.54010000004746, "cuda_event_ms": 11.431936264038086, "tokens_total": 9, "tokens_per_s": 779.8892557224796}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 403, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544824.9669664, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.54010000004746, "prefill_cuda_event_ms": 11.431936264038086, "kv_decode_ms": 649.6737999998459, "kv_decode_cuda_event_ms": 649.5887451171875, "gpu_peak_mb": 632.44970703125, "latency_ms": 649.6737999998459, "cuda_event_ms": 649.5887451171875, "tokens_total": 64, "tokens_per_s": 98.51097581588665}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 404, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544824.9669664, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.54010000004746, "prefill_cuda_event_ms": 11.431936264038086, "kv_decode_ms": 649.6737999998459, "kv_decode_cuda_event_ms": 649.5887451171875, "gpu_peak_mb": 632.44970703125, "latency_ms": 661.2138999998933, "cuda_event_ms": 661.0206813812256, "tokens_total": 73, "tokens_per_s": 110.4030027197126}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 405, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544825.6291137, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.945199999900069, "prefill_cuda_event_ms": 11.851072311401367, "kv_decode_ms": 620.3726000001097, "kv_decode_cuda_event_ms": 620.3329467773438, "gpu_peak_mb": 632.44970703125, "latency_ms": 11.945199999900069, "cuda_event_ms": 11.851072311401367, "tokens_total": 9, "tokens_per_s": 753.440712593786}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 406, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544825.6291137, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.945199999900069, "prefill_cuda_event_ms": 11.851072311401367, "kv_decode_ms": 620.3726000001097, "kv_decode_cuda_event_ms": 620.3329467773438, "gpu_peak_mb": 632.44970703125, "latency_ms": 620.3726000001097, "cuda_event_ms": 620.3329467773438, "tokens_total": 64, "tokens_per_s": 103.1638083306527}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 407, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544825.6291137, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.945199999900069, "prefill_cuda_event_ms": 11.851072311401367, "kv_decode_ms": 620.3726000001097, "kv_decode_cuda_event_ms": 620.3329467773438, "gpu_peak_mb": 632.44970703125, "latency_ms": 632.3178000000098, "cuda_event_ms": 632.1840190887451, "tokens_total": 73, "tokens_per_s": 115.44827616745704}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 408, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544826.2623856, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.351199999997334, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.02170000014485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 3.351199999997334, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 298.4005729293374}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 409, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544826.2623856, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.351199999997334, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.02170000014485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 193.02170000014485, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 331.56893758552525}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 410, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544826.2623856, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.351199999997334, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.02170000014485, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 196.37290000014218, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 331.0029031498386}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 411, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544826.4592292, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.9961000000184868, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.90330000008726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2.9961000000184868, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 333.7672307312272}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 412, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544826.4592292, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.9961000000184868, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.90330000008726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 184.90330000008726, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 346.126867394848}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 413, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544826.4592292, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.9961000000184868, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.90330000008726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 187.89940000010574, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 345.9297900896087}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 414, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544826.6475317, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.118400000175825, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.1856999999818, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 3.118400000175825, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 320.67727037699365}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 415, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544826.6475317, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.118400000175825, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.1856999999818, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 192.1856999999818, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 333.0112490159573}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 416, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544826.6475317, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.118400000175825, "prefill_cuda_event_ms": null, "kv_decode_ms": 192.1856999999818, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 195.30410000015763, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 332.81431367773405}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 417, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544826.8432174, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.891799999815703, "prefill_cuda_event_ms": null, "kv_decode_ms": 190.74959999989005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2.891799999815703, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 345.8053807537627}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 418, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544826.8432174, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.891799999815703, "prefill_cuda_event_ms": null, "kv_decode_ms": 190.74959999989005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 190.74959999989005, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 335.518396893293}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 419, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544826.8432174, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.891799999815703, "prefill_cuda_event_ms": null, "kv_decode_ms": 190.74959999989005, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 193.64139999970575, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 335.67202054983477}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 420, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544827.0373814, "prompt_tokens": 99, "prefill_ms": 472.3783, "prefill_cuda_event_ms": null, "kv_decode_ms": 3216.4143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3983.359099999916, "ollama_total_duration_ms": 3979.5296, "ollama_load_ms": 261.6033, "ollama_done_reason": "length", "latency_ms": 472.3783, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 209.57778966561332}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 421, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544827.0373814, "prompt_tokens": 99, "prefill_ms": 472.3783, "prefill_cuda_event_ms": null, "kv_decode_ms": 3216.4143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3983.359099999916, "ollama_total_duration_ms": 3979.5296, "ollama_load_ms": 261.6033, "ollama_done_reason": "length", "latency_ms": 3216.4143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.89793416849316}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 422, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544827.0373814, "prompt_tokens": 99, "prefill_ms": 472.3783, "prefill_cuda_event_ms": null, "kv_decode_ms": 3216.4143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3983.359099999916, "ollama_total_duration_ms": 3979.5296, "ollama_load_ms": 261.6033, "ollama_done_reason": "length", "latency_ms": 3688.7925999999998, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 44.18790039862908}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 423, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544831.020975, "prompt_tokens": 99, "prefill_ms": 60.6937, "prefill_cuda_event_ms": null, "kv_decode_ms": 3614.8665, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3957.0031000000654, "ollama_total_duration_ms": 3953.4562, "ollama_load_ms": 259.0103, "ollama_done_reason": "length", "latency_ms": 60.6937, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1631.1412881402846}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 424, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544831.020975, "prompt_tokens": 99, "prefill_ms": 60.6937, "prefill_cuda_event_ms": null, "kv_decode_ms": 3614.8665, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3957.0031000000654, "ollama_total_duration_ms": 3953.4562, "ollama_load_ms": 259.0103, "ollama_done_reason": "length", "latency_ms": 3614.8665, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 17.70466488873102}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 425, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544831.020975, "prompt_tokens": 99, "prefill_ms": 60.6937, "prefill_cuda_event_ms": null, "kv_decode_ms": 3614.8665, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 3957.0031000000654, "ollama_total_duration_ms": 3953.4562, "ollama_load_ms": 259.0103, "ollama_done_reason": "length", "latency_ms": 3675.5602, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 44.34698144788922}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 426, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544834.9783711, "prompt_tokens": 99, "prefill_ms": 72.9938, "prefill_cuda_event_ms": null, "kv_decode_ms": 3881.4593, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4232.652500000086, "ollama_total_duration_ms": 4229.024, "ollama_load_ms": 260.2576, "ollama_done_reason": "length", "latency_ms": 72.9938, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1356.279574429609}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 427, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544834.9783711, "prompt_tokens": 99, "prefill_ms": 72.9938, "prefill_cuda_event_ms": null, "kv_decode_ms": 3881.4593, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4232.652500000086, "ollama_total_duration_ms": 4229.024, "ollama_load_ms": 260.2576, "ollama_done_reason": "length", "latency_ms": 3881.4593, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.48864384588549}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 428, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544834.9783711, "prompt_tokens": 99, "prefill_ms": 72.9938, "prefill_cuda_event_ms": null, "kv_decode_ms": 3881.4593, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4232.652500000086, "ollama_total_duration_ms": 4229.024, "ollama_load_ms": 260.2576, "ollama_done_reason": "length", "latency_ms": 3954.4531, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 41.21935344232556}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 429, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544839.2112982, "prompt_tokens": 99, "prefill_ms": 68.4392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3880.3299, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4219.350100000156, "ollama_total_duration_ms": 4216.5272, "ollama_load_ms": 249.6954, "ollama_done_reason": "length", "latency_ms": 68.4392, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1446.5394101625968}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 430, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544839.2112982, "prompt_tokens": 99, "prefill_ms": 68.4392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3880.3299, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4219.350100000156, "ollama_total_duration_ms": 4216.5272, "ollama_load_ms": 249.6954, "ollama_done_reason": "length", "latency_ms": 3880.3299, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.493442993081594}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 431, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544839.2112982, "prompt_tokens": 99, "prefill_ms": 68.4392, "prefill_cuda_event_ms": null, "kv_decode_ms": 3880.3299, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4219.350100000156, "ollama_total_duration_ms": 4216.5272, "ollama_load_ms": 249.6954, "ollama_done_reason": "length", "latency_ms": 3948.7691, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 41.27868605941026}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 432, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544843.4308019, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.36040000009598, "prefill_cuda_event_ms": 19.264511108398438, "kv_decode_ms": 203.93829999989066, "kv_decode_cuda_event_ms": 203.8620147705078, "gpu_peak_mb": 631.30419921875, "latency_ms": 19.36040000009598, "cuda_event_ms": 19.264511108398438, "tokens_total": 9, "tokens_per_s": 464.86642837727436}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 433, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544843.4308019, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.36040000009598, "prefill_cuda_event_ms": 19.264511108398438, "kv_decode_ms": 203.93829999989066, "kv_decode_cuda_event_ms": 203.8620147705078, "gpu_peak_mb": 631.30419921875, "latency_ms": 203.93829999989066, "cuda_event_ms": 203.8620147705078, "tokens_total": 64, "tokens_per_s": 313.8204054855528}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 434, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544843.4308019, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.36040000009598, "prefill_cuda_event_ms": 19.264511108398438, "kv_decode_ms": 203.93829999989066, "kv_decode_cuda_event_ms": 203.8620147705078, "gpu_peak_mb": 631.30419921875, "latency_ms": 223.29869999998664, "cuda_event_ms": 223.12652587890625, "tokens_total": 73, "tokens_per_s": 326.9163680756062}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 435, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544843.6614144, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.541800000017247, "prefill_cuda_event_ms": 2.48524808883667, "kv_decode_ms": 163.74590000009448, "kv_decode_cuda_event_ms": 163.6802520751953, "gpu_peak_mb": 631.30419921875, "latency_ms": 2.541800000017247, "cuda_event_ms": 2.48524808883667, "tokens_total": 9, "tokens_per_s": 3540.797859760379}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 436, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544843.6614144, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.541800000017247, "prefill_cuda_event_ms": 2.48524808883667, "kv_decode_ms": 163.74590000009448, "kv_decode_cuda_event_ms": 163.6802520751953, "gpu_peak_mb": 631.30419921875, "latency_ms": 163.74590000009448, "cuda_event_ms": 163.6802520751953, "tokens_total": 64, "tokens_per_s": 390.84948081120245}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 437, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544843.6614144, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.541800000017247, "prefill_cuda_event_ms": 2.48524808883667, "kv_decode_ms": 163.74590000009448, "kv_decode_cuda_event_ms": 163.6802520751953, "gpu_peak_mb": 631.30419921875, "latency_ms": 166.28770000011173, "cuda_event_ms": 166.16550016403198, "tokens_total": 73, "tokens_per_s": 438.99819409343536}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 438, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544843.8283417, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.543500000001586, "prefill_cuda_event_ms": 2.475008010864258, "kv_decode_ms": 231.98530000013307, "kv_decode_cuda_event_ms": 231.90631103515625, "gpu_peak_mb": 631.30419921875, "latency_ms": 2.543500000001586, "cuda_event_ms": 2.475008010864258, "tokens_total": 9, "tokens_per_s": 3538.4312954568068}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 439, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544843.8283417, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.543500000001586, "prefill_cuda_event_ms": 2.475008010864258, "kv_decode_ms": 231.98530000013307, "kv_decode_cuda_event_ms": 231.90631103515625, "gpu_peak_mb": 631.30419921875, "latency_ms": 231.98530000013307, "cuda_event_ms": 231.90631103515625, "tokens_total": 64, "tokens_per_s": 275.8795492643857}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 440, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544843.8283417, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.543500000001586, "prefill_cuda_event_ms": 2.475008010864258, "kv_decode_ms": 231.98530000013307, "kv_decode_cuda_event_ms": 231.90631103515625, "gpu_peak_mb": 631.30419921875, "latency_ms": 234.52880000013465, "cuda_event_ms": 234.3813190460205, "tokens_total": 73, "tokens_per_s": 311.26241212148824}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 441, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.0639646, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.959999999982756, "prefill_cuda_event_ms": 4.840447902679443, "kv_decode_ms": 132.61710000006133, "kv_decode_cuda_event_ms": 132.5465545654297, "gpu_peak_mb": 631.30419921875, "latency_ms": 4.959999999982756, "cuda_event_ms": 4.840447902679443, "tokens_total": 9, "tokens_per_s": 1814.5161290385665}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 442, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.0639646, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.959999999982756, "prefill_cuda_event_ms": 4.840447902679443, "kv_decode_ms": 132.61710000006133, "kv_decode_cuda_event_ms": 132.5465545654297, "gpu_peak_mb": 631.30419921875, "latency_ms": 132.61710000006133, "cuda_event_ms": 132.5465545654297, "tokens_total": 64, "tokens_per_s": 482.5923655393641}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 443, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.0639646, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.959999999982756, "prefill_cuda_event_ms": 4.840447902679443, "kv_decode_ms": 132.61710000006133, "kv_decode_cuda_event_ms": 132.5465545654297, "gpu_peak_mb": 631.30419921875, "latency_ms": 137.57710000004408, "cuda_event_ms": 137.38700246810913, "tokens_total": 73, "tokens_per_s": 530.6115625345832}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 444, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.202446, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.376999999953114, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.66140000005726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 5.376999999953114, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3161.6142830850354}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 445, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.202446, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.376999999953114, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.66140000005726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 176.66140000005726, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 362.2749508380397}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 446, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.202446, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.376999999953114, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.66140000005726, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 182.03840000001037, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 444.9610631602749}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 447, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.3858438, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.802100000006249, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.04099999997925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 4.802100000006249, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3540.1178650960787}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 448, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.3858438, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.802100000006249, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.04099999997925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 184.04099999997925, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 347.74859949689045}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 449, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.3858438, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.802100000006249, "prefill_cuda_event_ms": null, "kv_decode_ms": 184.04099999997925, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 188.8430999999855, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 428.9275064855757}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 450, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.5752757, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.194300000108342, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.82650000001377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 4.194300000108342, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4053.119709978036}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 451, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.5752757, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.194300000108342, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.82650000001377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 178.82650000001377, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 357.88879164997957}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 452, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.5752757, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.194300000108342, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.82650000001377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 183.0208000001221, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 442.5726474802097}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 453, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.7592292, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.681500000060623, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.10730000008334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 4.681500000060623, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3631.3147494990617}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 454, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.7592292, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.681500000060623, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.10730000008334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 173.10730000008334, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 369.71288905764914}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 455, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.7592292, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.681500000060623, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.10730000008334, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 177.78880000014397, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 455.596753000945}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 456, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544844.9375281, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.180699999935314, "prefill_cuda_event_ms": 6.842368125915527, "kv_decode_ms": 199.39529999987826, "kv_decode_cuda_event_ms": 199.3420867919922, "gpu_peak_mb": 630.53759765625, "latency_ms": 7.180699999935314, "cuda_event_ms": 6.842368125915527, "tokens_total": 1, "tokens_per_s": 139.26218892434002}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 457, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544844.9375281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.180699999935314, "prefill_cuda_event_ms": 6.842368125915527, "kv_decode_ms": 199.39529999987826, "kv_decode_cuda_event_ms": 199.3420867919922, "gpu_peak_mb": 630.53759765625, "latency_ms": 199.39529999987826, "cuda_event_ms": 199.3420867919922, "tokens_total": 64, "tokens_per_s": 320.97045416837346}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 458, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544844.9375281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.180699999935314, "prefill_cuda_event_ms": 6.842368125915527, "kv_decode_ms": 199.39529999987826, "kv_decode_cuda_event_ms": 199.3420867919922, "gpu_peak_mb": 630.53759765625, "latency_ms": 206.57599999981358, "cuda_event_ms": 206.18445491790771, "tokens_total": 65, "tokens_per_s": 314.6541708623396}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 459, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544845.145635, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.0552999999381427, "prefill_cuda_event_ms": 2.0008959770202637, "kv_decode_ms": 117.43139999998675, "kv_decode_cuda_event_ms": 117.38931274414062, "gpu_peak_mb": 630.53759765625, "latency_ms": 2.0552999999381427, "cuda_event_ms": 2.0008959770202637, "tokens_total": 1, "tokens_per_s": 486.5469761251868}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 460, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544845.145635, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0552999999381427, "prefill_cuda_event_ms": 2.0008959770202637, "kv_decode_ms": 117.43139999998675, "kv_decode_cuda_event_ms": 117.38931274414062, "gpu_peak_mb": 630.53759765625, "latency_ms": 117.43139999998675, "cuda_event_ms": 117.38931274414062, "tokens_total": 64, "tokens_per_s": 544.9990377361355}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 461, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544845.145635, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0552999999381427, "prefill_cuda_event_ms": 2.0008959770202637, "kv_decode_ms": 117.43139999998675, "kv_decode_cuda_event_ms": 117.38931274414062, "gpu_peak_mb": 630.53759765625, "latency_ms": 119.4866999999249, "cuda_event_ms": 119.39020872116089, "tokens_total": 65, "tokens_per_s": 543.9935992879614}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 462, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544845.2662618, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.5051000000075874, "prefill_cuda_event_ms": 2.4310719966888428, "kv_decode_ms": 173.56089999998403, "kv_decode_cuda_event_ms": 173.52703857421875, "gpu_peak_mb": 630.53759765625, "latency_ms": 2.5051000000075874, "cuda_event_ms": 2.4310719966888428, "tokens_total": 1, "tokens_per_s": 399.1856612498388}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 463, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544845.2662618, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.5051000000075874, "prefill_cuda_event_ms": 2.4310719966888428, "kv_decode_ms": 173.56089999998403, "kv_decode_cuda_event_ms": 173.52703857421875, "gpu_peak_mb": 630.53759765625, "latency_ms": 173.56089999998403, "cuda_event_ms": 173.52703857421875, "tokens_total": 64, "tokens_per_s": 368.74664743041717}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 464, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544845.2662618, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.5051000000075874, "prefill_cuda_event_ms": 2.4310719966888428, "kv_decode_ms": 173.56089999998403, "kv_decode_cuda_event_ms": 173.52703857421875, "gpu_peak_mb": 630.53759765625, "latency_ms": 176.06599999999162, "cuda_event_ms": 175.9581105709076, "tokens_total": 65, "tokens_per_s": 369.17973941591845}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 465, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544845.4433868, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.5016000001869543, "prefill_cuda_event_ms": 2.4289278984069824, "kv_decode_ms": 114.99830000002476, "kv_decode_cuda_event_ms": 114.93580627441406, "gpu_peak_mb": 630.53759765625, "latency_ms": 2.5016000001869543, "cuda_event_ms": 2.4289278984069824, "tokens_total": 1, "tokens_per_s": 399.74416370533504}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 466, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544845.4433868, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.5016000001869543, "prefill_cuda_event_ms": 2.4289278984069824, "kv_decode_ms": 114.99830000002476, "kv_decode_cuda_event_ms": 114.93580627441406, "gpu_peak_mb": 630.53759765625, "latency_ms": 114.99830000002476, "cuda_event_ms": 114.93580627441406, "tokens_total": 64, "tokens_per_s": 556.5299660950311}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 124.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 467, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544845.4433868, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.5016000001869543, "prefill_cuda_event_ms": 2.4289278984069824, "kv_decode_ms": 114.99830000002476, "kv_decode_cuda_event_ms": 114.93580627441406, "gpu_peak_mb": 630.53759765625, "latency_ms": 117.49990000021171, "cuda_event_ms": 117.36473417282104, "tokens_total": 65, "tokens_per_s": 553.1919601623736}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 468, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544845.5614698, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 50.77649999998357, "prefill_cuda_event_ms": null, "kv_decode_ms": 1636.2320999999156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 50.77649999998357, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 334.8005474974742}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 469, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544845.5614698, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.77649999998357, "prefill_cuda_event_ms": null, "kv_decode_ms": 1636.2320999999156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1636.2320999999156, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.1142552453306}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 470, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544845.5614698, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 50.77649999998357, "prefill_cuda_event_ms": null, "kv_decode_ms": 1636.2320999999156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1687.0085999998992, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 48.0139816714656}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 471, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544847.249586, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 39.547000000084154, "prefill_cuda_event_ms": null, "kv_decode_ms": 1676.6503999999713, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 39.547000000084154, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 429.86825802118557}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 472, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544847.249586, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 39.547000000084154, "prefill_cuda_event_ms": null, "kv_decode_ms": 1676.6503999999713, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1676.6503999999713, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 38.17134448541037}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 473, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544847.249586, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 39.547000000084154, "prefill_cuda_event_ms": null, "kv_decode_ms": 1676.6503999999713, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1716.1974000000555, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 47.19736785523471}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 474, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544848.9664712, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 40.2248000000327, "prefill_cuda_event_ms": null, "kv_decode_ms": 1748.846199999889, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 40.2248000000327, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 422.62484835191674}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 475, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544848.9664712, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 40.2248000000327, "prefill_cuda_event_ms": null, "kv_decode_ms": 1748.846199999889, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1748.846199999889, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 36.59555654465445}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 476, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544848.9664712, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 40.2248000000327, "prefill_cuda_event_ms": null, "kv_decode_ms": 1748.846199999889, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1789.0709999999217, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 45.274894065134106}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 477, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544850.7561364, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 34.33650000010857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1702.0431000000826, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.33650000010857, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 495.09996650637794}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 478, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544850.7561364, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.33650000010857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1702.0431000000826, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1702.0431000000826, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 37.601868013798764}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 479, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544850.7561364, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.33650000010857, "prefill_cuda_event_ms": null, "kv_decode_ms": 1702.0431000000826, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1736.3796000001912, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 46.648785783932894}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 480, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544852.4929812, "prompt_tokens": 46, "prefill_ms": 22.3397, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.2414, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 7666.837900000019, "ollama_total_duration_ms": 7662.941, "ollama_load_ms": 6835.5106, "ollama_done_reason": "length", "latency_ms": 22.3397, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2059.1144912420486}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 481, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544852.4929812, "prompt_tokens": 46, "prefill_ms": 22.3397, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.2414, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 7666.837900000019, "ollama_total_duration_ms": 7662.941, "ollama_load_ms": 6835.5106, "ollama_done_reason": "length", "latency_ms": 739.2414, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 86.57523780459265}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 482, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544852.4929812, "prompt_tokens": 46, "prefill_ms": 22.3397, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.2414, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 7666.837900000019, "ollama_total_duration_ms": 7662.941, "ollama_load_ms": 6835.5106, "ollama_done_reason": "length", "latency_ms": 761.5811, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 144.43635746737937}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 483, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544860.1599422, "prompt_tokens": 46, "prefill_ms": 12.0075, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9024, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 951.0199000001194, "ollama_total_duration_ms": 946.5474, "ollama_load_ms": 145.3011, "ollama_done_reason": "length", "latency_ms": 12.0075, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3830.9389964605452}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 484, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544860.1599422, "prompt_tokens": 46, "prefill_ms": 12.0075, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9024, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 951.0199000001194, "ollama_total_duration_ms": 946.5474, "ollama_load_ms": 145.3011, "ollama_done_reason": "length", "latency_ms": 724.9024, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.28774742641217}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 485, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544860.1599422, "prompt_tokens": 46, "prefill_ms": 12.0075, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9024, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 951.0199000001194, "ollama_total_duration_ms": 946.5474, "ollama_load_ms": 145.3011, "ollama_done_reason": "length", "latency_ms": 736.9099, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.2719801973077}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 486, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544861.111207, "prompt_tokens": 46, "prefill_ms": 11.871, "prefill_cuda_event_ms": null, "kv_decode_ms": 733.4592, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 973.7252999998418, "ollama_total_duration_ms": 970.1765, "ollama_load_ms": 161.5688, "ollama_done_reason": "length", "latency_ms": 11.871, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3874.9894701373096}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 487, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544861.111207, "prompt_tokens": 46, "prefill_ms": 11.871, "prefill_cuda_event_ms": null, "kv_decode_ms": 733.4592, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 973.7252999998418, "ollama_total_duration_ms": 970.1765, "ollama_load_ms": 161.5688, "ollama_done_reason": "length", "latency_ms": 733.4592, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.25775066970324}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 488, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544861.111207, "prompt_tokens": 46, "prefill_ms": 11.871, "prefill_cuda_event_ms": null, "kv_decode_ms": 733.4592, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 973.7252999998418, "ollama_total_duration_ms": 970.1765, "ollama_load_ms": 161.5688, "ollama_done_reason": "length", "latency_ms": 745.3302, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.58559360669943}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 489, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544862.0850394, "prompt_tokens": 46, "prefill_ms": 12.1893, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.6277, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 973.846900000126, "ollama_total_duration_ms": 957.4087, "ollama_load_ms": 148.7193, "ollama_done_reason": "length", "latency_ms": 12.1893, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3773.8016128899935}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 490, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544862.0850394, "prompt_tokens": 46, "prefill_ms": 12.1893, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.6277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 973.846900000126, "ollama_total_duration_ms": 957.4087, "ollama_load_ms": 148.7193, "ollama_done_reason": "length", "latency_ms": 729.6277, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.71596802040274}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 491, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544862.0850394, "prompt_tokens": 46, "prefill_ms": 12.1893, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.6277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 973.846900000126, "ollama_total_duration_ms": 957.4087, "ollama_load_ms": 148.7193, "ollama_done_reason": "length", "latency_ms": 741.817, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 148.28454996313107}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 492, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544863.0591576, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.992999999956737, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.8713999999509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 16.992999999956737, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.84776084284976}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 493, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544863.0591576, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.992999999956737, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.8713999999509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 985.8713999999509, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 64.91718899645855}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 494, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544863.0591576, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.992999999956737, "prefill_cuda_event_ms": null, "kv_decode_ms": 985.8713999999509, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1002.8643999999076, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 64.81434578793103}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 495, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544864.0627382, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.39890000003652, "prefill_cuda_event_ms": null, "kv_decode_ms": 946.8401999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 15.39890000003652, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 64.93970348515988}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 496, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544864.0627382, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.39890000003652, "prefill_cuda_event_ms": null, "kv_decode_ms": 946.8401999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 946.8401999999969, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.5932432949089}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 497, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544864.0627382, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.39890000003652, "prefill_cuda_event_ms": null, "kv_decode_ms": 946.8401999999969, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 962.2391000000334, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 67.55077817976607}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 498, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544865.025488, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.326799999958894, "prefill_cuda_event_ms": null, "kv_decode_ms": 938.1065000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 13.326799999958894, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.03676801655945}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 499, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544865.025488, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.326799999958894, "prefill_cuda_event_ms": null, "kv_decode_ms": 938.1065000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 938.1065000000035, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.22253123712474}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 500, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544865.025488, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.326799999958894, "prefill_cuda_event_ms": null, "kv_decode_ms": 938.1065000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 951.4332999999624, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 68.31797878001807}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 501, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544865.9773247, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.79270000006727, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.7813000000024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 13.79270000006727, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 72.50212068667648}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 502, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544865.9773247, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.79270000006727, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.7813000000024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 941.7813000000024, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.95632913925965}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 503, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544865.9773247, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.79270000006727, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.7813000000024, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 955.5740000000696, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 68.0219428322613}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 504, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544866.933464, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.85140000008869, "prefill_cuda_event_ms": null, "kv_decode_ms": 2469.6019000000433, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 39.85140000008869, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 225.83899185423778}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 505, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544866.933464, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.85140000008869, "prefill_cuda_event_ms": null, "kv_decode_ms": 2469.6019000000433, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2469.6019000000433, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.91510801801654}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 506, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544866.933464, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.85140000008869, "prefill_cuda_event_ms": null, "kv_decode_ms": 2469.6019000000433, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2509.453300000132, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 29.090001395920044}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 507, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544869.4434426, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.63660000001801, "prefill_cuda_event_ms": null, "kv_decode_ms": 2512.7707999999984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 38.63660000001801, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 232.93975142729448}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 508, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544869.4434426, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.63660000001801, "prefill_cuda_event_ms": null, "kv_decode_ms": 2512.7707999999984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2512.7707999999984, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.469891643121624}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 509, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544869.4434426, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.63660000001801, "prefill_cuda_event_ms": null, "kv_decode_ms": 2512.7707999999984, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2551.4074000000164, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 28.611659588350935}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 510, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544871.9956498, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 39.02890000017578, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.4956000000457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 39.02890000017578, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 230.59835147696873}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 511, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544871.9956498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.02890000017578, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.4956000000457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2249.4956000000457, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.450822486604864}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 512, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544871.9956498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 39.02890000017578, "prefill_cuda_event_ms": null, "kv_decode_ms": 2249.4956000000457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2288.5245000002215, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 31.898282059026652}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 513, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544874.2850254, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 35.63510000003589, "prefill_cuda_event_ms": null, "kv_decode_ms": 1838.0611999998564, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 35.63510000003589, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 252.55997597848568}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 514, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544874.2850254, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.63510000003589, "prefill_cuda_event_ms": null, "kv_decode_ms": 1838.0611999998564, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1838.0611999998564, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 34.819297638188}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 515, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544874.2850254, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.63510000003589, "prefill_cuda_event_ms": null, "kv_decode_ms": 1838.0611999998564, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 1873.6962999998923, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 38.9604227750272}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 516, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544876.159398, "prompt_tokens": 92, "prefill_ms": 1069.2183, "prefill_cuda_event_ms": null, "kv_decode_ms": 3275.8164, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 15912.739500000043, "ollama_total_duration_ms": 15778.9057, "ollama_load_ms": 11334.1842, "ollama_done_reason": "length", "latency_ms": 1069.2183, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 86.04416890358125}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 517, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544876.159398, "prompt_tokens": 92, "prefill_ms": 1069.2183, "prefill_cuda_event_ms": null, "kv_decode_ms": 3275.8164, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 15912.739500000043, "ollama_total_duration_ms": 15778.9057, "ollama_load_ms": 11334.1842, "ollama_done_reason": "length", "latency_ms": 3275.8164, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.537114473204298}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 518, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544876.159398, "prompt_tokens": 92, "prefill_ms": 1069.2183, "prefill_cuda_event_ms": null, "kv_decode_ms": 3275.8164, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 15912.739500000043, "ollama_total_duration_ms": 15778.9057, "ollama_load_ms": 11334.1842, "ollama_done_reason": "length", "latency_ms": 4345.0347, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 35.90305044054078}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 519, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544892.0733225, "prompt_tokens": 92, "prefill_ms": 64.7106, "prefill_cuda_event_ms": null, "kv_decode_ms": 3790.4725, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4165.87329999993, "ollama_total_duration_ms": 4162.6569, "ollama_load_ms": 280.5496, "ollama_done_reason": "length", "latency_ms": 64.7106, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1421.714525904566}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 520, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544892.0733225, "prompt_tokens": 92, "prefill_ms": 64.7106, "prefill_cuda_event_ms": null, "kv_decode_ms": 3790.4725, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4165.87329999993, "ollama_total_duration_ms": 4162.6569, "ollama_load_ms": 280.5496, "ollama_done_reason": "length", "latency_ms": 3790.4725, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.884438549547582}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 521, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544892.0733225, "prompt_tokens": 92, "prefill_ms": 64.7106, "prefill_cuda_event_ms": null, "kv_decode_ms": 3790.4725, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4165.87329999993, "ollama_total_duration_ms": 4162.6569, "ollama_load_ms": 280.5496, "ollama_done_reason": "length", "latency_ms": 3855.1830999999997, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 40.465004113553}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 522, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544896.2393281, "prompt_tokens": 92, "prefill_ms": 74.4699, "prefill_cuda_event_ms": null, "kv_decode_ms": 3994.2114, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4356.054699999959, "ollama_total_duration_ms": 4352.9359, "ollama_load_ms": 255.5158, "ollama_done_reason": "length", "latency_ms": 74.4699, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1235.3984630031732}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 523, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544896.2393281, "prompt_tokens": 92, "prefill_ms": 74.4699, "prefill_cuda_event_ms": null, "kv_decode_ms": 3994.2114, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4356.054699999959, "ollama_total_duration_ms": 4352.9359, "ollama_load_ms": 255.5158, "ollama_done_reason": "length", "latency_ms": 3994.2114, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.023187956451178}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 524, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544896.2393281, "prompt_tokens": 92, "prefill_ms": 74.4699, "prefill_cuda_event_ms": null, "kv_decode_ms": 3994.2114, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4356.054699999959, "ollama_total_duration_ms": 4352.9359, "ollama_load_ms": 255.5158, "ollama_done_reason": "length", "latency_ms": 4068.6813, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 38.341661215883384}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 525, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544900.5964284, "prompt_tokens": 92, "prefill_ms": 71.829, "prefill_cuda_event_ms": null, "kv_decode_ms": 3860.2577, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 4205.076100000042, "ollama_total_duration_ms": 4187.9904, "ollama_load_ms": 232.9273, "ollama_done_reason": "length", "latency_ms": 71.829, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1280.8197246237594}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 526, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544900.5964284, "prompt_tokens": 92, "prefill_ms": 71.829, "prefill_cuda_event_ms": null, "kv_decode_ms": 3860.2577, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4205.076100000042, "ollama_total_duration_ms": 4187.9904, "ollama_load_ms": 232.9273, "ollama_done_reason": "length", "latency_ms": 3860.2577, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 16.579204025679424}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 527, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544900.5964284, "prompt_tokens": 92, "prefill_ms": 71.829, "prefill_cuda_event_ms": null, "kv_decode_ms": 3860.2577, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 4205.076100000042, "ollama_total_duration_ms": 4187.9904, "ollama_load_ms": 232.9273, "ollama_done_reason": "length", "latency_ms": 3932.0867000000003, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 39.67359112402074}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 528, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544904.8025343, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.235900000106994, "prefill_cuda_event_ms": 15.09887981414795, "kv_decode_ms": 508.0310999999256, "kv_decode_cuda_event_ms": 507.95928955078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 15.235900000106994, "cuda_event_ms": 15.09887981414795, "tokens_total": 1, "tokens_per_s": 65.63445546327932}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 529, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544904.8025343, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.235900000106994, "prefill_cuda_event_ms": 15.09887981414795, "kv_decode_ms": 508.0310999999256, "kv_decode_cuda_event_ms": 507.95928955078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 508.0310999999256, "cuda_event_ms": 507.95928955078125, "tokens_total": 64, "tokens_per_s": 125.97653962525006}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 530, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544904.8025343, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.235900000106994, "prefill_cuda_event_ms": 15.09887981414795, "kv_decode_ms": 508.0310999999256, "kv_decode_cuda_event_ms": 507.95928955078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 523.2670000000326, "cuda_event_ms": 523.0581693649292, "tokens_total": 65, "tokens_per_s": 124.21956668392227}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 531, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544905.3281875, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.698900000126741, "prefill_cuda_event_ms": 8.599552154541016, "kv_decode_ms": 477.60360000006585, "kv_decode_cuda_event_ms": 477.5270080566406, "gpu_peak_mb": 631.30419921875, "latency_ms": 8.698900000126741, "cuda_event_ms": 8.599552154541016, "tokens_total": 1, "tokens_per_s": 114.95706353509411}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 532, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544905.3281875, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.698900000126741, "prefill_cuda_event_ms": 8.599552154541016, "kv_decode_ms": 477.60360000006585, "kv_decode_cuda_event_ms": 477.5270080566406, "gpu_peak_mb": 631.30419921875, "latency_ms": 477.60360000006585, "cuda_event_ms": 477.5270080566406, "tokens_total": 64, "tokens_per_s": 134.00234001584406}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 533, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544905.3281875, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.698900000126741, "prefill_cuda_event_ms": 8.599552154541016, "kv_decode_ms": 477.60360000006585, "kv_decode_cuda_event_ms": 477.5270080566406, "gpu_peak_mb": 631.30419921875, "latency_ms": 486.3025000001926, "cuda_event_ms": 486.12656021118164, "tokens_total": 65, "tokens_per_s": 133.66166120876255}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 534, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544905.815311, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.562299999970492, "prefill_cuda_event_ms": 9.454591751098633, "kv_decode_ms": 462.1270000000095, "kv_decode_cuda_event_ms": 462.02960205078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 9.562299999970492, "cuda_event_ms": 9.454591751098633, "tokens_total": 1, "tokens_per_s": 104.57735063772166}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 535, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544905.815311, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.562299999970492, "prefill_cuda_event_ms": 9.454591751098633, "kv_decode_ms": 462.1270000000095, "kv_decode_cuda_event_ms": 462.02960205078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 462.1270000000095, "cuda_event_ms": 462.02960205078125, "tokens_total": 64, "tokens_per_s": 138.49006874733283}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 536, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544905.815311, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.562299999970492, "prefill_cuda_event_ms": 9.454591751098633, "kv_decode_ms": 462.1270000000095, "kv_decode_cuda_event_ms": 462.02960205078125, "gpu_peak_mb": 631.30419921875, "latency_ms": 471.68929999998, "cuda_event_ms": 471.4841938018799, "tokens_total": 65, "tokens_per_s": 137.8025747033116}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 537, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544906.2878673, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.140600000137056, "prefill_cuda_event_ms": 9.077568054199219, "kv_decode_ms": 468.25970000008965, "kv_decode_cuda_event_ms": 468.19830322265625, "gpu_peak_mb": 631.30419921875, "latency_ms": 9.140600000137056, "cuda_event_ms": 9.077568054199219, "tokens_total": 1, "tokens_per_s": 109.40200861923789}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 538, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544906.2878673, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.140600000137056, "prefill_cuda_event_ms": 9.077568054199219, "kv_decode_ms": 468.25970000008965, "kv_decode_cuda_event_ms": 468.19830322265625, "gpu_peak_mb": 631.30419921875, "latency_ms": 468.25970000008965, "cuda_event_ms": 468.19830322265625, "tokens_total": 64, "tokens_per_s": 136.67629309117942}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 539, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544906.2878673, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.140600000137056, "prefill_cuda_event_ms": 9.077568054199219, "kv_decode_ms": 468.25970000008965, "kv_decode_cuda_event_ms": 468.19830322265625, "gpu_peak_mb": 631.30419921875, "latency_ms": 477.4003000002267, "cuda_event_ms": 477.27587127685547, "tokens_total": 65, "tokens_per_s": 136.15408285241784}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 540, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544906.7665842, "prompt_tokens": 18, "prefill_ms": 12.406, "prefill_cuda_event_ms": null, "kv_decode_ms": 154.252, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 3758.92900000008, "ollama_total_duration_ms": 3678.6277, "ollama_load_ms": 3451.7701, "ollama_done_reason": "stop", "latency_ms": 12.406, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1450.9108495889086}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 541, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544906.7665842, "prompt_tokens": 18, "prefill_ms": 12.406, "prefill_cuda_event_ms": null, "kv_decode_ms": 154.252, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 3758.92900000008, "ollama_total_duration_ms": 3678.6277, "ollama_load_ms": 3451.7701, "ollama_done_reason": "stop", "latency_ms": 154.252, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 220.41853590228976}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 542, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544906.7665842, "prompt_tokens": 18, "prefill_ms": 12.406, "prefill_cuda_event_ms": null, "kv_decode_ms": 154.252, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 3758.92900000008, "ollama_total_duration_ms": 3678.6277, "ollama_load_ms": 3451.7701, "ollama_done_reason": "stop", "latency_ms": 166.65800000000002, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 312.0162248436918}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 543, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544910.5256479, "prompt_tokens": 18, "prefill_ms": 4.8047, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.4791, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 376.8457999999555, "ollama_total_duration_ms": 346.0633, "ollama_load_ms": 158.8267, "ollama_done_reason": "stop", "latency_ms": 4.8047, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3746.3317168605736}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 544, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544910.5256479, "prompt_tokens": 18, "prefill_ms": 4.8047, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.4791, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 376.8457999999555, "ollama_total_duration_ms": 346.0633, "ollama_load_ms": 158.8267, "ollama_done_reason": "stop", "latency_ms": 148.4791, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 228.9884569612828}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 545, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544910.5256479, "prompt_tokens": 18, "prefill_ms": 4.8047, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.4791, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 376.8457999999555, "ollama_total_duration_ms": 346.0633, "ollama_load_ms": 158.8267, "ollama_done_reason": "stop", "latency_ms": 153.28379999999999, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 339.24002405994634}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 546, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544910.9026322, "prompt_tokens": 18, "prefill_ms": 4.4624, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.7923, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 356.0179999999491, "ollama_total_duration_ms": 344.2621, "ollama_load_ms": 165.6123, "ollama_done_reason": "stop", "latency_ms": 4.4624, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4033.703836500538}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 547, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544910.9026322, "prompt_tokens": 18, "prefill_ms": 4.4624, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.7923, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 356.0179999999491, "ollama_total_duration_ms": 344.2621, "ollama_load_ms": 165.6123, "ollama_done_reason": "stop", "latency_ms": 146.7923, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 231.6197784216202}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 548, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544910.9026322, "prompt_tokens": 18, "prefill_ms": 4.4624, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.7923, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 356.0179999999491, "ollama_total_duration_ms": 344.2621, "ollama_load_ms": 165.6123, "ollama_done_reason": "stop", "latency_ms": 151.2547, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 343.79096980126894}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 549, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544911.2588139, "prompt_tokens": 18, "prefill_ms": 4.5194, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3094, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 358.24800000000323, "ollama_total_duration_ms": 331.2133, "ollama_load_ms": 154.5107, "ollama_done_reason": "stop", "latency_ms": 4.5194, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3982.829579147675}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 550, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544911.2588139, "prompt_tokens": 18, "prefill_ms": 4.5194, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3094, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 358.24800000000323, "ollama_total_duration_ms": 331.2133, "ollama_load_ms": 154.5107, "ollama_done_reason": "stop", "latency_ms": 148.3094, "cuda_event_ms": null, "tokens_total": 34, "tokens_per_s": 229.25047232339958}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 551, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544911.2588139, "prompt_tokens": 18, "prefill_ms": 4.5194, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.3094, "kv_decode_cuda_event_ms": null, "gen_tokens": 34, "ollama_wall_ms": 358.24800000000323, "ollama_total_duration_ms": 331.2133, "ollama_load_ms": 154.5107, "ollama_done_reason": "stop", "latency_ms": 152.8288, "cuda_event_ms": null, "tokens_total": 52, "tokens_per_s": 340.25000523461546}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 552, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544911.6173005, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.361199999977543, "prefill_cuda_event_ms": 22.229663848876953, "kv_decode_ms": 571.572900000092, "kv_decode_cuda_event_ms": 571.4974975585938, "gpu_peak_mb": 633.025390625, "latency_ms": 22.361199999977543, "cuda_event_ms": 22.229663848876953, "tokens_total": 17, "tokens_per_s": 760.2454251121171}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 553, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544911.6173005, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.361199999977543, "prefill_cuda_event_ms": 22.229663848876953, "kv_decode_ms": 571.572900000092, "kv_decode_cuda_event_ms": 571.4974975585938, "gpu_peak_mb": 633.025390625, "latency_ms": 571.572900000092, "cuda_event_ms": 571.4974975585938, "tokens_total": 64, "tokens_per_s": 111.97171874312043}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 554, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544911.6173005, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.361199999977543, "prefill_cuda_event_ms": 22.229663848876953, "kv_decode_ms": 571.572900000092, "kv_decode_cuda_event_ms": 571.4974975585938, "gpu_peak_mb": 633.025390625, "latency_ms": 593.9341000000695, "cuda_event_ms": 593.7271614074707, "tokens_total": 81, "tokens_per_s": 136.37876660052103}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 555, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544912.2150502, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 12.579100000039034, "prefill_cuda_event_ms": 12.422143936157227, "kv_decode_ms": 633.3352000001469, "kv_decode_cuda_event_ms": 633.2623901367188, "gpu_peak_mb": 633.025390625, "latency_ms": 12.579100000039034, "cuda_event_ms": 12.422143936157227, "tokens_total": 17, "tokens_per_s": 1351.4480368187906}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 556, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544912.2150502, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 12.579100000039034, "prefill_cuda_event_ms": 12.422143936157227, "kv_decode_ms": 633.3352000001469, "kv_decode_cuda_event_ms": 633.2623901367188, "gpu_peak_mb": 633.025390625, "latency_ms": 633.3352000001469, "cuda_event_ms": 633.2623901367188, "tokens_total": 64, "tokens_per_s": 101.0523337404666}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 557, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544912.2150502, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 12.579100000039034, "prefill_cuda_event_ms": 12.422143936157227, "kv_decode_ms": 633.3352000001469, "kv_decode_cuda_event_ms": 633.2623901367188, "gpu_peak_mb": 633.025390625, "latency_ms": 645.9143000001859, "cuda_event_ms": 645.684534072876, "tokens_total": 81, "tokens_per_s": 125.40363326834023}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 558, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544912.863168, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.625800000047093, "prefill_cuda_event_ms": 11.53433609008789, "kv_decode_ms": 635.2808999999979, "kv_decode_cuda_event_ms": 635.2169189453125, "gpu_peak_mb": 633.025390625, "latency_ms": 11.625800000047093, "cuda_event_ms": 11.53433609008789, "tokens_total": 17, "tokens_per_s": 1462.2649624052656}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 559, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544912.863168, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.625800000047093, "prefill_cuda_event_ms": 11.53433609008789, "kv_decode_ms": 635.2808999999979, "kv_decode_cuda_event_ms": 635.2169189453125, "gpu_peak_mb": 633.025390625, "latency_ms": 635.2808999999979, "cuda_event_ms": 635.2169189453125, "tokens_total": 64, "tokens_per_s": 100.74283675142793}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 560, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544912.863168, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.625800000047093, "prefill_cuda_event_ms": 11.53433609008789, "kv_decode_ms": 635.2808999999979, "kv_decode_cuda_event_ms": 635.2169189453125, "gpu_peak_mb": 633.025390625, "latency_ms": 646.906700000045, "cuda_event_ms": 646.7512550354004, "tokens_total": 81, "tokens_per_s": 125.21125534794177}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 561, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544913.510963, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.706200000186072, "prefill_cuda_event_ms": 10.597375869750977, "kv_decode_ms": 612.0935999999801, "kv_decode_cuda_event_ms": 612.0192260742188, "gpu_peak_mb": 633.025390625, "latency_ms": 10.706200000186072, "cuda_event_ms": 10.597375869750977, "tokens_total": 17, "tokens_per_s": 1587.8649754071978}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 562, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544913.510963, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.706200000186072, "prefill_cuda_event_ms": 10.597375869750977, "kv_decode_ms": 612.0935999999801, "kv_decode_cuda_event_ms": 612.0192260742188, "gpu_peak_mb": 633.025390625, "latency_ms": 612.0935999999801, "cuda_event_ms": 612.0192260742188, "tokens_total": 64, "tokens_per_s": 104.55917199592035}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 563, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544913.510963, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.706200000186072, "prefill_cuda_event_ms": 10.597375869750977, "kv_decode_ms": 612.0935999999801, "kv_decode_cuda_event_ms": 612.0192260742188, "gpu_peak_mb": 633.025390625, "latency_ms": 622.7998000001662, "cuda_event_ms": 622.6166019439697, "tokens_total": 81, "tokens_per_s": 130.05784523369852}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 564, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544914.1346436, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 40.34739999997328, "prefill_cuda_event_ms": null, "kv_decode_ms": 2410.035099999959, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 40.34739999997328, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 24.784744494085427}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 565, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544914.1346436, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 40.34739999997328, "prefill_cuda_event_ms": null, "kv_decode_ms": 2410.035099999959, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2410.035099999959, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.55562983294355}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 566, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544914.1346436, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 40.34739999997328, "prefill_cuda_event_ms": null, "kv_decode_ms": 2410.035099999959, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2450.3824999999324, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.526470867304102}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 567, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544916.5857608, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 36.52790000001005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2304.933099999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 36.52790000001005, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 27.376334254083176}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 568, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544916.5857608, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.52790000001005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2304.933099999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2304.933099999971, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.766532573114944}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 569, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544916.5857608, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.52790000001005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2304.933099999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2341.460999999981, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 27.760445294626102}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 570, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544918.9277365, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 34.394600000041464, "prefill_cuda_event_ms": null, "kv_decode_ms": 2155.741100000114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.394600000041464, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 29.074331435713585}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 571, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544918.9277365, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.394600000041464, "prefill_cuda_event_ms": null, "kv_decode_ms": 2155.741100000114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2155.741100000114, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.688166171715434}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 572, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544918.9277365, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.394600000041464, "prefill_cuda_event_ms": null, "kv_decode_ms": 2155.741100000114, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2190.1357000001553, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 29.678526312317263}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 573, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544921.1184843, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 32.03699999994569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2004.6784000001026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 32.03699999994569, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 31.21390891786669}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 574, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544921.1184843, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 32.03699999994569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2004.6784000001026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2004.6784000001026, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.925320290774184}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 575, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544921.1184843, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 32.03699999994569, "prefill_cuda_event_ms": null, "kv_decode_ms": 2004.6784000001026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2036.7154000000482, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.914129976136312}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 576, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544923.1556408, "prompt_tokens": 26, "prefill_ms": 116.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.3577, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 800.6616000000122, "ollama_total_duration_ms": 780.0268, "ollama_load_ms": 182.7693, "ollama_done_reason": "length", "latency_ms": 116.7791, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 222.64257902313}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 577, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544923.1556408, "prompt_tokens": 26, "prefill_ms": 116.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.3577, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 800.6616000000122, "ollama_total_duration_ms": 780.0268, "ollama_load_ms": 182.7693, "ollama_done_reason": "length", "latency_ms": 423.3577, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 151.17240102164197}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 578, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544923.1556408, "prompt_tokens": 26, "prefill_ms": 116.7791, "prefill_cuda_event_ms": null, "kv_decode_ms": 423.3577, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 800.6616000000122, "ollama_total_duration_ms": 780.0268, "ollama_load_ms": 182.7693, "ollama_done_reason": "length", "latency_ms": 540.1368, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 166.62445513803172}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 579, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544923.95722, "prompt_tokens": 26, "prefill_ms": 5.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 295.2216, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 565.9107000001313, "ollama_total_duration_ms": 533.0019, "ollama_load_ms": 178.6984, "ollama_done_reason": "length", "latency_ms": 5.5373, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4695.429180286421}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 580, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544923.95722, "prompt_tokens": 26, "prefill_ms": 5.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 295.2216, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 565.9107000001313, "ollama_total_duration_ms": 533.0019, "ollama_load_ms": 178.6984, "ollama_done_reason": "length", "latency_ms": 295.2216, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 216.78630560907465}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 581, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544923.95722, "prompt_tokens": 26, "prefill_ms": 5.5373, "prefill_cuda_event_ms": null, "kv_decode_ms": 295.2216, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 565.9107000001313, "ollama_total_duration_ms": 533.0019, "ollama_load_ms": 178.6984, "ollama_done_reason": "length", "latency_ms": 300.75890000000004, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 299.2430149199242}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 582, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544924.5233057, "prompt_tokens": 26, "prefill_ms": 4.8683, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.1989, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 542.102899999918, "ollama_total_duration_ms": 505.2284, "ollama_load_ms": 175.4643, "ollama_done_reason": "length", "latency_ms": 4.8683, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5340.6733356613195}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 583, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544924.5233057, "prompt_tokens": 26, "prefill_ms": 4.8683, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.1989, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 542.102899999918, "ollama_total_duration_ms": 505.2284, "ollama_load_ms": 175.4643, "ollama_done_reason": "length", "latency_ms": 280.1989, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.4091764814209}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 584, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544924.5233057, "prompt_tokens": 26, "prefill_ms": 4.8683, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.1989, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 542.102899999918, "ollama_total_duration_ms": 505.2284, "ollama_load_ms": 175.4643, "ollama_done_reason": "length", "latency_ms": 285.06719999999996, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 315.7150314031218}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 585, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544925.0655367, "prompt_tokens": 26, "prefill_ms": 5.3747, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.7067, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 527.9904999999872, "ollama_total_duration_ms": 506.5251, "ollama_load_ms": 172.0907, "ollama_done_reason": "length", "latency_ms": 5.3747, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4837.479301170299}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 586, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544925.0655367, "prompt_tokens": 26, "prefill_ms": 5.3747, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.7067, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 527.9904999999872, "ollama_total_duration_ms": 506.5251, "ollama_load_ms": 172.0907, "ollama_done_reason": "length", "latency_ms": 280.7067, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 227.99598299577457}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 587, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544925.0655367, "prompt_tokens": 26, "prefill_ms": 5.3747, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.7067, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 527.9904999999872, "ollama_total_duration_ms": 506.5251, "ollama_load_ms": 172.0907, "ollama_done_reason": "length", "latency_ms": 286.08140000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 314.595775887562}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 588, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544925.5936806, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 37.654400000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.800199999814, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 37.654400000064925, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 26.557321322296353}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 589, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544925.5936806, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 37.654400000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.800199999814, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2458.800199999814, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.02895509769555}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 590, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544925.5936806, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 37.654400000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 2458.800199999814, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2496.454599999879, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.03692452488547}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 591, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544928.0907915, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 38.94620000005489, "prefill_cuda_event_ms": null, "kv_decode_ms": 2429.957399999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 38.94620000005489, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.676445969018562}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 592, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544928.0907915, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.94620000005489, "prefill_cuda_event_ms": null, "kv_decode_ms": 2429.957399999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2429.957399999921, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.337910286000106}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 593, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544928.0907915, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.94620000005489, "prefill_cuda_event_ms": null, "kv_decode_ms": 2429.957399999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2468.903599999976, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.32747588848776}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 594, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544930.5608225, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 38.208499999882406, "prefill_cuda_event_ms": null, "kv_decode_ms": 2402.215099999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 38.208499999882406, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 26.17218681715005}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 595, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544930.5608225, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.208499999882406, "prefill_cuda_event_ms": null, "kv_decode_ms": 2402.215099999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2402.215099999921, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.64207713955428}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 596, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544930.5608225, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.208499999882406, "prefill_cuda_event_ms": null, "kv_decode_ms": 2402.215099999921, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2440.4235999998036, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.634720300199206}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 597, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544933.0030093, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 34.489399999984016, "prefill_cuda_event_ms": null, "kv_decode_ms": 2408.2469999998466, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 34.489399999984016, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.99441567555433}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 598, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544933.0030093, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.489399999984016, "prefill_cuda_event_ms": null, "kv_decode_ms": 2408.2469999998466, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2408.2469999998466, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.575347130092585}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 599, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544933.0030093, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.489399999984016, "prefill_cuda_event_ms": null, "kv_decode_ms": 2408.2469999998466, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 2442.7363999998306, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.609502359732513}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 600, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544935.4463158, "prompt_tokens": 10, "prefill_ms": 8.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.3477, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 1414.3775999998525, "ollama_total_duration_ms": 1354.6847, "ollama_load_ms": 1286.8232, "ollama_done_reason": "stop", "latency_ms": 8.0256, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1246.0127591706537}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 601, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544935.4463158, "prompt_tokens": 10, "prefill_ms": 8.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.3477, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 1414.3775999998525, "ollama_total_duration_ms": 1354.6847, "ollama_load_ms": 1286.8232, "ollama_done_reason": "stop", "latency_ms": 22.3477, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 492.2206759532301}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 602, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544935.4463158, "prompt_tokens": 10, "prefill_ms": 8.0256, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.3477, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 1414.3775999998525, "ollama_total_duration_ms": 1354.6847, "ollama_load_ms": 1286.8232, "ollama_done_reason": "stop", "latency_ms": 30.3733, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 691.3967201456543}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 603, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544936.8607707, "prompt_tokens": 10, "prefill_ms": 2.1135, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2793, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 212.60289999986526, "ollama_total_duration_ms": 173.4893, "ollama_load_ms": 137.6253, "ollama_done_reason": "stop", "latency_ms": 2.1135, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4731.488052992665}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 604, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544936.8607707, "prompt_tokens": 10, "prefill_ms": 2.1135, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2793, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 212.60289999986526, "ollama_total_duration_ms": 173.4893, "ollama_load_ms": 137.6253, "ollama_done_reason": "stop", "latency_ms": 21.2793, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 516.9342976507686}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 605, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544936.8607707, "prompt_tokens": 10, "prefill_ms": 2.1135, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.2793, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 212.60289999986526, "ollama_total_duration_ms": 173.4893, "ollama_load_ms": 137.6253, "ollama_done_reason": "stop", "latency_ms": 23.3928, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 897.7121165486816}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 606, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544937.0735703, "prompt_tokens": 10, "prefill_ms": 2.3968, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5926, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 188.7111000000914, "ollama_total_duration_ms": 172.4951, "ollama_load_ms": 138.802, "ollama_done_reason": "stop", "latency_ms": 2.3968, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4172.22963951936}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 607, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544937.0735703, "prompt_tokens": 10, "prefill_ms": 2.3968, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5926, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 188.7111000000914, "ollama_total_duration_ms": 172.4951, "ollama_load_ms": 138.802, "ollama_done_reason": "stop", "latency_ms": 22.5926, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 486.8850862671848}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 608, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544937.0735703, "prompt_tokens": 10, "prefill_ms": 2.3968, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5926, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 188.7111000000914, "ollama_total_duration_ms": 172.4951, "ollama_load_ms": 138.802, "ollama_done_reason": "stop", "latency_ms": 24.9894, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 840.3563110758962}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 609, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544937.2624328, "prompt_tokens": 10, "prefill_ms": 2.6474, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.9597, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 187.88689999996677, "ollama_total_duration_ms": 171.6826, "ollama_load_ms": 136.369, "ollama_done_reason": "stop", "latency_ms": 2.6474, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3777.290926947193}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 610, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544937.2624328, "prompt_tokens": 10, "prefill_ms": 2.6474, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.9597, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 187.88689999996677, "ollama_total_duration_ms": 171.6826, "ollama_load_ms": 136.369, "ollama_done_reason": "stop", "latency_ms": 22.9597, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 479.10033667687287}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 611, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544937.2624328, "prompt_tokens": 10, "prefill_ms": 2.6474, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.9597, "kv_decode_cuda_event_ms": null, "gen_tokens": 11, "ollama_wall_ms": 187.88689999996677, "ollama_total_duration_ms": 171.6826, "ollama_load_ms": 136.369, "ollama_done_reason": "stop", "latency_ms": 25.607100000000003, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 820.085054535656}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 612, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544937.4504268, "prompt_tokens": 11, "prefill_ms": 16614.8814, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 17277.158899999904, "ollama_total_duration_ms": 17225.4039, "ollama_load_ms": 141.0763, "ollama_done_reason": "length", "latency_ms": 16614.8814, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 0.6620570881715714}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 613, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544937.4504268, "prompt_tokens": 11, "prefill_ms": 16614.8814, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 17277.158899999904, "ollama_total_duration_ms": 17225.4039, "ollama_load_ms": 141.0763, "ollama_done_reason": "length", "latency_ms": 432.8872, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 147.8445193112663}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 614, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544937.4504268, "prompt_tokens": 11, "prefill_ms": 16614.8814, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.8872, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 17277.158899999904, "ollama_total_duration_ms": 17225.4039, "ollama_load_ms": 141.0763, "ollama_done_reason": "length", "latency_ms": 17047.7686, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 4.399402746468533}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 615, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544954.7277322, "prompt_tokens": 11, "prefill_ms": 4.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8787, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 501.5291999998226, "ollama_total_duration_ms": 460.2489, "ollama_load_ms": 145.2982, "ollama_done_reason": "length", "latency_ms": 4.2954, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2560.879079945989}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 616, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544954.7277322, "prompt_tokens": 11, "prefill_ms": 4.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8787, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 501.5291999998226, "ollama_total_duration_ms": 460.2489, "ollama_load_ms": 145.2982, "ollama_done_reason": "length", "latency_ms": 269.8787, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 237.14357598432187}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 617, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544954.7277322, "prompt_tokens": 11, "prefill_ms": 4.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8787, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 501.5291999998226, "ollama_total_duration_ms": 460.2489, "ollama_load_ms": 145.2982, "ollama_done_reason": "length", "latency_ms": 274.17409999999995, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 273.5488144212017}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 618, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544955.2296207, "prompt_tokens": 11, "prefill_ms": 4.5041, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.4526, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 556.4363999999387, "ollama_total_duration_ms": 509.7732, "ollama_load_ms": 179.1485, "ollama_done_reason": "length", "latency_ms": 4.5041, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2442.219311294154}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 619, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544955.2296207, "prompt_tokens": 11, "prefill_ms": 4.5041, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.4526, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 556.4363999999387, "ollama_total_duration_ms": 509.7732, "ollama_load_ms": 179.1485, "ollama_done_reason": "length", "latency_ms": 275.4526, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.34487530704013}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 620, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544955.2296207, "prompt_tokens": 11, "prefill_ms": 4.5041, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.4526, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 556.4363999999387, "ollama_total_duration_ms": 509.7732, "ollama_load_ms": 179.1485, "ollama_done_reason": "length", "latency_ms": 279.9567, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 267.89857145765757}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 621, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544955.7862027, "prompt_tokens": 11, "prefill_ms": 4.497, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9463, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 539.9933999999575, "ollama_total_duration_ms": 484.5937, "ollama_load_ms": 157.3831, "ollama_done_reason": "length", "latency_ms": 4.497, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2446.07516121859}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 622, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544955.7862027, "prompt_tokens": 11, "prefill_ms": 4.497, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9463, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 539.9933999999575, "ollama_total_duration_ms": 484.5937, "ollama_load_ms": 157.3831, "ollama_done_reason": "length", "latency_ms": 273.9463, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 233.62242892128856}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 1000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 623, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544955.7862027, "prompt_tokens": 11, "prefill_ms": 4.497, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.9463, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "ollama_wall_ms": 539.9933999999575, "ollama_total_duration_ms": 484.5937, "ollama_load_ms": 157.3831, "ollama_done_reason": "length", "latency_ms": 278.4433, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 269.3546585606477}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 624, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544956.3263278, "prompt_tokens": 11, "prefill_ms": 20.2403, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 5940.499399999908, "ollama_total_duration_ms": 5915.4423, "ollama_load_ms": 5771.1439, "ollama_done_reason": "stop", "latency_ms": 20.2403, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 543.470205481144}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 625, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544956.3263278, "prompt_tokens": 11, "prefill_ms": 20.2403, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 5940.499399999908, "ollama_total_duration_ms": 5915.4423, "ollama_load_ms": 5771.1439, "ollama_done_reason": "stop", "latency_ms": 107.3584, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 93.14594852382301}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 626, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544956.3263278, "prompt_tokens": 11, "prefill_ms": 20.2403, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3584, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 5940.499399999908, "ollama_total_duration_ms": 5915.4423, "ollama_load_ms": 5771.1439, "ollama_done_reason": "stop", "latency_ms": 127.59870000000001, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 164.5784792478293}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 627, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544962.2670116, "prompt_tokens": 11, "prefill_ms": 11.34, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.8435, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 312.42870000005496, "ollama_total_duration_ms": 292.4744, "ollama_load_ms": 168.2181, "ollama_done_reason": "stop", "latency_ms": 11.34, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 970.0176366843034}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 628, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544962.2670116, "prompt_tokens": 11, "prefill_ms": 11.34, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.8435, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 312.42870000005496, "ollama_total_duration_ms": 292.4744, "ollama_load_ms": 168.2181, "ollama_done_reason": "stop", "latency_ms": 102.8435, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 97.23511938041781}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 629, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544962.2670116, "prompt_tokens": 11, "prefill_ms": 11.34, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.8435, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 312.42870000005496, "ollama_total_duration_ms": 292.4744, "ollama_load_ms": 168.2181, "ollama_done_reason": "stop", "latency_ms": 114.18350000000001, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 183.91448852067066}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 630, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544962.5795557, "prompt_tokens": 11, "prefill_ms": 11.3762, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.6971, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 311.5891999998439, "ollama_total_duration_ms": 294.4358, "ollama_load_ms": 169.8483, "ollama_done_reason": "stop", "latency_ms": 11.3762, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 966.9309611293753}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 631, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544962.5795557, "prompt_tokens": 11, "prefill_ms": 11.3762, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.6971, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 311.5891999998439, "ollama_total_duration_ms": 294.4358, "ollama_load_ms": 169.8483, "ollama_done_reason": "stop", "latency_ms": 102.6971, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 97.3737330460159}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 632, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544962.5795557, "prompt_tokens": 11, "prefill_ms": 11.3762, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.6971, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 311.5891999998439, "ollama_total_duration_ms": 294.4358, "ollama_load_ms": 169.8483, "ollama_done_reason": "stop", "latency_ms": 114.0733, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 184.09215828769746}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 633, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544962.891259, "prompt_tokens": 11, "prefill_ms": 12.1581, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3867, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "ollama_wall_ms": 313.9932999999928, "ollama_total_duration_ms": 287.0695, "ollama_load_ms": 159.9716, "ollama_done_reason": "stop", "latency_ms": 12.1581, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 904.746629818804}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 634, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544962.891259, "prompt_tokens": 11, "prefill_ms": 12.1581, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3867, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 313.9932999999928, "ollama_total_duration_ms": 287.0695, "ollama_load_ms": 159.9716, "ollama_done_reason": "stop", "latency_ms": 105.3867, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 94.888633954759}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 635, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544962.891259, "prompt_tokens": 11, "prefill_ms": 12.1581, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3867, "kv_decode_cuda_event_ms": null, "gen_tokens": 10, "ollama_wall_ms": 313.9932999999928, "ollama_total_duration_ms": 287.0695, "ollama_load_ms": 159.9716, "ollama_done_reason": "stop", "latency_ms": 117.54480000000001, "cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 178.65528717561304}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 636, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544963.205409, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.917600000013408, "prefill_cuda_event_ms": 13.812735557556152, "kv_decode_ms": 301.359100000127, "kv_decode_cuda_event_ms": 301.3140563964844, "gpu_peak_mb": 633.26806640625, "latency_ms": 13.917600000013408, "cuda_event_ms": 13.812735557556152, "tokens_total": 17, "tokens_per_s": 1221.4749669471475}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 637, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544963.205409, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.917600000013408, "prefill_cuda_event_ms": 13.812735557556152, "kv_decode_ms": 301.359100000127, "kv_decode_cuda_event_ms": 301.3140563964844, "gpu_peak_mb": 633.26806640625, "latency_ms": 301.359100000127, "cuda_event_ms": 301.3140563964844, "tokens_total": 64, "tokens_per_s": 212.37122091210463}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 638, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544963.205409, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.917600000013408, "prefill_cuda_event_ms": 13.812735557556152, "kv_decode_ms": 301.359100000127, "kv_decode_cuda_event_ms": 301.3140563964844, "gpu_peak_mb": 633.26806640625, "latency_ms": 315.2767000001404, "cuda_event_ms": 315.1267919540405, "tokens_total": 81, "tokens_per_s": 256.91717783129525}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 639, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544963.5250416, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.597200000058365, "prefill_cuda_event_ms": 3.551232099533081, "kv_decode_ms": 217.1672000001763, "kv_decode_cuda_event_ms": 217.1327362060547, "gpu_peak_mb": 633.26806640625, "latency_ms": 3.597200000058365, "cuda_event_ms": 3.551232099533081, "tokens_total": 17, "tokens_per_s": 4725.897920528237}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 640, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544963.5250416, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.597200000058365, "prefill_cuda_event_ms": 3.551232099533081, "kv_decode_ms": 217.1672000001763, "kv_decode_cuda_event_ms": 217.1327362060547, "gpu_peak_mb": 633.26806640625, "latency_ms": 217.1672000001763, "cuda_event_ms": 217.1327362060547, "tokens_total": 64, "tokens_per_s": 294.70380425749397}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 641, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544963.5250416, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.597200000058365, "prefill_cuda_event_ms": 3.551232099533081, "kv_decode_ms": 217.1672000001763, "kv_decode_cuda_event_ms": 217.1327362060547, "gpu_peak_mb": 633.26806640625, "latency_ms": 220.76440000023467, "cuda_event_ms": 220.68396830558777, "tokens_total": 81, "tokens_per_s": 366.90698319074045}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 642, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544963.7464375, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.5546999999951367, "prefill_cuda_event_ms": 3.5093441009521484, "kv_decode_ms": 193.66609999997308, "kv_decode_cuda_event_ms": 193.6063995361328, "gpu_peak_mb": 633.26806640625, "latency_ms": 3.5546999999951367, "cuda_event_ms": 3.5093441009521484, "tokens_total": 17, "tokens_per_s": 4782.400765190665}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 643, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544963.7464375, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.5546999999951367, "prefill_cuda_event_ms": 3.5093441009521484, "kv_decode_ms": 193.66609999997308, "kv_decode_cuda_event_ms": 193.6063995361328, "gpu_peak_mb": 633.26806640625, "latency_ms": 193.66609999997308, "cuda_event_ms": 193.6063995361328, "tokens_total": 64, "tokens_per_s": 330.4656829461062}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 644, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544963.7464375, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.5546999999951367, "prefill_cuda_event_ms": 3.5093441009521484, "kv_decode_ms": 193.66609999997308, "kv_decode_cuda_event_ms": 193.6063995361328, "gpu_peak_mb": 633.26806640625, "latency_ms": 197.22079999996822, "cuda_event_ms": 197.11574363708496, "tokens_total": 81, "tokens_per_s": 410.7071870716124}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 645, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544963.944545, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.799800000133473, "prefill_cuda_event_ms": 8.683520317077637, "kv_decode_ms": 368.1516999999985, "kv_decode_cuda_event_ms": 368.10137939453125, "gpu_peak_mb": 633.26806640625, "latency_ms": 8.799800000133473, "cuda_event_ms": 8.683520317077637, "tokens_total": 17, "tokens_per_s": 1931.8620877454202}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 646, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544963.944545, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.799800000133473, "prefill_cuda_event_ms": 8.683520317077637, "kv_decode_ms": 368.1516999999985, "kv_decode_cuda_event_ms": 368.10137939453125, "gpu_peak_mb": 633.26806640625, "latency_ms": 368.1516999999985, "cuda_event_ms": 368.10137939453125, "tokens_total": 64, "tokens_per_s": 173.84138114804375}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 75.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 647, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544963.944545, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.799800000133473, "prefill_cuda_event_ms": 8.683520317077637, "kv_decode_ms": 368.1516999999985, "kv_decode_cuda_event_ms": 368.10137939453125, "gpu_peak_mb": 633.26806640625, "latency_ms": 376.951500000132, "cuda_event_ms": 376.7848997116089, "tokens_total": 81, "tokens_per_s": 214.88175534510842}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 648, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544964.3224623, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 26.79760000000897, "prefill_cuda_event_ms": 26.713247299194336, "kv_decode_ms": 467.62950000015735, "kv_decode_cuda_event_ms": 467.58502197265625, "gpu_peak_mb": 630.7822265625, "latency_ms": 26.79760000000897, "cuda_event_ms": 26.713247299194336, "tokens_total": 1, "tokens_per_s": 37.316774636522126}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 649, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544964.3224623, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 26.79760000000897, "prefill_cuda_event_ms": 26.713247299194336, "kv_decode_ms": 467.62950000015735, "kv_decode_cuda_event_ms": 467.58502197265625, "gpu_peak_mb": 630.7822265625, "latency_ms": 467.62950000015735, "cuda_event_ms": 467.58502197265625, "tokens_total": 64, "tokens_per_s": 136.8604846357607}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 650, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544964.3224623, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 26.79760000000897, "prefill_cuda_event_ms": 26.713247299194336, "kv_decode_ms": 467.62950000015735, "kv_decode_cuda_event_ms": 467.58502197265625, "gpu_peak_mb": 630.7822265625, "latency_ms": 494.4271000001663, "cuda_event_ms": 494.2982692718506, "tokens_total": 65, "tokens_per_s": 131.46528578222782}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 651, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544964.8263476, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.989900000007765, "prefill_cuda_event_ms": 6.9251837730407715, "kv_decode_ms": 598.6795000001166, "kv_decode_cuda_event_ms": 598.6273803710938, "gpu_peak_mb": 630.7822265625, "latency_ms": 6.989900000007765, "cuda_event_ms": 6.9251837730407715, "tokens_total": 1, "tokens_per_s": 143.06356314094467}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 652, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544964.8263476, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.989900000007765, "prefill_cuda_event_ms": 6.9251837730407715, "kv_decode_ms": 598.6795000001166, "kv_decode_cuda_event_ms": 598.6273803710938, "gpu_peak_mb": 630.7822265625, "latency_ms": 598.6795000001166, "cuda_event_ms": 598.6273803710938, "tokens_total": 64, "tokens_per_s": 106.90194001963911}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 653, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544964.8263476, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.989900000007765, "prefill_cuda_event_ms": 6.9251837730407715, "kv_decode_ms": 598.6795000001166, "kv_decode_cuda_event_ms": 598.6273803710938, "gpu_peak_mb": 630.7822265625, "latency_ms": 605.6694000001244, "cuda_event_ms": 605.5525641441345, "tokens_total": 65, "tokens_per_s": 107.31927351784101}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 654, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544965.4328003, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.759499999987383, "prefill_cuda_event_ms": 18.654207229614258, "kv_decode_ms": 555.2411999999549, "kv_decode_cuda_event_ms": 555.0806884765625, "gpu_peak_mb": 630.7822265625, "latency_ms": 18.759499999987383, "cuda_event_ms": 18.654207229614258, "tokens_total": 1, "tokens_per_s": 53.30632479547283}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 655, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544965.4328003, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.759499999987383, "prefill_cuda_event_ms": 18.654207229614258, "kv_decode_ms": 555.2411999999549, "kv_decode_cuda_event_ms": 555.0806884765625, "gpu_peak_mb": 630.7822265625, "latency_ms": 555.2411999999549, "cuda_event_ms": 555.0806884765625, "tokens_total": 64, "tokens_per_s": 115.26522167304083}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 656, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544965.4328003, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.759499999987383, "prefill_cuda_event_ms": 18.654207229614258, "kv_decode_ms": 555.2411999999549, "kv_decode_cuda_event_ms": 555.0806884765625, "gpu_peak_mb": 630.7822265625, "latency_ms": 574.0006999999423, "cuda_event_ms": 573.7348957061768, "tokens_total": 65, "tokens_per_s": 113.24028002057582}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 657, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544966.0083883, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 22.442199999886725, "prefill_cuda_event_ms": 22.354944229125977, "kv_decode_ms": 515.2236000001267, "kv_decode_cuda_event_ms": 515.1692504882812, "gpu_peak_mb": 630.7822265625, "latency_ms": 22.442199999886725, "cuda_event_ms": 22.354944229125977, "tokens_total": 1, "tokens_per_s": 44.55891133690313}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 658, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544966.0083883, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 22.442199999886725, "prefill_cuda_event_ms": 22.354944229125977, "kv_decode_ms": 515.2236000001267, "kv_decode_cuda_event_ms": 515.1692504882812, "gpu_peak_mb": 630.7822265625, "latency_ms": 515.2236000001267, "cuda_event_ms": 515.1692504882812, "tokens_total": 64, "tokens_per_s": 124.21791237820679}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 659, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544966.0083883, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 22.442199999886725, "prefill_cuda_event_ms": 22.354944229125977, "kv_decode_ms": 515.2236000001267, "kv_decode_cuda_event_ms": 515.1692504882812, "gpu_peak_mb": 630.7822265625, "latency_ms": 537.6658000000134, "cuda_event_ms": 537.5241947174072, "tokens_total": 65, "tokens_per_s": 120.89294130294019}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 660, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544966.5469313, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.042499999944994, "prefill_cuda_event_ms": 4.986879825592041, "kv_decode_ms": 330.70889999999054, "kv_decode_cuda_event_ms": 330.6690673828125, "gpu_peak_mb": 631.55810546875, "latency_ms": 5.042499999944994, "cuda_event_ms": 4.986879825592041, "tokens_total": 1, "tokens_per_s": 198.3143282123765}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 661, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544966.5469313, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.042499999944994, "prefill_cuda_event_ms": 4.986879825592041, "kv_decode_ms": 330.70889999999054, "kv_decode_cuda_event_ms": 330.6690673828125, "gpu_peak_mb": 631.55810546875, "latency_ms": 330.70889999999054, "cuda_event_ms": 330.6690673828125, "tokens_total": 64, "tokens_per_s": 193.52366991031033}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 662, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544966.5469313, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.042499999944994, "prefill_cuda_event_ms": 4.986879825592041, "kv_decode_ms": 330.70889999999054, "kv_decode_cuda_event_ms": 330.6690673828125, "gpu_peak_mb": 631.55810546875, "latency_ms": 335.75139999993553, "cuda_event_ms": 335.65594720840454, "tokens_total": 65, "tokens_per_s": 193.59561866313135}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 663, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544966.883805, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.977699999926699, "prefill_cuda_event_ms": 4.928512096405029, "kv_decode_ms": 647.8478999999879, "kv_decode_cuda_event_ms": 647.7967529296875, "gpu_peak_mb": 631.55810546875, "latency_ms": 4.977699999926699, "cuda_event_ms": 4.928512096405029, "tokens_total": 1, "tokens_per_s": 200.89599614575525}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 664, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544966.883805, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.977699999926699, "prefill_cuda_event_ms": 4.928512096405029, "kv_decode_ms": 647.8478999999879, "kv_decode_cuda_event_ms": 647.7967529296875, "gpu_peak_mb": 631.55810546875, "latency_ms": 647.8478999999879, "cuda_event_ms": 647.7967529296875, "tokens_total": 64, "tokens_per_s": 98.78861998317998}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 665, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544966.883805, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.977699999926699, "prefill_cuda_event_ms": 4.928512096405029, "kv_decode_ms": 647.8478999999879, "kv_decode_cuda_event_ms": 647.7967529296875, "gpu_peak_mb": 631.55810546875, "latency_ms": 652.8255999999146, "cuda_event_ms": 652.7252650260925, "tokens_total": 65, "tokens_per_s": 99.56717383633317}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 666, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544967.5373778, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.413799999876574, "prefill_cuda_event_ms": 12.346367835998535, "kv_decode_ms": 702.3950000000241, "kv_decode_cuda_event_ms": 702.3216552734375, "gpu_peak_mb": 631.55810546875, "latency_ms": 12.413799999876574, "cuda_event_ms": 12.346367835998535, "tokens_total": 1, "tokens_per_s": 80.55551080329494}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 667, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544967.5373778, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.413799999876574, "prefill_cuda_event_ms": 12.346367835998535, "kv_decode_ms": 702.3950000000241, "kv_decode_cuda_event_ms": 702.3216552734375, "gpu_peak_mb": 631.55810546875, "latency_ms": 702.3950000000241, "cuda_event_ms": 702.3216552734375, "tokens_total": 64, "tokens_per_s": 91.11682173135885}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 668, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544967.5373778, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.413799999876574, "prefill_cuda_event_ms": 12.346367835998535, "kv_decode_ms": 702.3950000000241, "kv_decode_cuda_event_ms": 702.3216552734375, "gpu_peak_mb": 631.55810546875, "latency_ms": 714.8087999999007, "cuda_event_ms": 714.668023109436, "tokens_total": 65, "tokens_per_s": 90.93340764692465}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 669, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544968.2529418, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.66649999981928, "prefill_cuda_event_ms": 11.582464218139648, "kv_decode_ms": 731.4022999998997, "kv_decode_cuda_event_ms": 731.3541259765625, "gpu_peak_mb": 631.55810546875, "latency_ms": 11.66649999981928, "cuda_event_ms": 11.582464218139648, "tokens_total": 1, "tokens_per_s": 85.71551022290237}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 670, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544968.2529418, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.66649999981928, "prefill_cuda_event_ms": 11.582464218139648, "kv_decode_ms": 731.4022999998997, "kv_decode_cuda_event_ms": 731.3541259765625, "gpu_peak_mb": 631.55810546875, "latency_ms": 731.4022999998997, "cuda_event_ms": 731.3541259765625, "tokens_total": 64, "tokens_per_s": 87.50314293516547}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 50.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 671, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544968.2529418, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.66649999981928, "prefill_cuda_event_ms": 11.582464218139648, "kv_decode_ms": 731.4022999998997, "kv_decode_cuda_event_ms": 731.3541259765625, "gpu_peak_mb": 631.55810546875, "latency_ms": 743.068799999719, "cuda_event_ms": 742.9365901947021, "tokens_total": 65, "tokens_per_s": 87.47507633213046}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 672, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544968.9970443, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.763899999996283, "prefill_cuda_event_ms": null, "kv_decode_ms": 892.0387999999093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 21.763899999996283, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 781.1100032624164}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 673, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544968.9970443, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.763899999996283, "prefill_cuda_event_ms": null, "kv_decode_ms": 892.0387999999093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 892.0387999999093, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.74575814415977}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 674, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544968.9970443, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.763899999996283, "prefill_cuda_event_ms": null, "kv_decode_ms": 892.0387999999093, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 913.8026999999056, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 88.64057854065037}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 675, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544969.9140596, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.60820000019703, "prefill_cuda_event_ms": null, "kv_decode_ms": 934.459299999844, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 20.60820000019703, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 824.9143544723686}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 676, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544969.9140596, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.60820000019703, "prefill_cuda_event_ms": null, "kv_decode_ms": 934.459299999844, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 934.459299999844, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.48880416729834}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 677, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544969.9140596, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.60820000019703, "prefill_cuda_event_ms": null, "kv_decode_ms": 934.459299999844, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 955.067500000041, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 84.81075944893583}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 678, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544970.8701563, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.45070000008309, "prefill_cuda_event_ms": null, "kv_decode_ms": 956.5838999999414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 20.45070000008309, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 831.2673893769373}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 679, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544970.8701563, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.45070000008309, "prefill_cuda_event_ms": null, "kv_decode_ms": 956.5838999999414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 956.5838999999414, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 66.90474301313655}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 680, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544970.8701563, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.45070000008309, "prefill_cuda_event_ms": null, "kv_decode_ms": 956.5838999999414, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 977.0346000000245, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 82.90392172395734}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 681, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766544971.848, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.662400000015623, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.3126999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 19.662400000015623, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 864.5943526724353}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 682, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766544971.848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.662400000015623, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.3126999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 958.3126999998512, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 66.78404658522206}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 683, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766544971.848, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.662400000015623, "prefill_cuda_event_ms": null, "kv_decode_ms": 958.3126999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "latency_ms": 977.9750999998669, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 82.82419460373892}
