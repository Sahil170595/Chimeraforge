{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554142.3858883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 51.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.0526, "kv_decode_ms_equiv": 394.0526, "kv_decode_ms_per_token": 6.157071875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 646.6161999996984, "ollama_total_duration_ms": 622.4966, "ollama_load_ms": 139.1815, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 51.8946, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 481.7456922300201, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554142.3858883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 51.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.0526, "kv_decode_ms_equiv": 394.0526, "kv_decode_ms_per_token": 6.157071875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 646.6161999996984, "ollama_total_duration_ms": 622.4966, "ollama_load_ms": 139.1815, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 394.0526, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 162.41486542659536, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554142.3858883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 51.8946, "prefill_cuda_event_ms": null, "kv_decode_ms": 394.0526, "kv_decode_ms_equiv": 394.0526, "kv_decode_ms_per_token": 6.157071875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 646.6161999996984, "ollama_total_duration_ms": 622.4966, "ollama_load_ms": 139.1815, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 445.94719999999995, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 199.57519634611455, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554143.032676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1624, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.6179, "kv_decode_ms_equiv": 120.6179, "kv_decode_ms_per_token": 1.8846546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.59359999891603, "ollama_total_duration_ms": 308.9586, "ollama_load_ms": 155.7979, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.1624, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11561.228264890862, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554143.032676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1624, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.6179, "kv_decode_ms_equiv": 120.6179, "kv_decode_ms_per_token": 1.8846546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.59359999891603, "ollama_total_duration_ms": 308.9586, "ollama_load_ms": 155.7979, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.6179, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 530.6011794269341, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554143.032676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.1624, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.6179, "kv_decode_ms_equiv": 120.6179, "kv_decode_ms_per_token": 1.8846546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.59359999891603, "ollama_total_duration_ms": 308.9586, "ollama_load_ms": 155.7979, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 122.78030000000001, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 724.8719867926694, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554143.3774085, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.4809, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.3632, "kv_decode_ms_equiv": 120.3632, "kv_decode_ms_per_token": 1.880675, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.5503000002209, "ollama_total_duration_ms": 303.3021, "ollama_load_ms": 152.2907, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.4809, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 16881.62603822, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554143.3774085, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.4809, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.3632, "kv_decode_ms_equiv": 120.3632, "kv_decode_ms_per_token": 1.880675, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.5503000002209, "ollama_total_duration_ms": 303.3021, "ollama_load_ms": 152.2907, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.3632, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 531.7239820809018, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554143.3774085, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.4809, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.3632, "kv_decode_ms_equiv": 120.3632, "kv_decode_ms_per_token": 1.880675, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.5503000002209, "ollama_total_duration_ms": 303.3021, "ollama_load_ms": 152.2907, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 121.84410000000001, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 730.4416052972609, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554143.695357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9094, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.0552, "kv_decode_ms_equiv": 115.0552, "kv_decode_ms_per_token": 1.7977375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 336.02079999946, "ollama_total_duration_ms": 298.9357, "ollama_load_ms": 148.3586, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.9094, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 13093.118257044098, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554143.695357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9094, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.0552, "kv_decode_ms_equiv": 115.0552, "kv_decode_ms_per_token": 1.7977375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 336.02079999946, "ollama_total_duration_ms": 298.9357, "ollama_load_ms": 148.3586, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 115.0552, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 556.2547368567435, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554143.695357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 1.9094, "prefill_cuda_event_ms": null, "kv_decode_ms": 115.0552, "kv_decode_ms_equiv": 115.0552, "kv_decode_ms_per_token": 1.7977375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 336.02079999946, "ollama_total_duration_ms": 298.9357, "ollama_load_ms": 148.3586, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 116.9646, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 760.9139859410454, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554144.0315094, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.4628, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.4924, "kv_decode_ms_equiv": 120.4924, "kv_decode_ms_per_token": 1.88269375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 345.2906000002258, "ollama_total_duration_ms": 331.2816, "ollama_load_ms": 180.1926, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4628, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10151.047588111092, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554144.0315094, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.4628, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.4924, "kv_decode_ms_equiv": 120.4924, "kv_decode_ms_per_token": 1.88269375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 345.2906000002258, "ollama_total_duration_ms": 331.2816, "ollama_load_ms": 180.1926, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.4924, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 531.1538321089131, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554144.0315094, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.4628, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.4924, "kv_decode_ms_equiv": 120.4924, "kv_decode_ms_per_token": 1.88269375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 345.2906000002258, "ollama_total_duration_ms": 331.2816, "ollama_load_ms": 180.1926, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 122.9552, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 723.840878628964, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554144.3769197, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2272, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.8796, "kv_decode_ms_equiv": 120.8796, "kv_decode_ms_per_token": 1.88874375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 338.22259999942617, "ollama_total_duration_ms": 299.1948, "ollama_load_ms": 145.0837, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2272, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11224.856321839081, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554144.3769197, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2272, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.8796, "kv_decode_ms_equiv": 120.8796, "kv_decode_ms_per_token": 1.88874375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 338.22259999942617, "ollama_total_duration_ms": 299.1948, "ollama_load_ms": 145.0837, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.8796, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 529.4524468975742, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554144.3769197, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2272, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.8796, "kv_decode_ms_equiv": 120.8796, "kv_decode_ms_per_token": 1.88874375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 338.22259999942617, "ollama_total_duration_ms": 299.1948, "ollama_load_ms": 145.0837, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 123.10679999999999, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 722.9495040079022, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554144.7153866, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 11.1309, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6893, "kv_decode_ms_equiv": 261.06221176470586, "kv_decode_ms_per_token": 4.079097058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 363.9911000009306, "ollama_total_duration_ms": 327.9561, "ollama_load_ms": 152.9829, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 11.1309, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1617.119909441285, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554144.7153866, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 11.1309, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6893, "kv_decode_ms_equiv": 261.06221176470586, "kv_decode_ms_per_token": 4.079097058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 363.9911000009306, "ollama_total_duration_ms": 327.9561, "ollama_load_ms": 152.9829, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 261.06221176470586, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 245.1522936520698, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554144.7153866, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 11.1309, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6893, "kv_decode_ms_equiv": 261.06221176470586, "kv_decode_ms_per_token": 4.079097058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 363.9911000009306, "ollama_total_duration_ms": 327.9561, "ollama_load_ms": 152.9829, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 272.19311176470586, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 301.2567050957702, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554145.079461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.2402, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8574, "kv_decode_ms_equiv": 251.9668705882353, "kv_decode_ms_per_token": 3.9369823529411767, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.209400000138, "ollama_total_duration_ms": 294.2049, "ollama_load_ms": 128.5373, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.2402, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4245.082779114193, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554145.079461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.2402, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8574, "kv_decode_ms_equiv": 251.9668705882353, "kv_decode_ms_per_token": 3.9369823529411767, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.209400000138, "ollama_total_duration_ms": 294.2049, "ollama_load_ms": 128.5373, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 251.9668705882353, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 254.00164652832044, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554145.079461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.2402, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8574, "kv_decode_ms_equiv": 251.9668705882353, "kv_decode_ms_per_token": 3.9369823529411767, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 317.209400000138, "ollama_total_duration_ms": 294.2049, "ollama_load_ms": 128.5373, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 256.2070705882353, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 320.0536183944228, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554145.396802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 3.9636, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.1703, "kv_decode_ms_equiv": 254.43821176470587, "kv_decode_ms_per_token": 3.975597058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 359.9635999999009, "ollama_total_duration_ms": 327.0869, "ollama_load_ms": 162.8824, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 3.9636, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4541.326067211626, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554145.396802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 3.9636, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.1703, "kv_decode_ms_equiv": 254.43821176470587, "kv_decode_ms_per_token": 3.975597058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 359.9635999999009, "ollama_total_duration_ms": 327.0869, "ollama_load_ms": 162.8824, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 254.43821176470587, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 251.53454568052302, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554145.396802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 3.9636, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.1703, "kv_decode_ms_equiv": 254.43821176470587, "kv_decode_ms_per_token": 3.975597058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 359.9635999999009, "ollama_total_duration_ms": 327.0869, "ollama_load_ms": 162.8824, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 258.4018117647059, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 317.3352363127667, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554145.7568514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.6661, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.1879, "kv_decode_ms_equiv": 258.23604705882354, "kv_decode_ms_per_token": 4.034938235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 375.6226000004972, "ollama_total_duration_ms": 347.2532, "ollama_load_ms": 183.9501, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.6661, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3857.6112813698805, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554145.7568514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.6661, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.1879, "kv_decode_ms_equiv": 258.23604705882354, "kv_decode_ms_per_token": 4.034938235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 375.6226000004972, "ollama_total_duration_ms": 347.2532, "ollama_load_ms": 183.9501, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 258.23604705882354, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 247.835268270744, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554145.7568514, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.6661, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.1879, "kv_decode_ms_equiv": 258.23604705882354, "kv_decode_ms_per_token": 4.034938235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 375.6226000004972, "ollama_total_duration_ms": 347.2532, "ollama_load_ms": 183.9501, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 262.9021470588235, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 311.9031202953727, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554146.1325889, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 5.0873, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.5408, "kv_decode_ms_equiv": 253.25327058823527, "kv_decode_ms_per_token": 3.957082352941176, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 322.26880000052915, "ollama_total_duration_ms": 295.2412, "ollama_load_ms": 128.0557, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.0873, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3538.2226328307743, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554146.1325889, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 5.0873, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.5408, "kv_decode_ms_equiv": 253.25327058823527, "kv_decode_ms_per_token": 3.957082352941176, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 322.26880000052915, "ollama_total_duration_ms": 295.2412, "ollama_load_ms": 128.0557, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 253.25327058823527, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 252.7114451526972, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554146.1325889, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 5.0873, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.5408, "kv_decode_ms_equiv": 253.25327058823527, "kv_decode_ms_per_token": 3.957082352941176, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 322.26880000052915, "ollama_total_duration_ms": 295.2412, "ollama_load_ms": 128.0557, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 258.34057058823527, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 317.4104625273838, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554146.455036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.3922, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.791, "kv_decode_ms_equiv": 266.90070588235295, "kv_decode_ms_per_token": 4.170323529411765, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.7918000009522, "ollama_total_duration_ms": 306.0233, "ollama_load_ms": 141.1498, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.3922, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4098.17403579072, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554146.455036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.3922, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.791, "kv_decode_ms_equiv": 266.90070588235295, "kv_decode_ms_per_token": 4.170323529411765, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.7918000009522, "ollama_total_duration_ms": 306.0233, "ollama_load_ms": 141.1498, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 266.90070588235295, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.78954940722613, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554146.455036, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.3922, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.791, "kv_decode_ms_equiv": 266.90070588235295, "kv_decode_ms_per_token": 4.170323529411765, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.7918000009522, "ollama_total_duration_ms": 306.0233, "ollama_load_ms": 141.1498, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 271.29290588235295, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 302.25633705128865, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554146.7859366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 20.8818, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0612, "kv_decode_ms_equiv": 530.3789714285714, "kv_decode_ms_per_token": 8.287171428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 42, "gen_tokens_equiv": 64, "ollama_wall_ms": 4638.3633999994345, "ollama_total_duration_ms": 4585.4545, "ollama_load_ms": 4164.5476, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 20.8818, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 814.1060636535165, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554146.7859366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 20.8818, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0612, "kv_decode_ms_equiv": 530.3789714285714, "kv_decode_ms_per_token": 8.287171428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 42, "gen_tokens_equiv": 64, "ollama_wall_ms": 4638.3633999994345, "ollama_total_duration_ms": 4585.4545, "ollama_load_ms": 4164.5476, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 530.3789714285714, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 120.6684341719215, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554146.7859366, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 20.8818, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0612, "kv_decode_ms_equiv": 530.3789714285714, "kv_decode_ms_per_token": 8.287171428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 42, "gen_tokens_equiv": 64, "ollama_wall_ms": 4638.3633999994345, "ollama_total_duration_ms": 4585.4545, "ollama_load_ms": 4164.5476, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 551.2607714285714, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 146.9359043816805, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554151.4244695, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.2199, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.9856, "kv_decode_ms_equiv": 511.9775219512195, "kv_decode_ms_per_token": 7.999648780487805, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 746.793100001014, "ollama_total_duration_ms": 720.4324, "ollama_load_ms": 360.5905, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.2199, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2068.15168067738, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554151.4244695, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.2199, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.9856, "kv_decode_ms_equiv": 511.9775219512195, "kv_decode_ms_per_token": 7.999648780487805, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 746.793100001014, "ollama_total_duration_ms": 720.4324, "ollama_load_ms": 360.5905, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 511.9775219512195, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 125.00548804581665, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554151.4244695, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.2199, "prefill_cuda_event_ms": null, "kv_decode_ms": 327.9856, "kv_decode_ms_equiv": 511.9775219512195, "kv_decode_ms_per_token": 7.999648780487805, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 746.793100001014, "ollama_total_duration_ms": 720.4324, "ollama_load_ms": 360.5905, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 520.1974219512196, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 155.71011424119592, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554152.1714537, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8387, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.4701, "kv_decode_ms_equiv": 520.5386926829268, "kv_decode_ms_per_token": 8.133417073170731, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.1618999997736, "ollama_total_duration_ms": 640.9698, "ollama_load_ms": 277.7879, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.8387, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1923.3597701019382, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554152.1714537, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8387, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.4701, "kv_decode_ms_equiv": 520.5386926829268, "kv_decode_ms_per_token": 8.133417073170731, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.1618999997736, "ollama_total_duration_ms": 640.9698, "ollama_load_ms": 277.7879, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 520.5386926829268, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 122.94955379807665, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554152.1714537, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8387, "prefill_cuda_event_ms": null, "kv_decode_ms": 333.4701, "kv_decode_ms_equiv": 520.5386926829268, "kv_decode_ms_per_token": 8.133417073170731, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.1618999997736, "ollama_total_duration_ms": 640.9698, "ollama_load_ms": 277.7879, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 529.3773926829268, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 153.00993415960878, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554152.8338084, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8552, "prefill_cuda_event_ms": null, "kv_decode_ms": 331.7729, "kv_decode_ms_equiv": 517.8894048780488, "kv_decode_ms_per_token": 8.092021951219513, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 669.3694999994477, "ollama_total_duration_ms": 651.5248, "ollama_load_ms": 293.1321, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.8552, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1919.7759508537356, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554152.8338084, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8552, "prefill_cuda_event_ms": null, "kv_decode_ms": 331.7729, "kv_decode_ms_equiv": 517.8894048780488, "kv_decode_ms_per_token": 8.092021951219513, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 669.3694999994477, "ollama_total_duration_ms": 651.5248, "ollama_load_ms": 293.1321, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 517.8894048780488, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 123.57850806982728, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554152.8338084, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.8552, "prefill_cuda_event_ms": null, "kv_decode_ms": 331.7729, "kv_decode_ms_equiv": 517.8894048780488, "kv_decode_ms_per_token": 8.092021951219513, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 669.3694999994477, "ollama_total_duration_ms": 651.5248, "ollama_load_ms": 293.1321, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 526.7446048780488, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 153.774712165781, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554153.5033972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 9.4353, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.8756, "kv_decode_ms_equiv": 507.1228878048781, "kv_decode_ms_per_token": 7.92379512195122, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 716.0489000016241, "ollama_total_duration_ms": 703.3005, "ollama_load_ms": 345.6099, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 9.4353, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1801.7445126281093, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554153.5033972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 9.4353, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.8756, "kv_decode_ms_equiv": 507.1228878048781, "kv_decode_ms_per_token": 7.92379512195122, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 716.0489000016241, "ollama_total_duration_ms": 703.3005, "ollama_load_ms": 345.6099, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 507.1228878048781, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 126.20215245466262, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554153.5033972, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 9.4353, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.8756, "kv_decode_ms_equiv": 507.1228878048781, "kv_decode_ms_per_token": 7.92379512195122, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 716.0489000016241, "ollama_total_duration_ms": 703.3005, "ollama_load_ms": 345.6099, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 516.5581878048781, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 156.80711662748146, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554154.2196062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.9919, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.1896, "kv_decode_ms_equiv": 509.1740097560975, "kv_decode_ms_per_token": 7.955843902439024, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.4242000004597, "ollama_total_duration_ms": 668.6581, "ollama_load_ms": 312.1603, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.9919, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1890.5904202671295, "gen_tokens": 0}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554154.2196062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.9919, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.1896, "kv_decode_ms_equiv": 509.1740097560975, "kv_decode_ms_per_token": 7.955843902439024, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.4242000004597, "ollama_total_duration_ms": 668.6581, "ollama_load_ms": 312.1603, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 509.1740097560975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 125.69376828691045, "gen_tokens": 64}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554154.2196062, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.9919, "prefill_cuda_event_ms": null, "kv_decode_ms": 326.1896, "kv_decode_ms_equiv": 509.1740097560975, "kv_decode_ms_per_token": 7.955843902439024, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 41, "gen_tokens_equiv": 64, "ollama_wall_ms": 692.4242000004597, "ollama_total_duration_ms": 668.6581, "ollama_load_ms": 312.1603, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 518.1659097560976, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 156.32058858933226, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554154.9121664, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 11.9988, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.0108, "kv_decode_ms_equiv": 265.0108, "kv_decode_ms_per_token": 4.14079375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2442.6726999990933, "ollama_total_duration_ms": 2401.7495, "ollama_load_ms": 2070.7065, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 11.9988, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 2166.8833550021673, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554154.9121664, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 11.9988, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.0108, "kv_decode_ms_equiv": 265.0108, "kv_decode_ms_per_token": 4.14079375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2442.6726999990933, "ollama_total_duration_ms": 2401.7495, "ollama_load_ms": 2070.7065, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 265.0108, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 241.49959171475277, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554154.9121664, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 11.9988, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.0108, "kv_decode_ms_equiv": 265.0108, "kv_decode_ms_per_token": 4.14079375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2442.6726999990933, "ollama_total_duration_ms": 2401.7495, "ollama_load_ms": 2070.7065, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 277.00960000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 324.89848727264325, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554157.3550293, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.9302, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.8822, "kv_decode_ms_equiv": 249.8822, "kv_decode_ms_per_token": 3.904409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.45099999885133, "ollama_total_duration_ms": 453.5723, "ollama_load_ms": 150.6252, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 3.9302, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 6615.439417841331, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554157.3550293, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.9302, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.8822, "kv_decode_ms_equiv": 249.8822, "kv_decode_ms_per_token": 3.904409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.45099999885133, "ollama_total_duration_ms": 453.5723, "ollama_load_ms": 150.6252, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 249.8822, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 256.12068406633205, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554157.3550293, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.9302, "prefill_cuda_event_ms": null, "kv_decode_ms": 249.8822, "kv_decode_ms_equiv": 249.8822, "kv_decode_ms_per_token": 3.904409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.45099999885133, "ollama_total_duration_ms": 453.5723, "ollama_load_ms": 150.6252, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 253.81240000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 354.592604616638, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554157.8416054, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2741, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.1821, "kv_decode_ms_equiv": 254.1821, "kv_decode_ms_per_token": 3.9715953125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0496000001003, "ollama_total_duration_ms": 450.5918, "ollama_load_ms": 153.4447, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.2741, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 6083.152008610001, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554157.8416054, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2741, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.1821, "kv_decode_ms_equiv": 254.1821, "kv_decode_ms_per_token": 3.9715953125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0496000001003, "ollama_total_duration_ms": 450.5918, "ollama_load_ms": 153.4447, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 254.1821, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 251.7879897915707, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554157.8416054, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2741, "prefill_cuda_event_ms": null, "kv_decode_ms": 254.1821, "kv_decode_ms_equiv": 254.1821, "kv_decode_ms_per_token": 3.9715953125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0496000001003, "ollama_total_duration_ms": 450.5918, "ollama_load_ms": 153.4447, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 258.45619999999997, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 348.221478146007, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554158.318781, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.78, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.0945, "kv_decode_ms_equiv": 250.0945, "kv_decode_ms_per_token": 3.9077265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0165999998426, "ollama_total_duration_ms": 447.8844, "ollama_load_ms": 143.7685, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.78, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5439.330543933054, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554158.318781, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.78, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.0945, "kv_decode_ms_equiv": 250.0945, "kv_decode_ms_per_token": 3.9077265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0165999998426, "ollama_total_duration_ms": 447.8844, "ollama_load_ms": 143.7685, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 250.0945, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 255.90326856448263, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554158.318781, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.78, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.0945, "kv_decode_ms_equiv": 250.0945, "kv_decode_ms_per_token": 3.9077265625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 477.0165999998426, "ollama_total_duration_ms": 447.8844, "ollama_load_ms": 143.7685, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 254.8745, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 353.11496442366735, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554158.795996, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 5.1785, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.0315, "kv_decode_ms_equiv": 258.0315, "kv_decode_ms_per_token": 4.0317421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9458000011946, "ollama_total_duration_ms": 456.3676, "ollama_load_ms": 145.1012, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.1785, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5020.758907019407, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554158.795996, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 5.1785, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.0315, "kv_decode_ms_equiv": 258.0315, "kv_decode_ms_per_token": 4.0317421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9458000011946, "ollama_total_duration_ms": 456.3676, "ollama_load_ms": 145.1012, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 258.0315, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 248.0317325597844, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554158.795996, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 5.1785, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.0315, "kv_decode_ms_equiv": 258.0315, "kv_decode_ms_per_token": 4.0317421875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9458000011946, "ollama_total_duration_ms": 456.3676, "ollama_load_ms": 145.1012, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 263.21, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 341.9322974051138, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554159.2830756, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.5946, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.8792, "kv_decode_ms_equiv": 259.8792, "kv_decode_ms_per_token": 4.0606125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.9645999994391, "ollama_total_duration_ms": 465.2434, "ollama_load_ms": 168.9785, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5946, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5658.816871980151, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554159.2830756, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.5946, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.8792, "kv_decode_ms_equiv": 259.8792, "kv_decode_ms_per_token": 4.0606125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.9645999994391, "ollama_total_duration_ms": 465.2434, "ollama_load_ms": 168.9785, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 259.8792, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 246.26826617905547, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554159.2830756, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.5946, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.8792, "kv_decode_ms_equiv": 259.8792, "kv_decode_ms_per_token": 4.0606125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.9645999994391, "ollama_total_duration_ms": 465.2434, "ollama_load_ms": 168.9785, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 264.47380000000004, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 340.2983584763405, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554159.773265, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.1332, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.8206, "kv_decode_ms_equiv": 117.31130810810811, "kv_decode_ms_per_token": 1.8329891891891892, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 1693.2804999996733, "ollama_total_duration_ms": 1648.8878, "ollama_load_ms": 1521.7459, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.1332, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2090.1981999704913, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554159.773265, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.1332, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.8206, "kv_decode_ms_equiv": 117.31130810810811, "kv_decode_ms_per_token": 1.8329891891891892, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 1693.2804999996733, "ollama_total_duration_ms": 1648.8878, "ollama_load_ms": 1521.7459, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.31130810810811, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 545.5569546715894, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554159.773265, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 8.1332, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.8206, "kv_decode_ms_equiv": 117.31130810810811, "kv_decode_ms_per_token": 1.8329891891891892, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 1693.2804999996733, "ollama_total_duration_ms": 1648.8878, "ollama_load_ms": 1521.7459, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 125.44450810810811, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 645.7038352782585, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554161.4666755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1638, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.9458, "kv_decode_ms_equiv": 123.92773333333334, "kv_decode_ms_per_token": 1.9363708333333334, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 308.0900000004476, "ollama_total_duration_ms": 284.6971, "ollama_load_ms": 156.8658, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1638, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7856.548664386727, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554161.4666755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1638, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.9458, "kv_decode_ms_equiv": 123.92773333333334, "kv_decode_ms_per_token": 1.9363708333333334, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 308.0900000004476, "ollama_total_duration_ms": 284.6971, "ollama_load_ms": 156.8658, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.92773333333334, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 516.430005444033, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554161.4666755, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1638, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.9458, "kv_decode_ms_equiv": 123.92773333333334, "kv_decode_ms_per_token": 1.9363708333333334, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 308.0900000004476, "ollama_total_duration_ms": 284.6971, "ollama_load_ms": 156.8658, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.09153333333333, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 642.3904750675831, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554161.774883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3405, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.5725, "kv_decode_ms_equiv": 122.09666666666668, "kv_decode_ms_per_token": 1.9077604166666668, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4787000011129, "ollama_total_duration_ms": 285.0565, "ollama_load_ms": 159.7607, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.3405, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7263.405255287332, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554161.774883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3405, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.5725, "kv_decode_ms_equiv": 122.09666666666668, "kv_decode_ms_per_token": 1.9077604166666668, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4787000011129, "ollama_total_duration_ms": 285.0565, "ollama_load_ms": 159.7607, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 122.09666666666668, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 524.1748341478063, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554161.774883, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.3405, "prefill_cuda_event_ms": null, "kv_decode_ms": 91.5725, "kv_decode_ms_equiv": 122.09666666666668, "kv_decode_ms_per_token": 1.9077604166666668, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4787000011129, "ollama_total_duration_ms": 285.0565, "ollama_load_ms": 159.7607, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 124.43716666666668, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 650.9309249782018, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554162.0855298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.371, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.6226, "kv_decode_ms_equiv": 123.49680000000001, "kv_decode_ms_per_token": 1.9296375000000001, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.51559999992605, "ollama_total_duration_ms": 270.5346, "ollama_load_ms": 143.1737, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.371, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7169.970476592156, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554162.0855298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.371, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.6226, "kv_decode_ms_equiv": 123.49680000000001, "kv_decode_ms_per_token": 1.9296375000000001, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.51559999992605, "ollama_total_duration_ms": 270.5346, "ollama_load_ms": 143.1737, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.49680000000001, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 518.2320513567963, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554162.0855298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.371, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.6226, "kv_decode_ms_equiv": 123.49680000000001, "kv_decode_ms_per_token": 1.9296375000000001, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.51559999992605, "ollama_total_duration_ms": 270.5346, "ollama_load_ms": 143.1737, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 125.8678, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 643.5323410753186, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554162.3861892, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8648, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.4485, "kv_decode_ms_equiv": 120.598, "kv_decode_ms_per_token": 1.88434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.1311999998579, "ollama_total_duration_ms": 270.8403, "ollama_load_ms": 137.1683, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.8648, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5934.096621055572, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554162.3861892, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8648, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.4485, "kv_decode_ms_equiv": 120.598, "kv_decode_ms_per_token": 1.88434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.1311999998579, "ollama_total_duration_ms": 270.8403, "ollama_load_ms": 137.1683, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 120.598, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 530.6887344732086, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554162.3861892, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.8648, "prefill_cuda_event_ms": null, "kv_decode_ms": 90.4485, "kv_decode_ms_equiv": 120.598, "kv_decode_ms_per_token": 1.88434375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.1311999998579, "ollama_total_duration_ms": 270.8403, "ollama_load_ms": 137.1683, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.4628, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 656.0680626067124, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554162.6784422, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.0576, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.2041, "kv_decode_ms_equiv": 126.9388, "kv_decode_ms_per_token": 1.98341875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.3171000008151, "ollama_total_duration_ms": 264.7837, "ollama_load_ms": 140.3458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.0576, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5559.916274201989, "gen_tokens": 0}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554162.6784422, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.0576, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.2041, "kv_decode_ms_equiv": 126.9388, "kv_decode_ms_per_token": 1.98341875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.3171000008151, "ollama_total_duration_ms": 264.7837, "ollama_load_ms": 140.3458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.9388, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 504.17996703923467, "gen_tokens": 64}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554162.6784422, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 3.0576, "prefill_cuda_event_ms": null, "kv_decode_ms": 95.2041, "kv_decode_ms_equiv": 126.9388, "kv_decode_ms_per_token": 1.98341875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.3171000008151, "ollama_total_duration_ms": 264.7837, "ollama_load_ms": 140.3458, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 129.9964, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 623.0941779926214, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554162.9649317, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 18.4851, "prefill_cuda_event_ms": null, "kv_decode_ms": 516.3649, "kv_decode_ms_equiv": 516.3649, "kv_decode_ms_per_token": 8.0682015625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 874.7299000006024, "ollama_total_duration_ms": 871.8102, "ollama_load_ms": 306.7569, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 18.4851, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1352.4406143326246, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554162.9649317, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 18.4851, "prefill_cuda_event_ms": null, "kv_decode_ms": 516.3649, "kv_decode_ms_equiv": 516.3649, "kv_decode_ms_per_token": 8.0682015625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 874.7299000006024, "ollama_total_duration_ms": 871.8102, "ollama_load_ms": 306.7569, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 516.3649, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 123.9433586597385, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554162.9649317, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 18.4851, "prefill_cuda_event_ms": null, "kv_decode_ms": 516.3649, "kv_decode_ms_equiv": 516.3649, "kv_decode_ms_per_token": 8.0682015625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 874.7299000006024, "ollama_total_duration_ms": 871.8102, "ollama_load_ms": 306.7569, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 534.85, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 166.40179489576516, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554163.8397934, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.5263, "prefill_cuda_event_ms": null, "kv_decode_ms": 503.1092, "kv_decode_ms_equiv": 503.1092, "kv_decode_ms_per_token": 7.86108125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 865.6437999998161, "ollama_total_duration_ms": 838.929, "ollama_load_ms": 290.4847, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.5263, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2624.3137419564778, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554163.8397934, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.5263, "prefill_cuda_event_ms": null, "kv_decode_ms": 503.1092, "kv_decode_ms_equiv": 503.1092, "kv_decode_ms_per_token": 7.86108125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 865.6437999998161, "ollama_total_duration_ms": 838.929, "ollama_load_ms": 290.4847, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 503.1092, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 127.20896377963273, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554163.8397934, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.5263, "prefill_cuda_event_ms": null, "kv_decode_ms": 503.1092, "kv_decode_ms_equiv": 503.1092, "kv_decode_ms_per_token": 7.86108125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 865.6437999998161, "ollama_total_duration_ms": 838.929, "ollama_load_ms": 290.4847, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 512.6355, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 173.61263509842763, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554164.7074323, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.7444, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.8591, "kv_decode_ms_equiv": 506.8591, "kv_decode_ms_per_token": 7.9196734375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 820.1468000006571, "ollama_total_duration_ms": 807.9645, "ollama_load_ms": 258.974, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.7444, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2565.576125774804, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554164.7074323, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.7444, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.8591, "kv_decode_ms_equiv": 506.8591, "kv_decode_ms_per_token": 7.9196734375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 820.1468000006571, "ollama_total_duration_ms": 807.9645, "ollama_load_ms": 258.974, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 506.8591, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 126.2678326185719, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554164.7074323, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.7444, "prefill_cuda_event_ms": null, "kv_decode_ms": 506.8591, "kv_decode_ms_equiv": 506.8591, "kv_decode_ms_per_token": 7.9196734375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 820.1468000006571, "ollama_total_duration_ms": 807.9645, "ollama_load_ms": 258.974, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 516.6035, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 172.27912702875605, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554165.5276966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.3728, "prefill_cuda_event_ms": null, "kv_decode_ms": 507.9475, "kv_decode_ms_equiv": 507.9475, "kv_decode_ms_per_token": 7.9366796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 838.5109999999258, "ollama_total_duration_ms": 835.8742, "ollama_load_ms": 289.6489, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.3728, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2667.2925913280983, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554165.5276966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.3728, "prefill_cuda_event_ms": null, "kv_decode_ms": 507.9475, "kv_decode_ms_equiv": 507.9475, "kv_decode_ms_per_token": 7.9366796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 838.5109999999258, "ollama_total_duration_ms": 835.8742, "ollama_load_ms": 289.6489, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 507.9475, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 125.99727334025663, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554165.5276966, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.3728, "prefill_cuda_event_ms": null, "kv_decode_ms": 507.9475, "kv_decode_ms_equiv": 507.9475, "kv_decode_ms_per_token": 7.9366796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 838.5109999999258, "ollama_total_duration_ms": 835.8742, "ollama_load_ms": 289.6489, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 517.3203, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 172.0404167398805, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554166.3663304, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 514.2779, "kv_decode_ms_equiv": 514.2779, "kv_decode_ms_per_token": 8.0355921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 908.6175000011281, "ollama_total_duration_ms": 883.7022, "ollama_load_ms": 327.1889, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 8.7628, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2852.969370520838, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554166.3663304, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 514.2779, "kv_decode_ms_equiv": 514.2779, "kv_decode_ms_per_token": 8.0355921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 908.6175000011281, "ollama_total_duration_ms": 883.7022, "ollama_load_ms": 327.1889, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 514.2779, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 124.44633533737303, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554166.3663304, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.7628, "prefill_cuda_event_ms": null, "kv_decode_ms": 514.2779, "kv_decode_ms_equiv": 514.2779, "kv_decode_ms_per_token": 8.0355921875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 908.6175000011281, "ollama_total_duration_ms": 883.7022, "ollama_load_ms": 327.1889, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 523.0407, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 170.1588423233603, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554167.2751093, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.4418, "prefill_cuda_event_ms": null, "kv_decode_ms": 513.5292, "kv_decode_ms_equiv": 513.5292, "kv_decode_ms_per_token": 8.02389375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 869.6388000007573, "ollama_total_duration_ms": 848.4497, "ollama_load_ms": 298.949, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.4418, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2647.800207587536, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554167.2751093, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.4418, "prefill_cuda_event_ms": null, "kv_decode_ms": 513.5292, "kv_decode_ms_equiv": 513.5292, "kv_decode_ms_per_token": 8.02389375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 869.6388000007573, "ollama_total_duration_ms": 848.4497, "ollama_load_ms": 298.949, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 513.5292, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 124.62777189690482, "gen_tokens": 64}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554167.2751093, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 9.4418, "prefill_cuda_event_ms": null, "kv_decode_ms": 513.5292, "kv_decode_ms_equiv": 513.5292, "kv_decode_ms_per_token": 8.02389375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 869.6388000007573, "ollama_total_duration_ms": 848.4497, "ollama_load_ms": 298.949, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 522.971, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 170.18152058144716, "gen_tokens": 64}
