{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555045.529683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 175.4088000016054, "prefill_cuda_event_ms": 175.30307006835938, "kv_decode_ms": 3284.776200000124, "kv_decode_cuda_event_ms": 3284.7724609375, "gpu_peak_mb": 34.4453125, "hf_load_ms": 5756.5254000001005, "params_millions_measured": 5.03672, "latency_ms": 175.4088000016054, "cuda_event_ms": 175.30307006835938, "tokens_total": 17, "tokens_per_s": 96.9164602907289, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555045.529683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 175.4088000016054, "prefill_cuda_event_ms": 175.30307006835938, "kv_decode_ms": 3284.776200000124, "kv_decode_cuda_event_ms": 3284.7724609375, "gpu_peak_mb": 34.4453125, "hf_load_ms": 5756.5254000001005, "params_millions_measured": 5.03672, "latency_ms": 3284.776200000124, "cuda_event_ms": 3284.7724609375, "tokens_total": 512, "tokens_per_s": 155.87058868728428, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555045.529683, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 175.4088000016054, "prefill_cuda_event_ms": 175.30307006835938, "kv_decode_ms": 3284.776200000124, "kv_decode_cuda_event_ms": 3284.7724609375, "gpu_peak_mb": 34.4453125, "hf_load_ms": 5756.5254000001005, "params_millions_measured": 5.03672, "latency_ms": 3460.1850000017293, "cuda_event_ms": 3460.0755310058594, "tokens_total": 529, "tokens_per_s": 152.8819990837876, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555054.7856994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.023199999821372, "prefill_cuda_event_ms": 6.946400165557861, "kv_decode_ms": 3215.315899999041, "kv_decode_cuda_event_ms": 3215.326171875, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 7.023199999821372, "cuda_event_ms": 6.946400165557861, "tokens_total": 17, "tokens_per_s": 2420.549037537359, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555054.7856994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.023199999821372, "prefill_cuda_event_ms": 6.946400165557861, "kv_decode_ms": 3215.315899999041, "kv_decode_cuda_event_ms": 3215.326171875, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 3215.315899999041, "cuda_event_ms": 3215.326171875, "tokens_total": 512, "tokens_per_s": 159.23785280325106, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555054.7856994, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.023199999821372, "prefill_cuda_event_ms": 6.946400165557861, "kv_decode_ms": 3215.315899999041, "kv_decode_cuda_event_ms": 3215.326171875, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 3222.3390999988624, "cuda_event_ms": 3222.272572040558, "tokens_total": 529, "tokens_per_s": 164.16645907942674, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555058.0088425, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.983799999943585, "prefill_cuda_event_ms": 6.926400184631348, "kv_decode_ms": 3082.447399998273, "kv_decode_cuda_event_ms": 3082.459228515625, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 6.983799999943585, "cuda_event_ms": 6.926400184631348, "tokens_total": 17, "tokens_per_s": 2434.2048741569524, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555058.0088425, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.983799999943585, "prefill_cuda_event_ms": 6.926400184631348, "kv_decode_ms": 3082.447399998273, "kv_decode_cuda_event_ms": 3082.459228515625, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 3082.447399998273, "cuda_event_ms": 3082.459228515625, "tokens_total": 512, "tokens_per_s": 166.1017800337118, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555058.0088425, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.983799999943585, "prefill_cuda_event_ms": 6.926400184631348, "kv_decode_ms": 3082.447399998273, "kv_decode_cuda_event_ms": 3082.459228515625, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 3089.4311999982165, "cuda_event_ms": 3089.3856287002563, "tokens_total": 529, "tokens_per_s": 171.22893042586784, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555061.0991266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.946999999854597, "prefill_cuda_event_ms": 6.883552074432373, "kv_decode_ms": 2999.3419000002177, "kv_decode_cuda_event_ms": 2999.343017578125, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 6.946999999854597, "cuda_event_ms": 6.883552074432373, "tokens_total": 17, "tokens_per_s": 2447.0994674472167, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555061.0991266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.946999999854597, "prefill_cuda_event_ms": 6.883552074432373, "kv_decode_ms": 2999.3419000002177, "kv_decode_cuda_event_ms": 2999.343017578125, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 2999.3419000002177, "cuda_event_ms": 2999.343017578125, "tokens_total": 512, "tokens_per_s": 170.70411345901007, "gen_tokens": 512}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555061.0991266, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.946999999854597, "prefill_cuda_event_ms": 6.883552074432373, "kv_decode_ms": 2999.3419000002177, "kv_decode_cuda_event_ms": 2999.343017578125, "gpu_peak_mb": 34.4453125, "params_millions_measured": 5.03672, "latency_ms": 3006.2889000000723, "cuda_event_ms": 3006.2265696525574, "tokens_total": 529, "tokens_per_s": 175.96445903784806, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555064.1063027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 15.08540000031644, "prefill_cuda_event_ms": 15.0065279006958, "kv_decode_ms": 1638.3683999993082, "kv_decode_cuda_event_ms": 1638.3118896484375, "gpu_peak_mb": 132.49267578125, "hf_load_ms": 213.7096999995265, "params_millions_measured": 45.1712, "latency_ms": 15.08540000031644, "cuda_event_ms": 15.0065279006958, "tokens_total": 17, "tokens_per_s": 1126.9174168164848, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555064.1063027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 15.08540000031644, "prefill_cuda_event_ms": 15.0065279006958, "kv_decode_ms": 1638.3683999993082, "kv_decode_cuda_event_ms": 1638.3118896484375, "gpu_peak_mb": 132.49267578125, "hf_load_ms": 213.7096999995265, "params_millions_measured": 45.1712, "latency_ms": 1638.3683999993082, "cuda_event_ms": 1638.3118896484375, "tokens_total": 512, "tokens_per_s": 312.5060273380616, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555064.1063027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 15.08540000031644, "prefill_cuda_event_ms": 15.0065279006958, "kv_decode_ms": 1638.3683999993082, "kv_decode_cuda_event_ms": 1638.3118896484375, "gpu_peak_mb": 132.49267578125, "hf_load_ms": 213.7096999995265, "params_millions_measured": 45.1712, "latency_ms": 1653.4537999996246, "cuda_event_ms": 1653.3184175491333, "tokens_total": 529, "tokens_per_s": 319.9363659269585, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555065.9744046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.8342000007105526, "prefill_cuda_event_ms": 3.7940480709075928, "kv_decode_ms": 1629.3423000006442, "kv_decode_cuda_event_ms": 1629.3427734375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 3.8342000007105526, "cuda_event_ms": 3.7940480709075928, "tokens_total": 17, "tokens_per_s": 4433.7801880052075, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555065.9744046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.8342000007105526, "prefill_cuda_event_ms": 3.7940480709075928, "kv_decode_ms": 1629.3423000006442, "kv_decode_cuda_event_ms": 1629.3427734375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1629.3423000006442, "cuda_event_ms": 1629.3427734375, "tokens_total": 512, "tokens_per_s": 314.2372232033733, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555065.9744046, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.8342000007105526, "prefill_cuda_event_ms": 3.7940480709075928, "kv_decode_ms": 1629.3423000006442, "kv_decode_cuda_event_ms": 1629.3427734375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1633.1765000013547, "cuda_event_ms": 1633.1368215084076, "tokens_total": 529, "tokens_per_s": 323.90865286119487, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555067.6083124, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.871400000207359, "prefill_cuda_event_ms": 3.8082239627838135, "kv_decode_ms": 1591.642100000172, "kv_decode_cuda_event_ms": 1591.6380615234375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 3.871400000207359, "cuda_event_ms": 3.8082239627838135, "tokens_total": 17, "tokens_per_s": 4391.176318409219, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555067.6083124, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.871400000207359, "prefill_cuda_event_ms": 3.8082239627838135, "kv_decode_ms": 1591.642100000172, "kv_decode_cuda_event_ms": 1591.6380615234375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1591.642100000172, "cuda_event_ms": 1591.6380615234375, "tokens_total": 512, "tokens_per_s": 321.68035766328666, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555067.6083124, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.871400000207359, "prefill_cuda_event_ms": 3.8082239627838135, "kv_decode_ms": 1591.642100000172, "kv_decode_cuda_event_ms": 1591.6380615234375, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1595.5135000003793, "cuda_event_ms": 1595.4462854862213, "tokens_total": 529, "tokens_per_s": 331.55470010117386, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555069.2045188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.4993000008398667, "prefill_cuda_event_ms": 3.458143949508667, "kv_decode_ms": 1606.764000000112, "kv_decode_cuda_event_ms": 1606.7584228515625, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 3.4993000008398667, "cuda_event_ms": 3.458143949508667, "tokens_total": 17, "tokens_per_s": 4858.114478872869, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555069.2045188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.4993000008398667, "prefill_cuda_event_ms": 3.458143949508667, "kv_decode_ms": 1606.764000000112, "kv_decode_cuda_event_ms": 1606.7584228515625, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1606.764000000112, "cuda_event_ms": 1606.7584228515625, "tokens_total": 512, "tokens_per_s": 318.65289488684357, "gen_tokens": 512}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555069.2045188, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.4993000008398667, "prefill_cuda_event_ms": 3.458143949508667, "kv_decode_ms": 1606.764000000112, "kv_decode_cuda_event_ms": 1606.7584228515625, "gpu_peak_mb": 132.49267578125, "params_millions_measured": 45.1712, "latency_ms": 1610.2633000009519, "cuda_event_ms": 1610.2165668010712, "tokens_total": 529, "tokens_per_s": 328.51770266371176, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555070.8154128, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.497899999681977, "prefill_cuda_event_ms": 5.437983989715576, "kv_decode_ms": 2127.5815999997576, "kv_decode_cuda_event_ms": 2127.582275390625, "gpu_peak_mb": 241.66650390625, "hf_load_ms": 205.31860000119195, "params_millions_measured": 51.475968, "latency_ms": 5.497899999681977, "cuda_event_ms": 5.437983989715576, "tokens_total": 17, "tokens_per_s": 3092.089707157889, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555070.8154128, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.497899999681977, "prefill_cuda_event_ms": 5.437983989715576, "kv_decode_ms": 2127.5815999997576, "kv_decode_cuda_event_ms": 2127.582275390625, "gpu_peak_mb": 241.66650390625, "hf_load_ms": 205.31860000119195, "params_millions_measured": 51.475968, "latency_ms": 2127.5815999997576, "cuda_event_ms": 2127.582275390625, "tokens_total": 512, "tokens_per_s": 240.64881929795706, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555070.8154128, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.497899999681977, "prefill_cuda_event_ms": 5.437983989715576, "kv_decode_ms": 2127.5815999997576, "kv_decode_cuda_event_ms": 2127.582275390625, "gpu_peak_mb": 241.66650390625, "hf_load_ms": 205.31860000119195, "params_millions_measured": 51.475968, "latency_ms": 2133.0794999994396, "cuda_event_ms": 2133.0202593803406, "tokens_total": 529, "tokens_per_s": 247.9982579177846, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555073.154779, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.836699999941629, "prefill_cuda_event_ms": 4.788671970367432, "kv_decode_ms": 2176.7008999995596, "kv_decode_cuda_event_ms": 2176.70654296875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 4.836699999941629, "cuda_event_ms": 4.788671970367432, "tokens_total": 17, "tokens_per_s": 3514.7931441282612, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555073.154779, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.836699999941629, "prefill_cuda_event_ms": 4.788671970367432, "kv_decode_ms": 2176.7008999995596, "kv_decode_cuda_event_ms": 2176.70654296875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2176.7008999995596, "cuda_event_ms": 2176.70654296875, "tokens_total": 512, "tokens_per_s": 235.21835269149912, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555073.154779, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.836699999941629, "prefill_cuda_event_ms": 4.788671970367432, "kv_decode_ms": 2176.7008999995596, "kv_decode_cuda_event_ms": 2176.70654296875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2181.5375999995013, "cuda_event_ms": 2181.4952149391174, "tokens_total": 529, "tokens_per_s": 242.4895174853374, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555075.33701, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.3768000016134465, "prefill_cuda_event_ms": 5.326111793518066, "kv_decode_ms": 2182.000799999514, "kv_decode_cuda_event_ms": 2181.981201171875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 5.3768000016134465, "cuda_event_ms": 5.326111793518066, "tokens_total": 17, "tokens_per_s": 3161.7318841873816, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555075.33701, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.3768000016134465, "prefill_cuda_event_ms": 5.326111793518066, "kv_decode_ms": 2182.000799999514, "kv_decode_cuda_event_ms": 2181.981201171875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2182.000799999514, "cuda_event_ms": 2181.981201171875, "tokens_total": 512, "tokens_per_s": 234.64702671058325, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555075.33701, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.3768000016134465, "prefill_cuda_event_ms": 5.326111793518066, "kv_decode_ms": 2182.000799999514, "kv_decode_cuda_event_ms": 2181.981201171875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2187.3776000011276, "cuda_event_ms": 2187.307312965393, "tokens_total": 529, "tokens_per_s": 241.84210353060547, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555077.525173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.7725999993417645, "prefill_cuda_event_ms": 4.717152118682861, "kv_decode_ms": 2114.728299999115, "kv_decode_cuda_event_ms": 2114.735107421875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 4.7725999993417645, "cuda_event_ms": 4.717152118682861, "tokens_total": 17, "tokens_per_s": 3561.9997490559936, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555077.525173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.7725999993417645, "prefill_cuda_event_ms": 4.717152118682861, "kv_decode_ms": 2114.728299999115, "kv_decode_cuda_event_ms": 2114.735107421875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2114.728299999115, "cuda_event_ms": 2114.735107421875, "tokens_total": 512, "tokens_per_s": 242.11148070426557, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555077.525173, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.7725999993417645, "prefill_cuda_event_ms": 4.717152118682861, "kv_decode_ms": 2114.728299999115, "kv_decode_cuda_event_ms": 2114.735107421875, "gpu_peak_mb": 241.66650390625, "params_millions_measured": 51.475968, "latency_ms": 2119.5008999984566, "cuda_event_ms": 2119.452259540558, "tokens_total": 529, "tokens_per_s": 249.58706080303398, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555079.6455302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.830300000117859, "prefill_cuda_event_ms": 3.7754878997802734, "kv_decode_ms": 883.6785999992571, "kv_decode_cuda_event_ms": 883.6557006835938, "gpu_peak_mb": 287.38818359375, "hf_load_ms": 126.29070000002685, "params_millions_measured": 25.016064, "latency_ms": 3.830300000117859, "cuda_event_ms": 3.7754878997802734, "tokens_total": 17, "tokens_per_s": 4438.294650412999, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555079.6455302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.830300000117859, "prefill_cuda_event_ms": 3.7754878997802734, "kv_decode_ms": 883.6785999992571, "kv_decode_cuda_event_ms": 883.6557006835938, "gpu_peak_mb": 287.38818359375, "hf_load_ms": 126.29070000002685, "params_millions_measured": 25.016064, "latency_ms": 883.6785999992571, "cuda_event_ms": 883.6557006835938, "tokens_total": 512, "tokens_per_s": 579.3961741298594, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555079.6455302, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.830300000117859, "prefill_cuda_event_ms": 3.7754878997802734, "kv_decode_ms": 883.6785999992571, "kv_decode_cuda_event_ms": 883.6557006835938, "gpu_peak_mb": 287.38818359375, "hf_load_ms": 126.29070000002685, "params_millions_measured": 25.016064, "latency_ms": 887.508899999375, "cuda_event_ms": 887.431188583374, "tokens_total": 529, "tokens_per_s": 596.0503607348304, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555080.6601884, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2767999998904997, "prefill_cuda_event_ms": 2.235743999481201, "kv_decode_ms": 882.8714999999647, "kv_decode_cuda_event_ms": 882.8477172851562, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 2.2767999998904997, "cuda_event_ms": 2.235743999481201, "tokens_total": 17, "tokens_per_s": 7466.6198176465205, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555080.6601884, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2767999998904997, "prefill_cuda_event_ms": 2.235743999481201, "kv_decode_ms": 882.8714999999647, "kv_decode_cuda_event_ms": 882.8477172851562, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 882.8714999999647, "cuda_event_ms": 882.8477172851562, "tokens_total": 512, "tokens_per_s": 579.9258442480253, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555080.6601884, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2767999998904997, "prefill_cuda_event_ms": 2.235743999481201, "kv_decode_ms": 882.8714999999647, "kv_decode_cuda_event_ms": 882.8477172851562, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 885.1482999998552, "cuda_event_ms": 885.0834612846375, "tokens_total": 529, "tokens_per_s": 597.6399660939151, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555081.5461414, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.192300000388059, "prefill_cuda_event_ms": 2.15283203125, "kv_decode_ms": 869.9970999987272, "kv_decode_cuda_event_ms": 869.983154296875, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 2.192300000388059, "cuda_event_ms": 2.15283203125, "tokens_total": 17, "tokens_per_s": 7754.413172006948, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555081.5461414, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.192300000388059, "prefill_cuda_event_ms": 2.15283203125, "kv_decode_ms": 869.9970999987272, "kv_decode_cuda_event_ms": 869.983154296875, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 869.9970999987272, "cuda_event_ms": 869.983154296875, "tokens_total": 512, "tokens_per_s": 588.5077088196605, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555081.5461414, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.192300000388059, "prefill_cuda_event_ms": 2.15283203125, "kv_decode_ms": 869.9970999987272, "kv_decode_cuda_event_ms": 869.983154296875, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 872.1893999991153, "cuda_event_ms": 872.135986328125, "tokens_total": 529, "tokens_per_s": 606.5196389689403, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555082.419125, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2676999997202074, "prefill_cuda_event_ms": 2.221247911453247, "kv_decode_ms": 884.0956000003644, "kv_decode_cuda_event_ms": 884.0775756835938, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 2.2676999997202074, "cuda_event_ms": 2.221247911453247, "tokens_total": 17, "tokens_per_s": 7496.5824412830125, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555082.419125, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2676999997202074, "prefill_cuda_event_ms": 2.221247911453247, "kv_decode_ms": 884.0956000003644, "kv_decode_cuda_event_ms": 884.0775756835938, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 884.0956000003644, "cuda_event_ms": 884.0775756835938, "tokens_total": 512, "tokens_per_s": 579.1228912346006, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555082.419125, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.2676999997202074, "prefill_cuda_event_ms": 2.221247911453247, "kv_decode_ms": 884.0956000003644, "kv_decode_cuda_event_ms": 884.0775756835938, "gpu_peak_mb": 287.38818359375, "params_millions_measured": 25.016064, "latency_ms": 886.3633000000846, "cuda_event_ms": 886.298823595047, "tokens_total": 529, "tokens_per_s": 596.8207393062748, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555083.3061094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 14.50259999910486, "prefill_cuda_event_ms": 14.44422435760498, "kv_decode_ms": 2038.898099999642, "kv_decode_cuda_event_ms": 2038.90478515625, "gpu_peak_mb": 493.1259765625, "hf_load_ms": 264.8895000002085, "params_millions_measured": 96.08832, "latency_ms": 14.50259999910486, "cuda_event_ms": 14.44422435760498, "tokens_total": 17, "tokens_per_s": 1172.2036049432024, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555083.3061094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 14.50259999910486, "prefill_cuda_event_ms": 14.44422435760498, "kv_decode_ms": 2038.898099999642, "kv_decode_cuda_event_ms": 2038.90478515625, "gpu_peak_mb": 493.1259765625, "hf_load_ms": 264.8895000002085, "params_millions_measured": 96.08832, "latency_ms": 2038.898099999642, "cuda_event_ms": 2038.90478515625, "tokens_total": 512, "tokens_per_s": 251.11603174287617, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555083.3061094, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 14.50259999910486, "prefill_cuda_event_ms": 14.44422435760498, "kv_decode_ms": 2038.898099999642, "kv_decode_cuda_event_ms": 2038.90478515625, "gpu_peak_mb": 493.1259765625, "hf_load_ms": 264.8895000002085, "params_millions_measured": 96.08832, "latency_ms": 2053.400699998747, "cuda_event_ms": 2053.349009513855, "tokens_total": 529, "tokens_per_s": 257.6214179727916, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555085.6253135, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.000799999834271, "prefill_cuda_event_ms": 4.947423934936523, "kv_decode_ms": 2059.6108000008826, "kv_decode_cuda_event_ms": 2059.6162109375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 5.000799999834271, "cuda_event_ms": 4.947423934936523, "tokens_total": 17, "tokens_per_s": 3399.4560871387353, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555085.6253135, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.000799999834271, "prefill_cuda_event_ms": 4.947423934936523, "kv_decode_ms": 2059.6108000008826, "kv_decode_cuda_event_ms": 2059.6162109375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2059.6108000008826, "cuda_event_ms": 2059.6162109375, "tokens_total": 512, "tokens_per_s": 248.59065605976653, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555085.6253135, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.000799999834271, "prefill_cuda_event_ms": 4.947423934936523, "kv_decode_ms": 2059.6108000008826, "kv_decode_cuda_event_ms": 2059.6162109375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2064.611600000717, "cuda_event_ms": 2064.5636348724365, "tokens_total": 529, "tokens_per_s": 256.22252630946, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555087.6906822, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.784299999504583, "prefill_cuda_event_ms": 5.737855911254883, "kv_decode_ms": 2122.908600000301, "kv_decode_cuda_event_ms": 2122.919677734375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 5.784299999504583, "cuda_event_ms": 5.737855911254883, "tokens_total": 17, "tokens_per_s": 2938.990024973813, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555087.6906822, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.784299999504583, "prefill_cuda_event_ms": 5.737855911254883, "kv_decode_ms": 2122.908600000301, "kv_decode_cuda_event_ms": 2122.919677734375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2122.908600000301, "cuda_event_ms": 2122.919677734375, "tokens_total": 512, "tokens_per_s": 241.1785415537567, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555087.6906822, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 5.784299999504583, "prefill_cuda_event_ms": 5.737855911254883, "kv_decode_ms": 2122.908600000301, "kv_decode_cuda_event_ms": 2122.919677734375, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2128.6928999998054, "cuda_event_ms": 2128.65753364563, "tokens_total": 529, "tokens_per_s": 248.5093082238628, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555089.8200364, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.982200000085868, "prefill_cuda_event_ms": 4.927264213562012, "kv_decode_ms": 2116.93539999942, "kv_decode_cuda_event_ms": 2116.936767578125, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 4.982200000085868, "cuda_event_ms": 4.927264213562012, "tokens_total": 17, "tokens_per_s": 3412.147244130506, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555089.8200364, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.982200000085868, "prefill_cuda_event_ms": 4.927264213562012, "kv_decode_ms": 2116.93539999942, "kv_decode_cuda_event_ms": 2116.936767578125, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2116.93539999942, "cuda_event_ms": 2116.936767578125, "tokens_total": 512, "tokens_per_s": 241.8590572013394, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555089.8200364, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.982200000085868, "prefill_cuda_event_ms": 4.927264213562012, "kv_decode_ms": 2116.93539999942, "kv_decode_cuda_event_ms": 2116.936767578125, "gpu_peak_mb": 493.1259765625, "params_millions_measured": 96.08832, "latency_ms": 2121.917599999506, "cuda_event_ms": 2121.864031791687, "tokens_total": 529, "tokens_per_s": 249.30280044810561, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555091.9426856, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 187.30949999917357, "prefill_cuda_event_ms": 187.23568725585938, "kv_decode_ms": 984.5607000006567, "kv_decode_cuda_event_ms": 984.5350341796875, "gpu_peak_mb": 482.9345703125, "hf_load_ms": 118.4291999998095, "params_millions_measured": 0.102714, "latency_ms": 187.30949999917357, "cuda_event_ms": 187.23568725585938, "tokens_total": 17, "tokens_per_s": 90.75887768679648, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555091.9426856, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 187.30949999917357, "prefill_cuda_event_ms": 187.23568725585938, "kv_decode_ms": 984.5607000006567, "kv_decode_cuda_event_ms": 984.5350341796875, "gpu_peak_mb": 482.9345703125, "hf_load_ms": 118.4291999998095, "params_millions_measured": 0.102714, "latency_ms": 984.5607000006567, "cuda_event_ms": 984.5350341796875, "tokens_total": 512, "tokens_per_s": 520.0288819162278, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555091.9426856, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 187.30949999917357, "prefill_cuda_event_ms": 187.23568725585938, "kv_decode_ms": 984.5607000006567, "kv_decode_cuda_event_ms": 984.5350341796875, "gpu_peak_mb": 482.9345703125, "hf_load_ms": 118.4291999998095, "params_millions_measured": 0.102714, "latency_ms": 1171.8701999998302, "cuda_event_ms": 1171.7707214355469, "tokens_total": 529, "tokens_per_s": 451.41518232998556, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555093.2339566, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.617399999508052, "prefill_cuda_event_ms": 2.5460479259490967, "kv_decode_ms": 874.1437000007863, "kv_decode_cuda_event_ms": 874.1140747070312, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 2.617399999508052, "cuda_event_ms": 2.5460479259490967, "tokens_total": 17, "tokens_per_s": 6494.995034459846, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555093.2339566, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.617399999508052, "prefill_cuda_event_ms": 2.5460479259490967, "kv_decode_ms": 874.1437000007863, "kv_decode_cuda_event_ms": 874.1140747070312, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 874.1437000007863, "cuda_event_ms": 874.1140747070312, "tokens_total": 512, "tokens_per_s": 585.7160556090943, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555093.2339566, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.617399999508052, "prefill_cuda_event_ms": 2.5460479259490967, "kv_decode_ms": 874.1437000007863, "kv_decode_cuda_event_ms": 874.1140747070312, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 876.7611000002944, "cuda_event_ms": 876.6601226329803, "tokens_total": 529, "tokens_per_s": 603.3570604350745, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555094.1113565, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.4829000012687175, "prefill_cuda_event_ms": 2.4242238998413086, "kv_decode_ms": 860.8312000014848, "kv_decode_cuda_event_ms": 860.8020629882812, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 2.4829000012687175, "cuda_event_ms": 2.4242238998413086, "tokens_total": 17, "tokens_per_s": 6846.8323296601975, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555094.1113565, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.4829000012687175, "prefill_cuda_event_ms": 2.4242238998413086, "kv_decode_ms": 860.8312000014848, "kv_decode_cuda_event_ms": 860.8020629882812, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 860.8312000014848, "cuda_event_ms": 860.8020629882812, "tokens_total": 512, "tokens_per_s": 594.773981239431, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555094.1113565, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.4829000012687175, "prefill_cuda_event_ms": 2.4242238998413086, "kv_decode_ms": 860.8312000014848, "kv_decode_cuda_event_ms": 860.8020629882812, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 863.3141000027535, "cuda_event_ms": 863.2262868881226, "tokens_total": 529, "tokens_per_s": 612.7549636897078, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555094.975331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.10010000046168, "prefill_cuda_event_ms": 2.0542399883270264, "kv_decode_ms": 837.5226999996812, "kv_decode_cuda_event_ms": 837.49169921875, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 2.10010000046168, "cuda_event_ms": 2.0542399883270264, "tokens_total": 17, "tokens_per_s": 8094.852624285876, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555094.975331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.10010000046168, "prefill_cuda_event_ms": 2.0542399883270264, "kv_decode_ms": 837.5226999996812, "kv_decode_cuda_event_ms": 837.49169921875, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 837.5226999996812, "cuda_event_ms": 837.49169921875, "tokens_total": 512, "tokens_per_s": 611.326713891092, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555094.975331, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 2.10010000046168, "prefill_cuda_event_ms": 2.0542399883270264, "kv_decode_ms": 837.5226999996812, "kv_decode_cuda_event_ms": 837.49169921875, "gpu_peak_mb": 482.9345703125, "params_millions_measured": 0.102714, "latency_ms": 839.6228000001429, "cuda_event_ms": 839.545939207077, "tokens_total": 529, "tokens_per_s": 630.0448248903078, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555095.81562, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.022399999186746, "prefill_cuda_event_ms": 6.966847896575928, "kv_decode_ms": 1380.5197999990924, "kv_decode_cuda_event_ms": 1380.5086669921875, "gpu_peak_mb": 641.03662109375, "hf_load_ms": 234.10560000047553, "params_millions_measured": 74.824704, "latency_ms": 7.022399999186746, "cuda_event_ms": 6.966847896575928, "tokens_total": 17, "tokens_per_s": 2420.8247895261943, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555095.81562, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.022399999186746, "prefill_cuda_event_ms": 6.966847896575928, "kv_decode_ms": 1380.5197999990924, "kv_decode_cuda_event_ms": 1380.5086669921875, "gpu_peak_mb": 641.03662109375, "hf_load_ms": 234.10560000047553, "params_millions_measured": 74.824704, "latency_ms": 1380.5197999990924, "cuda_event_ms": 1380.5086669921875, "tokens_total": 512, "tokens_per_s": 370.87479658048846, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555095.81562, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 7.022399999186746, "prefill_cuda_event_ms": 6.966847896575928, "kv_decode_ms": 1380.5197999990924, "kv_decode_cuda_event_ms": 1380.5086669921875, "gpu_peak_mb": 641.03662109375, "hf_load_ms": 234.10560000047553, "params_millions_measured": 74.824704, "latency_ms": 1387.542199998279, "cuda_event_ms": 1387.4755148887634, "tokens_total": 529, "tokens_per_s": 381.24966577640384, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555097.4381955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.66300000132469, "prefill_cuda_event_ms": 3.6135358810424805, "kv_decode_ms": 1356.4726000004157, "kv_decode_cuda_event_ms": 1356.462890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 3.66300000132469, "cuda_event_ms": 3.6135358810424805, "tokens_total": 17, "tokens_per_s": 4641.004639326265, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555097.4381955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.66300000132469, "prefill_cuda_event_ms": 3.6135358810424805, "kv_decode_ms": 1356.4726000004157, "kv_decode_cuda_event_ms": 1356.462890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1356.4726000004157, "cuda_event_ms": 1356.462890625, "tokens_total": 512, "tokens_per_s": 377.4495703045112, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555097.4381955, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 3.66300000132469, "prefill_cuda_event_ms": 3.6135358810424805, "kv_decode_ms": 1356.4726000004157, "kv_decode_cuda_event_ms": 1356.462890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1360.1356000017404, "cuda_event_ms": 1360.0764265060425, "tokens_total": 529, "tokens_per_s": 388.93180944556053, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555098.7990167, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.54229999963718, "prefill_cuda_event_ms": 6.449632167816162, "kv_decode_ms": 1376.7310999992333, "kv_decode_cuda_event_ms": 1376.712646484375, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 6.54229999963718, "cuda_event_ms": 6.449632167816162, "tokens_total": 17, "tokens_per_s": 2598.47454273616, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555098.7990167, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.54229999963718, "prefill_cuda_event_ms": 6.449632167816162, "kv_decode_ms": 1376.7310999992333, "kv_decode_cuda_event_ms": 1376.712646484375, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1376.7310999992333, "cuda_event_ms": 1376.712646484375, "tokens_total": 512, "tokens_per_s": 371.89542678325864, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555098.7990167, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 6.54229999963718, "prefill_cuda_event_ms": 6.449632167816162, "kv_decode_ms": 1376.7310999992333, "kv_decode_cuda_event_ms": 1376.712646484375, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1383.2733999988704, "cuda_event_ms": 1383.1622786521912, "tokens_total": 529, "tokens_per_s": 382.4262072851484, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766555100.183343, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.055200000948389, "prefill_cuda_event_ms": 4.003456115722656, "kv_decode_ms": 1357.5672000006307, "kv_decode_cuda_event_ms": 1357.558837890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 4.055200000948389, "cuda_event_ms": 4.003456115722656, "tokens_total": 17, "tokens_per_s": 4192.148351751877, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766555100.183343, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.055200000948389, "prefill_cuda_event_ms": 4.003456115722656, "kv_decode_ms": 1357.5672000006307, "kv_decode_cuda_event_ms": 1357.558837890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1357.5672000006307, "cuda_event_ms": 1357.558837890625, "tokens_total": 512, "tokens_per_s": 377.14523450460655, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 512, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766555100.183343, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 512, "prefill_ms": 4.055200000948389, "prefill_cuda_event_ms": 4.003456115722656, "kv_decode_ms": 1357.5672000006307, "kv_decode_cuda_event_ms": 1357.558837890625, "gpu_peak_mb": 641.03662109375, "params_millions_measured": 74.824704, "latency_ms": 1361.622400001579, "cuda_event_ms": 1361.5622940063477, "tokens_total": 529, "tokens_per_s": 388.50712209154796, "gen_tokens": 512}
