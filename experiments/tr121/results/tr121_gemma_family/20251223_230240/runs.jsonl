{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548965.9276817, "prompt_tokens": 25, "prefill_ms": 6.9533, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.7034, "kv_decode_ms_equiv": 145.7034, "kv_decode_ms_per_token": 2.276615625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2812.8195999997843, "ollama_total_duration_ms": 2717.3644, "ollama_load_ms": 2498.5249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 6.9533, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3595.4151266305207}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548965.9276817, "prompt_tokens": 25, "prefill_ms": 6.9533, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.7034, "kv_decode_ms_equiv": 145.7034, "kv_decode_ms_per_token": 2.276615625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2812.8195999997843, "ollama_total_duration_ms": 2717.3644, "ollama_load_ms": 2498.5249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 145.7034, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 439.24850072132847}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548965.9276817, "prompt_tokens": 25, "prefill_ms": 6.9533, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.7034, "kv_decode_ms_equiv": 145.7034, "kv_decode_ms_per_token": 2.276615625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2812.8195999997843, "ollama_total_duration_ms": 2717.3644, "ollama_load_ms": 2498.5249, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 152.6567, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 583.0074932839502}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548968.7406263, "prompt_tokens": 25, "prefill_ms": 2.3489, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.3086, "kv_decode_ms_equiv": 131.3086, "kv_decode_ms_per_token": 2.051696875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.63510000058886, "ollama_total_duration_ms": 465.5186, "ollama_load_ms": 291.2856, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3489, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10643.27983311337}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548968.7406263, "prompt_tokens": 25, "prefill_ms": 2.3489, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.3086, "kv_decode_ms_equiv": 131.3086, "kv_decode_ms_per_token": 2.051696875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.63510000058886, "ollama_total_duration_ms": 465.5186, "ollama_load_ms": 291.2856, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.3086, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 487.40143448334675}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548968.7406263, "prompt_tokens": 25, "prefill_ms": 2.3489, "prefill_cuda_event_ms": null, "kv_decode_ms": 131.3086, "kv_decode_ms_equiv": 131.3086, "kv_decode_ms_per_token": 2.051696875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.63510000058886, "ollama_total_duration_ms": 465.5186, "ollama_load_ms": 291.2856, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 133.6575, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 665.8810766324373}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548969.2433615, "prompt_tokens": 25, "prefill_ms": 2.2774, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8678, "kv_decode_ms_equiv": 136.8678, "kv_decode_ms_per_token": 2.138559375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 520.3145000004952, "ollama_total_duration_ms": 469.1872, "ollama_load_ms": 294.9559, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2774, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10977.430403091244}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548969.2433615, "prompt_tokens": 25, "prefill_ms": 2.2774, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8678, "kv_decode_ms_equiv": 136.8678, "kv_decode_ms_per_token": 2.138559375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 520.3145000004952, "ollama_total_duration_ms": 469.1872, "ollama_load_ms": 294.9559, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.8678, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 467.6045059539206}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548969.2433615, "prompt_tokens": 25, "prefill_ms": 2.2774, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8678, "kv_decode_ms_equiv": 136.8678, "kv_decode_ms_per_token": 2.138559375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 520.3145000004952, "ollama_total_duration_ms": 469.1872, "ollama_load_ms": 294.9559, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 139.1452, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 639.6196203677885}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548969.7640932, "prompt_tokens": 25, "prefill_ms": 2.6579, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7116, "kv_decode_ms_equiv": 128.7116, "kv_decode_ms_per_token": 2.01111875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.69869999968796, "ollama_total_duration_ms": 462.2386, "ollama_load_ms": 292.808, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.6579, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9405.921968471348}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548969.7640932, "prompt_tokens": 25, "prefill_ms": 2.6579, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7116, "kv_decode_ms_equiv": 128.7116, "kv_decode_ms_per_token": 2.01111875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.69869999968796, "ollama_total_duration_ms": 462.2386, "ollama_load_ms": 292.808, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 128.7116, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 497.2356803893355}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548969.7640932, "prompt_tokens": 25, "prefill_ms": 2.6579, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7116, "kv_decode_ms_equiv": 128.7116, "kv_decode_ms_per_token": 2.01111875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.69869999968796, "ollama_total_duration_ms": 462.2386, "ollama_load_ms": 292.808, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.36950000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 677.47841013325}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548970.2559166, "prompt_tokens": 25, "prefill_ms": 1.8965, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7867, "kv_decode_ms_equiv": 134.7867, "kv_decode_ms_per_token": 2.1060421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 493.01809999997204, "ollama_total_duration_ms": 465.696, "ollama_load_ms": 292.807, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.8965, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 13182.177695755337}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548970.2559166, "prompt_tokens": 25, "prefill_ms": 1.8965, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7867, "kv_decode_ms_equiv": 134.7867, "kv_decode_ms_per_token": 2.1060421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 493.01809999997204, "ollama_total_duration_ms": 465.696, "ollama_load_ms": 292.807, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.7867, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 474.82429646248477}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548970.2559166, "prompt_tokens": 25, "prefill_ms": 1.8965, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7867, "kv_decode_ms_equiv": 134.7867, "kv_decode_ms_per_token": 2.1060421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 493.01809999997204, "ollama_total_duration_ms": 465.696, "ollama_load_ms": 292.807, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.6832, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 651.1407400470576}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548970.7490275, "prompt_tokens": 25, "prefill_ms": 2.3223, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.6007, "kv_decode_ms_equiv": 142.6007, "kv_decode_ms_per_token": 2.2281359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 513.1214000002728, "ollama_total_duration_ms": 486.4509, "ollama_load_ms": 292.3877, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3223, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10765.189682642209}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548970.7490275, "prompt_tokens": 25, "prefill_ms": 2.3223, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.6007, "kv_decode_ms_equiv": 142.6007, "kv_decode_ms_per_token": 2.2281359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 513.1214000002728, "ollama_total_duration_ms": 486.4509, "ollama_load_ms": 292.3877, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 142.6007, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 448.80565102415346}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548970.7490275, "prompt_tokens": 25, "prefill_ms": 2.3223, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.6007, "kv_decode_ms_equiv": 142.6007, "kv_decode_ms_per_token": 2.2281359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 513.1214000002728, "ollama_total_duration_ms": 486.4509, "ollama_load_ms": 292.3877, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 144.923, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 614.1192219316465}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548971.2624123, "prompt_tokens": 18, "prefill_ms": 13.2524, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.7825, "kv_decode_ms_equiv": 281.9435294117647, "kv_decode_ms_per_token": 4.405367647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2086.9510000002265, "ollama_total_duration_ms": 2017.294, "ollama_load_ms": 1795.9427, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 13.2524, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1358.2445443844135}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548971.2624123, "prompt_tokens": 18, "prefill_ms": 13.2524, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.7825, "kv_decode_ms_equiv": 281.9435294117647, "kv_decode_ms_per_token": 4.405367647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2086.9510000002265, "ollama_total_duration_ms": 2017.294, "ollama_load_ms": 1795.9427, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 281.9435294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 226.9958105920251}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548971.2624123, "prompt_tokens": 18, "prefill_ms": 13.2524, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.7825, "kv_decode_ms_equiv": 281.9435294117647, "kv_decode_ms_per_token": 4.405367647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2086.9510000002265, "ollama_total_duration_ms": 2017.294, "ollama_load_ms": 1795.9427, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 295.1959294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 277.7816081793572}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548973.349499, "prompt_tokens": 18, "prefill_ms": 5.7518, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.728, "kv_decode_ms_equiv": 281.8409411764706, "kv_decode_ms_per_token": 4.403764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.99730000014097, "ollama_total_duration_ms": 477.1987, "ollama_load_ms": 300.2511, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.7518, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3129.4551270906495}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548973.349499, "prompt_tokens": 18, "prefill_ms": 5.7518, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.728, "kv_decode_ms_equiv": 281.8409411764706, "kv_decode_ms_per_token": 4.403764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.99730000014097, "ollama_total_duration_ms": 477.1987, "ollama_load_ms": 300.2511, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 281.8409411764706, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 227.0784355631545}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548973.349499, "prompt_tokens": 18, "prefill_ms": 5.7518, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.728, "kv_decode_ms_equiv": 281.8409411764706, "kv_decode_ms_per_token": 4.403764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.99730000014097, "ollama_total_duration_ms": 477.1987, "ollama_load_ms": 300.2511, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 287.5927411764706, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 285.1254161164094}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548973.8395817, "prompt_tokens": 18, "prefill_ms": 4.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.5274, "kv_decode_ms_equiv": 273.9339294117647, "kv_decode_ms_per_token": 4.280217647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2806000002238, "ollama_total_duration_ms": 489.047, "ollama_load_ms": 313.1227, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.7528, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3787.2412051843125}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548973.8395817, "prompt_tokens": 18, "prefill_ms": 4.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.5274, "kv_decode_ms_equiv": 273.9339294117647, "kv_decode_ms_per_token": 4.280217647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2806000002238, "ollama_total_duration_ms": 489.047, "ollama_load_ms": 313.1227, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 273.9339294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 233.6329790816025}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548973.8395817, "prompt_tokens": 18, "prefill_ms": 4.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.5274, "kv_decode_ms_equiv": 273.9339294117647, "kv_decode_ms_per_token": 4.280217647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2806000002238, "ollama_total_duration_ms": 489.047, "ollama_load_ms": 313.1227, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 278.6867294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 294.2371894531207}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548974.3400636, "prompt_tokens": 18, "prefill_ms": 5.5648, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.6307, "kv_decode_ms_equiv": 276.0107294117647, "kv_decode_ms_per_token": 4.312667647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.18329999997513, "ollama_total_duration_ms": 475.3276, "ollama_load_ms": 297.8763, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.5648, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3234.617596319724}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548974.3400636, "prompt_tokens": 18, "prefill_ms": 5.5648, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.6307, "kv_decode_ms_equiv": 276.0107294117647, "kv_decode_ms_per_token": 4.312667647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.18329999997513, "ollama_total_duration_ms": 475.3276, "ollama_load_ms": 297.8763, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 276.0107294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 231.87504390281163}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548974.3400636, "prompt_tokens": 18, "prefill_ms": 5.5648, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.6307, "kv_decode_ms_equiv": 276.0107294117647, "kv_decode_ms_per_token": 4.312667647058824, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 506.18329999997513, "ollama_total_duration_ms": 475.3276, "ollama_load_ms": 297.8763, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 281.5755294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 291.2184882376142}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548974.8463833, "prompt_tokens": 18, "prefill_ms": 5.2374, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.2861, "kv_decode_ms_equiv": 286.6561882352941, "kv_decode_ms_per_token": 4.4790029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 488.47920000025624, "ollama_total_duration_ms": 475.1984, "ollama_load_ms": 294.5531, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.2374, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3436.8197960820257}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548974.8463833, "prompt_tokens": 18, "prefill_ms": 5.2374, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.2861, "kv_decode_ms_equiv": 286.6561882352941, "kv_decode_ms_per_token": 4.4790029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 488.47920000025624, "ollama_total_duration_ms": 475.1984, "ollama_load_ms": 294.5531, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 286.6561882352941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 223.26397484734326}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548974.8463833, "prompt_tokens": 18, "prefill_ms": 5.2374, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.2861, "kv_decode_ms_equiv": 286.6561882352941, "kv_decode_ms_per_token": 4.4790029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 488.47920000025624, "ollama_total_duration_ms": 475.1984, "ollama_load_ms": 294.5531, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 291.8935882352941, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 280.9242933212365}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548975.33498, "prompt_tokens": 18, "prefill_ms": 5.0693, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.4478, "kv_decode_ms_equiv": 273.78409411764704, "kv_decode_ms_per_token": 4.277876470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.27579999964655, "ollama_total_duration_ms": 467.6742, "ollama_load_ms": 286.5197, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.0693, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3550.786104590377}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548975.33498, "prompt_tokens": 18, "prefill_ms": 5.0693, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.4478, "kv_decode_ms_equiv": 273.78409411764704, "kv_decode_ms_per_token": 4.277876470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.27579999964655, "ollama_total_duration_ms": 467.6742, "ollama_load_ms": 286.5197, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 273.78409411764704, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 233.7608406589856}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548975.33498, "prompt_tokens": 18, "prefill_ms": 5.0693, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.4478, "kv_decode_ms_equiv": 273.78409411764704, "kv_decode_ms_per_token": 4.277876470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.27579999964655, "ollama_total_duration_ms": 467.6742, "ollama_load_ms": 286.5197, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 278.85339411764704, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 294.0613301819972}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548975.833668, "prompt_tokens": 26, "prefill_ms": 11.3154, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.713, "kv_decode_ms_equiv": 274.713, "kv_decode_ms_per_token": 4.292390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 636.2686000002213, "ollama_total_duration_ms": 619.2403, "ollama_load_ms": 282.7034, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 11.3154, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 2297.7535040740936}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548975.833668, "prompt_tokens": 26, "prefill_ms": 11.3154, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.713, "kv_decode_ms_equiv": 274.713, "kv_decode_ms_per_token": 4.292390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 636.2686000002213, "ollama_total_duration_ms": 619.2403, "ollama_load_ms": 282.7034, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.713, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.97040911787934}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548975.833668, "prompt_tokens": 26, "prefill_ms": 11.3154, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.713, "kv_decode_ms_equiv": 274.713, "kv_decode_ms_per_token": 4.292390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 636.2686000002213, "ollama_total_duration_ms": 619.2403, "ollama_load_ms": 282.7034, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 286.02840000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 314.6540693161938}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548976.4700532, "prompt_tokens": 26, "prefill_ms": 5.347, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.6196, "kv_decode_ms_equiv": 281.6196, "kv_decode_ms_per_token": 4.40030625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.0544000000446, "ollama_total_duration_ms": 632.1539, "ollama_load_ms": 298.6571, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.347, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4862.539741911352}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548976.4700532, "prompt_tokens": 26, "prefill_ms": 5.347, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.6196, "kv_decode_ms_equiv": 281.6196, "kv_decode_ms_per_token": 4.40030625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.0544000000446, "ollama_total_duration_ms": 632.1539, "ollama_load_ms": 298.6571, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 281.6196, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 227.2569096753209}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548976.4700532, "prompt_tokens": 26, "prefill_ms": 5.347, "prefill_cuda_event_ms": null, "kv_decode_ms": 281.6196, "kv_decode_ms_equiv": 281.6196, "kv_decode_ms_per_token": 4.40030625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.0544000000446, "ollama_total_duration_ms": 632.1539, "ollama_load_ms": 298.6571, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 286.96659999999997, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 313.6253487339642}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548977.1202378, "prompt_tokens": 26, "prefill_ms": 4.5569, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.577, "kv_decode_ms_equiv": 280.577, "kv_decode_ms_per_token": 4.384015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 648.2359000001452, "ollama_total_duration_ms": 629.2938, "ollama_load_ms": 297.5368, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5569, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5705.633215563212}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548977.1202378, "prompt_tokens": 26, "prefill_ms": 4.5569, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.577, "kv_decode_ms_equiv": 280.577, "kv_decode_ms_per_token": 4.384015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 648.2359000001452, "ollama_total_duration_ms": 629.2938, "ollama_load_ms": 297.5368, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 280.577, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.10137680565404}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548977.1202378, "prompt_tokens": 26, "prefill_ms": 4.5569, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.577, "kv_decode_ms_equiv": 280.577, "kv_decode_ms_per_token": 4.384015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 648.2359000001452, "ollama_total_duration_ms": 629.2938, "ollama_load_ms": 297.5368, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 285.1339, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 315.6411777063338}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548977.7686296, "prompt_tokens": 26, "prefill_ms": 4.3335, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.0835, "kv_decode_ms_equiv": 279.0835, "kv_decode_ms_per_token": 4.3606796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4803000003958, "ollama_total_duration_ms": 616.8931, "ollama_load_ms": 289.6371, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.3335, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5999.769239644629}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548977.7686296, "prompt_tokens": 26, "prefill_ms": 4.3335, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.0835, "kv_decode_ms_equiv": 279.0835, "kv_decode_ms_per_token": 4.3606796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4803000003958, "ollama_total_duration_ms": 616.8931, "ollama_load_ms": 289.6371, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 279.0835, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 229.3220487775164}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548977.7686296, "prompt_tokens": 26, "prefill_ms": 4.3335, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.0835, "kv_decode_ms_equiv": 279.0835, "kv_decode_ms_per_token": 4.3606796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4803000003958, "ollama_total_duration_ms": 616.8931, "ollama_load_ms": 289.6371, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 283.41700000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 317.553287205778}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548978.4131975, "prompt_tokens": 26, "prefill_ms": 5.5623, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.7142, "kv_decode_ms_equiv": 275.7142, "kv_decode_ms_per_token": 4.308034375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 639.0504999999393, "ollama_total_duration_ms": 621.4127, "ollama_load_ms": 288.9431, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.5623, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4674.325369002032}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548978.4131975, "prompt_tokens": 26, "prefill_ms": 5.5623, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.7142, "kv_decode_ms_equiv": 275.7142, "kv_decode_ms_per_token": 4.308034375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 639.0504999999393, "ollama_total_duration_ms": 621.4127, "ollama_load_ms": 288.9431, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 275.7142, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.12442449463973}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548978.4131975, "prompt_tokens": 26, "prefill_ms": 5.5623, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.7142, "kv_decode_ms_equiv": 275.7142, "kv_decode_ms_per_token": 4.308034375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 639.0504999999393, "ollama_total_duration_ms": 621.4127, "ollama_load_ms": 288.9431, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 281.2765, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 319.96985172952594}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548979.0523474, "prompt_tokens": 26, "prefill_ms": 4.487, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.2252, "kv_decode_ms_equiv": 277.2252, "kv_decode_ms_per_token": 4.33164375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4912999995722, "ollama_total_duration_ms": 626.4728, "ollama_load_ms": 293.2707, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.487, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5794.5174949855145}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548979.0523474, "prompt_tokens": 26, "prefill_ms": 4.487, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.2252, "kv_decode_ms_equiv": 277.2252, "kv_decode_ms_per_token": 4.33164375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4912999995722, "ollama_total_duration_ms": 626.4728, "ollama_load_ms": 293.2707, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 277.2252, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 230.85924367626035}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548979.0523474, "prompt_tokens": 26, "prefill_ms": 4.487, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.2252, "kv_decode_ms_equiv": 277.2252, "kv_decode_ms_per_token": 4.33164375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 644.4912999995722, "ollama_total_duration_ms": 626.4728, "ollama_load_ms": 293.2707, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 281.7122, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 319.47498191416634}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548979.6970472, "prompt_tokens": 17, "prefill_ms": 8.1426, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0031, "kv_decode_ms_equiv": 140.11347027027028, "kv_decode_ms_per_token": 2.189272972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.39710000003106, "ollama_total_duration_ms": 412.8478, "ollama_load_ms": 295.5149, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.1426, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2087.7852283054553}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548979.6970472, "prompt_tokens": 17, "prefill_ms": 8.1426, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0031, "kv_decode_ms_equiv": 140.11347027027028, "kv_decode_ms_per_token": 2.189272972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.39710000003106, "ollama_total_duration_ms": 412.8478, "ollama_load_ms": 295.5149, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 140.11347027027028, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 456.77264203468746}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548979.6970472, "prompt_tokens": 17, "prefill_ms": 8.1426, "prefill_cuda_event_ms": null, "kv_decode_ms": 81.0031, "kv_decode_ms_equiv": 140.11347027027028, "kv_decode_ms_per_token": 2.189272972972973, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.39710000003106, "ollama_total_duration_ms": 412.8478, "ollama_load_ms": 295.5149, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 148.25607027027027, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 546.3519965984348}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548980.1235535, "prompt_tokens": 17, "prefill_ms": 2.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.8526, "kv_decode_ms_equiv": 139.80346666666665, "kv_decode_ms_per_token": 2.1844291666666664, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.03740000003745, "ollama_total_duration_ms": 438.7555, "ollama_load_ms": 292.5274, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4189, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7027.987928397206}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548980.1235535, "prompt_tokens": 17, "prefill_ms": 2.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.8526, "kv_decode_ms_equiv": 139.80346666666665, "kv_decode_ms_per_token": 2.1844291666666664, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.03740000003745, "ollama_total_duration_ms": 438.7555, "ollama_load_ms": 292.5274, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.80346666666665, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 457.7855007887263}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548980.1235535, "prompt_tokens": 17, "prefill_ms": 2.4189, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.8526, "kv_decode_ms_equiv": 139.80346666666665, "kv_decode_ms_per_token": 2.1844291666666664, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.03740000003745, "ollama_total_duration_ms": 438.7555, "ollama_load_ms": 292.5274, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.22236666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 569.5306715704118}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548980.578727, "prompt_tokens": 17, "prefill_ms": 2.3493, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.4952, "kv_decode_ms_equiv": 139.32693333333333, "kv_decode_ms_per_token": 2.1769833333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.2856000002721, "ollama_total_duration_ms": 437.5513, "ollama_load_ms": 290.4106, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.3493, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7236.198016430426}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548980.578727, "prompt_tokens": 17, "prefill_ms": 2.3493, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.4952, "kv_decode_ms_equiv": 139.32693333333333, "kv_decode_ms_per_token": 2.1769833333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.2856000002721, "ollama_total_duration_ms": 437.5513, "ollama_load_ms": 290.4106, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.32693333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 459.3512429279049}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548980.578727, "prompt_tokens": 17, "prefill_ms": 2.3493, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.4952, "kv_decode_ms_equiv": 139.32693333333333, "kv_decode_ms_per_token": 2.1769833333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.2856000002721, "ollama_total_duration_ms": 437.5513, "ollama_load_ms": 290.4106, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 141.67623333333333, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 571.7260975552946}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548981.0621748, "prompt_tokens": 17, "prefill_ms": 2.4021, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3162, "kv_decode_ms_equiv": 140.42159999999998, "kv_decode_ms_per_token": 2.1940874999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.1636000005601, "ollama_total_duration_ms": 433.2547, "ollama_load_ms": 292.2908, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4021, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7077.140835102618}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548981.0621748, "prompt_tokens": 17, "prefill_ms": 2.4021, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3162, "kv_decode_ms_equiv": 140.42159999999998, "kv_decode_ms_per_token": 2.1940874999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.1636000005601, "ollama_total_duration_ms": 433.2547, "ollama_load_ms": 292.2908, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 140.42159999999998, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 455.77033732702097}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548981.0621748, "prompt_tokens": 17, "prefill_ms": 2.4021, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.3162, "kv_decode_ms_equiv": 140.42159999999998, "kv_decode_ms_per_token": 2.1940874999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.1636000005601, "ollama_total_duration_ms": 433.2547, "ollama_load_ms": 292.2908, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.82369999999997, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 567.1327657804693}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548981.5174224, "prompt_tokens": 17, "prefill_ms": 2.7848, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.5762, "kv_decode_ms_equiv": 167.43493333333333, "kv_decode_ms_per_token": 2.6161708333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.9233999998469, "ollama_total_duration_ms": 418.2022, "ollama_load_ms": 256.1705, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.7848, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6104.567652973284}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548981.5174224, "prompt_tokens": 17, "prefill_ms": 2.7848, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.5762, "kv_decode_ms_equiv": 167.43493333333333, "kv_decode_ms_per_token": 2.6161708333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.9233999998469, "ollama_total_duration_ms": 418.2022, "ollama_load_ms": 256.1705, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 167.43493333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 382.2380355513226}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548981.5174224, "prompt_tokens": 17, "prefill_ms": 2.7848, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.5762, "kv_decode_ms_equiv": 167.43493333333333, "kv_decode_ms_per_token": 2.6161708333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 455.9233999998469, "ollama_total_duration_ms": 418.2022, "ollama_load_ms": 256.1705, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 170.21973333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 475.85552164731394}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548981.9734228, "prompt_tokens": 17, "prefill_ms": 2.5959, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.3152, "kv_decode_ms_equiv": 156.42026666666666, "kv_decode_ms_per_token": 2.4440666666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.82339999965916, "ollama_total_duration_ms": 441.1209, "ollama_load_ms": 281.5044, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.5959, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6548.788474132286}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548981.9734228, "prompt_tokens": 17, "prefill_ms": 2.5959, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.3152, "kv_decode_ms_equiv": 156.42026666666666, "kv_decode_ms_per_token": 2.4440666666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.82339999965916, "ollama_total_duration_ms": 441.1209, "ollama_load_ms": 281.5044, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 156.42026666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 409.1541420037642}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548981.9734228, "prompt_tokens": 17, "prefill_ms": 2.5959, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.3152, "kv_decode_ms_equiv": 156.42026666666666, "kv_decode_ms_per_token": 2.4440666666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.82339999965916, "ollama_total_duration_ms": 441.1209, "ollama_load_ms": 281.5044, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 159.01616666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 509.3821697374586}
