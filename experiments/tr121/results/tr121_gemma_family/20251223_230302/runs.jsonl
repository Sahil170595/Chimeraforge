{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548985.9255188, "prompt_tokens": 25, "prefill_ms": 26.1217, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.6356, "kv_decode_ms_equiv": 262.6356, "kv_decode_ms_per_token": 4.10368125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 668.8793000002988, "ollama_total_duration_ms": 633.2332, "ollama_load_ms": 291.0138, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 26.1217, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 957.0586906671464}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548985.9255188, "prompt_tokens": 25, "prefill_ms": 26.1217, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.6356, "kv_decode_ms_equiv": 262.6356, "kv_decode_ms_per_token": 4.10368125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 668.8793000002988, "ollama_total_duration_ms": 633.2332, "ollama_load_ms": 291.0138, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 262.6356, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.68364380152573}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548985.9255188, "prompt_tokens": 25, "prefill_ms": 26.1217, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.6356, "kv_decode_ms_equiv": 262.6356, "kv_decode_ms_per_token": 4.10368125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 668.8793000002988, "ollama_total_duration_ms": 633.2332, "ollama_load_ms": 291.0138, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 288.7573, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 308.2173160643904}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548986.5945973, "prompt_tokens": 25, "prefill_ms": 3.0561, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.086, "kv_decode_ms_equiv": 138.086, "kv_decode_ms_per_token": 2.15759375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 504.6118000000206, "ollama_total_duration_ms": 483.4136, "ollama_load_ms": 294.4117, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.0561, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8180.36059029482}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548986.5945973, "prompt_tokens": 25, "prefill_ms": 3.0561, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.086, "kv_decode_ms_equiv": 138.086, "kv_decode_ms_per_token": 2.15759375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 504.6118000000206, "ollama_total_duration_ms": 483.4136, "ollama_load_ms": 294.4117, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 138.086, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 463.47928102776524}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548986.5945973, "prompt_tokens": 25, "prefill_ms": 3.0561, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.086, "kv_decode_ms_equiv": 138.086, "kv_decode_ms_per_token": 2.15759375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 504.6118000000206, "ollama_total_duration_ms": 483.4136, "ollama_load_ms": 294.4117, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 141.1421, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 630.5701842327697}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548987.099326, "prompt_tokens": 25, "prefill_ms": 2.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8664, "kv_decode_ms_equiv": 136.8664, "kv_decode_ms_per_token": 2.1385375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.4884000005113, "ollama_total_duration_ms": 486.7269, "ollama_load_ms": 299.736, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.8188, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8869.022278983966}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548987.099326, "prompt_tokens": 25, "prefill_ms": 2.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8664, "kv_decode_ms_equiv": 136.8664, "kv_decode_ms_per_token": 2.1385375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.4884000005113, "ollama_total_duration_ms": 486.7269, "ollama_load_ms": 299.736, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.8664, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 467.60928905852717}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548987.099326, "prompt_tokens": 25, "prefill_ms": 2.8188, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.8664, "kv_decode_ms_equiv": 136.8664, "kv_decode_ms_per_token": 2.1385375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.4884000005113, "ollama_total_duration_ms": 486.7269, "ollama_load_ms": 299.736, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 139.6852, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 637.1469561556986}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548987.60704, "prompt_tokens": 25, "prefill_ms": 2.4854, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.136, "kv_decode_ms_equiv": 135.136, "kv_decode_ms_per_token": 2.1115, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.7150000001711, "ollama_total_duration_ms": 483.4996, "ollama_load_ms": 299.4285, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4854, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10058.74305946729}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548987.60704, "prompt_tokens": 25, "prefill_ms": 2.4854, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.136, "kv_decode_ms_equiv": 135.136, "kv_decode_ms_per_token": 2.1115, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.7150000001711, "ollama_total_duration_ms": 483.4996, "ollama_load_ms": 299.4285, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.136, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 473.5969689793985}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548987.60704, "prompt_tokens": 25, "prefill_ms": 2.4854, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.136, "kv_decode_ms_equiv": 135.136, "kv_decode_ms_per_token": 2.1115, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.7150000001711, "ollama_total_duration_ms": 483.4996, "ollama_load_ms": 299.4285, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.6214, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 646.7017484199405}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548988.1348474, "prompt_tokens": 25, "prefill_ms": 3.2422, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.4584, "kv_decode_ms_equiv": 134.4584, "kv_decode_ms_per_token": 2.1009125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 515.3752999995049, "ollama_total_duration_ms": 475.7408, "ollama_load_ms": 297.6124, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 3.2422, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 7710.813645055827}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548988.1348474, "prompt_tokens": 25, "prefill_ms": 3.2422, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.4584, "kv_decode_ms_equiv": 134.4584, "kv_decode_ms_per_token": 2.1009125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 515.3752999995049, "ollama_total_duration_ms": 475.7408, "ollama_load_ms": 297.6124, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.4584, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 475.9836499616238}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548988.1348474, "prompt_tokens": 25, "prefill_ms": 3.2422, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.4584, "kv_decode_ms_equiv": 134.4584, "kv_decode_ms_per_token": 2.1009125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 515.3752999995049, "ollama_total_duration_ms": 475.7408, "ollama_load_ms": 297.6124, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.7006, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 646.3297908651087}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548988.6503072, "prompt_tokens": 25, "prefill_ms": 2.5822, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0602, "kv_decode_ms_equiv": 138.0602, "kv_decode_ms_per_token": 2.157190625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.3183999997855, "ollama_total_duration_ms": 485.2793, "ollama_load_ms": 291.4198, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5822, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9681.666795755558}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548988.6503072, "prompt_tokens": 25, "prefill_ms": 2.5822, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0602, "kv_decode_ms_equiv": 138.0602, "kv_decode_ms_per_token": 2.157190625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.3183999997855, "ollama_total_duration_ms": 485.2793, "ollama_load_ms": 291.4198, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 138.0602, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 463.56589371882694}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548988.6503072, "prompt_tokens": 25, "prefill_ms": 2.5822, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0602, "kv_decode_ms_equiv": 138.0602, "kv_decode_ms_per_token": 2.157190625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 527.3183999997855, "ollama_total_duration_ms": 485.2793, "ollama_load_ms": 291.4198, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 140.6424, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 632.8105891253277}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548989.1778376, "prompt_tokens": 18, "prefill_ms": 10.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.3559, "kv_decode_ms_equiv": 286.7875764705882, "kv_decode_ms_per_token": 4.4810558823529405, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.32989999958954, "ollama_total_duration_ms": 486.3966, "ollama_load_ms": 298.9012, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 10.6491, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1690.2836859452912}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548989.1778376, "prompt_tokens": 18, "prefill_ms": 10.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.3559, "kv_decode_ms_equiv": 286.7875764705882, "kv_decode_ms_per_token": 4.4810558823529405, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.32989999958954, "ollama_total_duration_ms": 486.3966, "ollama_load_ms": 298.9012, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 286.7875764705882, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 223.1616891764612}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548989.1778376, "prompt_tokens": 18, "prefill_ms": 10.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 152.3559, "kv_decode_ms_equiv": 286.7875764705882, "kv_decode_ms_per_token": 4.4810558823529405, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.32989999958954, "ollama_total_duration_ms": 486.3966, "ollama_load_ms": 298.9012, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 297.43667647058817, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 275.6889330966839}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548989.676281, "prompt_tokens": 18, "prefill_ms": 4.8193, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.5702, "kv_decode_ms_equiv": 268.3674352941176, "kv_decode_ms_per_token": 4.193241176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.58359999961976, "ollama_total_duration_ms": 469.012, "ollama_load_ms": 292.4613, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.8193, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3734.9822588342704}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548989.676281, "prompt_tokens": 18, "prefill_ms": 4.8193, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.5702, "kv_decode_ms_equiv": 268.3674352941176, "kv_decode_ms_per_token": 4.193241176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.58359999961976, "ollama_total_duration_ms": 469.012, "ollama_load_ms": 292.4613, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 268.3674352941176, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 238.4790089373516}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548989.676281, "prompt_tokens": 18, "prefill_ms": 4.8193, "prefill_cuda_event_ms": null, "kv_decode_ms": 142.5702, "kv_decode_ms_equiv": 268.3674352941176, "kv_decode_ms_per_token": 4.193241176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 498.58359999961976, "ollama_total_duration_ms": 469.012, "ollama_load_ms": 292.4613, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 273.1867352941176, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 300.1609866295937}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548990.17497, "prompt_tokens": 18, "prefill_ms": 4.9875, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3966, "kv_decode_ms_equiv": 275.5700705882353, "kv_decode_ms_per_token": 4.3057823529411765, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.5800999993153, "ollama_total_duration_ms": 467.1757, "ollama_load_ms": 288.5695, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9875, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3609.0225563909776}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548990.17497, "prompt_tokens": 18, "prefill_ms": 4.9875, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3966, "kv_decode_ms_equiv": 275.5700705882353, "kv_decode_ms_per_token": 4.3057823529411765, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.5800999993153, "ollama_total_duration_ms": 467.1757, "ollama_load_ms": 288.5695, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 275.5700705882353, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.24583084579834}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548990.17497, "prompt_tokens": 18, "prefill_ms": 4.9875, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3966, "kv_decode_ms_equiv": 275.5700705882353, "kv_decode_ms_per_token": 4.3057823529411765, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.5800999993153, "ollama_total_duration_ms": 467.1757, "ollama_load_ms": 288.5695, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 280.5575705882353, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 292.27512851666575}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548990.6558118, "prompt_tokens": 18, "prefill_ms": 4.874, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3432, "kv_decode_ms_equiv": 275.46955294117646, "kv_decode_ms_per_token": 4.304211764705882, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.1916000004712, "ollama_total_duration_ms": 462.9285, "ollama_load_ms": 288.7669, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.874, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3693.0652441526468}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548990.6558118, "prompt_tokens": 18, "prefill_ms": 4.874, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3432, "kv_decode_ms_equiv": 275.46955294117646, "kv_decode_ms_per_token": 4.304211764705882, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.1916000004712, "ollama_total_duration_ms": 462.9285, "ollama_load_ms": 288.7669, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 275.46955294117646, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.33057634382737}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548990.6558118, "prompt_tokens": 18, "prefill_ms": 4.874, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.3432, "kv_decode_ms_equiv": 275.46955294117646, "kv_decode_ms_per_token": 4.304211764705882, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.1916000004712, "ollama_total_duration_ms": 462.9285, "ollama_load_ms": 288.7669, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 280.3435529411765, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 292.4982548723201}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548991.1310842, "prompt_tokens": 18, "prefill_ms": 5.0056, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.6105, "kv_decode_ms_equiv": 279.7374117647059, "kv_decode_ms_per_token": 4.370897058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.4730000000709, "ollama_total_duration_ms": 473.0533, "ollama_load_ms": 293.3677, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.0056, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3595.972510787917}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548991.1310842, "prompt_tokens": 18, "prefill_ms": 5.0056, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.6105, "kv_decode_ms_equiv": 279.7374117647059, "kv_decode_ms_per_token": 4.370897058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.4730000000709, "ollama_total_duration_ms": 473.0533, "ollama_load_ms": 293.3677, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 279.7374117647059, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.78598753116367}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548991.1310842, "prompt_tokens": 18, "prefill_ms": 5.0056, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.6105, "kv_decode_ms_equiv": 279.7374117647059, "kv_decode_ms_per_token": 4.370897058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.4730000000709, "ollama_total_duration_ms": 473.0533, "ollama_load_ms": 293.3677, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 284.7430117647059, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 287.9789726595986}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548991.640673, "prompt_tokens": 18, "prefill_ms": 5.4025, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.837, "kv_decode_ms_equiv": 278.28141176470587, "kv_decode_ms_per_token": 4.348147058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.00199999926554, "ollama_total_duration_ms": 478.6752, "ollama_load_ms": 294.9055, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.4025, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3331.7908375751967}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548991.640673, "prompt_tokens": 18, "prefill_ms": 5.4025, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.837, "kv_decode_ms_equiv": 278.28141176470587, "kv_decode_ms_per_token": 4.348147058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.00199999926554, "ollama_total_duration_ms": 478.6752, "ollama_load_ms": 294.9055, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 278.28141176470587, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 229.98302184162287}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548991.640673, "prompt_tokens": 18, "prefill_ms": 5.4025, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.837, "kv_decode_ms_equiv": 278.28141176470587, "kv_decode_ms_per_token": 4.348147058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.00199999926554, "ollama_total_duration_ms": 478.6752, "ollama_load_ms": 294.9055, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 283.68391176470584, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 289.05410775642696}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548992.142879, "prompt_tokens": 26, "prefill_ms": 10.2278, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.3216, "kv_decode_ms_equiv": 271.3216, "kv_decode_ms_per_token": 4.2394, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.8378999996203, "ollama_total_duration_ms": 625.9078, "ollama_load_ms": 291.5468, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 10.2278, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 2542.0911633000255}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548992.142879, "prompt_tokens": 26, "prefill_ms": 10.2278, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.3216, "kv_decode_ms_equiv": 271.3216, "kv_decode_ms_per_token": 4.2394, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.8378999996203, "ollama_total_duration_ms": 625.9078, "ollama_load_ms": 291.5468, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.3216, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 235.88243619380103}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548992.142879, "prompt_tokens": 26, "prefill_ms": 10.2278, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.3216, "kv_decode_ms_equiv": 271.3216, "kv_decode_ms_per_token": 4.2394, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.8378999996203, "ollama_total_duration_ms": 625.9078, "ollama_load_ms": 291.5468, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 281.5494, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 319.65971158169754}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548992.7978377, "prompt_tokens": 26, "prefill_ms": 5.0673, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.2701, "kv_decode_ms_equiv": 276.2701, "kv_decode_ms_per_token": 4.3167203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.987700000187, "ollama_total_duration_ms": 613.8048, "ollama_load_ms": 286.3346, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.0673, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5130.937580170899}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548992.7978377, "prompt_tokens": 26, "prefill_ms": 5.0673, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.2701, "kv_decode_ms_equiv": 276.2701, "kv_decode_ms_per_token": 4.3167203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.987700000187, "ollama_total_duration_ms": 613.8048, "ollama_load_ms": 286.3346, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 276.2701, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 231.65735271388397}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548992.7978377, "prompt_tokens": 26, "prefill_ms": 5.0673, "prefill_cuda_event_ms": null, "kv_decode_ms": 276.2701, "kv_decode_ms_equiv": 276.2701, "kv_decode_ms_per_token": 4.3167203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.987700000187, "ollama_total_duration_ms": 613.8048, "ollama_load_ms": 286.3346, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 281.3374, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 319.9005891147071}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548993.4639726, "prompt_tokens": 26, "prefill_ms": 5.4264, "prefill_cuda_event_ms": null, "kv_decode_ms": 283.4435, "kv_decode_ms_equiv": 283.4435, "kv_decode_ms_per_token": 4.4288046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 664.1158000002179, "ollama_total_duration_ms": 625.9259, "ollama_load_ms": 290.8557, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.4264, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4791.390240306649}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548993.4639726, "prompt_tokens": 26, "prefill_ms": 5.4264, "prefill_cuda_event_ms": null, "kv_decode_ms": 283.4435, "kv_decode_ms_equiv": 283.4435, "kv_decode_ms_per_token": 4.4288046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 664.1158000002179, "ollama_total_duration_ms": 625.9259, "ollama_load_ms": 290.8557, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 283.4435, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 225.79455870393926}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548993.4639726, "prompt_tokens": 26, "prefill_ms": 5.4264, "prefill_cuda_event_ms": null, "kv_decode_ms": 283.4435, "kv_decode_ms_equiv": 283.4435, "kv_decode_ms_per_token": 4.4288046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 664.1158000002179, "ollama_total_duration_ms": 625.9259, "ollama_load_ms": 290.8557, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 288.8699, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 311.55894054728446}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548994.1282666, "prompt_tokens": 26, "prefill_ms": 4.2486, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.3411, "kv_decode_ms_equiv": 273.3411, "kv_decode_ms_per_token": 4.2709546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 671.0674999994808, "ollama_total_duration_ms": 628.107, "ollama_load_ms": 290.595, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.2486, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 6119.662947794568}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548994.1282666, "prompt_tokens": 26, "prefill_ms": 4.2486, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.3411, "kv_decode_ms_equiv": 273.3411, "kv_decode_ms_per_token": 4.2709546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 671.0674999994808, "ollama_total_duration_ms": 628.107, "ollama_load_ms": 290.595, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 273.3411, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 234.13968846982763}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548994.1282666, "prompt_tokens": 26, "prefill_ms": 4.2486, "prefill_cuda_event_ms": null, "kv_decode_ms": 273.3411, "kv_decode_ms_equiv": 273.3411, "kv_decode_ms_per_token": 4.2709546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 671.0674999994808, "ollama_total_duration_ms": 628.107, "ollama_load_ms": 290.595, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 277.5897, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 324.2195225543311}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548994.7994804, "prompt_tokens": 26, "prefill_ms": 5.2575, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.7777, "kv_decode_ms_equiv": 278.7777, "kv_decode_ms_per_token": 4.3559015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.1217999997243, "ollama_total_duration_ms": 626.0386, "ollama_load_ms": 297.4313, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.2575, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4945.316214931051}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548994.7994804, "prompt_tokens": 26, "prefill_ms": 5.2575, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.7777, "kv_decode_ms_equiv": 278.7777, "kv_decode_ms_per_token": 4.3559015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.1217999997243, "ollama_total_duration_ms": 626.0386, "ollama_load_ms": 297.4313, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 278.7777, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 229.57359932304487}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548994.7994804, "prompt_tokens": 26, "prefill_ms": 5.2575, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.7777, "kv_decode_ms_equiv": 278.7777, "kv_decode_ms_per_token": 4.3559015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 665.1217999997243, "ollama_total_duration_ms": 626.0386, "ollama_load_ms": 297.4313, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 284.0352, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 316.8621353973029}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548995.4647088, "prompt_tokens": 26, "prefill_ms": 5.3344, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.7469, "kv_decode_ms_equiv": 277.7469, "kv_decode_ms_per_token": 4.3397953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 661.9733000006818, "ollama_total_duration_ms": 620.7685, "ollama_load_ms": 293.119, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.3344, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4874.025194961008}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548995.4647088, "prompt_tokens": 26, "prefill_ms": 5.3344, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.7469, "kv_decode_ms_equiv": 277.7469, "kv_decode_ms_per_token": 4.3397953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 661.9733000006818, "ollama_total_duration_ms": 620.7685, "ollama_load_ms": 293.119, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 277.7469, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 230.42561411126465}
{"task_idx": 2, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548995.4647088, "prompt_tokens": 26, "prefill_ms": 5.3344, "prefill_cuda_event_ms": null, "kv_decode_ms": 277.7469, "kv_decode_ms_equiv": 277.7469, "kv_decode_ms_per_token": 4.3397953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 661.9733000006818, "ollama_total_duration_ms": 620.7685, "ollama_load_ms": 293.119, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 283.0813, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 317.92986679091837}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548996.1268568, "prompt_tokens": 17, "prefill_ms": 6.5144, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.678, "kv_decode_ms_equiv": 136.09167567567567, "kv_decode_ms_per_token": 2.1264324324324324, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.3532999993913, "ollama_total_duration_ms": 413.5527, "ollama_load_ms": 299.8014, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 6.5144, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2609.6033402922753}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548996.1268568, "prompt_tokens": 17, "prefill_ms": 6.5144, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.678, "kv_decode_ms_equiv": 136.09167567567567, "kv_decode_ms_per_token": 2.1264324324324324, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.3532999993913, "ollama_total_duration_ms": 413.5527, "ollama_load_ms": 299.8014, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 136.09167567567567, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 470.27123211062815}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548996.1268568, "prompt_tokens": 17, "prefill_ms": 6.5144, "prefill_cuda_event_ms": null, "kv_decode_ms": 78.678, "kv_decode_ms_equiv": 136.09167567567567, "kv_decode_ms_per_token": 2.1264324324324324, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 426.3532999993913, "ollama_total_duration_ms": 413.5527, "ollama_load_ms": 299.8014, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.60607567567567, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 567.9982400204017}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548996.5533247, "prompt_tokens": 17, "prefill_ms": 2.4417, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.4213, "kv_decode_ms_equiv": 141.89506666666668, "kv_decode_ms_per_token": 2.217110416666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 451.45279999997, "ollama_total_duration_ms": 436.4575, "ollama_load_ms": 297.4751, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4417, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6962.36228856944}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548996.5533247, "prompt_tokens": 17, "prefill_ms": 2.4417, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.4213, "kv_decode_ms_equiv": 141.89506666666668, "kv_decode_ms_per_token": 2.217110416666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 451.45279999997, "ollama_total_duration_ms": 436.4575, "ollama_load_ms": 297.4751, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 141.89506666666668, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 451.03752726192965}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548996.5533247, "prompt_tokens": 17, "prefill_ms": 2.4417, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.4213, "kv_decode_ms_equiv": 141.89506666666668, "kv_decode_ms_per_token": 2.217110416666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 451.45279999997, "ollama_total_duration_ms": 436.4575, "ollama_load_ms": 297.4751, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 144.33676666666668, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 561.1875745218994}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548997.0048752, "prompt_tokens": 17, "prefill_ms": 2.5846, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.6129, "kv_decode_ms_equiv": 142.15053333333333, "kv_decode_ms_per_token": 2.2211020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7551999996722, "ollama_total_duration_ms": 433.1515, "ollama_load_ms": 295.8693, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.5846, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6577.420103691094}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548997.0048752, "prompt_tokens": 17, "prefill_ms": 2.5846, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.6129, "kv_decode_ms_equiv": 142.15053333333333, "kv_decode_ms_per_token": 2.2211020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7551999996722, "ollama_total_duration_ms": 433.1515, "ollama_load_ms": 295.8693, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.15053333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 450.2269425182131}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548997.0048752, "prompt_tokens": 17, "prefill_ms": 2.5846, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.6129, "kv_decode_ms_equiv": 142.15053333333333, "kv_decode_ms_per_token": 2.2211020833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.7551999996722, "ollama_total_duration_ms": 433.1515, "ollama_load_ms": 295.8693, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 144.73513333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 559.6429708151949}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548997.4699018, "prompt_tokens": 17, "prefill_ms": 2.966, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.9682, "kv_decode_ms_equiv": 151.95759999999999, "kv_decode_ms_per_token": 2.3743374999999998, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.07929999986663, "ollama_total_duration_ms": 450.2674, "ollama_load_ms": 295.9855, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.966, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5731.625084288604}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548997.4699018, "prompt_tokens": 17, "prefill_ms": 2.966, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.9682, "kv_decode_ms_equiv": 151.95759999999999, "kv_decode_ms_per_token": 2.3743374999999998, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.07929999986663, "ollama_total_duration_ms": 450.2674, "ollama_load_ms": 295.9855, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 151.95759999999999, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 421.1701158744281}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548997.4699018, "prompt_tokens": 17, "prefill_ms": 2.966, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.9682, "kv_decode_ms_equiv": 151.95759999999999, "kv_decode_ms_per_token": 2.3743374999999998, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 464.07929999986663, "ollama_total_duration_ms": 450.2674, "ollama_load_ms": 295.9855, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 154.9236, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 522.8383538724894}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548997.9341865, "prompt_tokens": 17, "prefill_ms": 3.2192, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3939, "kv_decode_ms_equiv": 167.19186666666667, "kv_decode_ms_per_token": 2.6123729166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 472.1241999995982, "ollama_total_duration_ms": 458.6063, "ollama_load_ms": 298.1943, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.2192, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5280.8151093439365}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548997.9341865, "prompt_tokens": 17, "prefill_ms": 3.2192, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3939, "kv_decode_ms_equiv": 167.19186666666667, "kv_decode_ms_per_token": 2.6123729166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 472.1241999995982, "ollama_total_duration_ms": 458.6063, "ollama_load_ms": 298.1943, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 167.19186666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 382.7937403653607}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548997.9341865, "prompt_tokens": 17, "prefill_ms": 3.2192, "prefill_cuda_event_ms": null, "kv_decode_ms": 125.3939, "kv_decode_ms_equiv": 167.19186666666667, "kv_decode_ms_per_token": 2.6123729166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 472.1241999995982, "ollama_total_duration_ms": 458.6063, "ollama_load_ms": 298.1943, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 170.41106666666667, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 475.32124283008227}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548998.406397, "prompt_tokens": 17, "prefill_ms": 3.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.4588, "kv_decode_ms_equiv": 155.2784, "kv_decode_ms_per_token": 2.426225, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9754000001194, "ollama_total_duration_ms": 440.7635, "ollama_load_ms": 291.3111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.2653, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5206.259761737053}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548998.406397, "prompt_tokens": 17, "prefill_ms": 3.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.4588, "kv_decode_ms_equiv": 155.2784, "kv_decode_ms_per_token": 2.426225, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9754000001194, "ollama_total_duration_ms": 440.7635, "ollama_load_ms": 291.3111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 155.2784, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 412.1629280054405}
{"task_idx": 3, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548998.406397, "prompt_tokens": 17, "prefill_ms": 3.2653, "prefill_cuda_event_ms": null, "kv_decode_ms": 116.4588, "kv_decode_ms_equiv": 155.2784, "kv_decode_ms_per_token": 2.426225, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 486.9754000001194, "ollama_total_duration_ms": 440.7635, "ollama_load_ms": 291.3111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 158.5437, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 510.9001492963769}
