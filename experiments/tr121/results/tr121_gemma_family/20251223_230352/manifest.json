{
  "argv": [
    "scripts/tr121/run_scaling.py",
    "--config",
    "scripts/tr121/configs/gemma_family.yaml",
    "--out-dir",
    "scripts/tr121/results/tr121_gemma_family/20251223_230352",
    "--ollama-timeout-s",
    "600"
  ],
  "config_path": "scripts\\tr121\\configs\\gemma_family.yaml",
  "config_sha256": "387dba6b4bebb1cc3f9753bc99d661ea1bc38c9b7ce0c403cda511d9a57c97c5",
  "cuda_available": true,
  "gen_tokens": 64,
  "git_head": "326c5b676047ee1be98782ee5a238ba7921eac81",
  "measured_params": {
    "hf_params_millions": [],
    "ollama_params_millions": [
      {
        "family": "gemma3",
        "model": "gemma3:270m",
        "parameter_size_raw": "268.10M",
        "params_millions_measured": 268.1,
        "quantization_level": "Q8_0"
      },
      {
        "family": "gemma3",
        "model": "gemma3:1b-it-qat",
        "parameter_size_raw": "999.89M",
        "params_millions_measured": 999.89,
        "quantization_level": "Q4_0"
      },
      {
        "family": "gemma3",
        "model": "gemma3:latest",
        "parameter_size_raw": "4.3B",
        "params_millions_measured": 4300.0,
        "quantization_level": "Q4_K_M"
      }
    ]
  },
  "modes": [
    "prefill",
    "kv_decode",
    "e2e_kv"
  ],
  "nvml": {
    "memory_total_mb": 12282.0,
    "name": "NVIDIA GeForce RTX 4080 Laptop GPU"
  },
  "ollama_timeout_s": 600.0,
  "platform": {
    "machine": "AMD64",
    "os": "Windows-11-10.0.26200-SP0",
    "processor": "Intel64 Family 6 Model 183 Stepping 1, GenuineIntel",
    "python": "3.13.1"
  },
  "records_expected": 108,
  "repetitions": 5,
  "resolved": {
    "backends": [
      {
        "device": null,
        "dtype": null,
        "kind": "ollama",
        "name": "ollama",
        "options": {
          "temperature": 0,
          "top_p": 1
        },
        "url": "http://localhost:11434"
      }
    ],
    "filters": {
      "backends_allowlist": null,
      "models_allowlist": null,
      "scenarios_allowlist": null
    },
    "models": [
      {
        "kind": "ollama",
        "name": "gemma3:270m",
        "params_millions": 268.1
      },
      {
        "kind": "ollama",
        "name": "gemma3:1b-it-qat",
        "params_millions": 999.9
      },
      {
        "kind": "ollama",
        "name": "gemma3:latest",
        "params_millions": 4300
      }
    ],
    "scenarios": [
      {
        "name": "short",
        "prompt": "Summarize RLHF in one sentence."
      },
      {
        "name": "medium",
        "prompt": "Explain how backpressure works in an inference service and when to enable queueing."
      }
    ]
  },
  "results": {
    "hf_load_ms_csv": "scripts\\tr121\\results\\tr121_gemma_family\\20251223_230352\\hf_load_ms.csv",
    "metrics_csv": "scripts\\tr121\\results\\tr121_gemma_family\\20251223_230352\\metrics.csv",
    "resolved_model_params_csv": "scripts\\tr121\\results\\tr121_gemma_family\\20251223_230352\\resolved_model_params.csv",
    "runs_jsonl": "scripts\\tr121\\results\\tr121_gemma_family\\20251223_230352\\runs.jsonl"
  },
  "run_id": "20251223_230352",
  "run_name": "tr121_gemma_family_v1",
  "seed": 42,
  "task_count": 6,
  "torch": {
    "cuda": "12.8",
    "torch": "2.8.0+cu128",
    "transformers": "4.57.0"
  },
  "warmup_repetitions": 1
}