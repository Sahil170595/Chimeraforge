{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549035.7868097, "prompt_tokens": 25, "prefill_ms": 48.7032, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.9502, "kv_decode_ms_equiv": 389.9502, "kv_decode_ms_per_token": 6.092971875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 808.6802999996507, "ollama_total_duration_ms": 774.193, "ollama_load_ms": 283.3702, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 48.7032, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 513.3132935823519}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549035.7868097, "prompt_tokens": 25, "prefill_ms": 48.7032, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.9502, "kv_decode_ms_equiv": 389.9502, "kv_decode_ms_per_token": 6.092971875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 808.6802999996507, "ollama_total_duration_ms": 774.193, "ollama_load_ms": 283.3702, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 389.9502, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 164.12352141376002}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549035.7868097, "prompt_tokens": 25, "prefill_ms": 48.7032, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.9502, "kv_decode_ms_equiv": 389.9502, "kv_decode_ms_per_token": 6.092971875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 808.6802999996507, "ollama_total_duration_ms": 774.193, "ollama_load_ms": 283.3702, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 438.6534, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 202.893674139993}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549036.5956094, "prompt_tokens": 25, "prefill_ms": 2.4389, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.0329, "kv_decode_ms_equiv": 133.0329, "kv_decode_ms_per_token": 2.0786390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.4194000006901, "ollama_total_duration_ms": 468.2238, "ollama_load_ms": 294.7641, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.4389, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10250.52277666161}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549036.5956094, "prompt_tokens": 25, "prefill_ms": 2.4389, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.0329, "kv_decode_ms_equiv": 133.0329, "kv_decode_ms_per_token": 2.0786390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.4194000006901, "ollama_total_duration_ms": 468.2238, "ollama_load_ms": 294.7641, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 133.0329, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 481.08400252869774}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549036.5956094, "prompt_tokens": 25, "prefill_ms": 2.4389, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.0329, "kv_decode_ms_equiv": 133.0329, "kv_decode_ms_per_token": 2.0786390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 502.4194000006901, "ollama_total_duration_ms": 468.2238, "ollama_load_ms": 294.7641, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.4718, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 656.963294205879}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549037.0981226, "prompt_tokens": 25, "prefill_ms": 2.2613, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1868, "kv_decode_ms_equiv": 134.1868, "kv_decode_ms_per_token": 2.09666875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.1914000000761, "ollama_total_duration_ms": 463.6098, "ollama_load_ms": 290.1596, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2613, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11055.587493919427}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549037.0981226, "prompt_tokens": 25, "prefill_ms": 2.2613, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1868, "kv_decode_ms_equiv": 134.1868, "kv_decode_ms_per_token": 2.09666875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.1914000000761, "ollama_total_duration_ms": 463.6098, "ollama_load_ms": 290.1596, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.1868, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 476.94706185705303}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549037.0981226, "prompt_tokens": 25, "prefill_ms": 2.2613, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.1868, "kv_decode_ms_equiv": 134.1868, "kv_decode_ms_per_token": 2.09666875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.1914000000761, "ollama_total_duration_ms": 463.6098, "ollama_load_ms": 290.1596, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.4481, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 652.2626551780493}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549037.5835984, "prompt_tokens": 25, "prefill_ms": 2.2637, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7773, "kv_decode_ms_equiv": 134.7773, "kv_decode_ms_per_token": 2.1058953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 521.9486999994842, "ollama_total_duration_ms": 471.7121, "ollama_load_ms": 295.2371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2637, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11043.866236692142}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549037.5835984, "prompt_tokens": 25, "prefill_ms": 2.2637, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7773, "kv_decode_ms_equiv": 134.7773, "kv_decode_ms_per_token": 2.1058953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 521.9486999994842, "ollama_total_duration_ms": 471.7121, "ollama_load_ms": 295.2371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.7773, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 474.8574129322965}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549037.5835984, "prompt_tokens": 25, "prefill_ms": 2.2637, "prefill_cuda_event_ms": null, "kv_decode_ms": 134.7773, "kv_decode_ms_equiv": 134.7773, "kv_decode_ms_per_token": 2.1058953125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 521.9486999994842, "ollama_total_duration_ms": 471.7121, "ollama_load_ms": 295.2371, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.041, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 649.44067833714}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549038.1056316, "prompt_tokens": 25, "prefill_ms": 2.3691, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6114, "kv_decode_ms_equiv": 128.6114, "kv_decode_ms_per_token": 2.009553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.86570000044594, "ollama_total_duration_ms": 464.8244, "ollama_load_ms": 289.7026, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3691, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10552.530496813137}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549038.1056316, "prompt_tokens": 25, "prefill_ms": 2.3691, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6114, "kv_decode_ms_equiv": 128.6114, "kv_decode_ms_per_token": 2.009553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.86570000044594, "ollama_total_duration_ms": 464.8244, "ollama_load_ms": 289.7026, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 128.6114, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 497.6230722937468}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549038.1056316, "prompt_tokens": 25, "prefill_ms": 2.3691, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.6114, "kv_decode_ms_equiv": 128.6114, "kv_decode_ms_per_token": 2.009553125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.86570000044594, "ollama_total_duration_ms": 464.8244, "ollama_load_ms": 289.7026, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 130.9805, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 679.4904585033651}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549038.6005857, "prompt_tokens": 25, "prefill_ms": 2.545, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0066, "kv_decode_ms_equiv": 128.0066, "kv_decode_ms_per_token": 2.000103125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 499.85110000034183, "ollama_total_duration_ms": 463.1472, "ollama_load_ms": 292.577, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.545, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9823.182711198428}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549038.6005857, "prompt_tokens": 25, "prefill_ms": 2.545, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0066, "kv_decode_ms_equiv": 128.0066, "kv_decode_ms_per_token": 2.000103125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 499.85110000034183, "ollama_total_duration_ms": 463.1472, "ollama_load_ms": 292.577, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 128.0066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 499.9742200792772}
{"task_idx": 0, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549038.6005857, "prompt_tokens": 25, "prefill_ms": 2.545, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.0066, "kv_decode_ms_equiv": 128.0066, "kv_decode_ms_per_token": 2.000103125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 499.85110000034183, "ollama_total_duration_ms": 463.1472, "ollama_load_ms": 292.577, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 130.55159999999998, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 681.7227824094075}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549039.1006248, "prompt_tokens": 18, "prefill_ms": 10.7113, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.8234, "kv_decode_ms_equiv": 278.25581176470587, "kv_decode_ms_per_token": 4.347747058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 495.96529999962513, "ollama_total_duration_ms": 483.6718, "ollama_load_ms": 297.0091, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 10.7113, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1680.4682904969518}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549039.1006248, "prompt_tokens": 18, "prefill_ms": 10.7113, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.8234, "kv_decode_ms_equiv": 278.25581176470587, "kv_decode_ms_per_token": 4.347747058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 495.96529999962513, "ollama_total_duration_ms": 483.6718, "ollama_load_ms": 297.0091, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 278.25581176470587, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 230.00418066422503}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549039.1006248, "prompt_tokens": 18, "prefill_ms": 10.7113, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.8234, "kv_decode_ms_equiv": 278.25581176470587, "kv_decode_ms_per_token": 4.347747058823529, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 495.96529999962513, "ollama_total_duration_ms": 483.6718, "ollama_load_ms": 297.0091, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 288.96711176470586, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 283.7693172044065}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549039.596709, "prompt_tokens": 18, "prefill_ms": 4.9931, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.974, "kv_decode_ms_equiv": 272.89223529411765, "kv_decode_ms_per_token": 4.263941176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 479.12709999945946, "ollama_total_duration_ms": 467.1715, "ollama_load_ms": 293.1582, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9931, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3604.9748653141332}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549039.596709, "prompt_tokens": 18, "prefill_ms": 4.9931, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.974, "kv_decode_ms_equiv": 272.89223529411765, "kv_decode_ms_per_token": 4.263941176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 479.12709999945946, "ollama_total_duration_ms": 467.1715, "ollama_load_ms": 293.1582, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 272.89223529411765, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 234.52481134548262}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549039.596709, "prompt_tokens": 18, "prefill_ms": 4.9931, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.974, "kv_decode_ms_equiv": 272.89223529411765, "kv_decode_ms_per_token": 4.263941176470588, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 479.12709999945946, "ollama_total_duration_ms": 467.1715, "ollama_load_ms": 293.1582, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 277.8853352941177, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 295.0857407182357}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549040.0759442, "prompt_tokens": 18, "prefill_ms": 4.6836, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.7839, "kv_decode_ms_equiv": 278.1814588235294, "kv_decode_ms_per_token": 4.346585294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.427399999935, "ollama_total_duration_ms": 464.5429, "ollama_load_ms": 285.4848, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.6836, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3843.197540353574}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549040.0759442, "prompt_tokens": 18, "prefill_ms": 4.6836, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.7839, "kv_decode_ms_equiv": 278.1814588235294, "kv_decode_ms_per_token": 4.346585294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.427399999935, "ollama_total_duration_ms": 464.5429, "ollama_load_ms": 285.4848, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 278.1814588235294, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 230.06565667843387}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549040.0759442, "prompt_tokens": 18, "prefill_ms": 4.6836, "prefill_cuda_event_ms": null, "kv_decode_ms": 147.7839, "kv_decode_ms_equiv": 278.1814588235294, "kv_decode_ms_per_token": 4.346585294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.427399999935, "ollama_total_duration_ms": 464.5429, "ollama_load_ms": 285.4848, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 282.8650588235294, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 289.8908770883477}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549040.5615394, "prompt_tokens": 18, "prefill_ms": 4.7875, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7982, "kv_decode_ms_equiv": 280.0907294117647, "kv_decode_ms_per_token": 4.376417647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 514.1890000004423, "ollama_total_duration_ms": 473.8006, "ollama_load_ms": 292.179, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.7875, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3759.7911227154054}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549040.5615394, "prompt_tokens": 18, "prefill_ms": 4.7875, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7982, "kv_decode_ms_equiv": 280.0907294117647, "kv_decode_ms_per_token": 4.376417647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 514.1890000004423, "ollama_total_duration_ms": 473.8006, "ollama_load_ms": 292.179, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 280.0907294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.49738773721725}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549040.5615394, "prompt_tokens": 18, "prefill_ms": 4.7875, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7982, "kv_decode_ms_equiv": 280.0907294117647, "kv_decode_ms_per_token": 4.376417647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 514.1890000004423, "ollama_total_duration_ms": 473.8006, "ollama_load_ms": 292.179, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 284.8782294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 287.8422832426296}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549041.075833, "prompt_tokens": 18, "prefill_ms": 5.6292, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7304, "kv_decode_ms_equiv": 279.9631058823529, "kv_decode_ms_per_token": 4.374423529411764, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 510.8559999998761, "ollama_total_duration_ms": 472.4756, "ollama_load_ms": 287.9244, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.6292, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3197.6124493711363}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549041.075833, "prompt_tokens": 18, "prefill_ms": 5.6292, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7304, "kv_decode_ms_equiv": 279.9631058823529, "kv_decode_ms_per_token": 4.374423529411764, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 510.8559999998761, "ollama_total_duration_ms": 472.4756, "ollama_load_ms": 287.9244, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 279.9631058823529, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.6015501874533}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549041.075833, "prompt_tokens": 18, "prefill_ms": 5.6292, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.7304, "kv_decode_ms_equiv": 279.9631058823529, "kv_decode_ms_per_token": 4.374423529411764, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 510.8559999998761, "ollama_total_duration_ms": 472.4756, "ollama_load_ms": 287.9244, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 285.59230588235295, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 287.12258107464254}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549041.586768, "prompt_tokens": 18, "prefill_ms": 4.4686, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.4777, "kv_decode_ms_equiv": 275.7227294117647, "kv_decode_ms_per_token": 4.3081676470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.60799999989104, "ollama_total_duration_ms": 471.8308, "ollama_load_ms": 291.9646, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.4686, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 4028.1072371660025}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549041.586768, "prompt_tokens": 18, "prefill_ms": 4.4686, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.4777, "kv_decode_ms_equiv": 275.7227294117647, "kv_decode_ms_per_token": 4.3081676470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.60799999989104, "ollama_total_duration_ms": 471.8308, "ollama_load_ms": 291.9646, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 275.7227294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.1172437852315}
{"task_idx": 1, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549041.586768, "prompt_tokens": 18, "prefill_ms": 4.4686, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.4777, "kv_decode_ms_equiv": 275.7227294117647, "kv_decode_ms_per_token": 4.3081676470588235, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.60799999989104, "ollama_total_duration_ms": 471.8308, "ollama_load_ms": 291.9646, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 280.1913294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 292.65716456019993}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549042.0945837, "prompt_tokens": 17, "prefill_ms": 25.6526, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.2958, "kv_decode_ms_equiv": 603.8793142857143, "kv_decode_ms_per_token": 9.435614285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 13413.18179999962, "ollama_total_duration_ms": 13324.5825, "ollama_load_ms": 12848.3027, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 25.6526, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 662.7008568332254}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549042.0945837, "prompt_tokens": 17, "prefill_ms": 25.6526, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.2958, "kv_decode_ms_equiv": 603.8793142857143, "kv_decode_ms_per_token": 9.435614285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13413.18179999962, "ollama_total_duration_ms": 13324.5825, "ollama_load_ms": 12848.3027, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 603.8793142857143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 105.98144113563656}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549042.0945837, "prompt_tokens": 17, "prefill_ms": 25.6526, "prefill_cuda_event_ms": null, "kv_decode_ms": 396.2958, "kv_decode_ms_equiv": 603.8793142857143, "kv_decode_ms_per_token": 9.435614285714285, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13413.18179999962, "ollama_total_duration_ms": 13324.5825, "ollama_load_ms": 12848.3027, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 629.5319142857143, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 128.66702729742465}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549055.5078645, "prompt_tokens": 17, "prefill_ms": 9.614, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.0034, "kv_decode_ms_equiv": 544.7857951219512, "kv_decode_ms_per_token": 8.512278048780487, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1005.4947999997239, "ollama_total_duration_ms": 1002.6111, "ollama_load_ms": 618.4202, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 9.614, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1768.2546286665279}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549055.5078645, "prompt_tokens": 17, "prefill_ms": 9.614, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.0034, "kv_decode_ms_equiv": 544.7857951219512, "kv_decode_ms_per_token": 8.512278048780487, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1005.4947999997239, "ollama_total_duration_ms": 1002.6111, "ollama_load_ms": 618.4202, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 544.7857951219512, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 117.47736555002042}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549055.5078645, "prompt_tokens": 17, "prefill_ms": 9.614, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.0034, "kv_decode_ms_equiv": 544.7857951219512, "kv_decode_ms_per_token": 8.512278048780487, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1005.4947999997239, "ollama_total_duration_ms": 1002.6111, "ollama_load_ms": 618.4202, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 554.3997951219512, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 146.10395009648667}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549056.5134902, "prompt_tokens": 17, "prefill_ms": 9.3649, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.1681, "kv_decode_ms_equiv": 545.042887804878, "kv_decode_ms_per_token": 8.516295121951218, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1026.8245000006573, "ollama_total_duration_ms": 1023.8735, "ollama_load_ms": 636.377, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 9.3649, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1815.2890046877167}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549056.5134902, "prompt_tokens": 17, "prefill_ms": 9.3649, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.1681, "kv_decode_ms_equiv": 545.042887804878, "kv_decode_ms_per_token": 8.516295121951218, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1026.8245000006573, "ollama_total_duration_ms": 1023.8735, "ollama_load_ms": 636.377, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 545.042887804878, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 117.42195234902617}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549056.5134902, "prompt_tokens": 17, "prefill_ms": 9.3649, "prefill_cuda_event_ms": null, "kv_decode_ms": 349.1681, "kv_decode_ms_equiv": 545.042887804878, "kv_decode_ms_per_token": 8.516295121951218, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1026.8245000006573, "ollama_total_duration_ms": 1023.8735, "ollama_load_ms": 636.377, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 554.407787804878, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 146.10184377227344}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549057.540867, "prompt_tokens": 17, "prefill_ms": 8.6003, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.6917, "kv_decode_ms_equiv": 539.616312195122, "kv_decode_ms_per_token": 8.43150487804878, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.7275000004302, "ollama_total_duration_ms": 941.9883, "ollama_load_ms": 560.388, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.6003, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1976.6752322593397}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549057.540867, "prompt_tokens": 17, "prefill_ms": 8.6003, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.6917, "kv_decode_ms_equiv": 539.616312195122, "kv_decode_ms_per_token": 8.43150487804878, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.7275000004302, "ollama_total_duration_ms": 941.9883, "ollama_load_ms": 560.388, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 539.616312195122, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 118.60278971117906}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549057.540867, "prompt_tokens": 17, "prefill_ms": 8.6003, "prefill_cuda_event_ms": null, "kv_decode_ms": 345.6917, "kv_decode_ms_equiv": 539.616312195122, "kv_decode_ms_per_token": 8.43150487804878, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.7275000004302, "ollama_total_duration_ms": 941.9883, "ollama_load_ms": 560.388, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 548.2166121951219, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 147.75181597592737}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549058.486712, "prompt_tokens": 17, "prefill_ms": 8.9491, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0133, "kv_decode_ms_equiv": 543.2402731707317, "kv_decode_ms_per_token": 8.488129268292683, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 996.7176000000109, "ollama_total_duration_ms": 971.3504, "ollama_load_ms": 584.5194, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 8.9491, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1899.632365265781}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549058.486712, "prompt_tokens": 17, "prefill_ms": 8.9491, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0133, "kv_decode_ms_equiv": 543.2402731707317, "kv_decode_ms_per_token": 8.488129268292683, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 996.7176000000109, "ollama_total_duration_ms": 971.3504, "ollama_load_ms": 584.5194, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 543.2402731707317, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 117.81158938465856}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549058.486712, "prompt_tokens": 17, "prefill_ms": 8.9491, "prefill_cuda_event_ms": null, "kv_decode_ms": 348.0133, "kv_decode_ms_equiv": 543.2402731707317, "kv_decode_ms_per_token": 8.488129268292683, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 996.7176000000109, "ollama_total_duration_ms": 971.3504, "ollama_load_ms": 584.5194, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 552.1893731707318, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 146.68880629645068}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549059.4835532, "prompt_tokens": 17, "prefill_ms": 9.077, "prefill_cuda_event_ms": null, "kv_decode_ms": 344.1526, "kv_decode_ms_equiv": 537.2138146341464, "kv_decode_ms_per_token": 8.393965853658537, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.0200999998051, "ollama_total_duration_ms": 950.9207, "ollama_load_ms": 567.1924, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 9.077, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1872.865484190812}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549059.4835532, "prompt_tokens": 17, "prefill_ms": 9.077, "prefill_cuda_event_ms": null, "kv_decode_ms": 344.1526, "kv_decode_ms_equiv": 537.2138146341464, "kv_decode_ms_per_token": 8.393965853658537, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.0200999998051, "ollama_total_duration_ms": 950.9207, "ollama_load_ms": 567.1924, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 537.2138146341464, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 119.1331984706784}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549059.4835532, "prompt_tokens": 17, "prefill_ms": 9.077, "prefill_cuda_event_ms": null, "kv_decode_ms": 344.1526, "kv_decode_ms_equiv": 537.2138146341464, "kv_decode_ms_per_token": 8.393965853658537, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 971.0200999998051, "ollama_total_duration_ms": 950.9207, "ollama_load_ms": 567.1924, "ollama_done_reason": "stop", "params_millions_measured": 4300.0, "latency_ms": 546.2908146341464, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 148.27267424264875}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549060.4548283, "prompt_tokens": 26, "prefill_ms": 11.1539, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.2822, "kv_decode_ms_equiv": 280.2822, "kv_decode_ms_per_token": 4.379409375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 645.991000000322, "ollama_total_duration_ms": 617.3994, "ollama_load_ms": 274.6224, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 11.1539, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 2331.02322954303}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549060.4548283, "prompt_tokens": 26, "prefill_ms": 11.1539, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.2822, "kv_decode_ms_equiv": 280.2822, "kv_decode_ms_per_token": 4.379409375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 645.991000000322, "ollama_total_duration_ms": 617.3994, "ollama_load_ms": 274.6224, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 280.2822, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.34129316809987}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549060.4548283, "prompt_tokens": 26, "prefill_ms": 11.1539, "prefill_cuda_event_ms": null, "kv_decode_ms": 280.2822, "kv_decode_ms_equiv": 280.2822, "kv_decode_ms_per_token": 4.379409375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 645.991000000322, "ollama_total_duration_ms": 617.3994, "ollama_load_ms": 274.6224, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 291.4361, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 308.8155516766797}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549061.1009228, "prompt_tokens": 26, "prefill_ms": 5.0429, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.6352, "kv_decode_ms_equiv": 278.6352, "kv_decode_ms_per_token": 4.353675, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 656.2872999993488, "ollama_total_duration_ms": 623.6221, "ollama_load_ms": 291.1396, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.0429, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5155.76354875171}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549061.1009228, "prompt_tokens": 26, "prefill_ms": 5.0429, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.6352, "kv_decode_ms_equiv": 278.6352, "kv_decode_ms_per_token": 4.353675, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 656.2872999993488, "ollama_total_duration_ms": 623.6221, "ollama_load_ms": 291.1396, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 278.6352, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 229.69100817125764}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549061.1009228, "prompt_tokens": 26, "prefill_ms": 5.0429, "prefill_cuda_event_ms": null, "kv_decode_ms": 278.6352, "kv_decode_ms_equiv": 278.6352, "kv_decode_ms_per_token": 4.353675, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 656.2872999993488, "ollama_total_duration_ms": 623.6221, "ollama_load_ms": 291.1396, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 283.6781, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 317.26100816383075}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549061.7573237, "prompt_tokens": 26, "prefill_ms": 4.8893, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.3477, "kv_decode_ms_equiv": 279.3477, "kv_decode_ms_per_token": 4.3648078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.5862999992096, "ollama_total_duration_ms": 620.3115, "ollama_load_ms": 282.6777, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.8893, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5317.734645041212}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549061.7573237, "prompt_tokens": 26, "prefill_ms": 4.8893, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.3477, "kv_decode_ms_equiv": 279.3477, "kv_decode_ms_per_token": 4.3648078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.5862999992096, "ollama_total_duration_ms": 620.3115, "ollama_load_ms": 282.6777, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 279.3477, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 229.10516177509248}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549061.7573237, "prompt_tokens": 26, "prefill_ms": 4.8893, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.3477, "kv_decode_ms_equiv": 279.3477, "kv_decode_ms_per_token": 4.3648078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 654.5862999992096, "ollama_total_duration_ms": 620.3115, "ollama_load_ms": 282.6777, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 284.23699999999997, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 316.63717250041344}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549062.4120996, "prompt_tokens": 26, "prefill_ms": 5.4755, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.7504, "kv_decode_ms_equiv": 287.7504, "kv_decode_ms_per_token": 4.4961, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 622.9577999993126, "ollama_total_duration_ms": 589.3438, "ollama_load_ms": 246.5106, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.4755, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 4748.424801388001}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549062.4120996, "prompt_tokens": 26, "prefill_ms": 5.4755, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.7504, "kv_decode_ms_equiv": 287.7504, "kv_decode_ms_per_token": 4.4961, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 622.9577999993126, "ollama_total_duration_ms": 589.3438, "ollama_load_ms": 246.5106, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 287.7504, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 222.41498187317896}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549062.4120996, "prompt_tokens": 26, "prefill_ms": 5.4755, "prefill_cuda_event_ms": null, "kv_decode_ms": 287.7504, "kv_decode_ms_equiv": 287.7504, "kv_decode_ms_per_token": 4.4961, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 622.9577999993126, "ollama_total_duration_ms": 589.3438, "ollama_load_ms": 246.5106, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 293.2259, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 306.93059514865496}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549063.035188, "prompt_tokens": 26, "prefill_ms": 4.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.8463, "kv_decode_ms_equiv": 274.8463, "kv_decode_ms_per_token": 4.2944734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.8499000001393, "ollama_total_duration_ms": 618.8416, "ollama_load_ms": 283.1918, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.7695, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5451.305168256631}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549063.035188, "prompt_tokens": 26, "prefill_ms": 4.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.8463, "kv_decode_ms_equiv": 274.8463, "kv_decode_ms_per_token": 4.2944734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.8499000001393, "ollama_total_duration_ms": 618.8416, "ollama_load_ms": 283.1918, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.8463, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.85741885555674}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549063.035188, "prompt_tokens": 26, "prefill_ms": 4.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.8463, "kv_decode_ms_equiv": 274.8463, "kv_decode_ms_per_token": 4.2944734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 650.8499000001393, "ollama_total_duration_ms": 618.8416, "ollama_load_ms": 283.1918, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 279.6158, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 321.870223356477}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549063.6861463, "prompt_tokens": 26, "prefill_ms": 4.8218, "prefill_cuda_event_ms": null, "kv_decode_ms": 285.6245, "kv_decode_ms_equiv": 285.6245, "kv_decode_ms_per_token": 4.4628828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.7588999999716, "ollama_total_duration_ms": 615.9087, "ollama_load_ms": 278.8561, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.8218, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5392.177195238293}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549063.6861463, "prompt_tokens": 26, "prefill_ms": 4.8218, "prefill_cuda_event_ms": null, "kv_decode_ms": 285.6245, "kv_decode_ms_equiv": 285.6245, "kv_decode_ms_per_token": 4.4628828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.7588999999716, "ollama_total_duration_ms": 615.9087, "ollama_load_ms": 278.8561, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 285.6245, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 224.0704141276396}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549063.6861463, "prompt_tokens": 26, "prefill_ms": 4.8218, "prefill_cuda_event_ms": null, "kv_decode_ms": 285.6245, "kv_decode_ms_equiv": 285.6245, "kv_decode_ms_per_token": 4.4628828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 662.7588999999716, "ollama_total_duration_ms": 615.9087, "ollama_load_ms": 278.8561, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 290.4463, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 309.8679514939595}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549064.3490832, "prompt_tokens": 17, "prefill_ms": 7.6055, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.3704, "kv_decode_ms_equiv": 137.28934054054054, "kv_decode_ms_per_token": 2.145145945945946, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 448.0468000001565, "ollama_total_duration_ms": 416.1573, "ollama_load_ms": 284.3321, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 7.6055, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2235.2245085793174}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549064.3490832, "prompt_tokens": 17, "prefill_ms": 7.6055, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.3704, "kv_decode_ms_equiv": 137.28934054054054, "kv_decode_ms_per_token": 2.145145945945946, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 448.0468000001565, "ollama_total_duration_ms": 416.1573, "ollama_load_ms": 284.3321, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 137.28934054054054, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 466.1687480471309}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549064.3490832, "prompt_tokens": 17, "prefill_ms": 7.6055, "prefill_cuda_event_ms": null, "kv_decode_ms": 79.3704, "kv_decode_ms_equiv": 137.28934054054054, "kv_decode_ms_per_token": 2.145145945945946, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 448.0468000001565, "ollama_total_duration_ms": 416.1573, "ollama_load_ms": 284.3321, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 144.89484054054054, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 559.0261164429576}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549064.7972853, "prompt_tokens": 17, "prefill_ms": 2.9543, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.9532, "kv_decode_ms_equiv": 146.60426666666666, "kv_decode_ms_per_token": 2.2906916666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.92340000025433, "ollama_total_duration_ms": 443.6707, "ollama_load_ms": 292.2356, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9543, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5754.3242053955255}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549064.7972853, "prompt_tokens": 17, "prefill_ms": 2.9543, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.9532, "kv_decode_ms_equiv": 146.60426666666666, "kv_decode_ms_per_token": 2.2906916666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.92340000025433, "ollama_total_duration_ms": 443.6707, "ollama_load_ms": 292.2356, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 146.60426666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 436.549368276685}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549064.7972853, "prompt_tokens": 17, "prefill_ms": 2.9543, "prefill_cuda_event_ms": null, "kv_decode_ms": 109.9532, "kv_decode_ms_equiv": 146.60426666666666, "kv_decode_ms_per_token": 2.2906916666666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.92340000025433, "ollama_total_duration_ms": 443.6707, "ollama_load_ms": 292.2356, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 149.55856666666665, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 541.5938505250007}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549065.2553058, "prompt_tokens": 17, "prefill_ms": 2.6068, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4881, "kv_decode_ms_equiv": 143.31746666666666, "kv_decode_ms_per_token": 2.2393354166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.69149999978254, "ollama_total_duration_ms": 425.0621, "ollama_load_ms": 278.038, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6068, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6521.405554703085}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549065.2553058, "prompt_tokens": 17, "prefill_ms": 2.6068, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4881, "kv_decode_ms_equiv": 143.31746666666666, "kv_decode_ms_per_token": 2.2393354166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.69149999978254, "ollama_total_duration_ms": 425.0621, "ollama_load_ms": 278.038, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 143.31746666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 446.561061177935}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549065.2553058, "prompt_tokens": 17, "prefill_ms": 2.6068, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4881, "kv_decode_ms_equiv": 143.31746666666666, "kv_decode_ms_per_token": 2.2393354166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 457.69149999978254, "ollama_total_duration_ms": 425.0621, "ollama_load_ms": 278.038, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 145.92426666666665, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 555.0824537294232}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549065.713155, "prompt_tokens": 17, "prefill_ms": 3.0414, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3217, "kv_decode_ms_equiv": 143.09560000000002, "kv_decode_ms_per_token": 2.2358687500000003, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 459.02989999922283, "ollama_total_duration_ms": 434.2504, "ollama_load_ms": 284.919, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.0414, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5589.531136976392}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549065.713155, "prompt_tokens": 17, "prefill_ms": 3.0414, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3217, "kv_decode_ms_equiv": 143.09560000000002, "kv_decode_ms_per_token": 2.2358687500000003, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 459.02989999922283, "ollama_total_duration_ms": 434.2504, "ollama_load_ms": 284.919, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 143.09560000000002, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 447.2534445503565}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549065.713155, "prompt_tokens": 17, "prefill_ms": 3.0414, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.3217, "kv_decode_ms_equiv": 143.09560000000002, "kv_decode_ms_per_token": 2.2358687500000003, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 459.02989999922283, "ollama_total_duration_ms": 434.2504, "ollama_load_ms": 284.919, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 146.13700000000003, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 554.2744137350568}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549066.1723034, "prompt_tokens": 17, "prefill_ms": 2.6837, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.6342, "kv_decode_ms_equiv": 151.51226666666668, "kv_decode_ms_per_token": 2.367379166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 437.1545000003607, "ollama_total_duration_ms": 422.0874, "ollama_load_ms": 268.9134, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6837, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6334.538137645788}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549066.1723034, "prompt_tokens": 17, "prefill_ms": 2.6837, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.6342, "kv_decode_ms_equiv": 151.51226666666668, "kv_decode_ms_per_token": 2.367379166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 437.1545000003607, "ollama_total_duration_ms": 422.0874, "ollama_load_ms": 268.9134, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 151.51226666666668, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 422.408042649132}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549066.1723034, "prompt_tokens": 17, "prefill_ms": 2.6837, "prefill_cuda_event_ms": null, "kv_decode_ms": 113.6342, "kv_decode_ms_equiv": 151.51226666666668, "kv_decode_ms_per_token": 2.367379166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 437.1545000003607, "ollama_total_duration_ms": 422.0874, "ollama_load_ms": 268.9134, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 154.19596666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 525.3055689524089}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549066.6095893, "prompt_tokens": 17, "prefill_ms": 2.2903, "prefill_cuda_event_ms": null, "kv_decode_ms": 111.7723, "kv_decode_ms_equiv": 149.02973333333333, "kv_decode_ms_per_token": 2.328589583333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 467.4462000002677, "ollama_total_duration_ms": 453.9217, "ollama_load_ms": 288.9704, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2903, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7422.608391913724}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549066.6095893, "prompt_tokens": 17, "prefill_ms": 2.2903, "prefill_cuda_event_ms": null, "kv_decode_ms": 111.7723, "kv_decode_ms_equiv": 149.02973333333333, "kv_decode_ms_per_token": 2.328589583333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 467.4462000002677, "ollama_total_duration_ms": 453.9217, "ollama_load_ms": 288.9704, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 149.02973333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 429.4445045865568}
{"task_idx": 4, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549066.6095893, "prompt_tokens": 17, "prefill_ms": 2.2903, "prefill_cuda_event_ms": null, "kv_decode_ms": 111.7723, "kv_decode_ms_equiv": 149.02973333333333, "kv_decode_ms_per_token": 2.328589583333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 467.4462000002677, "ollama_total_duration_ms": 453.9217, "ollama_load_ms": 288.9704, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 151.32003333333333, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 535.2893348996971}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549067.0772438, "prompt_tokens": 25, "prefill_ms": 21.2396, "prefill_cuda_event_ms": null, "kv_decode_ms": 559.1407, "kv_decode_ms_equiv": 559.1407, "kv_decode_ms_per_token": 8.7365734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1194.8380000003453, "ollama_total_duration_ms": 1191.2477, "ollama_load_ms": 565.5097, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 21.2396, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1177.0466487127817}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549067.0772438, "prompt_tokens": 25, "prefill_ms": 21.2396, "prefill_cuda_event_ms": null, "kv_decode_ms": 559.1407, "kv_decode_ms_equiv": 559.1407, "kv_decode_ms_per_token": 8.7365734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1194.8380000003453, "ollama_total_duration_ms": 1191.2477, "ollama_load_ms": 565.5097, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 559.1407, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 114.46135114113495}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549067.0772438, "prompt_tokens": 25, "prefill_ms": 21.2396, "prefill_cuda_event_ms": null, "kv_decode_ms": 559.1407, "kv_decode_ms_equiv": 559.1407, "kv_decode_ms_per_token": 8.7365734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1194.8380000003453, "ollama_total_duration_ms": 1191.2477, "ollama_load_ms": 565.5097, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 580.3803, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 153.34772734360556}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549068.2721782, "prompt_tokens": 25, "prefill_ms": 9.1658, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.7061, "kv_decode_ms_equiv": 547.7061, "kv_decode_ms_per_token": 8.5579078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1190.978300000097, "ollama_total_duration_ms": 1163.0731, "ollama_load_ms": 568.3833, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.1658, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2727.5306028933646}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549068.2721782, "prompt_tokens": 25, "prefill_ms": 9.1658, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.7061, "kv_decode_ms_equiv": 547.7061, "kv_decode_ms_per_token": 8.5579078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1190.978300000097, "ollama_total_duration_ms": 1163.0731, "ollama_load_ms": 568.3833, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 547.7061, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 116.85098997436765}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549068.2721782, "prompt_tokens": 25, "prefill_ms": 9.1658, "prefill_cuda_event_ms": null, "kv_decode_ms": 547.7061, "kv_decode_ms_equiv": 547.7061, "kv_decode_ms_per_token": 8.5579078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1190.978300000097, "ollama_total_duration_ms": 1163.0731, "ollama_load_ms": 568.3833, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 556.8719, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 159.82131617702385}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549069.463313, "prompt_tokens": 25, "prefill_ms": 9.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.9545, "kv_decode_ms_equiv": 542.9545, "kv_decode_ms_per_token": 8.4836640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3726000004535, "ollama_total_duration_ms": 938.6335, "ollama_load_ms": 344.5722, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.3721, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2667.491810800141}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549069.463313, "prompt_tokens": 25, "prefill_ms": 9.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.9545, "kv_decode_ms_equiv": 542.9545, "kv_decode_ms_per_token": 8.4836640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3726000004535, "ollama_total_duration_ms": 938.6335, "ollama_load_ms": 344.5722, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 542.9545, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 117.8735971430387}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549069.463313, "prompt_tokens": 25, "prefill_ms": 9.3721, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.9545, "kv_decode_ms_equiv": 542.9545, "kv_decode_ms_per_token": 8.4836640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3726000004535, "ollama_total_duration_ms": 938.6335, "ollama_load_ms": 344.5722, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 552.3266000000001, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 161.13654493555077}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549070.4059174, "prompt_tokens": 25, "prefill_ms": 9.0806, "prefill_cuda_event_ms": null, "kv_decode_ms": 537.9306, "kv_decode_ms_equiv": 537.9306, "kv_decode_ms_per_token": 8.405165625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3785999997563, "ollama_total_duration_ms": 928.4195, "ollama_load_ms": 327.8305, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.0806, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2753.1220403938064}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549070.4059174, "prompt_tokens": 25, "prefill_ms": 9.0806, "prefill_cuda_event_ms": null, "kv_decode_ms": 537.9306, "kv_decode_ms_equiv": 537.9306, "kv_decode_ms_per_token": 8.405165625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3785999997563, "ollama_total_duration_ms": 928.4195, "ollama_load_ms": 327.8305, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 537.9306, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 118.97445506911114}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549070.4059174, "prompt_tokens": 25, "prefill_ms": 9.0806, "prefill_cuda_event_ms": null, "kv_decode_ms": 537.9306, "kv_decode_ms_equiv": 537.9306, "kv_decode_ms_per_token": 8.405165625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 942.3785999997563, "ollama_total_duration_ms": 928.4195, "ollama_load_ms": 327.8305, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 547.0112, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 162.70233589367092}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549071.3484323, "prompt_tokens": 25, "prefill_ms": 9.0093, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.0109, "kv_decode_ms_equiv": 534.0109, "kv_decode_ms_per_token": 8.3439203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 970.1857999998538, "ollama_total_duration_ms": 966.4845, "ollama_load_ms": 377.2686, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.0093, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2774.9103703950364}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549071.3484323, "prompt_tokens": 25, "prefill_ms": 9.0093, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.0109, "kv_decode_ms_equiv": 534.0109, "kv_decode_ms_per_token": 8.3439203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 970.1857999998538, "ollama_total_duration_ms": 966.4845, "ollama_load_ms": 377.2686, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 534.0109, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 119.84774093562511}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549071.3484323, "prompt_tokens": 25, "prefill_ms": 9.0093, "prefill_cuda_event_ms": null, "kv_decode_ms": 534.0109, "kv_decode_ms_equiv": 534.0109, "kv_decode_ms_per_token": 8.3439203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 970.1857999998538, "ollama_total_duration_ms": 966.4845, "ollama_load_ms": 377.2686, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 543.0202, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 163.8981385959491}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549072.3189576, "prompt_tokens": 25, "prefill_ms": 9.4275, "prefill_cuda_event_ms": null, "kv_decode_ms": 528.0323, "kv_decode_ms_equiv": 528.0323, "kv_decode_ms_per_token": 8.2505046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 894.4541000000754, "ollama_total_duration_ms": 891.3437, "ollama_load_ms": 311.5903, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 9.4275, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2651.8164942985945}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549072.3189576, "prompt_tokens": 25, "prefill_ms": 9.4275, "prefill_cuda_event_ms": null, "kv_decode_ms": 528.0323, "kv_decode_ms_equiv": 528.0323, "kv_decode_ms_per_token": 8.2505046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 894.4541000000754, "ollama_total_duration_ms": 891.3437, "ollama_load_ms": 311.5903, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 528.0323, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 121.20470660601633}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:latest", "model_kind": "ollama", "params_millions": 4300.0, "params_millions_config": 4300.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549072.3189576, "prompt_tokens": 25, "prefill_ms": 9.4275, "prefill_cuda_event_ms": null, "kv_decode_ms": 528.0323, "kv_decode_ms_equiv": 528.0323, "kv_decode_ms_per_token": 8.2505046875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 894.4541000000754, "ollama_total_duration_ms": 891.3437, "ollama_load_ms": 311.5903, "ollama_done_reason": "length", "params_millions_measured": 4300.0, "latency_ms": 537.4598, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 165.59378022319066}
