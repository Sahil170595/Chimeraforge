{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549181.640857, "prompt_tokens": 19, "prefill_ms": 19.4729, "prefill_cuda_event_ms": null, "kv_decode_ms": 657.7689, "kv_decode_ms_equiv": 725.8139586206897, "kv_decode_ms_per_token": 11.340843103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 6245.928500000446, "ollama_total_duration_ms": 6230.3272, "ollama_load_ms": 5500.2763, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 19.4729, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 975.7149679811431}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549181.640857, "prompt_tokens": 19, "prefill_ms": 19.4729, "prefill_cuda_event_ms": null, "kv_decode_ms": 657.7689, "kv_decode_ms_equiv": 725.8139586206897, "kv_decode_ms_per_token": 11.340843103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6245.928500000446, "ollama_total_duration_ms": 6230.3272, "ollama_load_ms": 5500.2763, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 725.8139586206897, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.17686576546868}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549181.640857, "prompt_tokens": 19, "prefill_ms": 19.4729, "prefill_cuda_event_ms": null, "kv_decode_ms": 657.7689, "kv_decode_ms_equiv": 725.8139586206897, "kv_decode_ms_per_token": 11.340843103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6245.928500000446, "ollama_total_duration_ms": 6230.3272, "ollama_load_ms": 5500.2763, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 745.2868586206897, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 111.36651483914393}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549187.8869264, "prompt_tokens": 19, "prefill_ms": 11.7867, "prefill_cuda_event_ms": null, "kv_decode_ms": 648.2933, "kv_decode_ms_equiv": 727.9082666666667, "kv_decode_ms_per_token": 11.373566666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 859.1462999993382, "ollama_total_duration_ms": 844.261, "ollama_load_ms": 143.8104, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.7867, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1611.9863914412008}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549187.8869264, "prompt_tokens": 19, "prefill_ms": 11.7867, "prefill_cuda_event_ms": null, "kv_decode_ms": 648.2933, "kv_decode_ms_equiv": 727.9082666666667, "kv_decode_ms_per_token": 11.373566666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 859.1462999993382, "ollama_total_duration_ms": 844.261, "ollama_load_ms": 143.8104, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 727.9082666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.92316687523996}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549187.8869264, "prompt_tokens": 19, "prefill_ms": 11.7867, "prefill_cuda_event_ms": null, "kv_decode_ms": 648.2933, "kv_decode_ms_equiv": 727.9082666666667, "kv_decode_ms_per_token": 11.373566666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 859.1462999993382, "ollama_total_duration_ms": 844.261, "ollama_load_ms": 143.8104, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 739.6949666666667, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 112.20841527964973}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549188.7462034, "prompt_tokens": 19, "prefill_ms": 12.0927, "prefill_cuda_event_ms": null, "kv_decode_ms": 647.3482, "kv_decode_ms_equiv": 726.847101754386, "kv_decode_ms_per_token": 11.356985964912282, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 867.9105000001073, "ollama_total_duration_ms": 847.1316, "ollama_load_ms": 148.1128, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 12.0927, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1571.1958454274065}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549188.7462034, "prompt_tokens": 19, "prefill_ms": 12.0927, "prefill_cuda_event_ms": null, "kv_decode_ms": 647.3482, "kv_decode_ms_equiv": 726.847101754386, "kv_decode_ms_per_token": 11.356985964912282, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 867.9105000001073, "ollama_total_duration_ms": 847.1316, "ollama_load_ms": 148.1128, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 726.847101754386, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.05153084537811}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549188.7462034, "prompt_tokens": 19, "prefill_ms": 12.0927, "prefill_cuda_event_ms": null, "kv_decode_ms": 647.3482, "kv_decode_ms_equiv": 726.847101754386, "kv_decode_ms_per_token": 11.356985964912282, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 867.9105000001073, "ollama_total_duration_ms": 847.1316, "ollama_load_ms": 148.1128, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 738.939801754386, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 112.32308748688587}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549189.6158977, "prompt_tokens": 19, "prefill_ms": 11.7376, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.2389, "kv_decode_ms_equiv": 719.9875368421052, "kv_decode_ms_per_token": 11.249805263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 851.3029999994615, "ollama_total_duration_ms": 835.9083, "ollama_load_ms": 136.5304, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.7376, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1618.729552889858}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549189.6158977, "prompt_tokens": 19, "prefill_ms": 11.7376, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.2389, "kv_decode_ms_equiv": 719.9875368421052, "kv_decode_ms_per_token": 11.249805263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 851.3029999994615, "ollama_total_duration_ms": 835.9083, "ollama_load_ms": 136.5304, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 719.9875368421052, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.89042757699198}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549189.6158977, "prompt_tokens": 19, "prefill_ms": 11.7376, "prefill_cuda_event_ms": null, "kv_decode_ms": 641.2389, "kv_decode_ms_equiv": 719.9875368421052, "kv_decode_ms_per_token": 11.249805263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 851.3029999994615, "ollama_total_duration_ms": 835.9083, "ollama_load_ms": 136.5304, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 731.7251368421053, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.43057088103028}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549190.4673257, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 312.8159999996569, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.1146000001318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 12764.581700000235, "params_millions_measured": 45.1712, "latency_ms": 312.8159999996569, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 28.77090685901575}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549190.4673257, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 312.8159999996569, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.1146000001318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 12764.581700000235, "params_millions_measured": 45.1712, "latency_ms": 1137.1146000001318, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.2828056204648}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549190.4673257, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 312.8159999996569, "prefill_cuda_event_ms": null, "kv_decode_ms": 1137.1146000001318, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 12764.581700000235, "params_millions_measured": 45.1712, "latency_ms": 1449.9305999997887, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 50.34723730915855}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549204.7002115, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 20.518799999990733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.403300000391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.518799999990733, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 438.62214164590836}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549204.7002115, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.518799999990733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.403300000391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1013.403300000391, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.15353423456911}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549204.7002115, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 20.518799999990733, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.403300000391, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1033.9221000003818, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 70.60493242186529}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549205.734616, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 18.244899999444897, "prefill_cuda_event_ms": null, "kv_decode_ms": 860.1080999997066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 18.244899999444897, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 493.28853544134665}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549205.734616, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.244899999444897, "prefill_cuda_event_ms": null, "kv_decode_ms": 860.1080999997066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 860.1080999997066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.40925158130918}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549205.734616, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 18.244899999444897, "prefill_cuda_event_ms": null, "kv_decode_ms": 860.1080999997066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 878.3529999991515, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.11009355016778}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549206.6135368, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.507199999381555, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.5458999998518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 22.507199999381555, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 399.8720409578845}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549206.6135368, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.507199999381555, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.5458999998518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 935.5458999998518, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.40925709792553}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549206.6135368, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.507199999381555, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.5458999998518, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 958.0530999992334, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 76.19619413585573}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549207.571938, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 1070.8737000004476, "prefill_cuda_event_ms": 903.8058471679688, "kv_decode_ms": 331.4516000000367, "kv_decode_cuda_event_ms": 331.3283386230469, "gpu_peak_mb": 13.068359375, "hf_load_ms": 319.4110999993427, "params_millions_measured": 0.102714, "latency_ms": 1070.8737000004476, "cuda_event_ms": 903.8058471679688, "tokens_total": 17, "tokens_per_s": 15.874887953633463}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549207.571938, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1070.8737000004476, "prefill_cuda_event_ms": 903.8058471679688, "kv_decode_ms": 331.4516000000367, "kv_decode_cuda_event_ms": 331.3283386230469, "gpu_peak_mb": 13.068359375, "hf_load_ms": 319.4110999993427, "params_millions_measured": 0.102714, "latency_ms": 331.4516000000367, "cuda_event_ms": 331.3283386230469, "tokens_total": 64, "tokens_per_s": 193.09003184776574}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549207.571938, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1070.8737000004476, "prefill_cuda_event_ms": 903.8058471679688, "kv_decode_ms": 331.4516000000367, "kv_decode_cuda_event_ms": 331.3283386230469, "gpu_peak_mb": 13.068359375, "hf_load_ms": 319.4110999993427, "params_millions_measured": 0.102714, "latency_ms": 1402.3253000004843, "cuda_event_ms": 1235.1341857910156, "tokens_total": 81, "tokens_per_s": 57.76120562038781}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549209.2972772, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.25230000018928, "prefill_cuda_event_ms": 4.188223838806152, "kv_decode_ms": 110.82230000010895, "kv_decode_cuda_event_ms": 110.76496124267578, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 4.25230000018928, "cuda_event_ms": 4.188223838806152, "tokens_total": 17, "tokens_per_s": 3997.8364647939447}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549209.2972772, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.25230000018928, "prefill_cuda_event_ms": 4.188223838806152, "kv_decode_ms": 110.82230000010895, "kv_decode_cuda_event_ms": 110.76496124267578, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 110.82230000010895, "cuda_event_ms": 110.76496124267578, "tokens_total": 64, "tokens_per_s": 577.5010986050378}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549209.2972772, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.25230000018928, "prefill_cuda_event_ms": 4.188223838806152, "kv_decode_ms": 110.82230000010895, "kv_decode_cuda_event_ms": 110.76496124267578, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 115.07460000029823, "cuda_event_ms": 114.95318508148193, "tokens_total": 81, "tokens_per_s": 703.8912149144127}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549209.4130268, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.001599999857717, "prefill_cuda_event_ms": 1.955839991569519, "kv_decode_ms": 110.68249999971158, "kv_decode_cuda_event_ms": 110.64006042480469, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 2.001599999857717, "cuda_event_ms": 1.955839991569519, "tokens_total": 17, "tokens_per_s": 8493.205436255213}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549209.4130268, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.001599999857717, "prefill_cuda_event_ms": 1.955839991569519, "kv_decode_ms": 110.68249999971158, "kv_decode_cuda_event_ms": 110.64006042480469, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 110.68249999971158, "cuda_event_ms": 110.64006042480469, "tokens_total": 64, "tokens_per_s": 578.2305242487906}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549209.4130268, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.001599999857717, "prefill_cuda_event_ms": 1.955839991569519, "kv_decode_ms": 110.68249999971158, "kv_decode_cuda_event_ms": 110.64006042480469, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 112.6840999995693, "cuda_event_ms": 112.5959004163742, "tokens_total": 81, "tokens_per_s": 718.8236849769364}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549209.5262232, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.7244000002610846, "prefill_cuda_event_ms": 2.65830397605896, "kv_decode_ms": 118.2771000003413, "kv_decode_cuda_event_ms": 118.23004913330078, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 2.7244000002610846, "cuda_event_ms": 2.65830397605896, "tokens_total": 17, "tokens_per_s": 6239.906033758206}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549209.5262232, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7244000002610846, "prefill_cuda_event_ms": 2.65830397605896, "kv_decode_ms": 118.2771000003413, "kv_decode_cuda_event_ms": 118.23004913330078, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 118.2771000003413, "cuda_event_ms": 118.23004913330078, "tokens_total": 64, "tokens_per_s": 541.1022082872789}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549209.5262232, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.7244000002610846, "prefill_cuda_event_ms": 2.65830397605896, "kv_decode_ms": 118.2771000003413, "kv_decode_cuda_event_ms": 118.23004913330078, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 121.00150000060239, "cuda_event_ms": 120.88835310935974, "tokens_total": 81, "tokens_per_s": 669.4131890893646}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549209.6479924, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 29.606600000079197, "prefill_cuda_event_ms": 29.52889633178711, "kv_decode_ms": 543.4358000002248, "kv_decode_cuda_event_ms": 543.3875732421875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 541.504800000439, "params_millions_measured": 96.08832, "latency_ms": 29.606600000079197, "cuda_event_ms": 29.52889633178711, "tokens_total": 1, "tokens_per_s": 33.776252592237036}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549209.6479924, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.606600000079197, "prefill_cuda_event_ms": 29.52889633178711, "kv_decode_ms": 543.4358000002248, "kv_decode_cuda_event_ms": 543.3875732421875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 541.504800000439, "params_millions_measured": 96.08832, "latency_ms": 543.4358000002248, "cuda_event_ms": 543.3875732421875, "tokens_total": 64, "tokens_per_s": 117.76920107209264}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549209.6479924, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 29.606600000079197, "prefill_cuda_event_ms": 29.52889633178711, "kv_decode_ms": 543.4358000002248, "kv_decode_cuda_event_ms": 543.3875732421875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 541.504800000439, "params_millions_measured": 96.08832, "latency_ms": 573.042400000304, "cuda_event_ms": 572.9164695739746, "tokens_total": 65, "tokens_per_s": 113.42965197682669}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549210.7636995, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.49070000044594, "prefill_cuda_event_ms": 10.431488037109375, "kv_decode_ms": 393.7452000000121, "kv_decode_cuda_event_ms": 393.67852783203125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 10.49070000044594, "cuda_event_ms": 10.431488037109375, "tokens_total": 1, "tokens_per_s": 95.32252375508706}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549210.7636995, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.49070000044594, "prefill_cuda_event_ms": 10.431488037109375, "kv_decode_ms": 393.7452000000121, "kv_decode_cuda_event_ms": 393.67852783203125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 393.7452000000121, "cuda_event_ms": 393.67852783203125, "tokens_total": 64, "tokens_per_s": 162.54166399996248}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549210.7636995, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.49070000044594, "prefill_cuda_event_ms": 10.431488037109375, "kv_decode_ms": 393.7452000000121, "kv_decode_cuda_event_ms": 393.67852783203125, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 404.23590000045806, "cuda_event_ms": 404.1100158691406, "tokens_total": 65, "tokens_per_s": 160.79719787363356}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549211.1687982, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.17589999982738, "prefill_cuda_event_ms": 14.1015043258667, "kv_decode_ms": 618.061399999533, "kv_decode_cuda_event_ms": 618.00244140625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 14.17589999982738, "cuda_event_ms": 14.1015043258667, "tokens_total": 1, "tokens_per_s": 70.5422583407175}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549211.1687982, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.17589999982738, "prefill_cuda_event_ms": 14.1015043258667, "kv_decode_ms": 618.061399999533, "kv_decode_cuda_event_ms": 618.00244140625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 618.061399999533, "cuda_event_ms": 618.00244140625, "tokens_total": 64, "tokens_per_s": 103.5495826143622}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549211.1687982, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.17589999982738, "prefill_cuda_event_ms": 14.1015043258667, "kv_decode_ms": 618.061399999533, "kv_decode_cuda_event_ms": 618.00244140625, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 632.2372999993604, "cuda_event_ms": 632.1039457321167, "tokens_total": 65, "tokens_per_s": 102.8094989018613}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549211.8018281, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.478099999796541, "prefill_cuda_event_ms": 9.400320053100586, "kv_decode_ms": 610.5770000003758, "kv_decode_cuda_event_ms": 610.5057373046875, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 9.478099999796541, "cuda_event_ms": 9.400320053100586, "tokens_total": 1, "tokens_per_s": 105.50637786280649}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549211.8018281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.478099999796541, "prefill_cuda_event_ms": 9.400320053100586, "kv_decode_ms": 610.5770000003758, "kv_decode_cuda_event_ms": 610.5057373046875, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 610.5770000003758, "cuda_event_ms": 610.5057373046875, "tokens_total": 64, "tokens_per_s": 104.81888443220201}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549211.8018281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.478099999796541, "prefill_cuda_event_ms": 9.400320053100586, "kv_decode_ms": 610.5770000003758, "kv_decode_cuda_event_ms": 610.5057373046875, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 620.0551000001724, "cuda_event_ms": 619.9060573577881, "tokens_total": 65, "tokens_per_s": 104.82939338775203}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549212.422698, "prompt_tokens": 30, "prefill_ms": 66.8825, "prefill_cuda_event_ms": null, "kv_decode_ms": 329.9231, "kv_decode_ms_equiv": 728.1061517241379, "kv_decode_ms_per_token": 11.376658620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 5835.542900000291, "ollama_total_duration_ms": 5808.5893, "ollama_load_ms": 5388.9049, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 66.8825, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 448.5478264119912}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549212.422698, "prompt_tokens": 30, "prefill_ms": 66.8825, "prefill_cuda_event_ms": null, "kv_decode_ms": 329.9231, "kv_decode_ms_equiv": 728.1061517241379, "kv_decode_ms_per_token": 11.376658620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5835.542900000291, "ollama_total_duration_ms": 5808.5893, "ollama_load_ms": 5388.9049, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 728.1061517241379, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.89927107256206}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549212.422698, "prompt_tokens": 30, "prefill_ms": 66.8825, "prefill_cuda_event_ms": null, "kv_decode_ms": 329.9231, "kv_decode_ms_equiv": 728.1061517241379, "kv_decode_ms_per_token": 11.376658620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5835.542900000291, "ollama_total_duration_ms": 5808.5893, "ollama_load_ms": 5388.9049, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 794.988651724138, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 118.24068154449343}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549218.258883, "prompt_tokens": 30, "prefill_ms": 11.621, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.6845, "kv_decode_ms_equiv": 709.9244137931034, "kv_decode_ms_per_token": 11.092568965517241, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.8708999999435, "ollama_total_duration_ms": 485.4968, "ollama_load_ms": 123.6705, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.621, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2581.5334308579295}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549218.258883, "prompt_tokens": 30, "prefill_ms": 11.621, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.6845, "kv_decode_ms_equiv": 709.9244137931034, "kv_decode_ms_per_token": 11.092568965517241, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.8708999999435, "ollama_total_duration_ms": 485.4968, "ollama_load_ms": 123.6705, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 709.9244137931034, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.15044243661103}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549218.258883, "prompt_tokens": 30, "prefill_ms": 11.621, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.6845, "kv_decode_ms_equiv": 709.9244137931034, "kv_decode_ms_per_token": 11.092568965517241, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 507.8708999999435, "ollama_total_duration_ms": 485.4968, "ollama_load_ms": 123.6705, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 721.5454137931034, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.27593025066838}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549218.766877, "prompt_tokens": 30, "prefill_ms": 12.2473, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.7118, "kv_decode_ms_equiv": 716.6053517241379, "kv_decode_ms_per_token": 11.196958620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 468.1275999992067, "ollama_total_duration_ms": 465.2833, "ollama_load_ms": 104.0262, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.2473, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2449.5194859275107}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549218.766877, "prompt_tokens": 30, "prefill_ms": 12.2473, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.7118, "kv_decode_ms_equiv": 716.6053517241379, "kv_decode_ms_per_token": 11.196958620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 468.1275999992067, "ollama_total_duration_ms": 465.2833, "ollama_load_ms": 104.0262, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 716.6053517241379, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.30996656111667}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549218.766877, "prompt_tokens": 30, "prefill_ms": 12.2473, "prefill_cuda_event_ms": null, "kv_decode_ms": 324.7118, "kv_decode_ms_equiv": 716.6053517241379, "kv_decode_ms_per_token": 11.196958620689655, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 468.1275999992067, "ollama_total_duration_ms": 465.2833, "ollama_load_ms": 104.0262, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 728.8526517241379, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 128.96982644933544}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549219.2351062, "prompt_tokens": 30, "prefill_ms": 11.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5188, "kv_decode_ms_equiv": 711.7656275862068, "kv_decode_ms_per_token": 11.121337931034482, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.24150000009104, "ollama_total_duration_ms": 470.8469, "ollama_load_ms": 111.5594, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.147, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2691.3070781376155}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549219.2351062, "prompt_tokens": 30, "prefill_ms": 11.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5188, "kv_decode_ms_equiv": 711.7656275862068, "kv_decode_ms_per_token": 11.121337931034482, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.24150000009104, "ollama_total_duration_ms": 470.8469, "ollama_load_ms": 111.5594, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 711.7656275862068, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.91723893304825}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549219.2351062, "prompt_tokens": 30, "prefill_ms": 11.147, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.5188, "kv_decode_ms_equiv": 711.7656275862068, "kv_decode_ms_per_token": 11.121337931034482, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.24150000009104, "ollama_total_duration_ms": 470.8469, "ollama_load_ms": 111.5594, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 722.9126275862069, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.02954494496024}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549219.727589, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 30.26300000055926, "prefill_cuda_event_ms": 30.180288314819336, "kv_decode_ms": 445.06120000005467, "kv_decode_cuda_event_ms": 445.0067138671875, "gpu_peak_mb": 229.62109375, "hf_load_ms": 280.0201999998535, "params_millions_measured": 5.03672, "latency_ms": 30.26300000055926, "cuda_event_ms": 30.180288314819336, "tokens_total": 9, "tokens_per_s": 297.39285595723095}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549219.727589, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.26300000055926, "prefill_cuda_event_ms": 30.180288314819336, "kv_decode_ms": 445.06120000005467, "kv_decode_cuda_event_ms": 445.0067138671875, "gpu_peak_mb": 229.62109375, "hf_load_ms": 280.0201999998535, "params_millions_measured": 5.03672, "latency_ms": 445.06120000005467, "cuda_event_ms": 445.0067138671875, "tokens_total": 64, "tokens_per_s": 143.8004481181288}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549219.727589, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 30.26300000055926, "prefill_cuda_event_ms": 30.180288314819336, "kv_decode_ms": 445.06120000005467, "kv_decode_cuda_event_ms": 445.0067138671875, "gpu_peak_mb": 229.62109375, "hf_load_ms": 280.0201999998535, "params_millions_measured": 5.03672, "latency_ms": 475.3242000006139, "cuda_event_ms": 475.18700218200684, "tokens_total": 73, "tokens_per_s": 153.5793885518678}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549220.4837575, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.535599999551778, "prefill_cuda_event_ms": 6.47049617767334, "kv_decode_ms": 449.7152999992977, "kv_decode_cuda_event_ms": 449.674560546875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.535599999551778, "cuda_event_ms": 6.47049617767334, "tokens_total": 9, "tokens_per_s": 1377.0732603918898}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549220.4837575, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.535599999551778, "prefill_cuda_event_ms": 6.47049617767334, "kv_decode_ms": 449.7152999992977, "kv_decode_cuda_event_ms": 449.674560546875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 449.7152999992977, "cuda_event_ms": 449.674560546875, "tokens_total": 64, "tokens_per_s": 142.31225844462028}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549220.4837575, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.535599999551778, "prefill_cuda_event_ms": 6.47049617767334, "kv_decode_ms": 449.7152999992977, "kv_decode_cuda_event_ms": 449.674560546875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 456.2508999988495, "cuda_event_ms": 456.14505672454834, "tokens_total": 73, "tokens_per_s": 159.99968438458768}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549220.9404953, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.484999999884167, "prefill_cuda_event_ms": 6.409215927124023, "kv_decode_ms": 372.46070000037435, "kv_decode_cuda_event_ms": 372.4185485839844, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.484999999884167, "cuda_event_ms": 6.409215927124023, "tokens_total": 9, "tokens_per_s": 1387.81804165933}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549220.9404953, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.484999999884167, "prefill_cuda_event_ms": 6.409215927124023, "kv_decode_ms": 372.46070000037435, "kv_decode_cuda_event_ms": 372.4185485839844, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 372.46070000037435, "cuda_event_ms": 372.4185485839844, "tokens_total": 64, "tokens_per_s": 171.83020920042216}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549220.9404953, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.484999999884167, "prefill_cuda_event_ms": 6.409215927124023, "kv_decode_ms": 372.46070000037435, "kv_decode_cuda_event_ms": 372.4185485839844, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 378.9457000002585, "cuda_event_ms": 378.8277645111084, "tokens_total": 73, "tokens_per_s": 192.63973703871082}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549221.3200135, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.6389000003255205, "prefill_cuda_event_ms": 6.578207969665527, "kv_decode_ms": 373.50009999954636, "kv_decode_cuda_event_ms": 373.465087890625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.6389000003255205, "cuda_event_ms": 6.578207969665527, "tokens_total": 9, "tokens_per_s": 1355.6462666343384}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549221.3200135, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.6389000003255205, "prefill_cuda_event_ms": 6.578207969665527, "kv_decode_ms": 373.50009999954636, "kv_decode_cuda_event_ms": 373.465087890625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 373.50009999954636, "cuda_event_ms": 373.465087890625, "tokens_total": 64, "tokens_per_s": 171.35202908935696}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549221.3200135, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.6389000003255205, "prefill_cuda_event_ms": 6.578207969665527, "kv_decode_ms": 373.50009999954636, "kv_decode_cuda_event_ms": 373.465087890625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 380.1389999998719, "cuda_event_ms": 380.0432958602905, "tokens_total": 73, "tokens_per_s": 192.03501876951483}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549221.7007031, "prompt_tokens": 25, "prefill_ms": 10.3903, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.0781, "kv_decode_ms_equiv": 137.0781, "kv_decode_ms_per_token": 2.1418453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1581.265099999655, "ollama_total_duration_ms": 1522.7303, "ollama_load_ms": 1313.4135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 10.3903, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2406.090295756619}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549221.7007031, "prompt_tokens": 25, "prefill_ms": 10.3903, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.0781, "kv_decode_ms_equiv": 137.0781, "kv_decode_ms_per_token": 2.1418453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1581.265099999655, "ollama_total_duration_ms": 1522.7303, "ollama_load_ms": 1313.4135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.0781, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 466.88712493097}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549221.7007031, "prompt_tokens": 25, "prefill_ms": 10.3903, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.0781, "kv_decode_ms_equiv": 137.0781, "kv_decode_ms_per_token": 2.1418453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1581.265099999655, "ollama_total_duration_ms": 1522.7303, "ollama_load_ms": 1313.4135, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 147.4684, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 603.5191268095402}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549223.2821033, "prompt_tokens": 25, "prefill_ms": 2.7738, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.625, "kv_decode_ms_equiv": 137.625, "kv_decode_ms_per_token": 2.150390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 373.62729999949806, "ollama_total_duration_ms": 329.9177, "ollama_load_ms": 147.9578, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.7738, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9012.906482082342}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549223.2821033, "prompt_tokens": 25, "prefill_ms": 2.7738, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.625, "kv_decode_ms_equiv": 137.625, "kv_decode_ms_per_token": 2.150390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 373.62729999949806, "ollama_total_duration_ms": 329.9177, "ollama_load_ms": 147.9578, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 137.625, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 465.03178928247047}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549223.2821033, "prompt_tokens": 25, "prefill_ms": 2.7738, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.625, "kv_decode_ms_equiv": 137.625, "kv_decode_ms_per_token": 2.150390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 373.62729999949806, "ollama_total_duration_ms": 329.9177, "ollama_load_ms": 147.9578, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 140.3988, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 633.9085519249453}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549223.6559927, "prompt_tokens": 25, "prefill_ms": 2.5563, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.6479, "kv_decode_ms_equiv": 135.6479, "kv_decode_ms_per_token": 2.1194984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.48440000010305, "ollama_total_duration_ms": 316.2057, "ollama_load_ms": 142.8513, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5563, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9779.75980909909}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549223.6559927, "prompt_tokens": 25, "prefill_ms": 2.5563, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.6479, "kv_decode_ms_equiv": 135.6479, "kv_decode_ms_per_token": 2.1194984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.48440000010305, "ollama_total_duration_ms": 316.2057, "ollama_load_ms": 142.8513, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.6479, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 471.80973682600325}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549223.6559927, "prompt_tokens": 25, "prefill_ms": 2.5563, "prefill_cuda_event_ms": null, "kv_decode_ms": 135.6479, "kv_decode_ms_equiv": 135.6479, "kv_decode_ms_per_token": 2.1194984375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.48440000010305, "ollama_total_duration_ms": 316.2057, "ollama_load_ms": 142.8513, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 138.2042, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 643.9746404233736}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549223.998599, "prompt_tokens": 25, "prefill_ms": 2.5912, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.2765, "kv_decode_ms_equiv": 132.2765, "kv_decode_ms_per_token": 2.0668203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.53539999966597, "ollama_total_duration_ms": 317.7267, "ollama_load_ms": 142.4133, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5912, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9648.039518369867}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549223.998599, "prompt_tokens": 25, "prefill_ms": 2.5912, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.2765, "kv_decode_ms_equiv": 132.2765, "kv_decode_ms_per_token": 2.0668203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.53539999966597, "ollama_total_duration_ms": 317.7267, "ollama_load_ms": 142.4133, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 132.2765, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 483.8349971461295}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549223.998599, "prompt_tokens": 25, "prefill_ms": 2.5912, "prefill_cuda_event_ms": null, "kv_decode_ms": 132.2765, "kv_decode_ms_equiv": 132.2765, "kv_decode_ms_per_token": 2.0668203125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.53539999966597, "ollama_total_duration_ms": 317.7267, "ollama_load_ms": 142.4133, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 134.86769999999999, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 659.9059671070243}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549224.3302805, "prompt_tokens": 27, "prefill_ms": 21.3518, "prefill_cuda_event_ms": null, "kv_decode_ms": 737.8131, "kv_decode_ms_equiv": 737.8131, "kv_decode_ms_per_token": 11.5283296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 6299.156000000039, "ollama_total_duration_ms": 6274.7269, "ollama_load_ms": 5465.8409, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 21.3518, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 1264.5303908803942}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549224.3302805, "prompt_tokens": 27, "prefill_ms": 21.3518, "prefill_cuda_event_ms": null, "kv_decode_ms": 737.8131, "kv_decode_ms_equiv": 737.8131, "kv_decode_ms_per_token": 11.5283296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6299.156000000039, "ollama_total_duration_ms": 6274.7269, "ollama_load_ms": 5465.8409, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 737.8131, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 86.74283500794442}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549224.3302805, "prompt_tokens": 27, "prefill_ms": 21.3518, "prefill_cuda_event_ms": null, "kv_decode_ms": 737.8131, "kv_decode_ms_equiv": 737.8131, "kv_decode_ms_per_token": 11.5283296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6299.156000000039, "ollama_total_duration_ms": 6274.7269, "ollama_load_ms": 5465.8409, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 759.1649, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 119.86855556678134}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549230.6300666, "prompt_tokens": 27, "prefill_ms": 11.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.0092, "kv_decode_ms_equiv": 729.0092, "kv_decode_ms_per_token": 11.39076875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 972.3396999997931, "ollama_total_duration_ms": 953.2917, "ollama_load_ms": 159.0217, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.7695, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2294.065168443859}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549230.6300666, "prompt_tokens": 27, "prefill_ms": 11.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.0092, "kv_decode_ms_equiv": 729.0092, "kv_decode_ms_per_token": 11.39076875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 972.3396999997931, "ollama_total_duration_ms": 953.2917, "ollama_load_ms": 159.0217, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 729.0092, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.7903872818066}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549230.6300666, "prompt_tokens": 27, "prefill_ms": 11.7695, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.0092, "kv_decode_ms_equiv": 729.0092, "kv_decode_ms_per_token": 11.39076875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 972.3396999997931, "ollama_total_duration_ms": 953.2917, "ollama_load_ms": 159.0217, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 740.7787, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.84370487434371}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549231.6025426, "prompt_tokens": 27, "prefill_ms": 11.1831, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.2451, "kv_decode_ms_equiv": 728.2451, "kv_decode_ms_per_token": 11.3788296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 944.1834999997809, "ollama_total_duration_ms": 920.6866, "ollama_load_ms": 135.5779, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.1831, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2414.357378544411}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549231.6025426, "prompt_tokens": 27, "prefill_ms": 11.1831, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.2451, "kv_decode_ms_equiv": 728.2451, "kv_decode_ms_per_token": 11.3788296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 944.1834999997809, "ollama_total_duration_ms": 920.6866, "ollama_load_ms": 135.5779, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 728.2451, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.88249999897013}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549231.6025426, "prompt_tokens": 27, "prefill_ms": 11.1831, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.2451, "kv_decode_ms_equiv": 728.2451, "kv_decode_ms_per_token": 11.3788296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 944.1834999997809, "ollama_total_duration_ms": 920.6866, "ollama_load_ms": 135.5779, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 739.4282, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.06806800173432}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549232.5469275, "prompt_tokens": 27, "prefill_ms": 11.8585, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.1596, "kv_decode_ms_equiv": 727.1596, "kv_decode_ms_per_token": 11.36186875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.930699999939, "ollama_total_duration_ms": 925.5301, "ollama_load_ms": 136.4777, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.8585, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2276.8478306699835}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549232.5469275, "prompt_tokens": 27, "prefill_ms": 11.8585, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.1596, "kv_decode_ms_equiv": 727.1596, "kv_decode_ms_per_token": 11.36186875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.930699999939, "ollama_total_duration_ms": 925.5301, "ollama_load_ms": 136.4777, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 727.1596, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.01369052956188}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549232.5469275, "prompt_tokens": 27, "prefill_ms": 11.8585, "prefill_cuda_event_ms": null, "kv_decode_ms": 727.1596, "kv_decode_ms_equiv": 727.1596, "kv_decode_ms_per_token": 11.36186875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.930699999939, "ollama_total_duration_ms": 925.5301, "ollama_load_ms": 136.4777, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 739.0181, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.13636161279406}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549233.4939818, "prompt_tokens": 17, "prefill_ms": 7.1536, "prefill_cuda_event_ms": null, "kv_decode_ms": 76.9103, "kv_decode_ms_equiv": 133.03403243243244, "kv_decode_ms_per_token": 2.078656756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 275.3199999997378, "ollama_total_duration_ms": 266.8384, "ollama_load_ms": 154.719, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 7.1536, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2376.4258555133083}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549233.4939818, "prompt_tokens": 17, "prefill_ms": 7.1536, "prefill_cuda_event_ms": null, "kv_decode_ms": 76.9103, "kv_decode_ms_equiv": 133.03403243243244, "kv_decode_ms_per_token": 2.078656756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 275.3199999997378, "ollama_total_duration_ms": 266.8384, "ollama_load_ms": 154.719, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 133.03403243243244, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 481.0799073726145}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549233.4939818, "prompt_tokens": 17, "prefill_ms": 7.1536, "prefill_cuda_event_ms": null, "kv_decode_ms": 76.9103, "kv_decode_ms_equiv": 133.03403243243244, "kv_decode_ms_per_token": 2.078656756756757, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 275.3199999997378, "ollama_total_duration_ms": 266.8384, "ollama_load_ms": 154.719, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 140.18763243243245, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 577.7970466762846}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549233.7694519, "prompt_tokens": 17, "prefill_ms": 2.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.2655, "kv_decode_ms_equiv": 135.02066666666667, "kv_decode_ms_per_token": 2.1096979166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4833999996117, "ollama_total_duration_ms": 276.2655, "ollama_load_ms": 143.131, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4481, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6944.160777746006}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549233.7694519, "prompt_tokens": 17, "prefill_ms": 2.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.2655, "kv_decode_ms_equiv": 135.02066666666667, "kv_decode_ms_per_token": 2.1096979166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4833999996117, "ollama_total_duration_ms": 276.2655, "ollama_load_ms": 143.131, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.02066666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 474.0015108798159}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549233.7694519, "prompt_tokens": 17, "prefill_ms": 2.4481, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.2655, "kv_decode_ms_equiv": 135.02066666666667, "kv_decode_ms_per_token": 2.1096979166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 310.4833999996117, "ollama_total_duration_ms": 276.2655, "ollama_load_ms": 143.131, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 137.46876666666668, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 589.2247523861784}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549234.0801578, "prompt_tokens": 17, "prefill_ms": 2.8168, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.2873, "kv_decode_ms_equiv": 124.38306666666666, "kv_decode_ms_per_token": 1.9434854166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.37540000043373, "ollama_total_duration_ms": 264.534, "ollama_load_ms": 132.5616, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.8168, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6035.217267821641}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549234.0801578, "prompt_tokens": 17, "prefill_ms": 2.8168, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.2873, "kv_decode_ms_equiv": 124.38306666666666, "kv_decode_ms_per_token": 1.9434854166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.37540000043373, "ollama_total_duration_ms": 264.534, "ollama_load_ms": 132.5616, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 124.38306666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 514.5394925139864}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549234.0801578, "prompt_tokens": 17, "prefill_ms": 2.8168, "prefill_cuda_event_ms": null, "kv_decode_ms": 93.2873, "kv_decode_ms_equiv": 124.38306666666666, "kv_decode_ms_per_token": 1.9434854166666666, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.37540000043373, "ollama_total_duration_ms": 264.534, "ollama_load_ms": 132.5616, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 127.19986666666667, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 636.7931203282184}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549234.386699, "prompt_tokens": 17, "prefill_ms": 3.6097, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.3111, "kv_decode_ms_equiv": 131.08146666666667, "kv_decode_ms_per_token": 2.0481479166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 305.2388999994946, "ollama_total_duration_ms": 282.157, "ollama_load_ms": 146.7605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.6097, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4709.5326481425045}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549234.386699, "prompt_tokens": 17, "prefill_ms": 3.6097, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.3111, "kv_decode_ms_equiv": 131.08146666666667, "kv_decode_ms_per_token": 2.0481479166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 305.2388999994946, "ollama_total_duration_ms": 282.157, "ollama_load_ms": 146.7605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 131.08146666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 488.2459864654144}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549234.386699, "prompt_tokens": 17, "prefill_ms": 3.6097, "prefill_cuda_event_ms": null, "kv_decode_ms": 98.3111, "kv_decode_ms_equiv": 131.08146666666667, "kv_decode_ms_per_token": 2.0481479166666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 305.2388999994946, "ollama_total_duration_ms": 282.157, "ollama_load_ms": 146.7605, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 134.69116666666667, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 601.3757398097129}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549234.6921241, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 59.63109999993321, "prefill_cuda_event_ms": null, "kv_decode_ms": 1287.5389000000723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 250.55720000000292, "params_millions_measured": 5.03672, "latency_ms": 59.63109999993321, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 150.9279553791575}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549234.6921241, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 59.63109999993321, "prefill_cuda_event_ms": null, "kv_decode_ms": 1287.5389000000723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 250.55720000000292, "params_millions_measured": 5.03672, "latency_ms": 1287.5389000000723, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.70723602991444}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549234.6921241, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 59.63109999993321, "prefill_cuda_event_ms": null, "kv_decode_ms": 1287.5389000000723, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 250.55720000000292, "params_millions_measured": 5.03672, "latency_ms": 1347.1700000000055, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 54.187667480718616}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549236.2911744, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.101499999451335, "prefill_cuda_event_ms": null, "kv_decode_ms": 1256.516599999486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 34.101499999451335, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 263.91800947597034}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549236.2911744, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.101499999451335, "prefill_cuda_event_ms": null, "kv_decode_ms": 1256.516599999486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1256.516599999486, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.934464375581015}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549236.2911744, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.101499999451335, "prefill_cuda_event_ms": null, "kv_decode_ms": 1256.516599999486, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1290.6180999989374, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 56.56204573611675}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549237.5822847, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.1873999994059, "prefill_cuda_event_ms": null, "kv_decode_ms": 1194.162600000709, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 33.1873999994059, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 271.18725781956744}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549237.5822847, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.1873999994059, "prefill_cuda_event_ms": null, "kv_decode_ms": 1194.162600000709, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1194.162600000709, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 53.59404154841393}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549237.5822847, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.1873999994059, "prefill_cuda_event_ms": null, "kv_decode_ms": 1194.162600000709, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1227.350000000115, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 59.477736586950066}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549238.8101673, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 31.973999999536318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1127.2568999993382, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 31.973999999536318, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 281.47870144900594}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549238.8101673, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.973999999536318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1127.2568999993382, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1127.2568999993382, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.774990687604195}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549238.8101673, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.973999999536318, "prefill_cuda_event_ms": null, "kv_decode_ms": 1127.2568999993382, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1159.2308999988745, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 62.97278652602417}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549239.9698174, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.81069999954343, "prefill_cuda_event_ms": 14.243840217590332, "kv_decode_ms": 415.38279999986116, "kv_decode_cuda_event_ms": 415.21051025390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 632.3296000000482, "params_millions_measured": 74.824704, "latency_ms": 14.81069999954343, "cuda_event_ms": 14.243840217590332, "tokens_total": 1, "tokens_per_s": 67.51875333581985}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549239.9698174, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.81069999954343, "prefill_cuda_event_ms": 14.243840217590332, "kv_decode_ms": 415.38279999986116, "kv_decode_cuda_event_ms": 415.21051025390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 632.3296000000482, "params_millions_measured": 74.824704, "latency_ms": 415.38279999986116, "cuda_event_ms": 415.21051025390625, "tokens_total": 64, "tokens_per_s": 154.0747474378366}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549239.9698174, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.81069999954343, "prefill_cuda_event_ms": 14.243840217590332, "kv_decode_ms": 415.38279999986116, "kv_decode_cuda_event_ms": 415.21051025390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 632.3296000000482, "params_millions_measured": 74.824704, "latency_ms": 430.1934999994046, "cuda_event_ms": 429.4543504714966, "tokens_total": 65, "tokens_per_s": 151.09479803876619}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549241.0343292, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.659699999749137, "prefill_cuda_event_ms": 7.59500789642334, "kv_decode_ms": 417.4440999995568, "kv_decode_cuda_event_ms": 417.3813781738281, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 7.659699999749137, "cuda_event_ms": 7.59500789642334, "tokens_total": 1, "tokens_per_s": 130.55341593440357}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549241.0343292, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.659699999749137, "prefill_cuda_event_ms": 7.59500789642334, "kv_decode_ms": 417.4440999995568, "kv_decode_cuda_event_ms": 417.3813781738281, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 417.4440999995568, "cuda_event_ms": 417.3813781738281, "tokens_total": 64, "tokens_per_s": 153.31394071701564}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549241.0343292, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.659699999749137, "prefill_cuda_event_ms": 7.59500789642334, "kv_decode_ms": 417.4440999995568, "kv_decode_cuda_event_ms": 417.3813781738281, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 425.1037999993059, "cuda_event_ms": 424.97638607025146, "tokens_total": 65, "tokens_per_s": 152.90383195846786}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549241.4602194, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.344900000338384, "prefill_cuda_event_ms": 8.283136367797852, "kv_decode_ms": 438.379399999576, "kv_decode_cuda_event_ms": 438.2812194824219, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 8.344900000338384, "cuda_event_ms": 8.283136367797852, "tokens_total": 1, "tokens_per_s": 119.83367085998036}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549241.4602194, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.344900000338384, "prefill_cuda_event_ms": 8.283136367797852, "kv_decode_ms": 438.379399999576, "kv_decode_cuda_event_ms": 438.2812194824219, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 438.379399999576, "cuda_event_ms": 438.2812194824219, "tokens_total": 64, "tokens_per_s": 145.99226149783019}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549241.4602194, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.344900000338384, "prefill_cuda_event_ms": 8.283136367797852, "kv_decode_ms": 438.379399999576, "kv_decode_cuda_event_ms": 438.2812194824219, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 446.72429999991436, "cuda_event_ms": 446.5643558502197, "tokens_total": 65, "tokens_per_s": 145.50361375016416}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549241.9077911, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.086799999342475, "prefill_cuda_event_ms": 7.000927925109863, "kv_decode_ms": 296.2868000004164, "kv_decode_cuda_event_ms": 296.2391052246094, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 7.086799999342475, "cuda_event_ms": 7.000927925109863, "tokens_total": 1, "tokens_per_s": 141.10741097431588}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549241.9077911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.086799999342475, "prefill_cuda_event_ms": 7.000927925109863, "kv_decode_ms": 296.2868000004164, "kv_decode_cuda_event_ms": 296.2391052246094, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 296.2868000004164, "cuda_event_ms": 296.2391052246094, "tokens_total": 64, "tokens_per_s": 216.00692302157927}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549241.9077911, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.086799999342475, "prefill_cuda_event_ms": 7.000927925109863, "kv_decode_ms": 296.2868000004164, "kv_decode_cuda_event_ms": 296.2391052246094, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 303.3735999997589, "cuda_event_ms": 303.24003314971924, "tokens_total": 65, "tokens_per_s": 214.25727222161606}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549242.2119768, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 58.98949999937031, "prefill_cuda_event_ms": 58.87180709838867, "kv_decode_ms": 686.8390999998155, "kv_decode_cuda_event_ms": 686.5725708007812, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 481.3285000000178, "params_millions_measured": 51.475968, "latency_ms": 58.98949999937031, "cuda_event_ms": 58.87180709838867, "tokens_total": 17, "tokens_per_s": 288.1868807191359}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549242.2119768, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 58.98949999937031, "prefill_cuda_event_ms": 58.87180709838867, "kv_decode_ms": 686.8390999998155, "kv_decode_cuda_event_ms": 686.5725708007812, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 481.3285000000178, "params_millions_measured": 51.475968, "latency_ms": 686.8390999998155, "cuda_event_ms": 686.5725708007812, "tokens_total": 64, "tokens_per_s": 93.18048433762317}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549242.2119768, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 58.98949999937031, "prefill_cuda_event_ms": 58.87180709838867, "kv_decode_ms": 686.8390999998155, "kv_decode_cuda_event_ms": 686.5725708007812, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 481.3285000000178, "params_millions_measured": 51.475968, "latency_ms": 745.8285999991858, "cuda_event_ms": 745.4443778991699, "tokens_total": 81, "tokens_per_s": 108.6040411967152}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549243.4402404, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.472299999695679, "prefill_cuda_event_ms": 11.410431861877441, "kv_decode_ms": 646.0747000000993, "kv_decode_cuda_event_ms": 646.0354614257812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 11.472299999695679, "cuda_event_ms": 11.410431861877441, "tokens_total": 17, "tokens_per_s": 1481.8301474378243}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549243.4402404, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.472299999695679, "prefill_cuda_event_ms": 11.410431861877441, "kv_decode_ms": 646.0747000000993, "kv_decode_cuda_event_ms": 646.0354614257812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 646.0747000000993, "cuda_event_ms": 646.0354614257812, "tokens_total": 64, "tokens_per_s": 99.05975268802534}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549243.4402404, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.472299999695679, "prefill_cuda_event_ms": 11.410431861877441, "kv_decode_ms": 646.0747000000993, "kv_decode_cuda_event_ms": 646.0354614257812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 657.5469999997949, "cuda_event_ms": 657.4458932876587, "tokens_total": 81, "tokens_per_s": 123.1851107221617}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549244.09909, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.785499999656167, "prefill_cuda_event_ms": 10.707967758178711, "kv_decode_ms": 634.6049000003404, "kv_decode_cuda_event_ms": 634.5523071289062, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 10.785499999656167, "cuda_event_ms": 10.707967758178711, "tokens_total": 17, "tokens_per_s": 1576.190255485786}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549244.09909, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.785499999656167, "prefill_cuda_event_ms": 10.707967758178711, "kv_decode_ms": 634.6049000003404, "kv_decode_cuda_event_ms": 634.5523071289062, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 634.6049000003404, "cuda_event_ms": 634.5523071289062, "tokens_total": 64, "tokens_per_s": 100.85015101516812}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549244.09909, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.785499999656167, "prefill_cuda_event_ms": 10.707967758178711, "kv_decode_ms": 634.6049000003404, "kv_decode_cuda_event_ms": 634.5523071289062, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 645.3903999999966, "cuda_event_ms": 645.260274887085, "tokens_total": 81, "tokens_per_s": 125.50543051151742}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549244.7455595, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 10.410500000034517, "prefill_cuda_event_ms": 10.32806396484375, "kv_decode_ms": 589.4328000003952, "kv_decode_cuda_event_ms": 589.3939208984375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 10.410500000034517, "cuda_event_ms": 10.32806396484375, "tokens_total": 17, "tokens_per_s": 1632.966716290633}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549244.7455595, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.410500000034517, "prefill_cuda_event_ms": 10.32806396484375, "kv_decode_ms": 589.4328000003952, "kv_decode_cuda_event_ms": 589.3939208984375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 589.4328000003952, "cuda_event_ms": 589.3939208984375, "tokens_total": 64, "tokens_per_s": 108.57895929774706}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549244.7455595, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 10.410500000034517, "prefill_cuda_event_ms": 10.32806396484375, "kv_decode_ms": 589.4328000003952, "kv_decode_cuda_event_ms": 589.3939208984375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 599.8433000004297, "cuda_event_ms": 599.7219848632812, "tokens_total": 81, "tokens_per_s": 135.03526671039248}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549245.346321, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.606699999902048, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.61340000014025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 166.45289999996749, "params_millions_measured": 0.102714, "latency_ms": 11.606699999902048, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 775.4142004252676}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549245.346321, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.606699999902048, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.61340000014025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 166.45289999996749, "params_millions_measured": 0.102714, "latency_ms": 168.61340000014025, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 379.5665113208486}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549245.346321, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.606699999902048, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.61340000014025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 166.45289999996749, "params_millions_measured": 0.102714, "latency_ms": 180.2201000000423, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 405.06025687469304}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549245.694046, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.2736999994303915, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.29099999965547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.2736999994303915, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2105.903549898107}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549245.694046, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.2736999994303915, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.29099999965547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 171.29099999965547, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 373.63317395618407}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549245.694046, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.2736999994303915, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.29099999965547, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 175.56469999908586, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 415.8011263105858}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549245.8701704, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.081099999893922, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.65810000005877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.081099999893922, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2205.287790113923}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549245.8701704, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.081099999893922, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.65810000005877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 171.65810000005877, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 372.8341394899401}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549245.8701704, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.081099999893922, "prefill_cuda_event_ms": null, "kv_decode_ms": 171.65810000005877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 175.7391999999527, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 415.38825714478986}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549246.0468895, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7604000008286675, "prefill_cuda_event_ms": null, "kv_decode_ms": 167.21569999936037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.7604000008286675, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2393.3624077270233}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549246.0468895, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7604000008286675, "prefill_cuda_event_ms": null, "kv_decode_ms": 167.21569999936037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 167.21569999936037, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 382.7391805927602}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549246.0468895, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7604000008286675, "prefill_cuda_event_ms": null, "kv_decode_ms": 167.21569999936037, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 170.97610000018904, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 426.9602593574148}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549246.2183423, "prompt_tokens": 38, "prefill_ms": 20.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.9909, "kv_decode_ms_equiv": 718.6119314285714, "kv_decode_ms_per_token": 11.228311428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 6031.075599999895, "ollama_total_duration_ms": 6027.629, "ollama_load_ms": 5581.296, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 20.9968, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 1809.7995885087253}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549246.2183423, "prompt_tokens": 38, "prefill_ms": 20.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.9909, "kv_decode_ms_equiv": 718.6119314285714, "kv_decode_ms_per_token": 11.228311428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6031.075599999895, "ollama_total_duration_ms": 6027.629, "ollama_load_ms": 5581.296, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 718.6119314285714, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.06058639016831}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549246.2183423, "prompt_tokens": 38, "prefill_ms": 20.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 392.9909, "kv_decode_ms_equiv": 718.6119314285714, "kv_decode_ms_per_token": 11.228311428571429, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6031.075599999895, "ollama_total_duration_ms": 6027.629, "ollama_load_ms": 5581.296, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 739.6087314285714, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 137.91075695251007}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549252.2495391, "prompt_tokens": 38, "prefill_ms": 11.953, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.169, "kv_decode_ms_equiv": 706.1376, "kv_decode_ms_per_token": 11.0334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 579.8482000000149, "ollama_total_duration_ms": 562.5603, "ollama_load_ms": 131.2744, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.953, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3179.11821300092}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549252.2495391, "prompt_tokens": 38, "prefill_ms": 11.953, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.169, "kv_decode_ms_equiv": 706.1376, "kv_decode_ms_per_token": 11.0334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 579.8482000000149, "ollama_total_duration_ms": 562.5603, "ollama_load_ms": 131.2744, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 706.1376, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.63389345079486}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549252.2495391, "prompt_tokens": 38, "prefill_ms": 11.953, "prefill_cuda_event_ms": null, "kv_decode_ms": 386.169, "kv_decode_ms_equiv": 706.1376, "kv_decode_ms_per_token": 11.0334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 579.8482000000149, "ollama_total_duration_ms": 562.5603, "ollama_load_ms": 131.2744, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 718.0906, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 142.04335776014892}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549252.8312829, "prompt_tokens": 38, "prefill_ms": 12.0369, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.672, "kv_decode_ms_equiv": 708.8859428571429, "kv_decode_ms_per_token": 11.076342857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.1015000000116, "ollama_total_duration_ms": 543.6014, "ollama_load_ms": 112.2339, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.0369, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3156.9590176872784}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549252.8312829, "prompt_tokens": 38, "prefill_ms": 12.0369, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.672, "kv_decode_ms_equiv": 708.8859428571429, "kv_decode_ms_per_token": 11.076342857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.1015000000116, "ollama_total_duration_ms": 543.6014, "ollama_load_ms": 112.2339, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 708.8859428571429, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.28250686147051}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549252.8312829, "prompt_tokens": 38, "prefill_ms": 12.0369, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.672, "kv_decode_ms_equiv": 708.8859428571429, "kv_decode_ms_per_token": 11.076342857142858, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.1015000000116, "ollama_total_duration_ms": 543.6014, "ollama_load_ms": 112.2339, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.9228428571429, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.48532122488479}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549253.3995318, "prompt_tokens": 38, "prefill_ms": 11.8388, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.5121, "kv_decode_ms_equiv": 714.0792685714285, "kv_decode_ms_per_token": 11.157488571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.8995000000432, "ollama_total_duration_ms": 555.698, "ollama_load_ms": 115.7285, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.8388, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3209.784775484002}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549253.3995318, "prompt_tokens": 38, "prefill_ms": 11.8388, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.5121, "kv_decode_ms_equiv": 714.0792685714285, "kv_decode_ms_per_token": 11.157488571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.8995000000432, "ollama_total_duration_ms": 555.698, "ollama_load_ms": 115.7285, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 714.0792685714285, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.62590403728848}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549253.3995318, "prompt_tokens": 38, "prefill_ms": 11.8388, "prefill_cuda_event_ms": null, "kv_decode_ms": 390.5121, "kv_decode_ms_equiv": 714.0792685714285, "kv_decode_ms_per_token": 11.157488571428571, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 568.8995000000432, "ollama_total_duration_ms": 555.698, "ollama_load_ms": 115.7285, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 725.9180685714285, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 140.51172496743476}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549253.9686158, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 82.21990000038204, "prefill_cuda_event_ms": 82.07965087890625, "kv_decode_ms": 324.3712000003143, "kv_decode_cuda_event_ms": 324.34893798828125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 82.21990000038204, "cuda_event_ms": 82.07965087890625, "tokens_total": 9, "tokens_per_s": 109.4625510364058}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549253.9686158, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 82.21990000038204, "prefill_cuda_event_ms": 82.07965087890625, "kv_decode_ms": 324.3712000003143, "kv_decode_cuda_event_ms": 324.34893798828125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 324.3712000003143, "cuda_event_ms": 324.34893798828125, "tokens_total": 64, "tokens_per_s": 197.30481621037254}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549253.9686158, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 82.21990000038204, "prefill_cuda_event_ms": 82.07965087890625, "kv_decode_ms": 324.3712000003143, "kv_decode_cuda_event_ms": 324.34893798828125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 406.5911000006963, "cuda_event_ms": 406.4285888671875, "tokens_total": 73, "tokens_per_s": 179.54155907464522}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549254.376536, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.608799999914481, "prefill_cuda_event_ms": 4.564864158630371, "kv_decode_ms": 324.63370000004943, "kv_decode_cuda_event_ms": 324.6029357910156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 4.608799999914481, "cuda_event_ms": 4.564864158630371, "tokens_total": 9, "tokens_per_s": 1952.7859746934125}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549254.376536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.608799999914481, "prefill_cuda_event_ms": 4.564864158630371, "kv_decode_ms": 324.63370000004943, "kv_decode_cuda_event_ms": 324.6029357910156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 324.63370000004943, "cuda_event_ms": 324.6029357910156, "tokens_total": 64, "tokens_per_s": 197.14527481278208}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549254.376536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.608799999914481, "prefill_cuda_event_ms": 4.564864158630371, "kv_decode_ms": 324.63370000004943, "kv_decode_cuda_event_ms": 324.6029357910156, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 329.2424999999639, "cuda_event_ms": 329.167799949646, "tokens_total": 73, "tokens_per_s": 221.7210718543566}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549254.7064235, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.907599999569356, "prefill_cuda_event_ms": 4.860767841339111, "kv_decode_ms": 265.60460000018793, "kv_decode_cuda_event_ms": 265.5784912109375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 4.907599999569356, "cuda_event_ms": 4.860767841339111, "tokens_total": 9, "tokens_per_s": 1833.8902927683089}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549254.7064235, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.907599999569356, "prefill_cuda_event_ms": 4.860767841339111, "kv_decode_ms": 265.60460000018793, "kv_decode_cuda_event_ms": 265.5784912109375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 265.60460000018793, "cuda_event_ms": 265.5784912109375, "tokens_total": 64, "tokens_per_s": 240.95968217400872}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549254.7064235, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.907599999569356, "prefill_cuda_event_ms": 4.860767841339111, "kv_decode_ms": 265.60460000018793, "kv_decode_cuda_event_ms": 265.5784912109375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 270.5121999997573, "cuda_event_ms": 270.4392590522766, "tokens_total": 73, "tokens_per_s": 269.85843891723}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549254.9776623, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.108400000608526, "prefill_cuda_event_ms": 5.050367832183838, "kv_decode_ms": 250.68690000080096, "kv_decode_cuda_event_ms": 250.6649627685547, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 5.108400000608526, "cuda_event_ms": 5.050367832183838, "tokens_total": 9, "tokens_per_s": 1761.804087175612}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549254.9776623, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.108400000608526, "prefill_cuda_event_ms": 5.050367832183838, "kv_decode_ms": 250.68690000080096, "kv_decode_cuda_event_ms": 250.6649627685547, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 250.68690000080096, "cuda_event_ms": 250.6649627685547, "tokens_total": 64, "tokens_per_s": 255.29854172593588}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549254.9776623, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.108400000608526, "prefill_cuda_event_ms": 5.050367832183838, "kv_decode_ms": 250.68690000080096, "kv_decode_cuda_event_ms": 250.6649627685547, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 255.7953000014095, "cuda_event_ms": 255.71533060073853, "tokens_total": 73, "tokens_per_s": 285.3844460769911}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549255.234211, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.854600000020582, "prefill_cuda_event_ms": 5.746496200561523, "kv_decode_ms": 147.03360000021348, "kv_decode_cuda_event_ms": 146.9687957763672, "gpu_peak_mb": 539.3515625, "hf_load_ms": 284.3991999998252, "params_millions_measured": 25.016064, "latency_ms": 5.854600000020582, "cuda_event_ms": 5.746496200561523, "tokens_total": 17, "tokens_per_s": 2903.6996549619503}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549255.234211, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.854600000020582, "prefill_cuda_event_ms": 5.746496200561523, "kv_decode_ms": 147.03360000021348, "kv_decode_cuda_event_ms": 146.9687957763672, "gpu_peak_mb": 539.3515625, "hf_load_ms": 284.3991999998252, "params_millions_measured": 25.016064, "latency_ms": 147.03360000021348, "cuda_event_ms": 146.9687957763672, "tokens_total": 64, "tokens_per_s": 435.2746583087612}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549255.234211, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.854600000020582, "prefill_cuda_event_ms": 5.746496200561523, "kv_decode_ms": 147.03360000021348, "kv_decode_cuda_event_ms": 146.9687957763672, "gpu_peak_mb": 539.3515625, "hf_load_ms": 284.3991999998252, "params_millions_measured": 25.016064, "latency_ms": 152.88820000023406, "cuda_event_ms": 152.7152919769287, "tokens_total": 81, "tokens_per_s": 529.7988988023667}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549255.6728942, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.303399999618705, "prefill_cuda_event_ms": 2.248703956604004, "kv_decode_ms": 106.4328000002206, "kv_decode_cuda_event_ms": 106.392578125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 2.303399999618705, "cuda_event_ms": 2.248703956604004, "tokens_total": 17, "tokens_per_s": 7380.39420110016}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549255.6728942, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.303399999618705, "prefill_cuda_event_ms": 2.248703956604004, "kv_decode_ms": 106.4328000002206, "kv_decode_cuda_event_ms": 106.392578125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 106.4328000002206, "cuda_event_ms": 106.392578125, "tokens_total": 64, "tokens_per_s": 601.3183905700813}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549255.6728942, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.303399999618705, "prefill_cuda_event_ms": 2.248703956604004, "kv_decode_ms": 106.4328000002206, "kv_decode_cuda_event_ms": 106.392578125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 108.7361999998393, "cuda_event_ms": 108.641282081604, "tokens_total": 81, "tokens_per_s": 744.9221142555995}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549255.782086, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 2.173900000343565, "prefill_cuda_event_ms": 2.1319680213928223, "kv_decode_ms": 103.37990000061836, "kv_decode_cuda_event_ms": 103.33814239501953, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 2.173900000343565, "cuda_event_ms": 2.1319680213928223, "tokens_total": 17, "tokens_per_s": 7820.046919045635}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549255.782086, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.173900000343565, "prefill_cuda_event_ms": 2.1319680213928223, "kv_decode_ms": 103.37990000061836, "kv_decode_cuda_event_ms": 103.33814239501953, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 103.37990000061836, "cuda_event_ms": 103.33814239501953, "tokens_total": 64, "tokens_per_s": 619.0758551673699}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549255.782086, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 2.173900000343565, "prefill_cuda_event_ms": 2.1319680213928223, "kv_decode_ms": 103.37990000061836, "kv_decode_cuda_event_ms": 103.33814239501953, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 105.55380000096193, "cuda_event_ms": 105.47011041641235, "tokens_total": 81, "tokens_per_s": 767.3811838063797}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549255.888099, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 1.9484999993437668, "prefill_cuda_event_ms": 1.913856029510498, "kv_decode_ms": 104.59759999957896, "kv_decode_cuda_event_ms": 104.56063842773438, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 1.9484999993437668, "cuda_event_ms": 1.913856029510498, "tokens_total": 17, "tokens_per_s": 8724.659997806215}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549255.888099, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1.9484999993437668, "prefill_cuda_event_ms": 1.913856029510498, "kv_decode_ms": 104.59759999957896, "kv_decode_cuda_event_ms": 104.56063842773438, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 104.59759999957896, "cuda_event_ms": 104.56063842773438, "tokens_total": 64, "tokens_per_s": 611.868723567822}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549255.888099, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 1.9484999993437668, "prefill_cuda_event_ms": 1.913856029510498, "kv_decode_ms": 104.59759999957896, "kv_decode_cuda_event_ms": 104.56063842773438, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 106.54609999892273, "cuda_event_ms": 106.47449445724487, "tokens_total": 81, "tokens_per_s": 760.2343023425445}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549255.995172, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.1490999997695326, "prefill_cuda_event_ms": 2.100224018096924, "kv_decode_ms": 114.41020000074786, "kv_decode_cuda_event_ms": 114.3325424194336, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 2.1490999997695326, "cuda_event_ms": 2.100224018096924, "tokens_total": 1, "tokens_per_s": 465.3110604938063}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549255.995172, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.1490999997695326, "prefill_cuda_event_ms": 2.100224018096924, "kv_decode_ms": 114.41020000074786, "kv_decode_cuda_event_ms": 114.3325424194336, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 114.41020000074786, "cuda_event_ms": 114.3325424194336, "tokens_total": 64, "tokens_per_s": 559.3906836941256}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549255.995172, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.1490999997695326, "prefill_cuda_event_ms": 2.100224018096924, "kv_decode_ms": 114.41020000074786, "kv_decode_cuda_event_ms": 114.3325424194336, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 116.5593000005174, "cuda_event_ms": 116.43276643753052, "tokens_total": 65, "tokens_per_s": 557.6560600459292}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549256.1124125, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.196399999476853, "prefill_cuda_event_ms": 5.143551826477051, "kv_decode_ms": 124.48560000029829, "kv_decode_cuda_event_ms": 124.44979095458984, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 5.196399999476853, "cuda_event_ms": 5.143551826477051, "tokens_total": 1, "tokens_per_s": 192.4409206567383}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549256.1124125, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.196399999476853, "prefill_cuda_event_ms": 5.143551826477051, "kv_decode_ms": 124.48560000029829, "kv_decode_cuda_event_ms": 124.44979095458984, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 124.48560000029829, "cuda_event_ms": 124.44979095458984, "tokens_total": 64, "tokens_per_s": 514.1156888816589}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549256.1124125, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.196399999476853, "prefill_cuda_event_ms": 5.143551826477051, "kv_decode_ms": 124.48560000029829, "kv_decode_cuda_event_ms": 124.44979095458984, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 129.68199999977514, "cuda_event_ms": 129.5933427810669, "tokens_total": 65, "tokens_per_s": 501.2260760946986}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549256.242742, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.0211000000927015, "prefill_cuda_event_ms": 1.97324800491333, "kv_decode_ms": 115.7351000001654, "kv_decode_cuda_event_ms": 115.68946838378906, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 2.0211000000927015, "cuda_event_ms": 1.97324800491333, "tokens_total": 1, "tokens_per_s": 494.78007023607597}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549256.242742, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0211000000927015, "prefill_cuda_event_ms": 1.97324800491333, "kv_decode_ms": 115.7351000001654, "kv_decode_cuda_event_ms": 115.68946838378906, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 115.7351000001654, "cuda_event_ms": 115.68946838378906, "tokens_total": 64, "tokens_per_s": 552.986950371223}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549256.242742, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0211000000927015, "prefill_cuda_event_ms": 1.97324800491333, "kv_decode_ms": 115.7351000001654, "kv_decode_cuda_event_ms": 115.68946838378906, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 117.7562000002581, "cuda_event_ms": 117.66271638870239, "tokens_total": 65, "tokens_per_s": 551.9879208046585}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549256.3610232, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.4576000005254173, "prefill_cuda_event_ms": 2.407423973083496, "kv_decode_ms": 105.01490000024205, "kv_decode_cuda_event_ms": 104.97126770019531, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 2.4576000005254173, "cuda_event_ms": 2.407423973083496, "tokens_total": 1, "tokens_per_s": 406.90104157967414}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549256.3610232, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.4576000005254173, "prefill_cuda_event_ms": 2.407423973083496, "kv_decode_ms": 105.01490000024205, "kv_decode_cuda_event_ms": 104.97126770019531, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 105.01490000024205, "cuda_event_ms": 104.97126770019531, "tokens_total": 64, "tokens_per_s": 609.437327463555}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549256.3610232, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.4576000005254173, "prefill_cuda_event_ms": 2.407423973083496, "kv_decode_ms": 105.01490000024205, "kv_decode_cuda_event_ms": 104.97126770019531, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 107.47250000076747, "cuda_event_ms": 107.37869167327881, "tokens_total": 65, "tokens_per_s": 604.8058805697814}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549256.4690838, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 36.87919999993028, "prefill_cuda_event_ms": null, "kv_decode_ms": 805.0559000002977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 106.51850000067498, "params_millions_measured": 25.016064, "latency_ms": 36.87919999993028, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 244.04000086815915}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549256.4690838, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.87919999993028, "prefill_cuda_event_ms": null, "kv_decode_ms": 805.0559000002977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 106.51850000067498, "params_millions_measured": 25.016064, "latency_ms": 805.0559000002977, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.4975851987127}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549256.4690838, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 36.87919999993028, "prefill_cuda_event_ms": null, "kv_decode_ms": 805.0559000002977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 106.51850000067498, "params_millions_measured": 25.016064, "latency_ms": 841.935100000228, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 86.70502037506245}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549257.4182353, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.254699999692093, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.2552000000069, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.254699999692093, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 467.4183446194395}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549257.4182353, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.254699999692093, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.2552000000069, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 859.2552000000069, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.48311048917654}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549257.4182353, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.254699999692093, "prefill_cuda_event_ms": null, "kv_decode_ms": 859.2552000000069, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 878.509899999699, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.09525026414046}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549258.2975528, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.07130000017787, "prefill_cuda_event_ms": null, "kv_decode_ms": 884.8590999996304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.07130000017787, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 471.9132937930849}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549258.2975528, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.07130000017787, "prefill_cuda_event_ms": null, "kv_decode_ms": 884.8590999996304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 884.8590999996304, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.32789943622294}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549258.2975528, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.07130000017787, "prefill_cuda_event_ms": null, "kv_decode_ms": 884.8590999996304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 903.9303999998083, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 80.75843007383698}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549259.201985, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.960200000241457, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.4717000000455, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.960200000241457, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 450.89728559288625}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549259.201985, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.960200000241457, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.4717000000455, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 928.4717000000455, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.93048005663162}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549259.201985, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.960200000241457, "prefill_cuda_event_ms": null, "kv_decode_ms": 928.4717000000455, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 948.431900000287, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 76.96915297764437}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549260.151411, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 816.3809999996374, "prefill_cuda_event_ms": null, "kv_decode_ms": 2696.2573000000702, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 234.14950000005774, "params_millions_measured": 96.08832, "latency_ms": 816.3809999996374, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 20.82361054459566}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549260.151411, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 816.3809999996374, "prefill_cuda_event_ms": null, "kv_decode_ms": 2696.2573000000702, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 234.14950000005774, "params_millions_measured": 96.08832, "latency_ms": 2696.2573000000702, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.736607036723957}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549260.151411, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 816.3809999996374, "prefill_cuda_event_ms": null, "kv_decode_ms": 2696.2573000000702, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 234.14950000005774, "params_millions_measured": 96.08832, "latency_ms": 3512.6382999997077, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 23.059590280048685}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549263.899257, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 66.77759999911359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1888.990499999636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 66.77759999911359, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 254.5763848989131}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549263.899257, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 66.77759999911359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1888.990499999636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1888.990499999636, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.88053036794644}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549263.899257, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 66.77759999911359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1888.990499999636, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1955.7680999987497, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 41.41595314907313}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549265.8556826, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 39.15620000043418, "prefill_cuda_event_ms": null, "kv_decode_ms": 1546.0114000006797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 39.15620000043418, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 434.1585751378197}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549265.8556826, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 39.15620000043418, "prefill_cuda_event_ms": null, "kv_decode_ms": 1546.0114000006797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1546.0114000006797, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.39684869074824}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549265.8556826, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 39.15620000043418, "prefill_cuda_event_ms": null, "kv_decode_ms": 1546.0114000006797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1585.167600001114, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 51.098697702339535}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549267.441418, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 35.70419999959995, "prefill_cuda_event_ms": null, "kv_decode_ms": 1589.0267000004314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 35.70419999959995, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 476.1344603769438}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549267.441418, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.70419999959995, "prefill_cuda_event_ms": null, "kv_decode_ms": 1589.0267000004314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1589.0267000004314, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.27622695073823}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549267.441418, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 35.70419999959995, "prefill_cuda_event_ms": null, "kv_decode_ms": 1589.0267000004314, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1624.7309000000314, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 49.85440973640523}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549269.066744, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 19.513199999892095, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.085799999797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 19.513199999892095, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 51.247360761204206}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549269.066744, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.513199999892095, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.085799999797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 935.085799999797, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.44291721680929}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549269.066744, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.513199999892095, "prefill_cuda_event_ms": null, "kv_decode_ms": 935.085799999797, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 954.5989999996891, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 68.09141849092778}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549270.0220783, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.25080000030721, "prefill_cuda_event_ms": null, "kv_decode_ms": 965.8497000000352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.25080000030721, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 61.53543210064094}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549270.0220783, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.25080000030721, "prefill_cuda_event_ms": null, "kv_decode_ms": 965.8497000000352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 965.8497000000352, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 66.26289784010666}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549270.0220783, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.25080000030721, "prefill_cuda_event_ms": null, "kv_decode_ms": 965.8497000000352, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 982.1005000003424, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 66.18467254621837}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549271.004702, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.054599999937636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.011300000362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 17.054599999937636, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.63520692385965}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549271.004702, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.054599999937636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.011300000362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1133.011300000362, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.48663874753901}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549271.004702, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.054599999937636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1133.011300000362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1150.0659000002997, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 56.518500374615975}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549272.1552424, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.76249999996071, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.8450999992565, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.76249999996071, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 67.73920406453253}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549272.1552424, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.76249999996071, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.8450999992565, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1084.8450999992565, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.99459747759737}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549272.1552424, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.76249999996071, "prefill_cuda_event_ms": null, "kv_decode_ms": 1084.8450999992565, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1099.6075999992172, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 59.111995952052595}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549273.2552965, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.268600000359584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1547.444999999243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.268600000359584, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 262.6310966863415}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549273.2552965, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.268600000359584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1547.444999999243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1547.444999999243, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.35849739411178}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549273.2552965, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.268600000359584, "prefill_cuda_event_ms": null, "kv_decode_ms": 1547.444999999243, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1581.7135999996026, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 46.15247665570957}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549274.837558, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 29.18580000005022, "prefill_cuda_event_ms": null, "kv_decode_ms": 1570.314500000677, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 29.18580000005022, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 308.36913841609663}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549274.837558, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.18580000005022, "prefill_cuda_event_ms": null, "kv_decode_ms": 1570.314500000677, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1570.314500000677, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.756166997103065}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549274.837558, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.18580000005022, "prefill_cuda_event_ms": null, "kv_decode_ms": 1570.314500000677, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1599.5003000007273, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 45.63925370940337}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549276.4375553, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 34.955400000399095, "prefill_cuda_event_ms": null, "kv_decode_ms": 2091.9272000001, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.955400000399095, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 257.4709486916827}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549276.4375553, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.955400000399095, "prefill_cuda_event_ms": null, "kv_decode_ms": 2091.9272000001, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2091.9272000001, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.593798866421803}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549276.4375553, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 34.955400000399095, "prefill_cuda_event_ms": null, "kv_decode_ms": 2091.9272000001, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2126.882600000499, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 34.32253383425247}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549278.5649164, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 31.564399999297166, "prefill_cuda_event_ms": null, "kv_decode_ms": 2202.7633999996397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 31.564399999297166, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 285.13135051514996}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549278.5649164, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.564399999297166, "prefill_cuda_event_ms": null, "kv_decode_ms": 2202.7633999996397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2202.7633999996397, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 29.054414105486984}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549278.5649164, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.564399999297166, "prefill_cuda_event_ms": null, "kv_decode_ms": 2202.7633999996397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 2234.327799998937, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 32.67201885060676}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549280.7998047, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 39.2706000002363, "prefill_cuda_event_ms": null, "kv_decode_ms": 1758.5896000000503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 39.2706000002363, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.464342281349985}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549280.7998047, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.2706000002363, "prefill_cuda_event_ms": null, "kv_decode_ms": 1758.5896000000503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1758.5896000000503, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 36.39280023036539}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549280.7998047, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.2706000002363, "prefill_cuda_event_ms": null, "kv_decode_ms": 1758.5896000000503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1797.8602000002866, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 36.154090290218136}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549282.5981739, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 23.711599999842292, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5361999999805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 23.711599999842292, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 42.173450969426405}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549282.5981739, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 23.711599999842292, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5361999999805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1508.5361999999805, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 42.42523314985801}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549282.5981739, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 23.711599999842292, "prefill_cuda_event_ms": null, "kv_decode_ms": 1508.5361999999805, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1532.2477999998227, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 42.42133680988644}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549284.130949, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 22.183699999914097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1583.9538999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 22.183699999914097, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 45.07814296099714}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549284.130949, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 22.183699999914097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1583.9538999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1583.9538999998695, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.40521633868591}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549284.130949, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 22.183699999914097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1583.9538999998695, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1606.1375999997836, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 40.46975800828569}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549285.7375028, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 25.54130000044097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1581.083000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 25.54130000044097, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 39.152274942259595}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549285.7375028, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 25.54130000044097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1581.083000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1581.083000000035, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.47858335077828}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549285.7375028, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 25.54130000044097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1581.083000000035, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1606.624300000476, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 40.45749837095128}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549287.3446646, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.230200000100012, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.5683999995963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 26.230200000100012, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 648.107906151504}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549287.3446646, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.230200000100012, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.5683999995963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 941.5683999995963, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.97169488698584}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549287.3446646, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.230200000100012, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.5683999995963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 967.7985999996963, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 83.69509937297431}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549288.3136075, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.687000000179978, "prefill_cuda_event_ms": null, "kv_decode_ms": 890.63570000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 20.687000000179978, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 821.7721274158698}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549288.3136075, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.687000000179978, "prefill_cuda_event_ms": null, "kv_decode_ms": 890.63570000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 890.63570000053, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.85878580879019}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549288.3136075, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.687000000179978, "prefill_cuda_event_ms": null, "kv_decode_ms": 890.63570000053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 911.32270000071, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 88.88179785265625}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549289.2254214, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.86179999939486, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.723599999932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 23.86179999939486, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 712.4357760282595}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549289.2254214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.86179999939486, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.723599999932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 941.723599999932, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.96049286648929}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549289.2254214, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.86179999939486, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.723599999932, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 965.5853999993269, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 83.88693532447411}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549290.1918359, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.117999999660242, "prefill_cuda_event_ms": null, "kv_decode_ms": 919.0684000004694, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 21.117999999660242, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 805.0004735426417}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549290.1918359, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.117999999660242, "prefill_cuda_event_ms": null, "kv_decode_ms": 919.0684000004694, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 919.0684000004694, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 69.6357311381474}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549290.1918359, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.117999999660242, "prefill_cuda_event_ms": null, "kv_decode_ms": 919.0684000004694, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 940.1864000001297, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 86.15312878381226}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549291.1325114, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.643699999382079, "prefill_cuda_event_ms": 5.570559978485107, "kv_decode_ms": 187.2349999994185, "kv_decode_cuda_event_ms": 187.13804626464844, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 5.643699999382079, "cuda_event_ms": 5.570559978485107, "tokens_total": 9, "tokens_per_s": 1594.6985135612094}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549291.1325114, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.643699999382079, "prefill_cuda_event_ms": 5.570559978485107, "kv_decode_ms": 187.2349999994185, "kv_decode_cuda_event_ms": 187.13804626464844, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 187.2349999994185, "cuda_event_ms": 187.13804626464844, "tokens_total": 64, "tokens_per_s": 341.81643389429735}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549291.1325114, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.643699999382079, "prefill_cuda_event_ms": 5.570559978485107, "kv_decode_ms": 187.2349999994185, "kv_decode_cuda_event_ms": 187.13804626464844, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 192.87869999880058, "cuda_event_ms": 192.70860624313354, "tokens_total": 73, "tokens_per_s": 378.4762132908089}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549291.327222, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.285899999762478, "prefill_cuda_event_ms": 5.187679767608643, "kv_decode_ms": 210.53029999984574, "kv_decode_cuda_event_ms": 210.47689819335938, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 5.285899999762478, "cuda_event_ms": 5.187679767608643, "tokens_total": 9, "tokens_per_s": 1702.6428801915313}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549291.327222, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.285899999762478, "prefill_cuda_event_ms": 5.187679767608643, "kv_decode_ms": 210.53029999984574, "kv_decode_cuda_event_ms": 210.47689819335938, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 210.53029999984574, "cuda_event_ms": 210.47689819335938, "tokens_total": 64, "tokens_per_s": 303.9942469091}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549291.327222, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.285899999762478, "prefill_cuda_event_ms": 5.187679767608643, "kv_decode_ms": 210.53029999984574, "kv_decode_cuda_event_ms": 210.47689819335938, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 215.81619999960822, "cuda_event_ms": 215.66457796096802, "tokens_total": 73, "tokens_per_s": 338.25078932968205}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549291.5437686, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.0450000001801527, "prefill_cuda_event_ms": 2.011136054992676, "kv_decode_ms": 114.15029999989201, "kv_decode_cuda_event_ms": 114.11251068115234, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 2.0450000001801527, "cuda_event_ms": 2.011136054992676, "tokens_total": 9, "tokens_per_s": 4400.977994722323}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549291.5437686, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.0450000001801527, "prefill_cuda_event_ms": 2.011136054992676, "kv_decode_ms": 114.15029999989201, "kv_decode_cuda_event_ms": 114.11251068115234, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 114.15029999989201, "cuda_event_ms": 114.11251068115234, "tokens_total": 64, "tokens_per_s": 560.664317133293}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549291.5437686, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.0450000001801527, "prefill_cuda_event_ms": 2.011136054992676, "kv_decode_ms": 114.15029999989201, "kv_decode_cuda_event_ms": 114.11251068115234, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 116.19530000007217, "cuda_event_ms": 116.12364673614502, "tokens_total": 73, "tokens_per_s": 628.2526057418386}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549291.660504, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 1.820899999984249, "prefill_cuda_event_ms": 1.7898880243301392, "kv_decode_ms": 102.63370000029681, "kv_decode_cuda_event_ms": 102.58326721191406, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 1.820899999984249, "cuda_event_ms": 1.7898880243301392, "tokens_total": 9, "tokens_per_s": 4942.61079690145}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549291.660504, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 1.820899999984249, "prefill_cuda_event_ms": 1.7898880243301392, "kv_decode_ms": 102.63370000029681, "kv_decode_cuda_event_ms": 102.58326721191406, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 102.63370000029681, "cuda_event_ms": 102.58326721191406, "tokens_total": 64, "tokens_per_s": 623.5768563329093}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549291.660504, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 1.820899999984249, "prefill_cuda_event_ms": 1.7898880243301392, "kv_decode_ms": 102.63370000029681, "kv_decode_cuda_event_ms": 102.58326721191406, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 104.45460000028106, "cuda_event_ms": 104.3731552362442, "tokens_total": 73, "tokens_per_s": 698.86821642899}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549291.765427, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.028700000068056, "prefill_cuda_event_ms": 7.969056129455566, "kv_decode_ms": 372.33820000074047, "kv_decode_cuda_event_ms": 372.2596435546875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 8.028700000068056, "cuda_event_ms": 7.969056129455566, "tokens_total": 17, "tokens_per_s": 2117.4038137999796}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549291.765427, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.028700000068056, "prefill_cuda_event_ms": 7.969056129455566, "kv_decode_ms": 372.33820000074047, "kv_decode_cuda_event_ms": 372.2596435546875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 372.33820000074047, "cuda_event_ms": 372.2596435546875, "tokens_total": 64, "tokens_per_s": 171.8867416769827}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549291.765427, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.028700000068056, "prefill_cuda_event_ms": 7.969056129455566, "kv_decode_ms": 372.33820000074047, "kv_decode_cuda_event_ms": 372.2596435546875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 380.3669000008085, "cuda_event_ms": 380.22869968414307, "tokens_total": 81, "tokens_per_s": 212.95228370246681}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549292.1465077, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.146900000203459, "prefill_cuda_event_ms": 6.096767902374268, "kv_decode_ms": 390.64980000057403, "kv_decode_cuda_event_ms": 390.59661865234375, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 6.146900000203459, "cuda_event_ms": 6.096767902374268, "tokens_total": 17, "tokens_per_s": 2765.6216953972425}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549292.1465077, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.146900000203459, "prefill_cuda_event_ms": 6.096767902374268, "kv_decode_ms": 390.64980000057403, "kv_decode_cuda_event_ms": 390.59661865234375, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 390.64980000057403, "cuda_event_ms": 390.59661865234375, "tokens_total": 64, "tokens_per_s": 163.82959878619153}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549292.1465077, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.146900000203459, "prefill_cuda_event_ms": 6.096767902374268, "kv_decode_ms": 390.64980000057403, "kv_decode_cuda_event_ms": 390.59661865234375, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 396.7967000007775, "cuda_event_ms": 396.693386554718, "tokens_total": 81, "tokens_per_s": 204.13476220906395}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549292.544191, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.751299999450566, "prefill_cuda_event_ms": 6.6829118728637695, "kv_decode_ms": 375.75699999979406, "kv_decode_cuda_event_ms": 375.69842529296875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 6.751299999450566, "cuda_event_ms": 6.6829118728637695, "tokens_total": 17, "tokens_per_s": 2518.0335641111333}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549292.544191, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.751299999450566, "prefill_cuda_event_ms": 6.6829118728637695, "kv_decode_ms": 375.75699999979406, "kv_decode_cuda_event_ms": 375.69842529296875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 375.75699999979406, "cuda_event_ms": 375.69842529296875, "tokens_total": 64, "tokens_per_s": 170.32284162380228}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549292.544191, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.751299999450566, "prefill_cuda_event_ms": 6.6829118728637695, "kv_decode_ms": 375.75699999979406, "kv_decode_cuda_event_ms": 375.69842529296875, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 382.5082999992446, "cuda_event_ms": 382.3813371658325, "tokens_total": 81, "tokens_per_s": 211.7601108267715}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549292.9274898, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.638599999860162, "prefill_cuda_event_ms": 6.561791896820068, "kv_decode_ms": 369.51570000019274, "kv_decode_cuda_event_ms": 369.4581298828125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 6.638599999860162, "cuda_event_ms": 6.561791896820068, "tokens_total": 17, "tokens_per_s": 2560.7808875904702}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549292.9274898, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.638599999860162, "prefill_cuda_event_ms": 6.561791896820068, "kv_decode_ms": 369.51570000019274, "kv_decode_cuda_event_ms": 369.4581298828125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 369.51570000019274, "cuda_event_ms": 369.4581298828125, "tokens_total": 64, "tokens_per_s": 173.19967730726088}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549292.9274898, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.638599999860162, "prefill_cuda_event_ms": 6.561791896820068, "kv_decode_ms": 369.51570000019274, "kv_decode_cuda_event_ms": 369.4581298828125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 376.1543000000529, "cuda_event_ms": 376.01992177963257, "tokens_total": 81, "tokens_per_s": 215.3371634990976}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549293.304445, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.023899999490823, "prefill_cuda_event_ms": 5.958655834197998, "kv_decode_ms": 186.27159999959986, "kv_decode_cuda_event_ms": 186.24000549316406, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 245.24730000030104, "params_millions_measured": 45.1712, "latency_ms": 6.023899999490823, "cuda_event_ms": 5.958655834197998, "tokens_total": 9, "tokens_per_s": 1494.0487061141014}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549293.304445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.023899999490823, "prefill_cuda_event_ms": 5.958655834197998, "kv_decode_ms": 186.27159999959986, "kv_decode_cuda_event_ms": 186.24000549316406, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 245.24730000030104, "params_millions_measured": 45.1712, "latency_ms": 186.27159999959986, "cuda_event_ms": 186.24000549316406, "tokens_total": 64, "tokens_per_s": 343.58431451781956}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549293.304445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.023899999490823, "prefill_cuda_event_ms": 5.958655834197998, "kv_decode_ms": 186.27159999959986, "kv_decode_cuda_event_ms": 186.24000549316406, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 245.24730000030104, "params_millions_measured": 45.1712, "latency_ms": 192.29549999909068, "cuda_event_ms": 192.19866132736206, "tokens_total": 73, "tokens_per_s": 379.62406816771687}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549293.7430694, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.9596000005985843, "prefill_cuda_event_ms": 3.8993918895721436, "kv_decode_ms": 193.38880000032077, "kv_decode_cuda_event_ms": 193.35781860351562, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.9596000005985843, "cuda_event_ms": 3.8993918895721436, "tokens_total": 9, "tokens_per_s": 2272.956863986121}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549293.7430694, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.9596000005985843, "prefill_cuda_event_ms": 3.8993918895721436, "kv_decode_ms": 193.38880000032077, "kv_decode_cuda_event_ms": 193.35781860351562, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 193.38880000032077, "cuda_event_ms": 193.35781860351562, "tokens_total": 64, "tokens_per_s": 330.93953734597784}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549293.7430694, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.9596000005985843, "prefill_cuda_event_ms": 3.8993918895721436, "kv_decode_ms": 193.38880000032077, "kv_decode_cuda_event_ms": 193.35781860351562, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 197.34840000091936, "cuda_event_ms": 197.25721049308777, "tokens_total": 73, "tokens_per_s": 369.90418974595144}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549293.9410486, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.8006000004315865, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 188.7045000003127, "kv_decode_cuda_event_ms": 188.6761016845703, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.8006000004315865, "cuda_event_ms": 3.7498879432678223, "tokens_total": 9, "tokens_per_s": 2368.0471501810193}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549293.9410486, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8006000004315865, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 188.7045000003127, "kv_decode_cuda_event_ms": 188.6761016845703, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 188.7045000003127, "cuda_event_ms": 188.6761016845703, "tokens_total": 64, "tokens_per_s": 339.1546041556717}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549293.9410486, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8006000004315865, "prefill_cuda_event_ms": 3.7498879432678223, "kv_decode_ms": 188.7045000003127, "kv_decode_cuda_event_ms": 188.6761016845703, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 192.50510000074428, "cuda_event_ms": 192.42598962783813, "tokens_total": 73, "tokens_per_s": 379.21073259730656}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549294.1341584, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7723999994341284, "prefill_cuda_event_ms": 3.732480049133301, "kv_decode_ms": 198.95110000015848, "kv_decode_cuda_event_ms": 198.89663696289062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.7723999994341284, "cuda_event_ms": 3.732480049133301, "tokens_total": 9, "tokens_per_s": 2385.7491255831906}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549294.1341584, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7723999994341284, "prefill_cuda_event_ms": 3.732480049133301, "kv_decode_ms": 198.95110000015848, "kv_decode_cuda_event_ms": 198.89663696289062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 198.95110000015848, "cuda_event_ms": 198.89663696289062, "tokens_total": 64, "tokens_per_s": 321.6870879324066}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549294.1341584, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7723999994341284, "prefill_cuda_event_ms": 3.732480049133301, "kv_decode_ms": 198.95110000015848, "kv_decode_cuda_event_ms": 198.89663696289062, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 202.7234999995926, "cuda_event_ms": 202.62911701202393, "tokens_total": 73, "tokens_per_s": 360.0963874447052}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549294.3375437, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.41500000045926, "prefill_cuda_event_ms": null, "kv_decode_ms": 906.5080999998827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.41500000045926, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.30355688162154}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549294.3375437, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.41500000045926, "prefill_cuda_event_ms": null, "kv_decode_ms": 906.5080999998827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 906.5080999998827, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.60058260925443}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549294.3375437, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.41500000045926, "prefill_cuda_event_ms": null, "kv_decode_ms": 906.5080999998827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 924.923100000342, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 70.27611268436908}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549295.2629445, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.312199999949371, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.4248999998817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.312199999949371, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 69.87046016709782}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549295.2629445, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.312199999949371, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.4248999998817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 913.4248999998817, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.06596820385374}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549295.2629445, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.312199999949371, "prefill_cuda_event_ms": null, "kv_decode_ms": 913.4248999998817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 927.7370999998311, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 70.06295210142166}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549296.190979, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.39670000026672, "prefill_cuda_event_ms": null, "kv_decode_ms": 905.0728000001982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.39670000026672, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.35757499907602}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549296.190979, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.39670000026672, "prefill_cuda_event_ms": null, "kv_decode_ms": 905.0728000001982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 905.0728000001982, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.71254378651749}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549296.190979, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.39670000026672, "prefill_cuda_event_ms": null, "kv_decode_ms": 905.0728000001982, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 923.4695000004649, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 70.38673177616292}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549297.1148741, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.004200000272249, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.4976000000606, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.004200000272249, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 71.4071492823981}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549297.1148741, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.004200000272249, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.4976000000606, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 902.4976000000606, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.9143160048245}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549297.1148741, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.004200000272249, "prefill_cuda_event_ms": null, "kv_decode_ms": 902.4976000000606, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 916.5018000003329, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 70.9218465255348}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549298.031885, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.93719999994937, "prefill_cuda_event_ms": null, "kv_decode_ms": 1100.0022999996872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 29.93719999994937, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 567.8553772573504}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549298.031885, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.93719999994937, "prefill_cuda_event_ms": null, "kv_decode_ms": 1100.0022999996872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1100.0022999996872, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.18169652919653}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549298.031885, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.93719999994937, "prefill_cuda_event_ms": null, "kv_decode_ms": 1100.0022999996872, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1129.9394999996366, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 71.68525394503516}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549299.1624105, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.053800000823685, "prefill_cuda_event_ms": null, "kv_decode_ms": 885.8111999998073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.053800000823685, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 847.7196341492258}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549299.1624105, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.053800000823685, "prefill_cuda_event_ms": null, "kv_decode_ms": 885.8111999998073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 885.8111999998073, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.2501589503654}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549299.1624105, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.053800000823685, "prefill_cuda_event_ms": null, "kv_decode_ms": 885.8111999998073, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 905.865000000631, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 89.41729727933365}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549300.0688827, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 29.179799999837996, "prefill_cuda_event_ms": null, "kv_decode_ms": 1296.9610999998622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 29.179799999837996, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 582.5948087407859}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549300.0688827, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.179799999837996, "prefill_cuda_event_ms": null, "kv_decode_ms": 1296.9610999998622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1296.9610999998622, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.34612148352545}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549300.0688827, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 29.179799999837996, "prefill_cuda_event_ms": null, "kv_decode_ms": 1296.9610999998622, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1326.1408999997002, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 61.07948257988145}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549301.3955824, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 28.20960000008199, "prefill_cuda_event_ms": null, "kv_decode_ms": 1178.82869999994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 28.20960000008199, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 602.6317282042493}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549301.3955824, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 28.20960000008199, "prefill_cuda_event_ms": null, "kv_decode_ms": 1178.82869999994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1178.82869999994, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.29117903220651}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549301.3955824, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 28.20960000008199, "prefill_cuda_event_ms": null, "kv_decode_ms": 1178.82869999994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1207.038300000022, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 67.10640416298185}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549302.6033485, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 361.2030999993294, "prefill_cuda_event_ms": null, "kv_decode_ms": 1164.2917999997735, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 209.40330000030372, "params_millions_measured": 51.475968, "latency_ms": 361.2030999993294, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 24.916729673739535}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549302.6033485, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 361.2030999993294, "prefill_cuda_event_ms": null, "kv_decode_ms": 1164.2917999997735, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 209.40330000030372, "params_millions_measured": 51.475968, "latency_ms": 1164.2917999997735, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.96903783056142}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549302.6033485, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 361.2030999993294, "prefill_cuda_event_ms": null, "kv_decode_ms": 1164.2917999997735, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 209.40330000030372, "params_millions_measured": 51.475968, "latency_ms": 1525.494899999103, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 47.85332287904924}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549304.3428855, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 37.92350000003353, "prefill_cuda_event_ms": null, "kv_decode_ms": 1053.2601999993858, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 37.92350000003353, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 237.3198676280418}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549304.3428855, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.92350000003353, "prefill_cuda_event_ms": null, "kv_decode_ms": 1053.2601999993858, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1053.2601999993858, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.763712518556495}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549304.3428855, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.92350000003353, "prefill_cuda_event_ms": null, "kv_decode_ms": 1053.2601999993858, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1091.1836999994193, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 66.89982630792491}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549305.4345613, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.733399999495305, "prefill_cuda_event_ms": null, "kv_decode_ms": 1087.3028000005434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 19.733399999495305, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 456.079540283488}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549305.4345613, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.733399999495305, "prefill_cuda_event_ms": null, "kv_decode_ms": 1087.3028000005434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1087.3028000005434, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.861248218957975}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549305.4345613, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.733399999495305, "prefill_cuda_event_ms": null, "kv_decode_ms": 1087.3028000005434, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1107.0362000000387, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 65.9418364096833}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549306.5421927, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.064700000853918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1046.1010999997598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 19.064700000853918, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 472.07666522929213}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549306.5421927, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.064700000853918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1046.1010999997598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1046.1010999997598, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.17955520744094}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549306.5421927, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.064700000853918, "prefill_cuda_event_ms": null, "kv_decode_ms": 1046.1010999997598, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1065.1658000006137, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 68.53393152498695}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549307.6077745, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 49.339499999405234, "prefill_cuda_event_ms": 49.20115280151367, "kv_decode_ms": 460.46170000045095, "kv_decode_cuda_event_ms": 460.3381652832031, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 49.339499999405234, "cuda_event_ms": 49.20115280151367, "tokens_total": 17, "tokens_per_s": 344.55152565804127}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549307.6077745, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.339499999405234, "prefill_cuda_event_ms": 49.20115280151367, "kv_decode_ms": 460.46170000045095, "kv_decode_cuda_event_ms": 460.3381652832031, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 460.46170000045095, "cuda_event_ms": 460.3381652832031, "tokens_total": 64, "tokens_per_s": 138.990930190149}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549307.6077745, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 49.339499999405234, "prefill_cuda_event_ms": 49.20115280151367, "kv_decode_ms": 460.46170000045095, "kv_decode_cuda_event_ms": 460.3381652832031, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 509.8011999998562, "cuda_event_ms": 509.5393180847168, "tokens_total": 81, "tokens_per_s": 158.8854635885966}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549308.1196465, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.103800000142655, "prefill_cuda_event_ms": 5.052351951599121, "kv_decode_ms": 283.4919999995691, "kv_decode_cuda_event_ms": 283.46881103515625, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 5.103800000142655, "cuda_event_ms": 5.052351951599121, "tokens_total": 17, "tokens_per_s": 3330.851522301978}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549308.1196465, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.103800000142655, "prefill_cuda_event_ms": 5.052351951599121, "kv_decode_ms": 283.4919999995691, "kv_decode_cuda_event_ms": 283.46881103515625, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 283.4919999995691, "cuda_event_ms": 283.46881103515625, "tokens_total": 64, "tokens_per_s": 225.75592962093208}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549308.1196465, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.103800000142655, "prefill_cuda_event_ms": 5.052351951599121, "kv_decode_ms": 283.4919999995691, "kv_decode_cuda_event_ms": 283.46881103515625, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 288.59579999971174, "cuda_event_ms": 288.52116298675537, "tokens_total": 81, "tokens_per_s": 280.6693652509181}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549308.4089415, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.202899999858346, "prefill_cuda_event_ms": 5.1466240882873535, "kv_decode_ms": 265.0000000003274, "kv_decode_cuda_event_ms": 264.9692077636719, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 5.202899999858346, "cuda_event_ms": 5.1466240882873535, "tokens_total": 17, "tokens_per_s": 3267.408560699387}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549308.4089415, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.202899999858346, "prefill_cuda_event_ms": 5.1466240882873535, "kv_decode_ms": 265.0000000003274, "kv_decode_cuda_event_ms": 264.9692077636719, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 265.0000000003274, "cuda_event_ms": 264.9692077636719, "tokens_total": 64, "tokens_per_s": 241.50943396196575}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549308.4089415, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.202899999858346, "prefill_cuda_event_ms": 5.1466240882873535, "kv_decode_ms": 265.0000000003274, "kv_decode_cuda_event_ms": 264.9692077636719, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 270.20290000018576, "cuda_event_ms": 270.11583185195923, "tokens_total": 81, "tokens_per_s": 299.7747248454562}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549308.679674, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.165800000213494, "prefill_cuda_event_ms": 5.1169281005859375, "kv_decode_ms": 265.130699999645, "kv_decode_cuda_event_ms": 265.09210205078125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 5.165800000213494, "cuda_event_ms": 5.1169281005859375, "tokens_total": 17, "tokens_per_s": 3290.874598183712}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549308.679674, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.165800000213494, "prefill_cuda_event_ms": 5.1169281005859375, "kv_decode_ms": 265.130699999645, "kv_decode_cuda_event_ms": 265.09210205078125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 265.130699999645, "cuda_event_ms": 265.09210205078125, "tokens_total": 64, "tokens_per_s": 241.39037840614344}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549308.679674, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.165800000213494, "prefill_cuda_event_ms": 5.1169281005859375, "kv_decode_ms": 265.130699999645, "kv_decode_cuda_event_ms": 265.09210205078125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 270.2964999998585, "cuda_event_ms": 270.2090301513672, "tokens_total": 81, "tokens_per_s": 299.6709169376681}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 360, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549308.9505923, "prompt_tokens": 84, "prefill_ms": 4362.4846, "prefill_cuda_event_ms": null, "kv_decode_ms": 1643.6693, "kv_decode_ms_equiv": 2768.285136842105, "kv_decode_ms_per_token": 43.254455263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 17365.205099999912, "ollama_total_duration_ms": 17218.1822, "ollama_load_ms": 11046.3117, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 4362.4846, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 19.255082298743243}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 361, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549308.9505923, "prompt_tokens": 84, "prefill_ms": 4362.4846, "prefill_cuda_event_ms": null, "kv_decode_ms": 1643.6693, "kv_decode_ms_equiv": 2768.285136842105, "kv_decode_ms_per_token": 43.254455263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 17365.205099999912, "ollama_total_duration_ms": 17218.1822, "ollama_load_ms": 11046.3117, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2768.285136842105, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.119005751339397}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 362, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549308.9505923, "prompt_tokens": 84, "prefill_ms": 4362.4846, "prefill_cuda_event_ms": null, "kv_decode_ms": 1643.6693, "kv_decode_ms_equiv": 2768.285136842105, "kv_decode_ms_per_token": 43.254455263157894, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 17365.205099999912, "ollama_total_duration_ms": 17218.1822, "ollama_load_ms": 11046.3117, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 7130.769736842105, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 20.75512258309753}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 363, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549326.3175044, "prompt_tokens": 84, "prefill_ms": 50.7091, "prefill_cuda_event_ms": null, "kv_decode_ms": 1665.1575, "kv_decode_ms_equiv": 3134.414117647059, "kv_decode_ms_per_token": 48.975220588235295, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2017.6206999994974, "ollama_total_duration_ms": 1986.5889, "ollama_load_ms": 263.0915, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 50.7091, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1656.507411884652}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 364, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549326.3175044, "prompt_tokens": 84, "prefill_ms": 50.7091, "prefill_cuda_event_ms": null, "kv_decode_ms": 1665.1575, "kv_decode_ms_equiv": 3134.414117647059, "kv_decode_ms_per_token": 48.975220588235295, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2017.6206999994974, "ollama_total_duration_ms": 1986.5889, "ollama_load_ms": 263.0915, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3134.414117647059, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.418488941736744}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 365, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549326.3175044, "prompt_tokens": 84, "prefill_ms": 50.7091, "prefill_cuda_event_ms": null, "kv_decode_ms": 1665.1575, "kv_decode_ms_equiv": 3134.414117647059, "kv_decode_ms_per_token": 48.975220588235295, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2017.6206999994974, "ollama_total_duration_ms": 1986.5889, "ollama_load_ms": 263.0915, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3185.123217647059, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 46.46602027199808}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 366, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549328.3353252, "prompt_tokens": 84, "prefill_ms": 62.9926, "prefill_cuda_event_ms": null, "kv_decode_ms": 1781.8883, "kv_decode_ms_equiv": 3354.1426823529414, "kv_decode_ms_per_token": 52.40847941176471, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2115.4519999990953, "ollama_total_duration_ms": 2111.8276, "ollama_load_ms": 253.9384, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 62.9926, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1333.4899654880096}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 367, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549328.3353252, "prompt_tokens": 84, "prefill_ms": 62.9926, "prefill_cuda_event_ms": null, "kv_decode_ms": 1781.8883, "kv_decode_ms_equiv": 3354.1426823529414, "kv_decode_ms_per_token": 52.40847941176471, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2115.4519999990953, "ollama_total_duration_ms": 2111.8276, "ollama_load_ms": 253.9384, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3354.1426823529414, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.08088178142255}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 368, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549328.3353252, "prompt_tokens": 84, "prefill_ms": 62.9926, "prefill_cuda_event_ms": null, "kv_decode_ms": 1781.8883, "kv_decode_ms_equiv": 3354.1426823529414, "kv_decode_ms_per_token": 52.40847941176471, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2115.4519999990953, "ollama_total_duration_ms": 2111.8276, "ollama_load_ms": 253.9384, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3417.1352823529414, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 43.311132797204166}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 369, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549330.450956, "prompt_tokens": 84, "prefill_ms": 62.4044, "prefill_cuda_event_ms": null, "kv_decode_ms": 1815.5558, "kv_decode_ms_equiv": 3417.5168000000003, "kv_decode_ms_per_token": 53.398700000000005, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2154.506300000321, "ollama_total_duration_ms": 2134.6714, "ollama_load_ms": 246.0909, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 62.4044, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1346.0589317419924}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 370, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549330.450956, "prompt_tokens": 84, "prefill_ms": 62.4044, "prefill_cuda_event_ms": null, "kv_decode_ms": 1815.5558, "kv_decode_ms_equiv": 3417.5168000000003, "kv_decode_ms_per_token": 53.398700000000005, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2154.506300000321, "ollama_total_duration_ms": 2134.6714, "ollama_load_ms": 246.0909, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3417.5168000000003, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.727047662209003}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 371, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549330.450956, "prompt_tokens": 84, "prefill_ms": 62.4044, "prefill_cuda_event_ms": null, "kv_decode_ms": 1815.5558, "kv_decode_ms_equiv": 3417.5168000000003, "kv_decode_ms_per_token": 53.398700000000005, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2154.506300000321, "ollama_total_duration_ms": 2134.6714, "ollama_load_ms": 246.0909, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3479.9212, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 42.529698660992665}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 372, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549332.6058643, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 133.45509999999194, "prefill_cuda_event_ms": 128.7229461669922, "kv_decode_ms": 409.89059999992605, "kv_decode_cuda_event_ms": 409.2886962890625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 133.45509999999194, "cuda_event_ms": 128.7229461669922, "tokens_total": 9, "tokens_per_s": 67.43841187036347}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 373, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549332.6058643, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 133.45509999999194, "prefill_cuda_event_ms": 128.7229461669922, "kv_decode_ms": 409.89059999992605, "kv_decode_cuda_event_ms": 409.2886962890625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 409.89059999992605, "cuda_event_ms": 409.2886962890625, "tokens_total": 64, "tokens_per_s": 156.13922349039365}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 374, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549332.6058643, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 133.45509999999194, "prefill_cuda_event_ms": 128.7229461669922, "kv_decode_ms": 409.89059999992605, "kv_decode_cuda_event_ms": 409.2886962890625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 543.345699999918, "cuda_event_ms": 538.0116424560547, "tokens_total": 73, "tokens_per_s": 134.3527702529182}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 375, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549333.2387583, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 8.295599999655678, "prefill_cuda_event_ms": 8.20633602142334, "kv_decode_ms": 381.1806000003344, "kv_decode_cuda_event_ms": 381.1010437011719, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 8.295599999655678, "cuda_event_ms": 8.20633602142334, "tokens_total": 9, "tokens_per_s": 1084.9124837713437}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 376, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549333.2387583, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.295599999655678, "prefill_cuda_event_ms": 8.20633602142334, "kv_decode_ms": 381.1806000003344, "kv_decode_cuda_event_ms": 381.1010437011719, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 381.1806000003344, "cuda_event_ms": 381.1010437011719, "tokens_total": 64, "tokens_per_s": 167.89941565741765}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 377, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549333.2387583, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 8.295599999655678, "prefill_cuda_event_ms": 8.20633602142334, "kv_decode_ms": 381.1806000003344, "kv_decode_cuda_event_ms": 381.1010437011719, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 389.47619999999006, "cuda_event_ms": 389.3073797225952, "tokens_total": 73, "tokens_per_s": 187.43122172805903}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 378, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549333.6297634, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.913800000096671, "prefill_cuda_event_ms": 7.829504013061523, "kv_decode_ms": 385.0869000007151, "kv_decode_cuda_event_ms": 385.01171875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 7.913800000096671, "cuda_event_ms": 7.829504013061523, "tokens_total": 9, "tokens_per_s": 1137.2539108759458}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 379, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549333.6297634, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.913800000096671, "prefill_cuda_event_ms": 7.829504013061523, "kv_decode_ms": 385.0869000007151, "kv_decode_cuda_event_ms": 385.01171875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 385.0869000007151, "cuda_event_ms": 385.01171875, "tokens_total": 64, "tokens_per_s": 166.1962533648409}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 380, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549333.6297634, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.913800000096671, "prefill_cuda_event_ms": 7.829504013061523, "kv_decode_ms": 385.0869000007151, "kv_decode_cuda_event_ms": 385.01171875, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 393.0007000008118, "cuda_event_ms": 392.8412227630615, "tokens_total": 73, "tokens_per_s": 185.75030527897079}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 381, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549334.0236979, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.0587000000159605, "prefill_cuda_event_ms": 6.975488185882568, "kv_decode_ms": 385.61010000012175, "kv_decode_cuda_event_ms": 385.5226745605469, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 7.0587000000159605, "cuda_event_ms": 6.975488185882568, "tokens_total": 9, "tokens_per_s": 1275.0223128875925}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 382, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549334.0236979, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.0587000000159605, "prefill_cuda_event_ms": 6.975488185882568, "kv_decode_ms": 385.61010000012175, "kv_decode_cuda_event_ms": 385.5226745605469, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 385.61010000012175, "cuda_event_ms": 385.5226745605469, "tokens_total": 64, "tokens_per_s": 165.97075647131595}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 383, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549334.0236979, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.0587000000159605, "prefill_cuda_event_ms": 6.975488185882568, "kv_decode_ms": 385.61010000012175, "kv_decode_cuda_event_ms": 385.5226745605469, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 392.6688000001377, "cuda_event_ms": 392.49816274642944, "tokens_total": 73, "tokens_per_s": 185.90730916226192}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 384, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549334.417319, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 530.6447000002663, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.1463999992557, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 322.4867999997514, "params_millions_measured": 74.824704, "latency_ms": 530.6447000002663, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 32.03650201347807}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 385, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549334.417319, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 530.6447000002663, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.1463999992557, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 322.4867999997514, "params_millions_measured": 74.824704, "latency_ms": 1462.1463999992557, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 43.77126668029452}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 386, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549334.417319, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 530.6447000002663, "prefill_cuda_event_ms": null, "kv_decode_ms": 1462.1463999992557, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 322.4867999997514, "params_millions_measured": 74.824704, "latency_ms": 1992.791099999522, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 40.646508306876434}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 387, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549336.7337458, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.43129999989469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1385.892999999669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 31.43129999989469, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 540.8621342437939}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 388, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549336.7337458, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.43129999989469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1385.892999999669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1385.892999999669, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.17961126870205}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 389, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549336.7337458, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.43129999989469, "prefill_cuda_event_ms": null, "kv_decode_ms": 1385.892999999669, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1417.3242999995637, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 57.149940913328685}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 390, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549338.1516776, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 33.69040000052337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.06389999963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 33.69040000052337, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 504.594780701206}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 391, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549338.1516776, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 33.69040000052337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.06389999963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1342.06389999963, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.68774422739308}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 392, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549338.1516776, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 33.69040000052337, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.06389999963, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1375.7543000001533, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 58.87679217138625}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 393, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549339.5280466, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.053499999870837, "prefill_cuda_event_ms": null, "kv_decode_ms": 1339.1797000003862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 31.053499999870837, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 547.4423172934036}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 394, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549339.5280466, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.053499999870837, "prefill_cuda_event_ms": null, "kv_decode_ms": 1339.1797000003862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1339.1797000003862, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.790449631204496}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 395, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549339.5280466, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.053499999870837, "prefill_cuda_event_ms": null, "kv_decode_ms": 1339.1797000003862, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1370.233200000257, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 59.11402526225814}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 396, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549340.8989022, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.490299999750277, "prefill_cuda_event_ms": 25.865215301513672, "kv_decode_ms": 588.5363000006691, "kv_decode_cuda_event_ms": 588.4088134765625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 26.490299999750277, "cuda_event_ms": 25.865215301513672, "tokens_total": 9, "tokens_per_s": 339.7470017359125}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 397, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549340.8989022, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.490299999750277, "prefill_cuda_event_ms": 25.865215301513672, "kv_decode_ms": 588.5363000006691, "kv_decode_cuda_event_ms": 588.4088134765625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 588.5363000006691, "cuda_event_ms": 588.4088134765625, "tokens_total": 64, "tokens_per_s": 108.74435442627284}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 398, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549340.8989022, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.490299999750277, "prefill_cuda_event_ms": 25.865215301513672, "kv_decode_ms": 588.5363000006691, "kv_decode_cuda_event_ms": 588.4088134765625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 615.0266000004194, "cuda_event_ms": 614.2740287780762, "tokens_total": 73, "tokens_per_s": 118.69405323273858}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 399, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549341.5169106, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.72229999974661, "prefill_cuda_event_ms": 11.659263610839844, "kv_decode_ms": 590.878299999531, "kv_decode_cuda_event_ms": 590.80908203125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 11.72229999974661, "cuda_event_ms": 11.659263610839844, "tokens_total": 9, "tokens_per_s": 767.7674176735405}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 400, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549341.5169106, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.72229999974661, "prefill_cuda_event_ms": 11.659263610839844, "kv_decode_ms": 590.878299999531, "kv_decode_cuda_event_ms": 590.80908203125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 590.878299999531, "cuda_event_ms": 590.80908203125, "tokens_total": 64, "tokens_per_s": 108.31333626577722}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 401, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549341.5169106, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.72229999974661, "prefill_cuda_event_ms": 11.659263610839844, "kv_decode_ms": 590.878299999531, "kv_decode_cuda_event_ms": 590.80908203125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 602.6005999992776, "cuda_event_ms": 602.4683456420898, "tokens_total": 73, "tokens_per_s": 121.1415985979561}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 402, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549342.1203673, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.948999999731313, "prefill_cuda_event_ms": 11.904000282287598, "kv_decode_ms": 626.4775999998164, "kv_decode_cuda_event_ms": 626.4105224609375, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 11.948999999731313, "cuda_event_ms": 11.904000282287598, "tokens_total": 9, "tokens_per_s": 753.2011047118901}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 403, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549342.1203673, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.948999999731313, "prefill_cuda_event_ms": 11.904000282287598, "kv_decode_ms": 626.4775999998164, "kv_decode_cuda_event_ms": 626.4105224609375, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 626.4775999998164, "cuda_event_ms": 626.4105224609375, "tokens_total": 64, "tokens_per_s": 102.15848100557587}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 404, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549342.1203673, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.948999999731313, "prefill_cuda_event_ms": 11.904000282287598, "kv_decode_ms": 626.4775999998164, "kv_decode_cuda_event_ms": 626.4105224609375, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 638.4265999995478, "cuda_event_ms": 638.3145227432251, "tokens_total": 73, "tokens_per_s": 114.3436066104572}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 405, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549342.7594526, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.900699999889184, "prefill_cuda_event_ms": 9.816032409667969, "kv_decode_ms": 601.0212000001047, "kv_decode_cuda_event_ms": 600.9804077148438, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 9.900699999889184, "cuda_event_ms": 9.816032409667969, "tokens_total": 9, "tokens_per_s": 909.0266344905648}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 406, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549342.7594526, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.900699999889184, "prefill_cuda_event_ms": 9.816032409667969, "kv_decode_ms": 601.0212000001047, "kv_decode_cuda_event_ms": 600.9804077148438, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 601.0212000001047, "cuda_event_ms": 600.9804077148438, "tokens_total": 64, "tokens_per_s": 106.48542846739657}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 407, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549342.7594526, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.900699999889184, "prefill_cuda_event_ms": 9.816032409667969, "kv_decode_ms": 601.0212000001047, "kv_decode_cuda_event_ms": 600.9804077148438, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 610.9218999999939, "cuda_event_ms": 610.7964401245117, "tokens_total": 73, "tokens_per_s": 119.49154220858792}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 408, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549343.3712022, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.624799999372044, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.81630000020232, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 6.624799999372044, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 150.9479531600635}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 409, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549343.3712022, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.624799999372044, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.81630000020232, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 174.81630000020232, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 366.0985846281264}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 410, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549343.3712022, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.624799999372044, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.81630000020232, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 181.44109999957436, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 358.2429780251138}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 411, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549343.5534515, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.690700000130164, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.37989999962156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.690700000130164, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 371.65049985194355}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 412, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549343.5534515, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.690700000130164, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.37989999962156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 174.37989999962156, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 367.0147763597691}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 413, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549343.5534515, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.690700000130164, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.37989999962156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 177.07059999975172, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 367.08521911650575}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 414, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549343.7312202, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.218699999706587, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.07049999960145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.218699999706587, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 310.68443784483145}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 415, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549343.7312202, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.218699999706587, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.07049999960145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 180.07049999960145, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 355.41635081893844}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 416, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549343.7312202, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.218699999706587, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.07049999960145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 183.28919999930804, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 354.63082385784537}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 417, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549343.9150033, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.019200000380806, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.15440000038507, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.019200000380806, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 331.2135664659088}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 418, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549343.9150033, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.019200000380806, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.15440000038507, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 173.15440000038507, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 369.6123228740227}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 419, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549343.9150033, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.019200000380806, "prefill_cuda_event_ms": null, "kv_decode_ms": 173.15440000038507, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 176.17360000076587, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 368.9542587522616}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 420, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549344.0915902, "prompt_tokens": 99, "prefill_ms": 481.7363, "prefill_cuda_event_ms": null, "kv_decode_ms": 2731.1083, "kv_decode_ms_equiv": 2731.1083, "kv_decode_ms_per_token": 42.6735671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3478.8886999995157, "ollama_total_duration_ms": 3453.3015, "ollama_load_ms": 224.3908, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 481.7363, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 205.50662260660033}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 421, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549344.0915902, "prompt_tokens": 99, "prefill_ms": 481.7363, "prefill_cuda_event_ms": null, "kv_decode_ms": 2731.1083, "kv_decode_ms_equiv": 2731.1083, "kv_decode_ms_per_token": 42.6735671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3478.8886999995157, "ollama_total_duration_ms": 3453.3015, "ollama_load_ms": 224.3908, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2731.1083, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.43371004364785}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 422, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549344.0915902, "prompt_tokens": 99, "prefill_ms": 481.7363, "prefill_cuda_event_ms": null, "kv_decode_ms": 2731.1083, "kv_decode_ms_equiv": 2731.1083, "kv_decode_ms_per_token": 42.6735671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3478.8886999995157, "ollama_total_duration_ms": 3453.3015, "ollama_load_ms": 224.3908, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3212.8446, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 50.733857466993584}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 423, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549347.5707357, "prompt_tokens": 99, "prefill_ms": 55.2612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2886.2963, "kv_decode_ms_equiv": 2886.2963, "kv_decode_ms_per_token": 45.0983796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3181.2613000001875, "ollama_total_duration_ms": 3178.7238, "ollama_load_ms": 226.4189, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.2612, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1791.4920414323249}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 424, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549347.5707357, "prompt_tokens": 99, "prefill_ms": 55.2612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2886.2963, "kv_decode_ms_equiv": 2886.2963, "kv_decode_ms_per_token": 45.0983796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3181.2613000001875, "ollama_total_duration_ms": 3178.7238, "ollama_load_ms": 226.4189, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2886.2963, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 22.173745640736882}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 425, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549347.5707357, "prompt_tokens": 99, "prefill_ms": 55.2612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2886.2963, "kv_decode_ms_equiv": 2886.2963, "kv_decode_ms_per_token": 45.0983796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3181.2613000001875, "ollama_total_duration_ms": 3178.7238, "ollama_load_ms": 226.4189, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2941.5575, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 55.4128212690046}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 426, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549350.7522209, "prompt_tokens": 99, "prefill_ms": 60.3593, "prefill_cuda_event_ms": null, "kv_decode_ms": 3309.4483, "kv_decode_ms_equiv": 3309.4483, "kv_decode_ms_per_token": 51.7101296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3637.1033999994324, "ollama_total_duration_ms": 3621.6726, "ollama_load_ms": 235.7621, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 60.3593, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1640.1780670087294}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 427, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549350.7522209, "prompt_tokens": 99, "prefill_ms": 60.3593, "prefill_cuda_event_ms": null, "kv_decode_ms": 3309.4483, "kv_decode_ms_equiv": 3309.4483, "kv_decode_ms_per_token": 51.7101296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3637.1033999994324, "ollama_total_duration_ms": 3621.6726, "ollama_load_ms": 235.7621, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3309.4483, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.338570721893433}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 428, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549350.7522209, "prompt_tokens": 99, "prefill_ms": 60.3593, "prefill_cuda_event_ms": null, "kv_decode_ms": 3309.4483, "kv_decode_ms_equiv": 3309.4483, "kv_decode_ms_per_token": 51.7101296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3637.1033999994324, "ollama_total_duration_ms": 3621.6726, "ollama_load_ms": 235.7621, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3369.8076, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 48.370714102490595}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 429, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549354.3895009, "prompt_tokens": 99, "prefill_ms": 61.8687, "prefill_cuda_event_ms": null, "kv_decode_ms": 3382.8876, "kv_decode_ms_equiv": 3382.8876, "kv_decode_ms_per_token": 52.85761875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3678.826100000151, "ollama_total_duration_ms": 3663.7361, "ollama_load_ms": 211.9906, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 61.8687, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1600.1629256797055}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 430, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549354.3895009, "prompt_tokens": 99, "prefill_ms": 61.8687, "prefill_cuda_event_ms": null, "kv_decode_ms": 3382.8876, "kv_decode_ms_equiv": 3382.8876, "kv_decode_ms_per_token": 52.85761875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3678.826100000151, "ollama_total_duration_ms": 3663.7361, "ollama_load_ms": 211.9906, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3382.8876, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.91874858626695}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 431, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549354.3895009, "prompt_tokens": 99, "prefill_ms": 61.8687, "prefill_cuda_event_ms": null, "kv_decode_ms": 3382.8876, "kv_decode_ms_equiv": 3382.8876, "kv_decode_ms_per_token": 52.85761875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3678.826100000151, "ollama_total_duration_ms": 3663.7361, "ollama_load_ms": 211.9906, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3444.7563, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 47.318296507651354}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 432, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549358.0685651, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 49.20490000040445, "prefill_cuda_event_ms": 48.707584381103516, "kv_decode_ms": 246.2386000006518, "kv_decode_cuda_event_ms": 246.17984008789062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 49.20490000040445, "cuda_event_ms": 48.707584381103516, "tokens_total": 9, "tokens_per_s": 182.90861275860783}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 433, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549358.0685651, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 49.20490000040445, "prefill_cuda_event_ms": 48.707584381103516, "kv_decode_ms": 246.2386000006518, "kv_decode_cuda_event_ms": 246.17984008789062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 246.2386000006518, "cuda_event_ms": 246.17984008789062, "tokens_total": 64, "tokens_per_s": 259.91050956198825}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 434, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549358.0685651, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 49.20490000040445, "prefill_cuda_event_ms": 48.707584381103516, "kv_decode_ms": 246.2386000006518, "kv_decode_cuda_event_ms": 246.17984008789062, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 295.44350000105624, "cuda_event_ms": 294.88742446899414, "tokens_total": 73, "tokens_per_s": 247.08616029710933}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 435, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549358.3693328, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.229100000178732, "prefill_cuda_event_ms": 5.169151782989502, "kv_decode_ms": 211.7207999999664, "kv_decode_cuda_event_ms": 211.65362548828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 5.229100000178732, "cuda_event_ms": 5.169151782989502, "tokens_total": 9, "tokens_per_s": 1721.1374805783744}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 436, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549358.3693328, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.229100000178732, "prefill_cuda_event_ms": 5.169151782989502, "kv_decode_ms": 211.7207999999664, "kv_decode_cuda_event_ms": 211.65362548828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 211.7207999999664, "cuda_event_ms": 211.65362548828125, "tokens_total": 64, "tokens_per_s": 302.28489595736534}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 437, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549358.3693328, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.229100000178732, "prefill_cuda_event_ms": 5.169151782989502, "kv_decode_ms": 211.7207999999664, "kv_decode_cuda_event_ms": 211.65362548828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 216.94990000014513, "cuda_event_ms": 216.82277727127075, "tokens_total": 73, "tokens_per_s": 336.4832157099458}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 438, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549358.5870936, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.522300000644464, "prefill_cuda_event_ms": 4.45747184753418, "kv_decode_ms": 230.39989999961108, "kv_decode_cuda_event_ms": 230.3231964111328, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 4.522300000644464, "cuda_event_ms": 4.45747184753418, "tokens_total": 9, "tokens_per_s": 1990.1377614747869}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 439, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549358.5870936, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.522300000644464, "prefill_cuda_event_ms": 4.45747184753418, "kv_decode_ms": 230.39989999961108, "kv_decode_cuda_event_ms": 230.3231964111328, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 230.39989999961108, "cuda_event_ms": 230.3231964111328, "tokens_total": 64, "tokens_per_s": 277.7778983415706}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 440, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549358.5870936, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.522300000644464, "prefill_cuda_event_ms": 4.45747184753418, "kv_decode_ms": 230.39989999961108, "kv_decode_cuda_event_ms": 230.3231964111328, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 234.92220000025554, "cuda_event_ms": 234.780668258667, "tokens_total": 73, "tokens_per_s": 310.74117303482}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 441, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549358.8237588, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.1373999997158535, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 215.60609999960434, "kv_decode_cuda_event_ms": 215.52537536621094, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 6.1373999997158535, "cuda_event_ms": 6.061056137084961, "tokens_total": 9, "tokens_per_s": 1466.4190048581936}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 442, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549358.8237588, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.1373999997158535, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 215.60609999960434, "kv_decode_cuda_event_ms": 215.52537536621094, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 215.60609999960434, "cuda_event_ms": 215.52537536621094, "tokens_total": 64, "tokens_per_s": 296.83761266549254}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 443, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549358.8237588, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.1373999997158535, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 215.60609999960434, "kv_decode_cuda_event_ms": 215.52537536621094, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 221.7434999993202, "cuda_event_ms": 221.5864315032959, "tokens_total": 73, "tokens_per_s": 329.2091989177757}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 444, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549359.0463417, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.540199999515607, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.68649999966146, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 8.540199999515607, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1990.5857006819776}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 445, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549359.0463417, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.540199999515607, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.68649999966146, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 179.68649999966146, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 356.17589524043586}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 446, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549359.0463417, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.540199999515607, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.68649999966146, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 188.22669999917707, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 430.33214735398394}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 447, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549359.2390285, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.202800000006391, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.13330000059068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 5.202800000006391, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3267.471361570523}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 448, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549359.2390285, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.202800000006391, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.13330000059068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 168.13330000059068, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 380.6503530221268}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 449, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549359.2390285, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.202800000006391, "prefill_cuda_event_ms": null, "kv_decode_ms": 168.13330000059068, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 173.33610000059707, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 467.30023347543295}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 450, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549359.4129236, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.100399999515503, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.29929999991145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.100399999515503, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4145.936982247755}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 451, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549359.4129236, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.100399999515503, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.29929999991145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 179.29929999991145, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 356.9450633662909}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 452, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549359.4129236, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.100399999515503, "prefill_cuda_event_ms": null, "kv_decode_ms": 179.29929999991145, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 183.39969999942696, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 441.65830151441406}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 453, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549359.597116, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.821400000764697, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.54980000022624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.821400000764697, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3525.9468198663703}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 454, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549359.597116, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.821400000764697, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.54980000022624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 175.54980000022624, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 364.56891434748155}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 455, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549359.597116, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.821400000764697, "prefill_cuda_event_ms": null, "kv_decode_ms": 175.54980000022624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 180.37120000099094, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 449.07390980131527}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 456, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549359.7779403, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 21.90950000021985, "prefill_cuda_event_ms": 21.47635269165039, "kv_decode_ms": 215.9480000000258, "kv_decode_cuda_event_ms": 215.91139221191406, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 21.90950000021985, "cuda_event_ms": 21.47635269165039, "tokens_total": 1, "tokens_per_s": 45.642301284372785}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 457, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549359.7779403, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.90950000021985, "prefill_cuda_event_ms": 21.47635269165039, "kv_decode_ms": 215.9480000000258, "kv_decode_cuda_event_ms": 215.91139221191406, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 215.9480000000258, "cuda_event_ms": 215.91139221191406, "tokens_total": 64, "tokens_per_s": 296.3676440624241}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 458, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549359.7779403, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.90950000021985, "prefill_cuda_event_ms": 21.47635269165039, "kv_decode_ms": 215.9480000000258, "kv_decode_cuda_event_ms": 215.91139221191406, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 237.85750000024564, "cuda_event_ms": 237.38774490356445, "tokens_total": 65, "tokens_per_s": 273.272862953377}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 459, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549360.0173848, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.393800000296324, "prefill_cuda_event_ms": 6.206463813781738, "kv_decode_ms": 219.10079999997834, "kv_decode_cuda_event_ms": 219.0417938232422, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 6.393800000296324, "cuda_event_ms": 6.206463813781738, "tokens_total": 1, "tokens_per_s": 156.40151395940669}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 460, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549360.0173848, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.393800000296324, "prefill_cuda_event_ms": 6.206463813781738, "kv_decode_ms": 219.10079999997834, "kv_decode_cuda_event_ms": 219.0417938232422, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 219.10079999997834, "cuda_event_ms": 219.0417938232422, "tokens_total": 64, "tokens_per_s": 292.1029955162479}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 461, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549360.0173848, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.393800000296324, "prefill_cuda_event_ms": 6.206463813781738, "kv_decode_ms": 219.10079999997834, "kv_decode_cuda_event_ms": 219.0417938232422, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 225.49460000027466, "cuda_event_ms": 225.24825763702393, "tokens_total": 65, "tokens_per_s": 288.25523981470434}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 462, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549360.2442071, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.424800000379037, "prefill_cuda_event_ms": 4.312064170837402, "kv_decode_ms": 207.07320000019536, "kv_decode_cuda_event_ms": 207.0128631591797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.424800000379037, "cuda_event_ms": 4.312064170837402, "tokens_total": 1, "tokens_per_s": 225.99891518584752}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 463, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549360.2442071, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.424800000379037, "prefill_cuda_event_ms": 4.312064170837402, "kv_decode_ms": 207.07320000019536, "kv_decode_cuda_event_ms": 207.0128631591797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 207.07320000019536, "cuda_event_ms": 207.0128631591797, "tokens_total": 64, "tokens_per_s": 309.06944983677084}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 464, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549360.2442071, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.424800000379037, "prefill_cuda_event_ms": 4.312064170837402, "kv_decode_ms": 207.07320000019536, "kv_decode_cuda_event_ms": 207.0128631591797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 211.4980000005744, "cuda_event_ms": 211.3249273300171, "tokens_total": 65, "tokens_per_s": 307.3315114082567}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 465, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549360.4564764, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.135199999836914, "prefill_cuda_event_ms": 4.060160160064697, "kv_decode_ms": 233.5998999997173, "kv_decode_cuda_event_ms": 233.5529022216797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.135199999836914, "cuda_event_ms": 4.060160160064697, "tokens_total": 1, "tokens_per_s": 241.82627201572802}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 466, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549360.4564764, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.135199999836914, "prefill_cuda_event_ms": 4.060160160064697, "kv_decode_ms": 233.5998999997173, "kv_decode_cuda_event_ms": 233.5529022216797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 233.5998999997173, "cuda_event_ms": 233.5529022216797, "tokens_total": 64, "tokens_per_s": 273.97272002290003}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 467, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549360.4564764, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.135199999836914, "prefill_cuda_event_ms": 4.060160160064697, "kv_decode_ms": 233.5998999997173, "kv_decode_cuda_event_ms": 233.5529022216797, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 237.73509999955422, "cuda_event_ms": 237.61306238174438, "tokens_total": 65, "tokens_per_s": 273.4135598829196}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 468, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549360.6949446, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 76.06029999988095, "prefill_cuda_event_ms": null, "kv_decode_ms": 1410.0413999994998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 76.06029999988095, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 223.50687546626304}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 469, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549360.6949446, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 76.06029999988095, "prefill_cuda_event_ms": null, "kv_decode_ms": 1410.0413999994998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1410.0413999994998, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 45.388738231390015}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 470, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549360.6949446, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 76.06029999988095, "prefill_cuda_event_ms": null, "kv_decode_ms": 1410.0413999994998, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1486.1016999993808, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 54.50501806170718}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 471, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549362.1824353, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.7519999997603, "prefill_cuda_event_ms": null, "kv_decode_ms": 1085.0663000001077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 26.7519999997603, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 635.4665071827274}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 472, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549362.1824353, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.7519999997603, "prefill_cuda_event_ms": null, "kv_decode_ms": 1085.0663000001077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1085.0663000001077, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.98257092676609}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 473, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549362.1824353, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.7519999997603, "prefill_cuda_event_ms": null, "kv_decode_ms": 1085.0663000001077, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1111.818299999868, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 72.85363084958182}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 474, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549363.294912, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 28.852700000243203, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.6718000000546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 28.852700000243203, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 589.199624293626}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 475, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549363.294912, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 28.852700000243203, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.6718000000546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1150.6718000000546, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 55.619682345562794}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 476, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549363.294912, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 28.852700000243203, "prefill_cuda_event_ms": null, "kv_decode_ms": 1150.6718000000546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1179.5245000002978, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 68.67174018002979}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 477, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549364.474979, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.986099999703583, "prefill_cuda_event_ms": null, "kv_decode_ms": 1077.4893999996493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 26.986099999703583, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 629.9539392571261}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 478, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549364.474979, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.986099999703583, "prefill_cuda_event_ms": null, "kv_decode_ms": 1077.4893999996493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1077.4893999996493, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.39733606662008}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 479, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549364.474979, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.986099999703583, "prefill_cuda_event_ms": null, "kv_decode_ms": 1077.4893999996493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1104.475499999353, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 73.33797807198752}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 480, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549365.5802603, "prompt_tokens": 46, "prefill_ms": 21.0011, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9446, "kv_decode_ms_equiv": 730.9446, "kv_decode_ms_per_token": 11.421009375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 7138.873599999897, "ollama_total_duration_ms": 7118.4994, "ollama_load_ms": 6312.0005, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 21.0011, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2190.3614572570004}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 481, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549365.5802603, "prompt_tokens": 46, "prefill_ms": 21.0011, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9446, "kv_decode_ms_equiv": 730.9446, "kv_decode_ms_per_token": 11.421009375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 7138.873599999897, "ollama_total_duration_ms": 7118.4994, "ollama_load_ms": 6312.0005, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 730.9446, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.55793530727226}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 482, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549365.5802603, "prompt_tokens": 46, "prefill_ms": 21.0011, "prefill_cuda_event_ms": null, "kv_decode_ms": 730.9446, "kv_decode_ms_equiv": 730.9446, "kv_decode_ms_per_token": 11.421009375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 7138.873599999897, "ollama_total_duration_ms": 7118.4994, "ollama_load_ms": 6312.0005, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 751.9457, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 146.2871587669163}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 483, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549372.7193751, "prompt_tokens": 46, "prefill_ms": 11.8437, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.1092, "kv_decode_ms_equiv": 734.1092, "kv_decode_ms_per_token": 11.47045625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 948.0447999994794, "ollama_total_duration_ms": 935.1567, "ollama_load_ms": 128.2088, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.8437, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3883.9214096946052}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 484, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549372.7193751, "prompt_tokens": 46, "prefill_ms": 11.8437, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.1092, "kv_decode_ms_equiv": 734.1092, "kv_decode_ms_per_token": 11.47045625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 948.0447999994794, "ollama_total_duration_ms": 935.1567, "ollama_load_ms": 128.2088, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 734.1092, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.18049031397508}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 485, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549372.7193751, "prompt_tokens": 46, "prefill_ms": 11.8437, "prefill_cuda_event_ms": null, "kv_decode_ms": 734.1092, "kv_decode_ms_equiv": 734.1092, "kv_decode_ms_per_token": 11.47045625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 948.0447999994794, "ollama_total_duration_ms": 935.1567, "ollama_load_ms": 128.2088, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 745.9529, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.46239340312238}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 486, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549373.6677334, "prompt_tokens": 46, "prefill_ms": 12.3444, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9506, "kv_decode_ms_equiv": 724.9506, "kv_decode_ms_per_token": 11.327353125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.6408000005467, "ollama_total_duration_ms": 917.7789, "ollama_load_ms": 128.5332, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.3444, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3726.3860535951526}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 487, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549373.6677334, "prompt_tokens": 46, "prefill_ms": 12.3444, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9506, "kv_decode_ms_equiv": 724.9506, "kv_decode_ms_per_token": 11.327353125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.6408000005467, "ollama_total_duration_ms": 917.7789, "ollama_load_ms": 128.5332, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 724.9506, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.28187741344031}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 488, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549373.6677334, "prompt_tokens": 46, "prefill_ms": 12.3444, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.9506, "kv_decode_ms_equiv": 724.9506, "kv_decode_ms_per_token": 11.327353125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 945.6408000005467, "ollama_total_duration_ms": 917.7789, "ollama_load_ms": 128.5332, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 737.295, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.194013251141}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 489, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549374.6135924, "prompt_tokens": 46, "prefill_ms": 12.3269, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.7451, "kv_decode_ms_equiv": 729.7451, "kv_decode_ms_per_token": 11.4022671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 961.7996999995739, "ollama_total_duration_ms": 934.4265, "ollama_load_ms": 140.4786, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.3269, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3731.676252748055}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 490, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549374.6135924, "prompt_tokens": 46, "prefill_ms": 12.3269, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.7451, "kv_decode_ms_equiv": 729.7451, "kv_decode_ms_per_token": 11.4022671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 961.7996999995739, "ollama_total_duration_ms": 934.4265, "ollama_load_ms": 140.4786, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 729.7451, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.70185644274967}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 491, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549374.6135924, "prompt_tokens": 46, "prefill_ms": 12.3269, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.7451, "kv_decode_ms_equiv": 729.7451, "kv_decode_ms_per_token": 11.4022671875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 961.7996999995739, "ollama_total_duration_ms": 934.4265, "ollama_load_ms": 140.4786, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 742.072, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 148.23359458381398}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 492, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549375.5756257, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 81.65780000035738, "prefill_cuda_event_ms": null, "kv_decode_ms": 903.1385999996928, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 81.65780000035738, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 12.246227549549747}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 493, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549375.5756257, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 81.65780000035738, "prefill_cuda_event_ms": null, "kv_decode_ms": 903.1385999996928, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 903.1385999996928, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.86398477489698}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 494, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549375.5756257, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 81.65780000035738, "prefill_cuda_event_ms": null, "kv_decode_ms": 903.1385999996928, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 984.7964000000502, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 66.00349067075864}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 495, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549376.5612173, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.727199999972072, "prefill_cuda_event_ms": null, "kv_decode_ms": 812.3264000005292, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.727199999972072, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 78.57187755375844}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 496, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549376.5612173, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.727199999972072, "prefill_cuda_event_ms": null, "kv_decode_ms": 812.3264000005292, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 812.3264000005292, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 78.78606432089158}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 497, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549376.5612173, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.727199999972072, "prefill_cuda_event_ms": null, "kv_decode_ms": 812.3264000005292, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 825.0536000005013, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 78.78276029576806}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 498, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549377.3867478, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.891399999309215, "prefill_cuda_event_ms": null, "kv_decode_ms": 832.2553999996671, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 10.891399999309215, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 91.81556090708493}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 499, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549377.3867478, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.891399999309215, "prefill_cuda_event_ms": null, "kv_decode_ms": 832.2553999996671, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 832.2553999996671, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.89947100376351}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 500, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549377.3867478, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.891399999309215, "prefill_cuda_event_ms": null, "kv_decode_ms": 832.2553999996671, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 843.1467999989763, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 77.09215050105026}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 501, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549378.230281, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 13.684200000170677, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.809400000202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.684200000170677, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 73.07697928907261}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 502, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549378.230281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.684200000170677, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.809400000202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 981.809400000202, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.18576823565432}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 503, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549378.230281, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 13.684200000170677, "prefill_cuda_event_ms": null, "kv_decode_ms": 981.809400000202, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 995.4936000003727, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 65.29424197199828}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 504, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549379.22619, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 55.50940000011906, "prefill_cuda_event_ms": null, "kv_decode_ms": 2565.381000000343, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 55.50940000011906, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 162.13470150966677}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 505, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549379.22619, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 55.50940000011906, "prefill_cuda_event_ms": null, "kv_decode_ms": 2565.381000000343, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2565.381000000343, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.947561395360548}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 506, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549379.22619, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 55.50940000011906, "prefill_cuda_event_ms": null, "kv_decode_ms": 2565.381000000343, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2620.8904000004623, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 27.85312960816184}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 507, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549381.8477216, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 42.39439999946626, "prefill_cuda_event_ms": null, "kv_decode_ms": 2620.423000000301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 42.39439999946626, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 212.29218953713956}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 508, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549381.8477216, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 42.39439999946626, "prefill_cuda_event_ms": null, "kv_decode_ms": 2620.423000000301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2620.423000000301, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.42353772653982}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 509, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549381.8477216, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 42.39439999946626, "prefill_cuda_event_ms": null, "kv_decode_ms": 2620.423000000301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2662.817399999767, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 27.414572249680504}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 510, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549384.5115612, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 37.7386000000115, "prefill_cuda_event_ms": null, "kv_decode_ms": 2262.7880999998524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 37.7386000000115, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 238.4826146173217}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 511, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549384.5115612, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.7386000000115, "prefill_cuda_event_ms": null, "kv_decode_ms": 2262.7880999998524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2262.7880999998524, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.283691256819044}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 512, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549384.5115612, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 37.7386000000115, "prefill_cuda_event_ms": null, "kv_decode_ms": 2262.7880999998524, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2300.526699999864, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 31.73186383796559}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 513, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549386.8126614, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 31.608700000106182, "prefill_cuda_event_ms": null, "kv_decode_ms": 1358.6737000005087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 31.608700000106182, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 284.7317352491487}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 514, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549386.8126614, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.608700000106182, "prefill_cuda_event_ms": null, "kv_decode_ms": 1358.6737000005087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1358.6737000005087, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.104761062185894}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 515, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549386.8126614, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 31.608700000106182, "prefill_cuda_event_ms": null, "kv_decode_ms": 1358.6737000005087, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1390.2824000006149, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.507317937684974}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 516, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549388.2035859, "prompt_tokens": 92, "prefill_ms": 851.1321, "prefill_cuda_event_ms": null, "kv_decode_ms": 2706.9835, "kv_decode_ms_equiv": 2706.9835, "kv_decode_ms_per_token": 42.2966171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 14769.565199999306, "ollama_total_duration_ms": 14662.0467, "ollama_load_ms": 11028.3146, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 851.1321, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 108.0913291838012}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 517, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549388.2035859, "prompt_tokens": 92, "prefill_ms": 851.1321, "prefill_cuda_event_ms": null, "kv_decode_ms": 2706.9835, "kv_decode_ms_equiv": 2706.9835, "kv_decode_ms_per_token": 42.2966171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14769.565199999306, "ollama_total_duration_ms": 14662.0467, "ollama_load_ms": 11028.3146, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2706.9835, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 23.64255267902446}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 518, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549388.2035859, "prompt_tokens": 92, "prefill_ms": 851.1321, "prefill_cuda_event_ms": null, "kv_decode_ms": 2706.9835, "kv_decode_ms_equiv": 2706.9835, "kv_decode_ms_per_token": 42.2966171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14769.565199999306, "ollama_total_duration_ms": 14662.0467, "ollama_load_ms": 11028.3146, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3558.1156, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 43.84343218078693}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 519, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549402.9741502, "prompt_tokens": 92, "prefill_ms": 51.2921, "prefill_cuda_event_ms": null, "kv_decode_ms": 2987.5386, "kv_decode_ms_equiv": 2987.5386, "kv_decode_ms_per_token": 46.680290625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3336.295500000233, "ollama_total_duration_ms": 3311.696, "ollama_load_ms": 251.224, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 51.2921, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1793.648534569651}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 520, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549402.9741502, "prompt_tokens": 92, "prefill_ms": 51.2921, "prefill_cuda_event_ms": null, "kv_decode_ms": 2987.5386, "kv_decode_ms_equiv": 2987.5386, "kv_decode_ms_per_token": 46.680290625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3336.295500000233, "ollama_total_duration_ms": 3311.696, "ollama_load_ms": 251.224, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2987.5386, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.422317355163212}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 521, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549402.9741502, "prompt_tokens": 92, "prefill_ms": 51.2921, "prefill_cuda_event_ms": null, "kv_decode_ms": 2987.5386, "kv_decode_ms_equiv": 2987.5386, "kv_decode_ms_per_token": 46.680290625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3336.295500000233, "ollama_total_duration_ms": 3311.696, "ollama_load_ms": 251.224, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3038.8307, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 51.33553507933166}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 522, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549406.310653, "prompt_tokens": 92, "prefill_ms": 53.1387, "prefill_cuda_event_ms": null, "kv_decode_ms": 3334.6758, "kv_decode_ms_equiv": 3334.6758, "kv_decode_ms_per_token": 52.104309375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3655.3383999998914, "ollama_total_duration_ms": 3637.1405, "ollama_load_ms": 222.205, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.1387, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1731.318229463649}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 523, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549406.310653, "prompt_tokens": 92, "prefill_ms": 53.1387, "prefill_cuda_event_ms": null, "kv_decode_ms": 3334.6758, "kv_decode_ms_equiv": 3334.6758, "kv_decode_ms_per_token": 52.104309375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3655.3383999998914, "ollama_total_duration_ms": 3637.1405, "ollama_load_ms": 222.205, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3334.6758, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.192270504976825}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 524, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549406.310653, "prompt_tokens": 92, "prefill_ms": 53.1387, "prefill_cuda_event_ms": null, "kv_decode_ms": 3334.6758, "kv_decode_ms_equiv": 3334.6758, "kv_decode_ms_per_token": 52.104309375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3655.3383999998914, "ollama_total_duration_ms": 3637.1405, "ollama_load_ms": 222.205, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3387.8145, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 46.04738541617317}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 525, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549409.966211, "prompt_tokens": 92, "prefill_ms": 62.7776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3388.969, "kv_decode_ms_equiv": 3388.969, "kv_decode_ms_per_token": 52.952640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3705.443299999388, "ollama_total_duration_ms": 3686.4603, "ollama_load_ms": 217.4342, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 62.7776, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1465.4908757263736}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 526, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549409.966211, "prompt_tokens": 92, "prefill_ms": 62.7776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3388.969, "kv_decode_ms_equiv": 3388.969, "kv_decode_ms_per_token": 52.952640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3705.443299999388, "ollama_total_duration_ms": 3686.4603, "ollama_load_ms": 217.4342, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3388.969, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 18.884799477363174}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 527, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549409.966211, "prompt_tokens": 92, "prefill_ms": 62.7776, "prefill_cuda_event_ms": null, "kv_decode_ms": 3388.969, "kv_decode_ms_equiv": 3388.969, "kv_decode_ms_per_token": 52.952640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3705.443299999388, "ollama_total_duration_ms": 3686.4603, "ollama_load_ms": 217.4342, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3451.7466, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 45.194511091862886}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 528, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549413.6719484, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 15.895499999714957, "prefill_cuda_event_ms": 15.747072219848633, "kv_decode_ms": 489.3696999997701, "kv_decode_cuda_event_ms": 489.2836608886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 15.895499999714957, "cuda_event_ms": 15.747072219848633, "tokens_total": 1, "tokens_per_s": 62.910886730076584}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 529, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549413.6719484, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.895499999714957, "prefill_cuda_event_ms": 15.747072219848633, "kv_decode_ms": 489.3696999997701, "kv_decode_cuda_event_ms": 489.2836608886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 489.3696999997701, "cuda_event_ms": 489.2836608886719, "tokens_total": 64, "tokens_per_s": 130.78047128792417}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 530, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549413.6719484, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 15.895499999714957, "prefill_cuda_event_ms": 15.747072219848633, "kv_decode_ms": 489.3696999997701, "kv_decode_cuda_event_ms": 489.2836608886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 505.26519999948505, "cuda_event_ms": 505.0307331085205, "tokens_total": 65, "tokens_per_s": 128.64531339198948}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 531, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549414.1803415, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.104399999865564, "prefill_cuda_event_ms": 8.009471893310547, "kv_decode_ms": 487.11739999998827, "kv_decode_cuda_event_ms": 487.044189453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 8.104399999865564, "cuda_event_ms": 8.009471893310547, "tokens_total": 1, "tokens_per_s": 123.38976358725976}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 532, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549414.1803415, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.104399999865564, "prefill_cuda_event_ms": 8.009471893310547, "kv_decode_ms": 487.11739999998827, "kv_decode_cuda_event_ms": 487.044189453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 487.11739999998827, "cuda_event_ms": 487.044189453125, "tokens_total": 64, "tokens_per_s": 131.38516505466967}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 533, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549414.1803415, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.104399999865564, "prefill_cuda_event_ms": 8.009471893310547, "kv_decode_ms": 487.11739999998827, "kv_decode_cuda_event_ms": 487.044189453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 495.22179999985383, "cuda_event_ms": 495.05366134643555, "tokens_total": 65, "tokens_per_s": 131.25431877195064}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 534, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549414.6764483, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.980200000223704, "prefill_cuda_event_ms": 8.92198371887207, "kv_decode_ms": 366.46440000004077, "kv_decode_cuda_event_ms": 366.39227294921875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 8.980200000223704, "cuda_event_ms": 8.92198371887207, "tokens_total": 1, "tokens_per_s": 111.35609451627906}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 535, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549414.6764483, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.980200000223704, "prefill_cuda_event_ms": 8.92198371887207, "kv_decode_ms": 366.46440000004077, "kv_decode_cuda_event_ms": 366.39227294921875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 366.46440000004077, "cuda_event_ms": 366.39227294921875, "tokens_total": 64, "tokens_per_s": 174.64179330923517}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 536, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549414.6764483, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.980200000223704, "prefill_cuda_event_ms": 8.92198371887207, "kv_decode_ms": 366.46440000004077, "kv_decode_cuda_event_ms": 366.39227294921875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 375.4446000002645, "cuda_event_ms": 375.3142566680908, "tokens_total": 65, "tokens_per_s": 173.1280726902297}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 537, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549415.0527043, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.6818000007770024, "prefill_cuda_event_ms": 3.639296054840088, "kv_decode_ms": 192.14160000046832, "kv_decode_cuda_event_ms": 192.11663818359375, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 3.6818000007770024, "cuda_event_ms": 3.639296054840088, "tokens_total": 1, "tokens_per_s": 271.6062794798635}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 538, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549415.0527043, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.6818000007770024, "prefill_cuda_event_ms": 3.639296054840088, "kv_decode_ms": 192.14160000046832, "kv_decode_cuda_event_ms": 192.11663818359375, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 192.14160000046832, "cuda_event_ms": 192.11663818359375, "tokens_total": 64, "tokens_per_s": 333.08768116765975}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 539, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549415.0527043, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.6818000007770024, "prefill_cuda_event_ms": 3.639296054840088, "kv_decode_ms": 192.14160000046832, "kv_decode_cuda_event_ms": 192.11663818359375, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 195.82340000124532, "cuda_event_ms": 195.75593423843384, "tokens_total": 65, "tokens_per_s": 331.9317303222528}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 540, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549415.2511222, "prompt_tokens": 18, "prefill_ms": 11.9924, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.1509, "kv_decode_ms_equiv": 273.2252235294118, "kv_decode_ms_per_token": 4.269144117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3286.222899999302, "ollama_total_duration_ms": 3243.9579, "ollama_load_ms": 3040.3815, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 11.9924, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1500.9506020479637}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 541, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549415.2511222, "prompt_tokens": 18, "prefill_ms": 11.9924, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.1509, "kv_decode_ms_equiv": 273.2252235294118, "kv_decode_ms_per_token": 4.269144117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3286.222899999302, "ollama_total_duration_ms": 3243.9579, "ollama_load_ms": 3040.3815, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 273.2252235294118, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 234.23898852849}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 542, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549415.2511222, "prompt_tokens": 18, "prefill_ms": 11.9924, "prefill_cuda_event_ms": null, "kv_decode_ms": 145.1509, "kv_decode_ms_equiv": 273.2252235294118, "kv_decode_ms_per_token": 4.269144117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3286.222899999302, "ollama_total_duration_ms": 3243.9579, "ollama_load_ms": 3040.3815, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 285.21762352941175, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 287.4997659166181}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 543, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549418.5374708, "prompt_tokens": 18, "prefill_ms": 4.9448, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.5965, "kv_decode_ms_equiv": 266.5345882352941, "kv_decode_ms_per_token": 4.1646029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 346.3541000000987, "ollama_total_duration_ms": 319.3612, "ollama_load_ms": 147.716, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9448, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3640.187671897751}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 544, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549418.5374708, "prompt_tokens": 18, "prefill_ms": 4.9448, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.5965, "kv_decode_ms_equiv": 266.5345882352941, "kv_decode_ms_per_token": 4.1646029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 346.3541000000987, "ollama_total_duration_ms": 319.3612, "ollama_load_ms": 147.716, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 266.5345882352941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 240.11892949331374}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 545, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549418.5374708, "prompt_tokens": 18, "prefill_ms": 4.9448, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.5965, "kv_decode_ms_equiv": 266.5345882352941, "kv_decode_ms_per_token": 4.1646029411764705, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 346.3541000000987, "ollama_total_duration_ms": 319.3612, "ollama_load_ms": 147.716, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 271.4793882352941, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 302.0487136538326}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 546, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549418.884012, "prompt_tokens": 18, "prefill_ms": 4.6194, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2209, "kv_decode_ms_equiv": 265.8275764705882, "kv_decode_ms_per_token": 4.153555882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.61250000054133, "ollama_total_duration_ms": 317.49, "ollama_load_ms": 153.3438, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.6194, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3896.6099493440706}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 547, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549418.884012, "prompt_tokens": 18, "prefill_ms": 4.6194, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2209, "kv_decode_ms_equiv": 265.8275764705882, "kv_decode_ms_per_token": 4.153555882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.61250000054133, "ollama_total_duration_ms": 317.49, "ollama_load_ms": 153.3438, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 265.8275764705882, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 240.7575649213396}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 548, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549418.884012, "prompt_tokens": 18, "prefill_ms": 4.6194, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.2209, "kv_decode_ms_equiv": 265.8275764705882, "kv_decode_ms_per_token": 4.153555882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 342.61250000054133, "ollama_total_duration_ms": 317.49, "ollama_load_ms": 153.3438, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 270.4469764705882, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 303.2017627637176}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 549, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549419.2268221, "prompt_tokens": 18, "prefill_ms": 5.0108, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.271, "kv_decode_ms_equiv": 262.1571764705882, "kv_decode_ms_per_token": 4.09620588235294, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.9557000004788, "ollama_total_duration_ms": 327.6209, "ollama_load_ms": 148.0937, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 5.0108, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3592.2407599584903}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 550, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549419.2268221, "prompt_tokens": 18, "prefill_ms": 5.0108, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.271, "kv_decode_ms_equiv": 262.1571764705882, "kv_decode_ms_per_token": 4.09620588235294, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.9557000004788, "ollama_total_duration_ms": 327.6209, "ollama_load_ms": 148.0937, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 262.1571764705882, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 244.1283540722764}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 551, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549419.2268221, "prompt_tokens": 18, "prefill_ms": 5.0108, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.271, "kv_decode_ms_equiv": 262.1571764705882, "kv_decode_ms_per_token": 4.09620588235294, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.9557000004788, "ollama_total_duration_ms": 327.6209, "ollama_load_ms": 148.0937, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 267.1679764705882, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 306.9230118192221}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 552, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549419.5859637, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 26.20759999990696, "prefill_cuda_event_ms": 26.03727912902832, "kv_decode_ms": 470.797500000117, "kv_decode_cuda_event_ms": 470.7502746582031, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 26.20759999990696, "cuda_event_ms": 26.03727912902832, "tokens_total": 17, "tokens_per_s": 648.6667989461206}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 553, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549419.5859637, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.20759999990696, "prefill_cuda_event_ms": 26.03727912902832, "kv_decode_ms": 470.797500000117, "kv_decode_cuda_event_ms": 470.7502746582031, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 470.797500000117, "cuda_event_ms": 470.7502746582031, "tokens_total": 64, "tokens_per_s": 135.93954938160059}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 554, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549419.5859637, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 26.20759999990696, "prefill_cuda_event_ms": 26.03727912902832, "kv_decode_ms": 470.797500000117, "kv_decode_cuda_event_ms": 470.7502746582031, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 497.00510000002396, "cuda_event_ms": 496.78755378723145, "tokens_total": 81, "tokens_per_s": 162.97619481167516}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 555, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549420.087799, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.535699999905773, "prefill_cuda_event_ms": 4.481023788452148, "kv_decode_ms": 216.6059999999561, "kv_decode_cuda_event_ms": 216.57498168945312, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 4.535699999905773, "cuda_event_ms": 4.481023788452148, "tokens_total": 17, "tokens_per_s": 3748.043301001646}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 556, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549420.087799, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.535699999905773, "prefill_cuda_event_ms": 4.481023788452148, "kv_decode_ms": 216.6059999999561, "kv_decode_cuda_event_ms": 216.57498168945312, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 216.6059999999561, "cuda_event_ms": 216.57498168945312, "tokens_total": 64, "tokens_per_s": 295.46734624162286}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 557, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549420.087799, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.535699999905773, "prefill_cuda_event_ms": 4.481023788452148, "kv_decode_ms": 216.6059999999561, "kv_decode_cuda_event_ms": 216.57498168945312, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 221.14169999986188, "cuda_event_ms": 221.05600547790527, "tokens_total": 81, "tokens_per_s": 366.28098635422714}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 558, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549420.309618, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.7168999999485095, "prefill_cuda_event_ms": 4.669312000274658, "kv_decode_ms": 349.95730000082403, "kv_decode_cuda_event_ms": 349.9294738769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 4.7168999999485095, "cuda_event_ms": 4.669312000274658, "tokens_total": 17, "tokens_per_s": 3604.061989905568}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 559, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549420.309618, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.7168999999485095, "prefill_cuda_event_ms": 4.669312000274658, "kv_decode_ms": 349.95730000082403, "kv_decode_cuda_event_ms": 349.9294738769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 349.95730000082403, "cuda_event_ms": 349.9294738769531, "tokens_total": 64, "tokens_per_s": 182.8794541501186}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 560, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549420.309618, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.7168999999485095, "prefill_cuda_event_ms": 4.669312000274658, "kv_decode_ms": 349.95730000082403, "kv_decode_cuda_event_ms": 349.9294738769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 354.67420000077254, "cuda_event_ms": 354.5987858772278, "tokens_total": 81, "tokens_per_s": 228.37860774711993}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 561, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549420.664913, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.001000000243948, "prefill_cuda_event_ms": 3.960832118988037, "kv_decode_ms": 242.78030000004946, "kv_decode_cuda_event_ms": 242.7535400390625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 4.001000000243948, "cuda_event_ms": 3.960832118988037, "tokens_total": 17, "tokens_per_s": 4248.9377652995445}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 562, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549420.664913, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.001000000243948, "prefill_cuda_event_ms": 3.960832118988037, "kv_decode_ms": 242.78030000004946, "kv_decode_cuda_event_ms": 242.7535400390625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 242.78030000004946, "cuda_event_ms": 242.7535400390625, "tokens_total": 64, "tokens_per_s": 263.61282196284856}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 563, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549420.664913, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.001000000243948, "prefill_cuda_event_ms": 3.960832118988037, "kv_decode_ms": 242.78030000004946, "kv_decode_cuda_event_ms": 242.7535400390625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 246.7813000002934, "cuda_event_ms": 246.71437215805054, "tokens_total": 81, "tokens_per_s": 328.2258420711119}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 564, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549420.9123523, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 39.69830000005459, "prefill_cuda_event_ms": null, "kv_decode_ms": 2250.44650000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 39.69830000005459, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.189995541336152}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 565, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549420.9123523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.69830000005459, "prefill_cuda_event_ms": null, "kv_decode_ms": 2250.44650000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2250.44650000018, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.438800922392456}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 566, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549420.9123523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.69830000005459, "prefill_cuda_event_ms": null, "kv_decode_ms": 2250.44650000018, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2290.1448000002347, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 28.382484810564527}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 567, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549423.2032142, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 36.21249999923748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2248.8133999995625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 36.21249999923748, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 27.61477390462014}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 568, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549423.2032142, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.21249999923748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2248.8133999995625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2248.8133999995625, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.45945332770271}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 569, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549423.2032142, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 36.21249999923748, "prefill_cuda_event_ms": null, "kv_decode_ms": 2248.8133999995625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2285.0258999988, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 28.44606706647576}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 570, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549425.488745, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 34.88329999981943, "prefill_cuda_event_ms": null, "kv_decode_ms": 2018.0080999998609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 34.88329999981943, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.667012582100217}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 571, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549425.488745, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.88329999981943, "prefill_cuda_event_ms": null, "kv_decode_ms": 2018.0080999998609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2018.0080999998609, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.714441582273338}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 572, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549425.488745, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.88329999981943, "prefill_cuda_event_ms": null, "kv_decode_ms": 2018.0080999998609, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2052.8913999996803, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.662658823554974}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 573, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549427.5421762, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 39.73909999967873, "prefill_cuda_event_ms": null, "kv_decode_ms": 2049.4099000006827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 39.73909999967873, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.164133058073396}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 574, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549427.5421762, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.73909999967873, "prefill_cuda_event_ms": null, "kv_decode_ms": 2049.4099000006827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2049.4099000006827, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.2285014334998}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 575, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549427.5421762, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 39.73909999967873, "prefill_cuda_event_ms": null, "kv_decode_ms": 2049.4099000006827, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2089.1490000003614, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.11314702780355}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 576, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549429.6319065, "prompt_tokens": 26, "prefill_ms": 118.1798, "prefill_cuda_event_ms": null, "kv_decode_ms": 454.8491, "kv_decode_ms_equiv": 454.8491, "kv_decode_ms_per_token": 7.1070171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 794.248900000639, "ollama_total_duration_ms": 761.2347, "ollama_load_ms": 148.4559, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 118.1798, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 220.0037569872347}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 577, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549429.6319065, "prompt_tokens": 26, "prefill_ms": 118.1798, "prefill_cuda_event_ms": null, "kv_decode_ms": 454.8491, "kv_decode_ms_equiv": 454.8491, "kv_decode_ms_per_token": 7.1070171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 794.248900000639, "ollama_total_duration_ms": 761.2347, "ollama_load_ms": 148.4559, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 454.8491, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 140.70600557415634}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 578, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549429.6319065, "prompt_tokens": 26, "prefill_ms": 118.1798, "prefill_cuda_event_ms": null, "kv_decode_ms": 454.8491, "kv_decode_ms_equiv": 454.8491, "kv_decode_ms_per_token": 7.1070171875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 794.248900000639, "ollama_total_duration_ms": 761.2347, "ollama_load_ms": 148.4559, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 573.0289, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 157.0601412947933}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 579, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549430.426332, "prompt_tokens": 26, "prefill_ms": 4.9515, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.8778, "kv_decode_ms_equiv": 262.8778, "kv_decode_ms_per_token": 4.107465625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 484.2605999992884, "ollama_total_duration_ms": 464.7239, "ollama_load_ms": 149.0584, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.9515, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5250.934060385742}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 580, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549430.426332, "prompt_tokens": 26, "prefill_ms": 4.9515, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.8778, "kv_decode_ms_equiv": 262.8778, "kv_decode_ms_per_token": 4.107465625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 484.2605999992884, "ollama_total_duration_ms": 464.7239, "ollama_load_ms": 149.0584, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 262.8778, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.45912815764586}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 581, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549430.426332, "prompt_tokens": 26, "prefill_ms": 4.9515, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.8778, "kv_decode_ms_equiv": 262.8778, "kv_decode_ms_per_token": 4.107465625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 484.2605999992884, "ollama_total_duration_ms": 464.7239, "ollama_load_ms": 149.0584, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.8293, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 336.03492971082704}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 582, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549430.9107158, "prompt_tokens": 26, "prefill_ms": 4.5108, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.9908, "kv_decode_ms_equiv": 262.9908, "kv_decode_ms_per_token": 4.10923125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.9822999994867, "ollama_total_duration_ms": 444.9563, "ollama_load_ms": 139.8122, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5108, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5763.944311430345}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 583, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549430.9107158, "prompt_tokens": 26, "prefill_ms": 4.5108, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.9908, "kv_decode_ms_equiv": 262.9908, "kv_decode_ms_per_token": 4.10923125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.9822999994867, "ollama_total_duration_ms": 444.9563, "ollama_load_ms": 139.8122, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 262.9908, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.35452038626448}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 584, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549430.9107158, "prompt_tokens": 26, "prefill_ms": 4.5108, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.9908, "kv_decode_ms_equiv": 262.9908, "kv_decode_ms_per_token": 4.10923125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 475.9822999994867, "ollama_total_duration_ms": 444.9563, "ollama_load_ms": 139.8122, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.5016, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 336.4465857400479}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 585, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549431.3869176, "prompt_tokens": 26, "prefill_ms": 4.6012, "prefill_cuda_event_ms": null, "kv_decode_ms": 261.9095, "kv_decode_ms_equiv": 261.9095, "kv_decode_ms_per_token": 4.0923359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 476.19810000014695, "ollama_total_duration_ms": 442.6366, "ollama_load_ms": 136.5929, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6012, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5650.699817438928}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 586, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549431.3869176, "prompt_tokens": 26, "prefill_ms": 4.6012, "prefill_cuda_event_ms": null, "kv_decode_ms": 261.9095, "kv_decode_ms_equiv": 261.9095, "kv_decode_ms_per_token": 4.0923359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 476.19810000014695, "ollama_total_duration_ms": 442.6366, "ollama_load_ms": 136.5929, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 261.9095, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 244.35921568327996}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 587, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549431.3869176, "prompt_tokens": 26, "prefill_ms": 4.6012, "prefill_cuda_event_ms": null, "kv_decode_ms": 261.9095, "kv_decode_ms_equiv": 261.9095, "kv_decode_ms_per_token": 4.0923359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 476.19810000014695, "ollama_total_duration_ms": 442.6366, "ollama_load_ms": 136.5929, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 266.5107, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 337.69751083164766}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 588, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549431.8632562, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 30.828699999801756, "prefill_cuda_event_ms": null, "kv_decode_ms": 2216.16400000039, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 30.828699999801756, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 32.437306795499985}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 589, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549431.8632562, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 30.828699999801756, "prefill_cuda_event_ms": null, "kv_decode_ms": 2216.16400000039, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2216.16400000039, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.878729191516847}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 590, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549431.8632562, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 30.828699999801756, "prefill_cuda_event_ms": null, "kv_decode_ms": 2216.16400000039, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2246.9927000001917, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 28.92755281314196}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 591, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549434.111156, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 32.25380000003497, "prefill_cuda_event_ms": null, "kv_decode_ms": 2499.4925000000876, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 32.25380000003497, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 31.00409874182006}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 592, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549434.111156, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 32.25380000003497, "prefill_cuda_event_ms": null, "kv_decode_ms": 2499.4925000000876, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2499.4925000000876, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.605197855163702}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 593, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549434.111156, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 32.25380000003497, "prefill_cuda_event_ms": null, "kv_decode_ms": 2499.4925000000876, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2531.7463000001226, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 25.673978470906366}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 594, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549436.6433668, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 34.18069999952422, "prefill_cuda_event_ms": null, "kv_decode_ms": 2398.56299999974, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 34.18069999952422, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 29.256276203059606}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 595, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549436.6433668, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.18069999952422, "prefill_cuda_event_ms": null, "kv_decode_ms": 2398.56299999974, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2398.56299999974, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.682642899105396}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 596, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549436.6433668, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 34.18069999952422, "prefill_cuda_event_ms": null, "kv_decode_ms": 2398.56299999974, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2432.743699999264, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 26.718803135743258}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 597, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549439.076716, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 33.133799999632174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2347.1197999997457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 33.133799999632174, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 30.180661439711148}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 598, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549439.076716, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 33.133799999632174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2347.1197999997457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2347.1197999997457, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.26746201877166}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 599, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549439.076716, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 33.133799999632174, "prefill_cuda_event_ms": null, "kv_decode_ms": 2347.1197999997457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2380.253599999378, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 27.3080145745886}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 600, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549441.4575508, "prompt_tokens": 10, "prefill_ms": 7.2326, "prefill_cuda_event_ms": null, "kv_decode_ms": 25.1443, "kv_decode_ms_equiv": 146.2941090909091, "kv_decode_ms_per_token": 2.2858454545454547, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1378.128000000288, "ollama_total_duration_ms": 1317.3257, "ollama_load_ms": 1252.0285, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 7.2326, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1382.6286535962172}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 601, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549441.4575508, "prompt_tokens": 10, "prefill_ms": 7.2326, "prefill_cuda_event_ms": null, "kv_decode_ms": 25.1443, "kv_decode_ms_equiv": 146.2941090909091, "kv_decode_ms_per_token": 2.2858454545454547, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1378.128000000288, "ollama_total_duration_ms": 1317.3257, "ollama_load_ms": 1252.0285, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 146.2941090909091, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 437.4748949065991}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 602, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549441.4575508, "prompt_tokens": 10, "prefill_ms": 7.2326, "prefill_cuda_event_ms": null, "kv_decode_ms": 25.1443, "kv_decode_ms_equiv": 146.2941090909091, "kv_decode_ms_per_token": 2.2858454545454547, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1378.128000000288, "ollama_total_duration_ms": 1317.3257, "ollama_load_ms": 1252.0285, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 153.5267090909091, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 482.0008221252352}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 603, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549442.8358755, "prompt_tokens": 10, "prefill_ms": 2.688, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.0206, "kv_decode_ms_equiv": 116.48349090909092, "kv_decode_ms_per_token": 1.8200545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 203.48630000080448, "ollama_total_duration_ms": 165.7502, "ollama_load_ms": 134.4353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.688, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3720.238095238095}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 604, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549442.8358755, "prompt_tokens": 10, "prefill_ms": 2.688, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.0206, "kv_decode_ms_equiv": 116.48349090909092, "kv_decode_ms_per_token": 1.8200545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 203.48630000080448, "ollama_total_duration_ms": 165.7502, "ollama_load_ms": 134.4353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 116.48349090909092, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 549.4340828946184}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 605, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549442.8358755, "prompt_tokens": 10, "prefill_ms": 2.688, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.0206, "kv_decode_ms_equiv": 116.48349090909092, "kv_decode_ms_per_token": 1.8200545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 203.48630000080448, "ollama_total_duration_ms": 165.7502, "ollama_load_ms": 134.4353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.17149090909092, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 620.9538828078466}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 606, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549443.0397542, "prompt_tokens": 10, "prefill_ms": 2.2855, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.4299, "kv_decode_ms_equiv": 124.68305454545454, "kv_decode_ms_per_token": 1.9481727272727272, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.84370000020135, "ollama_total_duration_ms": 167.1871, "ollama_load_ms": 132.0678, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2855, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4375.410194705754}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 607, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549443.0397542, "prompt_tokens": 10, "prefill_ms": 2.2855, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.4299, "kv_decode_ms_equiv": 124.68305454545454, "kv_decode_ms_per_token": 1.9481727272727272, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.84370000020135, "ollama_total_duration_ms": 167.1871, "ollama_load_ms": 132.0678, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 124.68305454545454, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 513.3015086397977}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 608, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549443.0397542, "prompt_tokens": 10, "prefill_ms": 2.2855, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.4299, "kv_decode_ms_equiv": 124.68305454545454, "kv_decode_ms_per_token": 1.9481727272727272, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 199.84370000020135, "ollama_total_duration_ms": 167.1871, "ollama_load_ms": 132.0678, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.96855454545454, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 582.8214731192212}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 609, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549443.2398684, "prompt_tokens": 10, "prefill_ms": 2.2224, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.599, "kv_decode_ms_equiv": 119.84872727272727, "kv_decode_ms_per_token": 1.8726363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 189.46259999938775, "ollama_total_duration_ms": 163.7734, "ollama_load_ms": 133.0327, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2224, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4499.640028797697}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 610, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549443.2398684, "prompt_tokens": 10, "prefill_ms": 2.2224, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.599, "kv_decode_ms_equiv": 119.84872727272727, "kv_decode_ms_per_token": 1.8726363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 189.46259999938775, "ollama_total_duration_ms": 163.7734, "ollama_load_ms": 133.0327, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.84872727272727, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 534.0065051701539}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 611, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549443.2398684, "prompt_tokens": 10, "prefill_ms": 2.2224, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.599, "kv_decode_ms_equiv": 119.84872727272727, "kv_decode_ms_per_token": 1.8726363636363637, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 189.46259999938775, "ollama_total_duration_ms": 163.7734, "ollama_load_ms": 133.0327, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 122.07112727272727, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 606.2039538200679}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 612, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549443.4294684, "prompt_tokens": 11, "prefill_ms": 73.0227, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9241, "kv_decode_ms_equiv": 264.9241, "kv_decode_ms_per_token": 4.1394390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 560.3662000003169, "ollama_total_duration_ms": 513.0583, "ollama_load_ms": 131.6944, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 73.0227, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 150.63808925169846}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 613, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549443.4294684, "prompt_tokens": 11, "prefill_ms": 73.0227, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9241, "kv_decode_ms_equiv": 264.9241, "kv_decode_ms_per_token": 4.1394390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 560.3662000003169, "ollama_total_duration_ms": 513.0583, "ollama_load_ms": 131.6944, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 264.9241, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 241.57862572714222}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 614, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549443.4294684, "prompt_tokens": 11, "prefill_ms": 73.0227, "prefill_cuda_event_ms": null, "kv_decode_ms": 264.9241, "kv_decode_ms_equiv": 264.9241, "kv_decode_ms_per_token": 4.1394390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 560.3662000003169, "ollama_total_duration_ms": 513.0583, "ollama_load_ms": 131.6944, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 337.9468, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 221.92842186995114}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 615, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549443.9901423, "prompt_tokens": 11, "prefill_ms": 4.7419, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.3848, "kv_decode_ms_equiv": 262.3848, "kv_decode_ms_per_token": 4.0997625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 478.948599999967, "ollama_total_duration_ms": 439.8547, "ollama_load_ms": 135.692, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.7419, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2319.745249794386}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 616, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549443.9901423, "prompt_tokens": 11, "prefill_ms": 4.7419, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.3848, "kv_decode_ms_equiv": 262.3848, "kv_decode_ms_per_token": 4.0997625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 478.948599999967, "ollama_total_duration_ms": 439.8547, "ollama_load_ms": 135.692, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 262.3848, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.91656833780007}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 617, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549443.9901423, "prompt_tokens": 11, "prefill_ms": 4.7419, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.3848, "kv_decode_ms_equiv": 262.3848, "kv_decode_ms_per_token": 4.0997625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 478.948599999967, "ollama_total_duration_ms": 439.8547, "ollama_load_ms": 135.692, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.12669999999997, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 280.76564416810453}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 618, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549444.469263, "prompt_tokens": 11, "prefill_ms": 5.1424, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.5691, "kv_decode_ms_equiv": 262.5691, "kv_decode_ms_per_token": 4.1026421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 470.6975999997667, "ollama_total_duration_ms": 450.2242, "ollama_load_ms": 141.6307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.1424, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2139.079029247044}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 619, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549444.469263, "prompt_tokens": 11, "prefill_ms": 5.1424, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.5691, "kv_decode_ms_equiv": 262.5691, "kv_decode_ms_per_token": 4.1026421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 470.6975999997667, "ollama_total_duration_ms": 450.2242, "ollama_load_ms": 141.6307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 262.5691, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.74536074503817}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 620, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549444.469263, "prompt_tokens": 11, "prefill_ms": 5.1424, "prefill_cuda_event_ms": null, "kv_decode_ms": 262.5691, "kv_decode_ms_equiv": 262.5691, "kv_decode_ms_per_token": 4.1026421875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 470.6975999997667, "ollama_total_duration_ms": 450.2242, "ollama_load_ms": 141.6307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.7115, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 280.1523281592311}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 621, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549444.9425914, "prompt_tokens": 11, "prefill_ms": 4.835, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.938, "kv_decode_ms_equiv": 266.938, "kv_decode_ms_per_token": 4.17090625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.0662000006414, "ollama_total_duration_ms": 456.7598, "ollama_load_ms": 147.6863, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.835, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2275.0775594622546}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 622, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549444.9425914, "prompt_tokens": 11, "prefill_ms": 4.835, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.938, "kv_decode_ms_equiv": 266.938, "kv_decode_ms_per_token": 4.17090625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.0662000006414, "ollama_total_duration_ms": 456.7598, "ollama_load_ms": 147.6863, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 266.938, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.75604822093518}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 623, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549444.9425914, "prompt_tokens": 11, "prefill_ms": 4.835, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.938, "kv_decode_ms_equiv": 266.938, "kv_decode_ms_per_token": 4.17090625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.0662000006414, "ollama_total_duration_ms": 456.7598, "ollama_load_ms": 147.6863, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.77299999999997, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 275.96560364716146}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 624, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549445.4539669, "prompt_tokens": 11, "prefill_ms": 19.1056, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4539, "kv_decode_ms_equiv": 687.70496, "kv_decode_ms_per_token": 10.74539, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 5521.175099999709, "ollama_total_duration_ms": 5498.4239, "ollama_load_ms": 5356.4491, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 19.1056, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 575.7474248387907}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 625, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549445.4539669, "prompt_tokens": 11, "prefill_ms": 19.1056, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4539, "kv_decode_ms_equiv": 687.70496, "kv_decode_ms_per_token": 10.74539, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5521.175099999709, "ollama_total_duration_ms": 5498.4239, "ollama_load_ms": 5356.4491, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 687.70496, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 93.06316476181878}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 626, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549445.4539669, "prompt_tokens": 11, "prefill_ms": 19.1056, "prefill_cuda_event_ms": null, "kv_decode_ms": 107.4539, "kv_decode_ms_equiv": 687.70496, "kv_decode_ms_per_token": 10.74539, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5521.175099999709, "ollama_total_duration_ms": 5498.4239, "ollama_load_ms": 5356.4491, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 706.81056, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 106.11046897771307}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 627, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549450.975309, "prompt_tokens": 11, "prefill_ms": 12.1012, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.677, "kv_decode_ms_equiv": 676.3328, "kv_decode_ms_per_token": 10.5677, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.37130000061006, "ollama_total_duration_ms": 283.8355, "ollama_load_ms": 157.1862, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 12.1012, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 909.0007602551813}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 628, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549450.975309, "prompt_tokens": 11, "prefill_ms": 12.1012, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.677, "kv_decode_ms_equiv": 676.3328, "kv_decode_ms_per_token": 10.5677, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.37130000061006, "ollama_total_duration_ms": 283.8355, "ollama_load_ms": 157.1862, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 676.3328, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.62797013541261}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 629, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549450.975309, "prompt_tokens": 11, "prefill_ms": 12.1012, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.677, "kv_decode_ms_equiv": 676.3328, "kv_decode_ms_per_token": 10.5677, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.37130000061006, "ollama_total_duration_ms": 283.8355, "ollama_load_ms": 157.1862, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 688.434, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 108.9429052022416}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 630, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549451.2618272, "prompt_tokens": 11, "prefill_ms": 11.7823, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.8525, "kv_decode_ms_equiv": 677.456, "kv_decode_ms_per_token": 10.58525, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.0940000003611, "ollama_total_duration_ms": 289.8562, "ollama_load_ms": 164.3807, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.7823, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 933.6037955237942}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 631, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549451.2618272, "prompt_tokens": 11, "prefill_ms": 11.7823, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.8525, "kv_decode_ms_equiv": 677.456, "kv_decode_ms_per_token": 10.58525, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.0940000003611, "ollama_total_duration_ms": 289.8562, "ollama_load_ms": 164.3807, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 677.456, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.47108004062255}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 632, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549451.2618272, "prompt_tokens": 11, "prefill_ms": 11.7823, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.8525, "kv_decode_ms_equiv": 677.456, "kv_decode_ms_per_token": 10.58525, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 292.0940000003611, "ollama_total_duration_ms": 289.8562, "ollama_load_ms": 164.3807, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 689.2383, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 108.81577532763342}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 633, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549451.5541785, "prompt_tokens": 11, "prefill_ms": 11.8948, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.5659, "kv_decode_ms_equiv": 669.22176, "kv_decode_ms_per_token": 10.45659, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 281.1139999994339, "ollama_total_duration_ms": 267.1633, "ollama_load_ms": 142.3196, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.8948, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 924.7738507583146}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 634, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549451.5541785, "prompt_tokens": 11, "prefill_ms": 11.8948, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.5659, "kv_decode_ms_equiv": 669.22176, "kv_decode_ms_per_token": 10.45659, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 281.1139999994339, "ollama_total_duration_ms": 267.1633, "ollama_load_ms": 142.3196, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 669.22176, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 95.63347133243246}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 635, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549451.5541785, "prompt_tokens": 11, "prefill_ms": 11.8948, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.5659, "kv_decode_ms_equiv": 669.22176, "kv_decode_ms_per_token": 10.45659, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 281.1139999994339, "ollama_total_duration_ms": 267.1633, "ollama_load_ms": 142.3196, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 681.11656, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 110.11331158942896}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 636, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549451.8354988, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.43209999960527, "prefill_cuda_event_ms": 22.194175720214844, "kv_decode_ms": 401.93490000001475, "kv_decode_cuda_event_ms": 401.8717041015625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 22.43209999960527, "cuda_event_ms": 22.194175720214844, "tokens_total": 17, "tokens_per_s": 757.8425559933819}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 637, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549451.8354988, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.43209999960527, "prefill_cuda_event_ms": 22.194175720214844, "kv_decode_ms": 401.93490000001475, "kv_decode_cuda_event_ms": 401.8717041015625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 401.93490000001475, "cuda_event_ms": 401.8717041015625, "tokens_total": 64, "tokens_per_s": 159.22976581530406}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 638, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549451.8354988, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.43209999960527, "prefill_cuda_event_ms": 22.194175720214844, "kv_decode_ms": 401.93490000001475, "kv_decode_cuda_event_ms": 401.8717041015625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 424.36699999962, "cuda_event_ms": 424.06587982177734, "tokens_total": 81, "tokens_per_s": 190.87252307571637}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 639, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549452.2644315, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.412699999898905, "prefill_cuda_event_ms": 3.3655359745025635, "kv_decode_ms": 255.83929999993416, "kv_decode_cuda_event_ms": 255.7902069091797, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 3.412699999898905, "cuda_event_ms": 3.3655359745025635, "tokens_total": 17, "tokens_per_s": 4981.393032057783}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 640, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549452.2644315, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.412699999898905, "prefill_cuda_event_ms": 3.3655359745025635, "kv_decode_ms": 255.83929999993416, "kv_decode_cuda_event_ms": 255.7902069091797, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 255.83929999993416, "cuda_event_ms": 255.7902069091797, "tokens_total": 64, "tokens_per_s": 250.15703216830437}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 641, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549452.2644315, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.412699999898905, "prefill_cuda_event_ms": 3.3655359745025635, "kv_decode_ms": 255.83929999993416, "kv_decode_cuda_event_ms": 255.7902069091797, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 259.25199999983306, "cuda_event_ms": 259.15574288368225, "tokens_total": 81, "tokens_per_s": 312.4373196737235}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 642, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549452.5246785, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.501900000126625, "prefill_cuda_event_ms": 5.448703765869141, "kv_decode_ms": 295.8805999996912, "kv_decode_cuda_event_ms": 295.8520202636719, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 5.501900000126625, "cuda_event_ms": 5.448703765869141, "tokens_total": 17, "tokens_per_s": 3089.84169098107}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 643, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549452.5246785, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.501900000126625, "prefill_cuda_event_ms": 5.448703765869141, "kv_decode_ms": 295.8805999996912, "kv_decode_cuda_event_ms": 295.8520202636719, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 295.8805999996912, "cuda_event_ms": 295.8520202636719, "tokens_total": 64, "tokens_per_s": 216.30346835874604}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 644, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549452.5246785, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.501900000126625, "prefill_cuda_event_ms": 5.448703765869141, "kv_decode_ms": 295.8805999996912, "kv_decode_cuda_event_ms": 295.8520202636719, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 301.3824999998178, "cuda_event_ms": 301.300724029541, "tokens_total": 81, "tokens_per_s": 268.7614576163147}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 645, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549452.826697, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.9702000003671856, "prefill_cuda_event_ms": 3.920639991760254, "kv_decode_ms": 276.0422999999719, "kv_decode_cuda_event_ms": 275.9710693359375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 3.9702000003671856, "cuda_event_ms": 3.920639991760254, "tokens_total": 17, "tokens_per_s": 4281.900155767405}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 646, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549452.826697, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.9702000003671856, "prefill_cuda_event_ms": 3.920639991760254, "kv_decode_ms": 276.0422999999719, "kv_decode_cuda_event_ms": 275.9710693359375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 276.0422999999719, "cuda_event_ms": 275.9710693359375, "tokens_total": 64, "tokens_per_s": 231.8485246645406}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 647, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549452.826697, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.9702000003671856, "prefill_cuda_event_ms": 3.920639991760254, "kv_decode_ms": 276.0422999999719, "kv_decode_cuda_event_ms": 275.9710693359375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 280.01250000033906, "cuda_event_ms": 279.89170932769775, "tokens_total": 81, "tokens_per_s": 289.2728003210639}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 648, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549453.1077313, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.530199999768229, "prefill_cuda_event_ms": 8.373344421386719, "kv_decode_ms": 474.8491999998805, "kv_decode_cuda_event_ms": 474.78375244140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 8.530199999768229, "cuda_event_ms": 8.373344421386719, "tokens_total": 1, "tokens_per_s": 117.23054559414442}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 649, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549453.1077313, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.530199999768229, "prefill_cuda_event_ms": 8.373344421386719, "kv_decode_ms": 474.8491999998805, "kv_decode_cuda_event_ms": 474.78375244140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 474.8491999998805, "cuda_event_ms": 474.78375244140625, "tokens_total": 64, "tokens_per_s": 134.77963109133617}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 650, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549453.1077313, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.530199999768229, "prefill_cuda_event_ms": 8.373344421386719, "kv_decode_ms": 474.8491999998805, "kv_decode_cuda_event_ms": 474.78375244140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 483.37939999964874, "cuda_event_ms": 483.15709686279297, "tokens_total": 65, "tokens_per_s": 134.46994224422315}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 651, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549453.5959244, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.311700000391284, "prefill_cuda_event_ms": 17.221471786499023, "kv_decode_ms": 599.0718000002744, "kv_decode_cuda_event_ms": 599.0390625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 17.311700000391284, "cuda_event_ms": 17.221471786499023, "tokens_total": 1, "tokens_per_s": 57.76440210825036}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 652, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549453.5959244, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.311700000391284, "prefill_cuda_event_ms": 17.221471786499023, "kv_decode_ms": 599.0718000002744, "kv_decode_cuda_event_ms": 599.0390625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 599.0718000002744, "cuda_event_ms": 599.0390625, "tokens_total": 64, "tokens_per_s": 106.83193567110101}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 653, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549453.5959244, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.311700000391284, "prefill_cuda_event_ms": 17.221471786499023, "kv_decode_ms": 599.0718000002744, "kv_decode_cuda_event_ms": 599.0390625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 616.3835000006657, "cuda_event_ms": 616.260534286499, "tokens_total": 65, "tokens_per_s": 105.4538286633724}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 654, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549454.2128682, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.9779000002890825, "prefill_cuda_event_ms": 5.909503936767578, "kv_decode_ms": 437.8047999998671, "kv_decode_cuda_event_ms": 437.76300048828125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 5.9779000002890825, "cuda_event_ms": 5.909503936767578, "tokens_total": 1, "tokens_per_s": 167.28282506426027}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 655, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549454.2128682, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.9779000002890825, "prefill_cuda_event_ms": 5.909503936767578, "kv_decode_ms": 437.8047999998671, "kv_decode_cuda_event_ms": 437.76300048828125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 437.8047999998671, "cuda_event_ms": 437.76300048828125, "tokens_total": 64, "tokens_per_s": 146.18387007182065}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 656, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549454.2128682, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.9779000002890825, "prefill_cuda_event_ms": 5.909503936767578, "kv_decode_ms": 437.8047999998671, "kv_decode_cuda_event_ms": 437.76300048828125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 443.7827000001562, "cuda_event_ms": 443.6725044250488, "tokens_total": 65, "tokens_per_s": 146.46807998594159}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 657, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549454.6572175, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.5728999998100335, "prefill_cuda_event_ms": 6.510591983795166, "kv_decode_ms": 710.6161000001521, "kv_decode_cuda_event_ms": 710.5863647460938, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 6.5728999998100335, "cuda_event_ms": 6.510591983795166, "tokens_total": 1, "tokens_per_s": 152.13984695171104}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 658, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549454.6572175, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.5728999998100335, "prefill_cuda_event_ms": 6.510591983795166, "kv_decode_ms": 710.6161000001521, "kv_decode_cuda_event_ms": 710.5863647460938, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 710.6161000001521, "cuda_event_ms": 710.5863647460938, "tokens_total": 64, "tokens_per_s": 90.06269348525358}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 659, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549454.6572175, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.5728999998100335, "prefill_cuda_event_ms": 6.510591983795166, "kv_decode_ms": 710.6161000001521, "kv_decode_cuda_event_ms": 710.5863647460938, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 717.1889999999621, "cuda_event_ms": 717.0969567298889, "tokens_total": 65, "tokens_per_s": 90.63161872254516}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 660, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549455.374982, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.3909999996903935, "prefill_cuda_event_ms": 5.342207908630371, "kv_decode_ms": 509.97450000068056, "kv_decode_cuda_event_ms": 509.8424377441406, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 5.3909999996903935, "cuda_event_ms": 5.342207908630371, "tokens_total": 1, "tokens_per_s": 185.4943424332091}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 661, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549455.374982, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.3909999996903935, "prefill_cuda_event_ms": 5.342207908630371, "kv_decode_ms": 509.97450000068056, "kv_decode_cuda_event_ms": 509.8424377441406, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 509.97450000068056, "cuda_event_ms": 509.8424377441406, "tokens_total": 64, "tokens_per_s": 125.49647090180899}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 662, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549455.374982, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.3909999996903935, "prefill_cuda_event_ms": 5.342207908630371, "kv_decode_ms": 509.97450000068056, "kv_decode_cuda_event_ms": 509.8424377441406, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 515.365500000371, "cuda_event_ms": 515.184645652771, "tokens_total": 65, "tokens_per_s": 126.12408087066987}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 663, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549455.8918536, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.538200000359211, "prefill_cuda_event_ms": 11.460576057434082, "kv_decode_ms": 524.7355000001335, "kv_decode_cuda_event_ms": 524.6904296875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 11.538200000359211, "cuda_event_ms": 11.460576057434082, "tokens_total": 1, "tokens_per_s": 86.66863115294133}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 664, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549455.8918536, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.538200000359211, "prefill_cuda_event_ms": 11.460576057434082, "kv_decode_ms": 524.7355000001335, "kv_decode_cuda_event_ms": 524.6904296875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 524.7355000001335, "cuda_event_ms": 524.6904296875, "tokens_total": 64, "tokens_per_s": 121.96620964273185}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 665, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549455.8918536, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.538200000359211, "prefill_cuda_event_ms": 11.460576057434082, "kv_decode_ms": 524.7355000001335, "kv_decode_cuda_event_ms": 524.6904296875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 536.2737000004927, "cuda_event_ms": 536.1510057449341, "tokens_total": 65, "tokens_per_s": 121.20676438158404}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 666, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549456.4289124, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.199800000009418, "prefill_cuda_event_ms": 10.136575698852539, "kv_decode_ms": 340.41700000034325, "kv_decode_cuda_event_ms": 340.3427734375, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 10.199800000009418, "cuda_event_ms": 10.136575698852539, "tokens_total": 1, "tokens_per_s": 98.0411380614401}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 667, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549456.4289124, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.199800000009418, "prefill_cuda_event_ms": 10.136575698852539, "kv_decode_ms": 340.41700000034325, "kv_decode_cuda_event_ms": 340.3427734375, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 340.41700000034325, "cuda_event_ms": 340.3427734375, "tokens_total": 64, "tokens_per_s": 188.00471186790162}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 668, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549456.4289124, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.199800000009418, "prefill_cuda_event_ms": 10.136575698852539, "kv_decode_ms": 340.41700000034325, "kv_decode_cuda_event_ms": 340.3427734375, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 350.61680000035267, "cuda_event_ms": 350.47934913635254, "tokens_total": 65, "tokens_per_s": 185.3875798305575}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 669, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549456.7803283, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.991700000180572, "prefill_cuda_event_ms": 11.919360160827637, "kv_decode_ms": 343.43769999941287, "kv_decode_cuda_event_ms": 343.3932800292969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 11.991700000180572, "cuda_event_ms": 11.919360160827637, "tokens_total": 1, "tokens_per_s": 83.39101211545835}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 670, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549456.7803283, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.991700000180572, "prefill_cuda_event_ms": 11.919360160827637, "kv_decode_ms": 343.43769999941287, "kv_decode_cuda_event_ms": 343.3932800292969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 343.43769999941287, "cuda_event_ms": 343.3932800292969, "tokens_total": 64, "tokens_per_s": 186.35111986863822}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 671, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549456.7803283, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.991700000180572, "prefill_cuda_event_ms": 11.919360160827637, "kv_decode_ms": 343.43769999941287, "kv_decode_cuda_event_ms": 343.3932800292969, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 355.42939999959344, "cuda_event_ms": 355.3126401901245, "tokens_total": 65, "tokens_per_s": 182.87738718314904}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 672, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549457.1364536, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.54140000038751, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.1475000001083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.54140000038751, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 789.1780478378464}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 673, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549457.1364536, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.54140000038751, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.1475000001083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 809.1475000001083, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.09559134767325}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 674, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549457.1364536, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.54140000038751, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.1475000001083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 830.6889000004958, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 97.50942861997031}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 675, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549457.9702475, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 15.960499999891908, "prefill_cuda_event_ms": null, "kv_decode_ms": 829.6575999993365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.960499999891908, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1065.1295385555047}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 676, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549457.9702475, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.960499999891908, "prefill_cuda_event_ms": null, "kv_decode_ms": 829.6575999993365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 829.6575999993365, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.140256414274}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 677, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549457.9702475, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.960499999891908, "prefill_cuda_event_ms": null, "kv_decode_ms": 829.6575999993365, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 845.6180999992284, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 95.78792128512139}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 678, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549458.8166702, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.730899999762187, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.4466000001194, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 20.730899999762187, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 820.0319330176218}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 679, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549458.8166702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.730899999762187, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.4466000001194, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 929.4466000001194, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.85817861939759}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 680, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549458.8166702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.730899999762187, "prefill_cuda_event_ms": null, "kv_decode_ms": 929.4466000001194, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 950.1774999998815, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 85.24723012280347}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 681, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766549459.7672935, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.298699999737437, "prefill_cuda_event_ms": null, "kv_decode_ms": 1001.7991000004258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 22.298699999737437, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 762.3762820343863}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 682, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766549459.7672935, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.298699999737437, "prefill_cuda_event_ms": null, "kv_decode_ms": 1001.7991000004258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1001.7991000004258, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.885064380645574}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 683, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766549459.7672935, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.298699999737437, "prefill_cuda_event_ms": null, "kv_decode_ms": 1001.7991000004258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1024.0978000001633, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 79.09400840426284}
