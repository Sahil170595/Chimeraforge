{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545730.2958233, "prompt_tokens": 19, "prefill_ms": 315.4429, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.6953, "kv_decode_ms_equiv": 724.6292965517241, "kv_decode_ms_per_token": 11.32233275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1163.7939000002007, "ollama_total_duration_ms": 1161.6723, "ollama_load_ms": 153.415, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 315.4429, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 60.23277112910133}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545730.2958233, "prompt_tokens": 19, "prefill_ms": 315.4429, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.6953, "kv_decode_ms_equiv": 724.6292965517241, "kv_decode_ms_per_token": 11.32233275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1163.7939000002007, "ollama_total_duration_ms": 1161.6723, "ollama_load_ms": 153.415, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 724.6292965517241, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.32102194122601}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545730.2958233, "prompt_tokens": 19, "prefill_ms": 315.4429, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.6953, "kv_decode_ms_equiv": 724.6292965517241, "kv_decode_ms_per_token": 11.32233275862069, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1163.7939000002007, "ollama_total_duration_ms": 1161.6723, "ollama_load_ms": 153.415, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 1040.0721965517241, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 79.80215246131935}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545731.4597547, "prompt_tokens": 19, "prefill_ms": 11.5016, "prefill_cuda_event_ms": null, "kv_decode_ms": 644.9704, "kv_decode_ms_equiv": 724.1772912280702, "kv_decode_ms_per_token": 11.315270175438597, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 823.0406000002404, "ollama_total_duration_ms": 821.2737, "ollama_load_ms": 128.8909, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.5016, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1651.9440773457607}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545731.4597547, "prompt_tokens": 19, "prefill_ms": 11.5016, "prefill_cuda_event_ms": null, "kv_decode_ms": 644.9704, "kv_decode_ms_equiv": 724.1772912280702, "kv_decode_ms_per_token": 11.315270175438597, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 823.0406000002404, "ollama_total_duration_ms": 821.2737, "ollama_load_ms": 128.8909, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 724.1772912280702, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.37614873488768}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545731.4597547, "prompt_tokens": 19, "prefill_ms": 11.5016, "prefill_cuda_event_ms": null, "kv_decode_ms": 644.9704, "kv_decode_ms_equiv": 724.1772912280702, "kv_decode_ms_per_token": 11.315270175438597, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 823.0406000002404, "ollama_total_duration_ms": 821.2737, "ollama_load_ms": 128.8909, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 735.6788912280703, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 112.82096168540045}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545732.2829344, "prompt_tokens": 19, "prefill_ms": 11.2996, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.7966, "kv_decode_ms_equiv": 721.7365333333333, "kv_decode_ms_per_token": 11.277133333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 835.1838999997199, "ollama_total_duration_ms": 817.2836, "ollama_load_ms": 128.2283, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.2996, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1681.4754504584232}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545732.2829344, "prompt_tokens": 19, "prefill_ms": 11.2996, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.7966, "kv_decode_ms_equiv": 721.7365333333333, "kv_decode_ms_per_token": 11.277133333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 835.1838999997199, "ollama_total_duration_ms": 817.2836, "ollama_load_ms": 128.2283, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 721.7365333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.67501788279527}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545732.2829344, "prompt_tokens": 19, "prefill_ms": 11.2996, "prefill_cuda_event_ms": null, "kv_decode_ms": 642.7966, "kv_decode_ms_equiv": 721.7365333333333, "kv_decode_ms_per_token": 11.277133333333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 835.1838999997199, "ollama_total_duration_ms": 817.2836, "ollama_load_ms": 128.2283, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 733.0361333333334, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.22770628314638}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545733.1188962, "prompt_tokens": 19, "prefill_ms": 12.2686, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.7468, "kv_decode_ms_equiv": 719.435003508772, "kv_decode_ms_per_token": 11.241171929824562, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 848.3644999996613, "ollama_total_duration_ms": 826.6498, "ollama_load_ms": 139.1418, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 12.2686, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1548.6689597835125}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545733.1188962, "prompt_tokens": 19, "prefill_ms": 12.2686, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.7468, "kv_decode_ms_equiv": 719.435003508772, "kv_decode_ms_per_token": 11.241171929824562, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 848.3644999996613, "ollama_total_duration_ms": 826.6498, "ollama_load_ms": 139.1418, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 719.435003508772, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.95869632123016}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545733.1188962, "prompt_tokens": 19, "prefill_ms": 12.2686, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.7468, "kv_decode_ms_equiv": 719.435003508772, "kv_decode_ms_per_token": 11.241171929824562, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 848.3644999996613, "ollama_total_duration_ms": 826.6498, "ollama_load_ms": 139.1418, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 731.703603508772, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.43390903363914}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545733.967382, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 92.01350000012098, "prefill_cuda_event_ms": null, "kv_decode_ms": 1148.7021000002642, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8927.160400000048, "params_millions_measured": 45.1712, "latency_ms": 92.01350000012098, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 97.81173414757798}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545733.967382, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 92.01350000012098, "prefill_cuda_event_ms": null, "kv_decode_ms": 1148.7021000002642, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8927.160400000048, "params_millions_measured": 45.1712, "latency_ms": 1148.7021000002642, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 55.71505440791419}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545733.967382, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 92.01350000012098, "prefill_cuda_event_ms": null, "kv_decode_ms": 1148.7021000002642, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 8927.160400000048, "params_millions_measured": 45.1712, "latency_ms": 1240.7156000003852, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.83701309145894}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545744.1404002, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.953600000164442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1228.1637000000956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 22.953600000164442, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 392.0953575881571}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545744.1404002, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.953600000164442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1228.1637000000956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1228.1637000000956, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.11031721585243}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545744.1404002, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.953600000164442, "prefill_cuda_event_ms": null, "kv_decode_ms": 1228.1637000000956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1251.11730000026, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.34784636099655}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545745.3919253, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 14.282499999808351, "prefill_cuda_event_ms": null, "kv_decode_ms": 853.5251999996945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.282499999808351, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 630.1417819093832}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545745.3919253, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 14.282499999808351, "prefill_cuda_event_ms": null, "kv_decode_ms": 853.5251999996945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 853.5251999996945, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.98314050952791}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545745.3919253, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 14.282499999808351, "prefill_cuda_event_ms": null, "kv_decode_ms": 853.5251999996945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 867.8076999995028, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 84.12001875535539}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545746.2602391, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.50130000007266, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.5225000001956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.50130000007266, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 545.4115736311909}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545746.2602391, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.50130000007266, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.5225000001956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 847.5225000001956, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.51421938648853}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545746.2602391, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.50130000007266, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.5225000001956, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 864.0238000002682, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 84.48841339784545}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545747.124647, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 686.113899999782, "prefill_cuda_event_ms": 612.8024291992188, "kv_decode_ms": 319.7454999999536, "kv_decode_cuda_event_ms": 319.6562805175781, "gpu_peak_mb": 13.068359375, "hf_load_ms": 309.4987000004039, "params_millions_measured": 0.102714, "latency_ms": 686.113899999782, "cuda_event_ms": 612.8024291992188, "tokens_total": 17, "tokens_per_s": 24.777227221319087}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545747.124647, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 686.113899999782, "prefill_cuda_event_ms": 612.8024291992188, "kv_decode_ms": 319.7454999999536, "kv_decode_cuda_event_ms": 319.6562805175781, "gpu_peak_mb": 13.068359375, "hf_load_ms": 309.4987000004039, "params_millions_measured": 0.102714, "latency_ms": 319.7454999999536, "cuda_event_ms": 319.6562805175781, "tokens_total": 64, "tokens_per_s": 200.15918910511417}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545747.124647, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 686.113899999782, "prefill_cuda_event_ms": 612.8024291992188, "kv_decode_ms": 319.7454999999536, "kv_decode_cuda_event_ms": 319.6562805175781, "gpu_peak_mb": 13.068359375, "hf_load_ms": 309.4987000004039, "params_millions_measured": 0.102714, "latency_ms": 1005.8593999997356, "cuda_event_ms": 932.4587097167969, "tokens_total": 81, "tokens_per_s": 80.52815333835056}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545748.4409878, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.467899999985093, "prefill_cuda_event_ms": 5.398399829864502, "kv_decode_ms": 218.41779999977007, "kv_decode_cuda_event_ms": 218.35487365722656, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 5.467899999985093, "cuda_event_ms": 5.398399829864502, "tokens_total": 17, "tokens_per_s": 3109.0546645049008}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545748.4409878, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.467899999985093, "prefill_cuda_event_ms": 5.398399829864502, "kv_decode_ms": 218.41779999977007, "kv_decode_cuda_event_ms": 218.35487365722656, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 218.41779999977007, "cuda_event_ms": 218.35487365722656, "tokens_total": 64, "tokens_per_s": 293.01641166639064}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545748.4409878, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.467899999985093, "prefill_cuda_event_ms": 5.398399829864502, "kv_decode_ms": 218.41779999977007, "kv_decode_cuda_event_ms": 218.35487365722656, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 223.88569999975516, "cuda_event_ms": 223.75327348709106, "tokens_total": 81, "tokens_per_s": 361.7917535603595}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545748.6656497, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.353999999897496, "prefill_cuda_event_ms": 5.250847816467285, "kv_decode_ms": 228.7293999997928, "kv_decode_cuda_event_ms": 228.65919494628906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 5.353999999897496, "cuda_event_ms": 5.250847816467285, "tokens_total": 17, "tokens_per_s": 3175.196115114955}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545748.6656497, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.353999999897496, "prefill_cuda_event_ms": 5.250847816467285, "kv_decode_ms": 228.7293999997928, "kv_decode_cuda_event_ms": 228.65919494628906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 228.7293999997928, "cuda_event_ms": 228.65919494628906, "tokens_total": 64, "tokens_per_s": 279.8066186509385}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545748.6656497, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.353999999897496, "prefill_cuda_event_ms": 5.250847816467285, "kv_decode_ms": 228.7293999997928, "kv_decode_cuda_event_ms": 228.65919494628906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 234.0833999996903, "cuda_event_ms": 233.91004276275635, "tokens_total": 81, "tokens_per_s": 346.03051732889713}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545748.9004328, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.042099999878701, "prefill_cuda_event_ms": 3.980288028717041, "kv_decode_ms": 214.7173999996994, "kv_decode_cuda_event_ms": 214.65699768066406, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 4.042099999878701, "cuda_event_ms": 3.980288028717041, "tokens_total": 17, "tokens_per_s": 4205.734643009859}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545748.9004328, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.042099999878701, "prefill_cuda_event_ms": 3.980288028717041, "kv_decode_ms": 214.7173999996994, "kv_decode_cuda_event_ms": 214.65699768066406, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 214.7173999996994, "cuda_event_ms": 214.65699768066406, "tokens_total": 64, "tokens_per_s": 298.0662023668766}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545748.9004328, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.042099999878701, "prefill_cuda_event_ms": 3.980288028717041, "kv_decode_ms": 214.7173999996994, "kv_decode_cuda_event_ms": 214.65699768066406, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 218.7594999995781, "cuda_event_ms": 218.6372857093811, "tokens_total": 81, "tokens_per_s": 370.2696340051802}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545749.1198332, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 38.18820000014966, "prefill_cuda_event_ms": 38.11532974243164, "kv_decode_ms": 284.6101999998609, "kv_decode_cuda_event_ms": 284.574462890625, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 401.59170000015365, "params_millions_measured": 96.08832, "latency_ms": 38.18820000014966, "cuda_event_ms": 38.11532974243164, "tokens_total": 1, "tokens_per_s": 26.186099370907268}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545749.1198332, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.18820000014966, "prefill_cuda_event_ms": 38.11532974243164, "kv_decode_ms": 284.6101999998609, "kv_decode_cuda_event_ms": 284.574462890625, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 401.59170000015365, "params_millions_measured": 96.08832, "latency_ms": 284.6101999998609, "cuda_event_ms": 284.574462890625, "tokens_total": 64, "tokens_per_s": 224.86896112659093}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545749.1198332, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 38.18820000014966, "prefill_cuda_event_ms": 38.11532974243164, "kv_decode_ms": 284.6101999998609, "kv_decode_cuda_event_ms": 284.574462890625, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 401.59170000015365, "params_millions_measured": 96.08832, "latency_ms": 322.7984000000106, "cuda_event_ms": 322.68979263305664, "tokens_total": 65, "tokens_per_s": 201.3640711973723}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545749.8631268, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.055599999963306, "prefill_cuda_event_ms": 10.982175827026367, "kv_decode_ms": 362.47610000009445, "kv_decode_cuda_event_ms": 362.4264831542969, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 11.055599999963306, "cuda_event_ms": 10.982175827026367, "tokens_total": 1, "tokens_per_s": 90.45189768111356}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545749.8631268, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.055599999963306, "prefill_cuda_event_ms": 10.982175827026367, "kv_decode_ms": 362.47610000009445, "kv_decode_cuda_event_ms": 362.4264831542969, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 362.47610000009445, "cuda_event_ms": 362.4264831542969, "tokens_total": 64, "tokens_per_s": 176.56336514320068}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545749.8631268, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.055599999963306, "prefill_cuda_event_ms": 10.982175827026367, "kv_decode_ms": 362.47610000009445, "kv_decode_cuda_event_ms": 362.4264831542969, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 373.53170000005775, "cuda_event_ms": 373.40865898132324, "tokens_total": 65, "tokens_per_s": 174.0146820202675}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545750.2374978, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.429099999877508, "prefill_cuda_event_ms": 6.393824100494385, "kv_decode_ms": 267.40990000007514, "kv_decode_cuda_event_ms": 267.3879089355469, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 6.429099999877508, "cuda_event_ms": 6.393824100494385, "tokens_total": 1, "tokens_per_s": 155.5427664866082}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545750.2374978, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.429099999877508, "prefill_cuda_event_ms": 6.393824100494385, "kv_decode_ms": 267.40990000007514, "kv_decode_cuda_event_ms": 267.3879089355469, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 267.40990000007514, "cuda_event_ms": 267.3879089355469, "tokens_total": 64, "tokens_per_s": 239.33294915402166}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545750.2374978, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.429099999877508, "prefill_cuda_event_ms": 6.393824100494385, "kv_decode_ms": 267.40990000007514, "kv_decode_cuda_event_ms": 267.3879089355469, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 273.83899999995265, "cuda_event_ms": 273.78173303604126, "tokens_total": 65, "tokens_per_s": 237.36575140871548}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545750.5117464, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.072200000337034, "prefill_cuda_event_ms": 4.030367851257324, "kv_decode_ms": 229.98760000018592, "kv_decode_cuda_event_ms": 229.96351623535156, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 4.072200000337034, "cuda_event_ms": 4.030367851257324, "tokens_total": 1, "tokens_per_s": 245.56750648721462}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545750.5117464, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.072200000337034, "prefill_cuda_event_ms": 4.030367851257324, "kv_decode_ms": 229.98760000018592, "kv_decode_cuda_event_ms": 229.96351623535156, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 229.98760000018592, "cuda_event_ms": 229.96351623535156, "tokens_total": 64, "tokens_per_s": 278.2758722641928}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545750.5117464, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.072200000337034, "prefill_cuda_event_ms": 4.030367851257324, "kv_decode_ms": 229.98760000018592, "kv_decode_cuda_event_ms": 229.96351623535156, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 234.05980000052296, "cuda_event_ms": 233.9938840866089, "tokens_total": 65, "tokens_per_s": 277.70680825949086}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545750.7462249, "prompt_tokens": 30, "prefill_ms": 60.4143, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4647, "kv_decode_ms_equiv": 711.6462344827586, "kv_decode_ms_per_token": 11.119472413793103, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 5340.594200000396, "ollama_total_duration_ms": 5319.0277, "ollama_load_ms": 4916.4475, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 60.4143, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 496.5711760295162}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545750.7462249, "prompt_tokens": 30, "prefill_ms": 60.4143, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4647, "kv_decode_ms_equiv": 711.6462344827586, "kv_decode_ms_per_token": 11.119472413793103, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5340.594200000396, "ollama_total_duration_ms": 5319.0277, "ollama_load_ms": 4916.4475, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 711.6462344827586, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.93232437535025}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545750.7462249, "prompt_tokens": 30, "prefill_ms": 60.4143, "prefill_cuda_event_ms": null, "kv_decode_ms": 322.4647, "kv_decode_ms_equiv": 711.6462344827586, "kv_decode_ms_per_token": 11.119472413793103, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5340.594200000396, "ollama_total_duration_ms": 5319.0277, "ollama_load_ms": 4916.4475, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 772.0605344827586, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 121.75211113850708}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545756.0870082, "prompt_tokens": 30, "prefill_ms": 11.4397, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4793, "kv_decode_ms_equiv": 709.4715586206897, "kv_decode_ms_per_token": 11.085493103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 469.027399999959, "ollama_total_duration_ms": 455.1994, "ollama_load_ms": 101.7337, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4397, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2622.4463928249866}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545756.0870082, "prompt_tokens": 30, "prefill_ms": 11.4397, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4793, "kv_decode_ms_equiv": 709.4715586206897, "kv_decode_ms_per_token": 11.085493103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 469.027399999959, "ollama_total_duration_ms": 455.1994, "ollama_load_ms": 101.7337, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 709.4715586206897, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.20798539750459}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545756.0870082, "prompt_tokens": 30, "prefill_ms": 11.4397, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.4793, "kv_decode_ms_equiv": 709.4715586206897, "kv_decode_ms_per_token": 11.085493103448277, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 469.027399999959, "ollama_total_duration_ms": 455.1994, "ollama_load_ms": 101.7337, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.9112586206898, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.39052848175655}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545756.5561116, "prompt_tokens": 30, "prefill_ms": 12.3334, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.795, "kv_decode_ms_equiv": 707.9613793103449, "kv_decode_ms_per_token": 11.06189655172414, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.7116999999962, "ollama_total_duration_ms": 467.6911, "ollama_load_ms": 112.4759, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.3334, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2432.4192842200855}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545756.5561116, "prompt_tokens": 30, "prefill_ms": 12.3334, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.795, "kv_decode_ms_equiv": 707.9613793103449, "kv_decode_ms_per_token": 11.06189655172414, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.7116999999962, "ollama_total_duration_ms": 467.6911, "ollama_load_ms": 112.4759, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 707.9613793103449, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.40041147773499}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545756.5561116, "prompt_tokens": 30, "prefill_ms": 12.3334, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.795, "kv_decode_ms_equiv": 707.9613793103449, "kv_decode_ms_per_token": 11.06189655172414, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 491.7116999999962, "ollama_total_duration_ms": 467.6911, "ollama_load_ms": 112.4759, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.2947793103449, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.5021259351643}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545757.0479312, "prompt_tokens": 30, "prefill_ms": 12.0605, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.2389, "kv_decode_ms_equiv": 706.7341241379311, "kv_decode_ms_per_token": 11.042720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7757999997957, "ollama_total_duration_ms": 482.5752, "ollama_load_ms": 130.535, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.0605, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2487.459060569628}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545757.0479312, "prompt_tokens": 30, "prefill_ms": 12.0605, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.2389, "kv_decode_ms_equiv": 706.7341241379311, "kv_decode_ms_per_token": 11.042720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7757999997957, "ollama_total_duration_ms": 482.5752, "ollama_load_ms": 130.535, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 706.7341241379311, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.5573932461047}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545757.0479312, "prompt_tokens": 30, "prefill_ms": 12.0605, "prefill_cuda_event_ms": null, "kv_decode_ms": 320.2389, "kv_decode_ms_equiv": 706.7341241379311, "kv_decode_ms_per_token": 11.042720689655173, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7757999997957, "ollama_total_duration_ms": 482.5752, "ollama_load_ms": 130.535, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 718.7946241379311, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 130.7744894624617}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545757.5449917, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.042200000058074, "prefill_cuda_event_ms": 18.980863571166992, "kv_decode_ms": 373.631800000112, "kv_decode_cuda_event_ms": 373.5980224609375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 255.69910000012896, "params_millions_measured": 5.03672, "latency_ms": 19.042200000058074, "cuda_event_ms": 18.980863571166992, "tokens_total": 9, "tokens_per_s": 472.6344645037103}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545757.5449917, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.042200000058074, "prefill_cuda_event_ms": 18.980863571166992, "kv_decode_ms": 373.631800000112, "kv_decode_cuda_event_ms": 373.5980224609375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 255.69910000012896, "params_millions_measured": 5.03672, "latency_ms": 373.631800000112, "cuda_event_ms": 373.5980224609375, "tokens_total": 64, "tokens_per_s": 171.29162988798282}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545757.5449917, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.042200000058074, "prefill_cuda_event_ms": 18.980863571166992, "kv_decode_ms": 373.631800000112, "kv_decode_cuda_event_ms": 373.5980224609375, "gpu_peak_mb": 229.62109375, "hf_load_ms": 255.69910000012896, "params_millions_measured": 5.03672, "latency_ms": 392.67400000017005, "cuda_event_ms": 392.5788860321045, "tokens_total": 73, "tokens_per_s": 185.90484727781413}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545758.1940155, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.7125999999007036, "prefill_cuda_event_ms": 6.65008020401001, "kv_decode_ms": 371.9874000003074, "kv_decode_cuda_event_ms": 371.947509765625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.7125999999007036, "cuda_event_ms": 6.65008020401001, "tokens_total": 9, "tokens_per_s": 1340.7621488146372}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545758.1940155, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.7125999999007036, "prefill_cuda_event_ms": 6.65008020401001, "kv_decode_ms": 371.9874000003074, "kv_decode_cuda_event_ms": 371.947509765625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 371.9874000003074, "cuda_event_ms": 371.947509765625, "tokens_total": 64, "tokens_per_s": 172.04883821319515}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545758.1940155, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.7125999999007036, "prefill_cuda_event_ms": 6.65008020401001, "kv_decode_ms": 371.9874000003074, "kv_decode_cuda_event_ms": 371.947509765625, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 378.7000000002081, "cuda_event_ms": 378.597589969635, "tokens_total": 73, "tokens_per_s": 192.76472141526244}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545758.573173, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.199800000307732, "prefill_cuda_event_ms": 7.149727821350098, "kv_decode_ms": 366.21459999969375, "kv_decode_cuda_event_ms": 366.18035888671875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 7.199800000307732, "cuda_event_ms": 7.149727821350098, "tokens_total": 9, "tokens_per_s": 1250.0347231333265}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545758.573173, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.199800000307732, "prefill_cuda_event_ms": 7.149727821350098, "kv_decode_ms": 366.21459999969375, "kv_decode_cuda_event_ms": 366.18035888671875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 366.21459999969375, "cuda_event_ms": 366.18035888671875, "tokens_total": 64, "tokens_per_s": 174.76091887121245}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545758.573173, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.199800000307732, "prefill_cuda_event_ms": 7.149727821350098, "kv_decode_ms": 366.21459999969375, "kv_decode_cuda_event_ms": 366.18035888671875, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 373.4144000000015, "cuda_event_ms": 373.33008670806885, "tokens_total": 73, "tokens_per_s": 195.49326431974694}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545758.9471045, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.105100000240782, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 365.0708999998642, "kv_decode_cuda_event_ms": 365.03448486328125, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 6.105100000240782, "cuda_event_ms": 6.061056137084961, "tokens_total": 9, "tokens_per_s": 1474.1773270945675}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545758.9471045, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.105100000240782, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 365.0708999998642, "kv_decode_cuda_event_ms": 365.03448486328125, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 365.0708999998642, "cuda_event_ms": 365.03448486328125, "tokens_total": 64, "tokens_per_s": 175.30841269469522}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545758.9471045, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.105100000240782, "prefill_cuda_event_ms": 6.061056137084961, "kv_decode_ms": 365.0708999998642, "kv_decode_cuda_event_ms": 365.03448486328125, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 371.176000000105, "cuda_event_ms": 371.0955410003662, "tokens_total": 73, "tokens_per_s": 196.6721986334767}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545759.3187582, "prompt_tokens": 25, "prefill_ms": 16.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.2583, "kv_decode_ms_equiv": 193.2583, "kv_decode_ms_per_token": 3.0196609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 453.15549999986615, "ollama_total_duration_ms": 425.6302, "ollama_load_ms": 177.7733, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 16.7528, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 1492.28785635834}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545759.3187582, "prompt_tokens": 25, "prefill_ms": 16.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.2583, "kv_decode_ms_equiv": 193.2583, "kv_decode_ms_per_token": 3.0196609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 453.15549999986615, "ollama_total_duration_ms": 425.6302, "ollama_load_ms": 177.7733, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 193.2583, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 331.1630082640694}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545759.3187582, "prompt_tokens": 25, "prefill_ms": 16.7528, "prefill_cuda_event_ms": null, "kv_decode_ms": 193.2583, "kv_decode_ms_equiv": 193.2583, "kv_decode_ms_per_token": 3.0196609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 453.15549999986615, "ollama_total_duration_ms": 425.6302, "ollama_load_ms": 177.7733, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 210.0111, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 423.78712363298894}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545759.7721043, "prompt_tokens": 25, "prefill_ms": 1.9618, "prefill_cuda_event_ms": null, "kv_decode_ms": 124.652, "kv_decode_ms_equiv": 124.652, "kv_decode_ms_per_token": 1.9476875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 364.3953999999212, "ollama_total_duration_ms": 330.586, "ollama_load_ms": 162.7637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 1.9618, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 12743.398919359772}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545759.7721043, "prompt_tokens": 25, "prefill_ms": 1.9618, "prefill_cuda_event_ms": null, "kv_decode_ms": 124.652, "kv_decode_ms_equiv": 124.652, "kv_decode_ms_per_token": 1.9476875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 364.3953999999212, "ollama_total_duration_ms": 330.586, "ollama_load_ms": 162.7637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 124.652, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 513.4293874145621}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545759.7721043, "prompt_tokens": 25, "prefill_ms": 1.9618, "prefill_cuda_event_ms": null, "kv_decode_ms": 124.652, "kv_decode_ms_equiv": 124.652, "kv_decode_ms_per_token": 1.9476875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 364.3953999999212, "ollama_total_duration_ms": 330.586, "ollama_load_ms": 162.7637, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 126.6138, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 702.9249576270518}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545760.1366806, "prompt_tokens": 25, "prefill_ms": 2.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.2897, "kv_decode_ms_equiv": 117.2897, "kv_decode_ms_per_token": 1.8326515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 381.82019999976546, "ollama_total_duration_ms": 313.9575, "ollama_load_ms": 153.9866, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2286, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11217.804899937179}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545760.1366806, "prompt_tokens": 25, "prefill_ms": 2.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.2897, "kv_decode_ms_equiv": 117.2897, "kv_decode_ms_per_token": 1.8326515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 381.82019999976546, "ollama_total_duration_ms": 313.9575, "ollama_load_ms": 153.9866, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 117.2897, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 545.6574618231609}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545760.1366806, "prompt_tokens": 25, "prefill_ms": 2.2286, "prefill_cuda_event_ms": null, "kv_decode_ms": 117.2897, "kv_decode_ms_equiv": 117.2897, "kv_decode_ms_per_token": 1.8326515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 381.82019999976546, "ollama_total_duration_ms": 313.9575, "ollama_load_ms": 153.9866, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 119.5183, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 744.6558393149836}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545760.5186539, "prompt_tokens": 25, "prefill_ms": 2.7305, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.3413, "kv_decode_ms_equiv": 119.3413, "kv_decode_ms_per_token": 1.8647078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 368.99399999992966, "ollama_total_duration_ms": 332.3741, "ollama_load_ms": 152.6891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.7305, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9155.832265152902}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545760.5186539, "prompt_tokens": 25, "prefill_ms": 2.7305, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.3413, "kv_decode_ms_equiv": 119.3413, "kv_decode_ms_per_token": 1.8647078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 368.99399999992966, "ollama_total_duration_ms": 332.3741, "ollama_load_ms": 152.6891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 119.3413, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 536.2770474261634}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545760.5186539, "prompt_tokens": 25, "prefill_ms": 2.7305, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.3413, "kv_decode_ms_equiv": 119.3413, "kv_decode_ms_per_token": 1.8647078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 368.99399999992966, "ollama_total_duration_ms": 332.3741, "ollama_load_ms": 152.6891, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 122.07180000000001, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 729.0791157335273}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545760.8877912, "prompt_tokens": 27, "prefill_ms": 19.4007, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.6505, "kv_decode_ms_equiv": 722.6505, "kv_decode_ms_per_token": 11.2914140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.4265000000814, "ollama_total_duration_ms": 936.3292, "ollama_load_ms": 154.591, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 19.4007, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 1391.7023612550063}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545760.8877912, "prompt_tokens": 27, "prefill_ms": 19.4007, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.6505, "kv_decode_ms_equiv": 722.6505, "kv_decode_ms_per_token": 11.2914140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.4265000000814, "ollama_total_duration_ms": 936.3292, "ollama_load_ms": 154.591, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 722.6505, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.56286683535126}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545760.8877912, "prompt_tokens": 27, "prefill_ms": 19.4007, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.6505, "kv_decode_ms_equiv": 722.6505, "kv_decode_ms_per_token": 11.2914140625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.4265000000814, "ollama_total_duration_ms": 936.3292, "ollama_load_ms": 154.591, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 742.0512, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.63304742314277}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545761.8273485, "prompt_tokens": 27, "prefill_ms": 11.5853, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.7341, "kv_decode_ms_equiv": 719.7341, "kv_decode_ms_per_token": 11.2458453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 931.979400000273, "ollama_total_duration_ms": 911.2014, "ollama_load_ms": 138.9507, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.5853, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2330.53956306699}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545761.8273485, "prompt_tokens": 27, "prefill_ms": 11.5853, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.7341, "kv_decode_ms_equiv": 719.7341, "kv_decode_ms_per_token": 11.2458453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 931.979400000273, "ollama_total_duration_ms": 911.2014, "ollama_load_ms": 138.9507, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 719.7341, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.92172817711429}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545761.8273485, "prompt_tokens": 27, "prefill_ms": 11.5853, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.7341, "kv_decode_ms_equiv": 719.7341, "kv_decode_ms_per_token": 11.2458453125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 931.979400000273, "ollama_total_duration_ms": 911.2014, "ollama_load_ms": 138.9507, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 731.3194, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 124.43263504291012}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545762.7594383, "prompt_tokens": 27, "prefill_ms": 11.7399, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.2862, "kv_decode_ms_equiv": 723.2862, "kv_decode_ms_per_token": 11.301346875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.5704000003498, "ollama_total_duration_ms": 936.0713, "ollama_load_ms": 141.186, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.7399, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2299.849232105895}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545762.7594383, "prompt_tokens": 27, "prefill_ms": 11.7399, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.2862, "kv_decode_ms_equiv": 723.2862, "kv_decode_ms_per_token": 11.301346875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.5704000003498, "ollama_total_duration_ms": 936.0713, "ollama_load_ms": 141.186, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 723.2862, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.48502847144049}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545762.7594383, "prompt_tokens": 27, "prefill_ms": 11.7399, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.2862, "kv_decode_ms_equiv": 723.2862, "kv_decode_ms_per_token": 11.301346875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.5704000003498, "ollama_total_duration_ms": 936.0713, "ollama_load_ms": 141.186, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 735.0261, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.8051274641812}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545763.7171748, "prompt_tokens": 27, "prefill_ms": 12.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.2393, "kv_decode_ms_equiv": 724.2393, "kv_decode_ms_per_token": 11.3162390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 906.6186000000016, "ollama_total_duration_ms": 903.6204, "ollama_load_ms": 129.7616, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 12.6262, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2138.4106065166084}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545763.7171748, "prompt_tokens": 27, "prefill_ms": 12.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.2393, "kv_decode_ms_equiv": 724.2393, "kv_decode_ms_per_token": 11.3162390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 906.6186000000016, "ollama_total_duration_ms": 903.6204, "ollama_load_ms": 129.7616, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 724.2393, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.36858204187484}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545763.7171748, "prompt_tokens": 27, "prefill_ms": 12.6262, "prefill_cuda_event_ms": null, "kv_decode_ms": 724.2393, "kv_decode_ms_equiv": 724.2393, "kv_decode_ms_per_token": 11.3162390625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 906.6186000000016, "ollama_total_duration_ms": 903.6204, "ollama_load_ms": 129.7616, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 736.8655, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.49607899949177}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545764.623915, "prompt_tokens": 17, "prefill_ms": 6.7009, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2357, "kv_decode_ms_equiv": 119.75904864864864, "kv_decode_ms_per_token": 1.871235135135135, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 273.9947000000029, "ollama_total_duration_ms": 249.7065, "ollama_load_ms": 154.3834, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 6.7009, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2536.9726454655342}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545764.623915, "prompt_tokens": 17, "prefill_ms": 6.7009, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2357, "kv_decode_ms_equiv": 119.75904864864864, "kv_decode_ms_per_token": 1.871235135135135, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 273.9947000000029, "ollama_total_duration_ms": 249.7065, "ollama_load_ms": 154.3834, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.75904864864864, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 534.4063828342893}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545764.623915, "prompt_tokens": 17, "prefill_ms": 6.7009, "prefill_cuda_event_ms": null, "kv_decode_ms": 69.2357, "kv_decode_ms_equiv": 119.75904864864864, "kv_decode_ms_per_token": 1.871235135135135, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 273.9947000000029, "ollama_total_duration_ms": 249.7065, "ollama_load_ms": 154.3834, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.45994864864865, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 640.5190011981359}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545764.898036, "prompt_tokens": 17, "prefill_ms": 2.6142, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8182, "kv_decode_ms_equiv": 119.75760000000001, "kv_decode_ms_per_token": 1.8712125000000002, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 307.5004000002082, "ollama_total_duration_ms": 274.2205, "ollama_load_ms": 149.0188, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6142, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6502.945451763447}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545764.898036, "prompt_tokens": 17, "prefill_ms": 2.6142, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8182, "kv_decode_ms_equiv": 119.75760000000001, "kv_decode_ms_per_token": 1.8712125000000002, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 307.5004000002082, "ollama_total_duration_ms": 274.2205, "ollama_load_ms": 149.0188, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.75760000000001, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 534.4128472848487}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545764.898036, "prompt_tokens": 17, "prefill_ms": 2.6142, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8182, "kv_decode_ms_equiv": 119.75760000000001, "kv_decode_ms_per_token": 1.8712125000000002, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 307.5004000002082, "ollama_total_duration_ms": 274.2205, "ollama_load_ms": 149.0188, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 122.37180000000001, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 661.9172064152035}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545765.2056513, "prompt_tokens": 17, "prefill_ms": 2.5893, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8292, "kv_decode_ms_equiv": 119.77226666666667, "kv_decode_ms_per_token": 1.8714416666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 304.99879999979385, "ollama_total_duration_ms": 275.3038, "ollama_load_ms": 155.3125, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.5893, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6565.481018035763}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545765.2056513, "prompt_tokens": 17, "prefill_ms": 2.5893, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8292, "kv_decode_ms_equiv": 119.77226666666667, "kv_decode_ms_per_token": 1.8714416666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 304.99879999979385, "ollama_total_duration_ms": 275.3038, "ollama_load_ms": 155.3125, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.77226666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 534.3474059659887}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545765.2056513, "prompt_tokens": 17, "prefill_ms": 2.5893, "prefill_cuda_event_ms": null, "kv_decode_ms": 89.8292, "kv_decode_ms_equiv": 119.77226666666667, "kv_decode_ms_per_token": 1.8714416666666667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 304.99879999979385, "ollama_total_duration_ms": 275.3038, "ollama_load_ms": 155.3125, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 122.36156666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 661.9725638251881}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545765.5107818, "prompt_tokens": 17, "prefill_ms": 2.496, "prefill_cuda_event_ms": null, "kv_decode_ms": 87.8059, "kv_decode_ms_equiv": 117.07453333333332, "kv_decode_ms_per_token": 1.8292895833333331, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 294.16079999964495, "ollama_total_duration_ms": 266.477, "ollama_load_ms": 149.1698, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.496, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6810.897435897436}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545765.5107818, "prompt_tokens": 17, "prefill_ms": 2.496, "prefill_cuda_event_ms": null, "kv_decode_ms": 87.8059, "kv_decode_ms_equiv": 117.07453333333332, "kv_decode_ms_per_token": 1.8292895833333331, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 294.16079999964495, "ollama_total_duration_ms": 266.477, "ollama_load_ms": 149.1698, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.07453333333332, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 546.6603041481268}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545765.5107818, "prompt_tokens": 17, "prefill_ms": 2.496, "prefill_cuda_event_ms": null, "kv_decode_ms": 87.8059, "kv_decode_ms_equiv": 117.07453333333332, "kv_decode_ms_per_token": 1.8292895833333331, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 294.16079999964495, "ollama_total_duration_ms": 266.477, "ollama_load_ms": 149.1698, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.57053333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 677.4244267539718}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545765.8050697, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 38.22949999994307, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.0332999997154, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 188.81340000007185, "params_millions_measured": 5.03672, "latency_ms": 38.22949999994307, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 235.4202906136204}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545765.8050697, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.22949999994307, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.0332999997154, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 188.81340000007185, "params_millions_measured": 5.03672, "latency_ms": 1033.0332999997154, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.95347236145982}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545765.8050697, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 38.22949999994307, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.0332999997154, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 188.81340000007185, "params_millions_measured": 5.03672, "latency_ms": 1071.2627999996585, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 68.14387655393548}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545767.0681024, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 26.84389999967607, "prefill_cuda_event_ms": null, "kv_decode_ms": 918.483599999945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 26.84389999967607, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 335.2717004648581}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545767.0681024, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.84389999967607, "prefill_cuda_event_ms": null, "kv_decode_ms": 918.483599999945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 918.483599999945, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 69.68006832131117}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545767.0681024, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 26.84389999967607, "prefill_cuda_event_ms": null, "kv_decode_ms": 918.483599999945, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 945.3274999996211, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 77.22191515641856}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545768.0138538, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.950900000225374, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.1121999996503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 25.950900000225374, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 346.80878119532804}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545768.0138538, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.950900000225374, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.1121999996503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 923.1121999996503, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 69.33068374572912}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545768.0138538, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.950900000225374, "prefill_cuda_event_ms": null, "kv_decode_ms": 923.1121999996503, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 949.0630999998757, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 76.9179625675148}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545768.9632866, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.48960000012812, "prefill_cuda_event_ms": null, "kv_decode_ms": 921.80849999977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 25.48960000012812, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 353.08517983627684}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545768.9632866, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.48960000012812, "prefill_cuda_event_ms": null, "kv_decode_ms": 921.80849999977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 921.80849999977, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 69.42873709671366}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545768.9632866, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.48960000012812, "prefill_cuda_event_ms": null, "kv_decode_ms": 921.80849999977, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 947.2980999998981, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 77.06127564280753}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545769.9109712, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.246799999848008, "prefill_cuda_event_ms": 7.179264068603516, "kv_decode_ms": 370.53270000023986, "kv_decode_cuda_event_ms": 370.48626708984375, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 418.45699999976205, "params_millions_measured": 74.824704, "latency_ms": 7.246799999848008, "cuda_event_ms": 7.179264068603516, "tokens_total": 1, "tokens_per_s": 137.991941273524}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545769.9109712, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.246799999848008, "prefill_cuda_event_ms": 7.179264068603516, "kv_decode_ms": 370.53270000023986, "kv_decode_cuda_event_ms": 370.48626708984375, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 418.45699999976205, "params_millions_measured": 74.824704, "latency_ms": 370.53270000023986, "cuda_event_ms": 370.48626708984375, "tokens_total": 64, "tokens_per_s": 172.72429666790157}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545769.9109712, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.246799999848008, "prefill_cuda_event_ms": 7.179264068603516, "kv_decode_ms": 370.53270000023986, "kv_decode_cuda_event_ms": 370.48626708984375, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 418.45699999976205, "params_millions_measured": 74.824704, "latency_ms": 377.77950000008786, "cuda_event_ms": 377.66553115844727, "tokens_total": 65, "tokens_per_s": 172.05803914713445}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545770.7085605, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.729599999947823, "prefill_cuda_event_ms": 7.662591934204102, "kv_decode_ms": 340.27199999991353, "kv_decode_cuda_event_ms": 340.2086486816406, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 7.729599999947823, "cuda_event_ms": 7.662591934204102, "tokens_total": 1, "tokens_per_s": 129.37280066326204}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545770.7085605, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.729599999947823, "prefill_cuda_event_ms": 7.662591934204102, "kv_decode_ms": 340.27199999991353, "kv_decode_cuda_event_ms": 340.2086486816406, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 340.27199999991353, "cuda_event_ms": 340.2086486816406, "tokens_total": 64, "tokens_per_s": 188.08482625668955}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545770.7085605, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.729599999947823, "prefill_cuda_event_ms": 7.662591934204102, "kv_decode_ms": 340.27199999991353, "kv_decode_cuda_event_ms": 340.2086486816406, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 348.00159999986136, "cuda_event_ms": 347.8712406158447, "tokens_total": 65, "tokens_per_s": 186.78075043340576}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545771.057214, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.986699999994016, "prefill_cuda_event_ms": 5.914624214172363, "kv_decode_ms": 352.93000000001484, "kv_decode_cuda_event_ms": 352.869384765625, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.986699999994016, "cuda_event_ms": 5.914624214172363, "tokens_total": 1, "tokens_per_s": 167.03693186580244}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545771.057214, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.986699999994016, "prefill_cuda_event_ms": 5.914624214172363, "kv_decode_ms": 352.93000000001484, "kv_decode_cuda_event_ms": 352.869384765625, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 352.93000000001484, "cuda_event_ms": 352.869384765625, "tokens_total": 64, "tokens_per_s": 181.33907573739071}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545771.057214, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.986699999994016, "prefill_cuda_event_ms": 5.914624214172363, "kv_decode_ms": 352.93000000001484, "kv_decode_cuda_event_ms": 352.869384765625, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 358.91670000000886, "cuda_event_ms": 358.78400897979736, "tokens_total": 65, "tokens_per_s": 181.100517195211}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545771.4168355, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.538600000363658, "prefill_cuda_event_ms": 5.483520030975342, "kv_decode_ms": 380.4768999998487, "kv_decode_cuda_event_ms": 380.4375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.538600000363658, "cuda_event_ms": 5.483520030975342, "tokens_total": 1, "tokens_per_s": 180.5510417676563}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545771.4168355, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.538600000363658, "prefill_cuda_event_ms": 5.483520030975342, "kv_decode_ms": 380.4768999998487, "kv_decode_cuda_event_ms": 380.4375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 380.4768999998487, "cuda_event_ms": 380.4375, "tokens_total": 64, "tokens_per_s": 168.20994914546836}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545771.4168355, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.538600000363658, "prefill_cuda_event_ms": 5.483520030975342, "kv_decode_ms": 380.4768999998487, "kv_decode_cuda_event_ms": 380.4375, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 386.01550000021234, "cuda_event_ms": 385.92102003097534, "tokens_total": 65, "tokens_per_s": 168.38702072834963}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545771.8035302, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 42.58040000013352, "prefill_cuda_event_ms": 42.475521087646484, "kv_decode_ms": 583.6801000000378, "kv_decode_cuda_event_ms": 583.6441650390625, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 390.92470000014146, "params_millions_measured": 51.475968, "latency_ms": 42.58040000013352, "cuda_event_ms": 42.475521087646484, "tokens_total": 17, "tokens_per_s": 399.24472292291034}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545771.8035302, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 42.58040000013352, "prefill_cuda_event_ms": 42.475521087646484, "kv_decode_ms": 583.6801000000378, "kv_decode_cuda_event_ms": 583.6441650390625, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 390.92470000014146, "params_millions_measured": 51.475968, "latency_ms": 583.6801000000378, "cuda_event_ms": 583.6441650390625, "tokens_total": 64, "tokens_per_s": 109.64910402118534}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545771.8035302, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 42.58040000013352, "prefill_cuda_event_ms": 42.475521087646484, "kv_decode_ms": 583.6801000000378, "kv_decode_cuda_event_ms": 583.6441650390625, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 390.92470000014146, "params_millions_measured": 51.475968, "latency_ms": 626.2605000001713, "cuda_event_ms": 626.119686126709, "tokens_total": 81, "tokens_per_s": 129.3391488046553}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545772.8219235, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.448799999972834, "prefill_cuda_event_ms": 9.396224021911621, "kv_decode_ms": 482.9893999999513, "kv_decode_cuda_event_ms": 482.9542541503906, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.448799999972834, "cuda_event_ms": 9.396224021911621, "tokens_total": 17, "tokens_per_s": 1799.1702650123693}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545772.8219235, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.448799999972834, "prefill_cuda_event_ms": 9.396224021911621, "kv_decode_ms": 482.9893999999513, "kv_decode_cuda_event_ms": 482.9542541503906, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 482.9893999999513, "cuda_event_ms": 482.9542541503906, "tokens_total": 64, "tokens_per_s": 132.5080840283585}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545772.8219235, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.448799999972834, "prefill_cuda_event_ms": 9.396224021911621, "kv_decode_ms": 482.9893999999513, "kv_decode_cuda_event_ms": 482.9542541503906, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 492.4381999999241, "cuda_event_ms": 492.35047817230225, "tokens_total": 81, "tokens_per_s": 164.48764535329}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545773.3150558, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.155100000043603, "prefill_cuda_event_ms": 9.096192359924316, "kv_decode_ms": 550.1528000004328, "kv_decode_cuda_event_ms": 550.0989379882812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.155100000043603, "cuda_event_ms": 9.096192359924316, "tokens_total": 17, "tokens_per_s": 1856.8885102204272}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545773.3150558, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.155100000043603, "prefill_cuda_event_ms": 9.096192359924316, "kv_decode_ms": 550.1528000004328, "kv_decode_cuda_event_ms": 550.0989379882812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 550.1528000004328, "cuda_event_ms": 550.0989379882812, "tokens_total": 64, "tokens_per_s": 116.3313174084539}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545773.3150558, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.155100000043603, "prefill_cuda_event_ms": 9.096192359924316, "kv_decode_ms": 550.1528000004328, "kv_decode_cuda_event_ms": 550.0989379882812, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 559.3079000004764, "cuda_event_ms": 559.1951303482056, "tokens_total": 81, "tokens_per_s": 144.82184142210582}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545773.8751252, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.109099999932369, "prefill_cuda_event_ms": 9.053183555603027, "kv_decode_ms": 543.6632999999347, "kv_decode_cuda_event_ms": 543.6293334960938, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.109099999932369, "cuda_event_ms": 9.053183555603027, "tokens_total": 17, "tokens_per_s": 1866.265602543195}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545773.8751252, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.109099999932369, "prefill_cuda_event_ms": 9.053183555603027, "kv_decode_ms": 543.6632999999347, "kv_decode_cuda_event_ms": 543.6293334960938, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 543.6632999999347, "cuda_event_ms": 543.6293334960938, "tokens_total": 64, "tokens_per_s": 117.71991966352647}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545773.8751252, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.109099999932369, "prefill_cuda_event_ms": 9.053183555603027, "kv_decode_ms": 543.6632999999347, "kv_decode_cuda_event_ms": 543.6293334960938, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 552.772399999867, "cuda_event_ms": 552.6825170516968, "tokens_total": 81, "tokens_per_s": 146.534088894488}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545774.4285731, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.222199999840086, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.6763000001156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.31960000012623, "params_millions_measured": 0.102714, "latency_ms": 9.222199999840086, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 975.9059660553947}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545774.4285731, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.222199999840086, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.6763000001156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.31960000012623, "params_millions_measured": 0.102714, "latency_ms": 183.6763000001156, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 348.4390746109309}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545774.4285731, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.222199999840086, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.6763000001156, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 149.31960000012623, "params_millions_measured": 0.102714, "latency_ms": 192.89849999995567, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 378.43736472816937}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545774.7713249, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.559200000130659, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.21809999996185, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.559200000130659, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1974.030531615651}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545774.7713249, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.559200000130659, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.21809999996185, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 174.21809999996185, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 367.3556306722092}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545774.7713249, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.559200000130659, "prefill_cuda_event_ms": null, "kv_decode_ms": 174.21809999996185, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 178.7773000000925, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 408.3292453793755}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545774.9504626, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.202399999892805, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.27940000011222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 6.202399999892805, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1451.0512060098583}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545774.9504626, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.202399999892805, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.27940000011222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 176.27940000011222, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 363.06000587680273}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545774.9504626, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.202399999892805, "prefill_cuda_event_ms": null, "kv_decode_ms": 176.27940000011222, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 182.48180000000502, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 400.0398943894569}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545775.1338527, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.508500000109052, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.42059999986304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 6.508500000109052, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1382.8070983866025}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545775.1338527, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.508500000109052, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.42059999986304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 183.42059999986304, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 348.9248208764326}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545775.1338527, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.508500000109052, "prefill_cuda_event_ms": null, "kv_decode_ms": 183.42059999986304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 189.9290999999721, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 384.35395102704496}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545775.3243427, "prompt_tokens": 38, "prefill_ms": 206.388, "prefill_cuda_event_ms": null, "kv_decode_ms": 576.6146, "kv_decode_ms_equiv": 1025.0926222222222, "kv_decode_ms_per_token": 16.01707222222222, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 956.1616999999387, "ollama_total_duration_ms": 927.8522, "ollama_load_ms": 117.5611, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 206.388, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 184.11923173827935}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545775.3243427, "prompt_tokens": 38, "prefill_ms": 206.388, "prefill_cuda_event_ms": null, "kv_decode_ms": 576.6146, "kv_decode_ms_equiv": 1025.0926222222222, "kv_decode_ms_per_token": 16.01707222222222, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 956.1616999999387, "ollama_total_duration_ms": 927.8522, "ollama_load_ms": 117.5611, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1025.0926222222222, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.433382713514376}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545775.3243427, "prompt_tokens": 38, "prefill_ms": 206.388, "prefill_cuda_event_ms": null, "kv_decode_ms": 576.6146, "kv_decode_ms_equiv": 1025.0926222222222, "kv_decode_ms_per_token": 16.01707222222222, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 956.1616999999387, "ollama_total_duration_ms": 927.8522, "ollama_load_ms": 117.5611, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 1231.480622222222, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 82.82712546133266}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545776.280634, "prompt_tokens": 38, "prefill_ms": 12.8124, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.3127, "kv_decode_ms_equiv": 790.5146514285715, "kv_decode_ms_per_token": 12.35179142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 604.9498000002131, "ollama_total_duration_ms": 586.93, "ollama_load_ms": 115.1428, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.8124, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 2965.8768068433706}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545776.280634, "prompt_tokens": 38, "prefill_ms": 12.8124, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.3127, "kv_decode_ms_equiv": 790.5146514285715, "kv_decode_ms_per_token": 12.35179142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 604.9498000002131, "ollama_total_duration_ms": 586.93, "ollama_load_ms": 115.1428, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 790.5146514285715, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 80.95991628282027}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545776.280634, "prompt_tokens": 38, "prefill_ms": 12.8124, "prefill_cuda_event_ms": null, "kv_decode_ms": 432.3127, "kv_decode_ms_equiv": 790.5146514285715, "kv_decode_ms_per_token": 12.35179142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 604.9498000002131, "ollama_total_duration_ms": 586.93, "ollama_load_ms": 115.1428, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 803.3270514285715, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 126.97194725188389}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545776.8857512, "prompt_tokens": 38, "prefill_ms": 12.0335, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.8899, "kv_decode_ms_equiv": 712.9415314285715, "kv_decode_ms_per_token": 11.13971142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 570.9225000000515, "ollama_total_duration_ms": 552.7222, "ollama_load_ms": 127.1352, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.0335, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3157.8509992936383}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545776.8857512, "prompt_tokens": 38, "prefill_ms": 12.0335, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.8899, "kv_decode_ms_equiv": 712.9415314285715, "kv_decode_ms_per_token": 11.13971142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 570.9225000000515, "ollama_total_duration_ms": 552.7222, "ollama_load_ms": 127.1352, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 712.9415314285715, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.76893220368109}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545776.8857512, "prompt_tokens": 38, "prefill_ms": 12.0335, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.8899, "kv_decode_ms_equiv": 712.9415314285715, "kv_decode_ms_per_token": 11.13971142857143, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 570.9225000000515, "ollama_total_duration_ms": 552.7222, "ollama_load_ms": 127.1352, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 724.9750314285715, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 140.69450060784555}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545777.456797, "prompt_tokens": 38, "prefill_ms": 11.7897, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.0593, "kv_decode_ms_equiv": 711.42272, "kv_decode_ms_per_token": 11.11598, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 553.6024999996698, "ollama_total_duration_ms": 531.6098, "ollama_load_ms": 106.5515, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.7897, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3223.1524126992203}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545777.456797, "prompt_tokens": 38, "prefill_ms": 11.7897, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.0593, "kv_decode_ms_equiv": 711.42272, "kv_decode_ms_per_token": 11.11598, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 553.6024999996698, "ollama_total_duration_ms": 531.6098, "ollama_load_ms": 106.5515, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 711.42272, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.96057927416206}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545777.456797, "prompt_tokens": 38, "prefill_ms": 11.7897, "prefill_cuda_event_ms": null, "kv_decode_ms": 389.0593, "kv_decode_ms_equiv": 711.42272, "kv_decode_ms_per_token": 11.11598, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 553.6024999996698, "ollama_total_duration_ms": 531.6098, "ollama_load_ms": 106.5515, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 723.2124200000001, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.03740087870725}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545778.0105257, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 60.81429999994725, "prefill_cuda_event_ms": 60.701568603515625, "kv_decode_ms": 553.7663000000066, "kv_decode_cuda_event_ms": 553.7176513671875, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 60.81429999994725, "cuda_event_ms": 60.701568603515625, "tokens_total": 9, "tokens_per_s": 147.99150857623627}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545778.0105257, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 60.81429999994725, "prefill_cuda_event_ms": 60.701568603515625, "kv_decode_ms": 553.7663000000066, "kv_decode_cuda_event_ms": 553.7176513671875, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 553.7663000000066, "cuda_event_ms": 553.7176513671875, "tokens_total": 64, "tokens_per_s": 115.57221882227076}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545778.0105257, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 60.81429999994725, "prefill_cuda_event_ms": 60.701568603515625, "kv_decode_ms": 553.7663000000066, "kv_decode_cuda_event_ms": 553.7176513671875, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 614.5805999999538, "cuda_event_ms": 614.4192199707031, "tokens_total": 73, "tokens_per_s": 118.78018928681686}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545778.62613, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.226099999930739, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 551.4015000003383, "kv_decode_cuda_event_ms": 551.3635864257812, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 11.226099999930739, "cuda_event_ms": 11.151359558105469, "tokens_total": 9, "tokens_per_s": 801.7031738587334}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545778.62613, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.226099999930739, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 551.4015000003383, "kv_decode_cuda_event_ms": 551.3635864257812, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 551.4015000003383, "cuda_event_ms": 551.3635864257812, "tokens_total": 64, "tokens_per_s": 116.06787431655651}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545778.62613, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.226099999930739, "prefill_cuda_event_ms": 11.151359558105469, "kv_decode_ms": 551.4015000003383, "kv_decode_cuda_event_ms": 551.3635864257812, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 562.627600000269, "cuda_event_ms": 562.5149459838867, "tokens_total": 73, "tokens_per_s": 129.748345086457}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545779.1895387, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.431899999981397, "prefill_cuda_event_ms": 9.381888389587402, "kv_decode_ms": 541.1174000000756, "kv_decode_cuda_event_ms": 541.0597534179688, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 9.431899999981397, "cuda_event_ms": 9.381888389587402, "tokens_total": 9, "tokens_per_s": 954.2085899996556}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545779.1895387, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.431899999981397, "prefill_cuda_event_ms": 9.381888389587402, "kv_decode_ms": 541.1174000000756, "kv_decode_cuda_event_ms": 541.0597534179688, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 541.1174000000756, "cuda_event_ms": 541.0597534179688, "tokens_total": 64, "tokens_per_s": 118.27377940533988}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545779.1895387, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.431899999981397, "prefill_cuda_event_ms": 9.381888389587402, "kv_decode_ms": 541.1174000000756, "kv_decode_cuda_event_ms": 541.0597534179688, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 550.549300000057, "cuda_event_ms": 550.4416418075562, "tokens_total": 73, "tokens_per_s": 132.59484663769882}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545779.740725, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 9.704399999918678, "prefill_cuda_event_ms": 9.647104263305664, "kv_decode_ms": 542.3204000003352, "kv_decode_cuda_event_ms": 542.2807006835938, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 9.704399999918678, "cuda_event_ms": 9.647104263305664, "tokens_total": 9, "tokens_per_s": 927.4143687477247}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545779.740725, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.704399999918678, "prefill_cuda_event_ms": 9.647104263305664, "kv_decode_ms": 542.3204000003352, "kv_decode_cuda_event_ms": 542.2807006835938, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 542.3204000003352, "cuda_event_ms": 542.2807006835938, "tokens_total": 64, "tokens_per_s": 118.01141907986577}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545779.740725, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 9.704399999918678, "prefill_cuda_event_ms": 9.647104263305664, "kv_decode_ms": 542.3204000003352, "kv_decode_cuda_event_ms": 542.2807006835938, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 552.0248000002539, "cuda_event_ms": 551.9278049468994, "tokens_total": 73, "tokens_per_s": 132.24043557457279}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545780.293487, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.453199999716162, "prefill_cuda_event_ms": 4.386816024780273, "kv_decode_ms": 216.800400000011, "kv_decode_cuda_event_ms": 216.76646423339844, "gpu_peak_mb": 539.3515625, "hf_load_ms": 241.37400000017806, "params_millions_measured": 25.016064, "latency_ms": 4.453199999716162, "cuda_event_ms": 4.386816024780273, "tokens_total": 17, "tokens_per_s": 3817.4795654997633}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545780.293487, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.453199999716162, "prefill_cuda_event_ms": 4.386816024780273, "kv_decode_ms": 216.800400000011, "kv_decode_cuda_event_ms": 216.76646423339844, "gpu_peak_mb": 539.3515625, "hf_load_ms": 241.37400000017806, "params_millions_measured": 25.016064, "latency_ms": 216.800400000011, "cuda_event_ms": 216.76646423339844, "tokens_total": 64, "tokens_per_s": 295.20240737561716}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545780.293487, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.453199999716162, "prefill_cuda_event_ms": 4.386816024780273, "kv_decode_ms": 216.800400000011, "kv_decode_cuda_event_ms": 216.76646423339844, "gpu_peak_mb": 539.3515625, "hf_load_ms": 241.37400000017806, "params_millions_measured": 25.016064, "latency_ms": 221.25359999972716, "cuda_event_ms": 221.1532802581787, "tokens_total": 81, "tokens_per_s": 366.09573810369585}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545780.7570124, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.3551000001352804, "prefill_cuda_event_ms": 3.2931840419769287, "kv_decode_ms": 216.84779999986858, "kv_decode_cuda_event_ms": 216.80015563964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 3.3551000001352804, "cuda_event_ms": 3.2931840419769287, "tokens_total": 17, "tokens_per_s": 5066.913057528702}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545780.7570124, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.3551000001352804, "prefill_cuda_event_ms": 3.2931840419769287, "kv_decode_ms": 216.84779999986858, "kv_decode_cuda_event_ms": 216.80015563964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 216.84779999986858, "cuda_event_ms": 216.80015563964844, "tokens_total": 64, "tokens_per_s": 295.13788011701655}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545780.7570124, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.3551000001352804, "prefill_cuda_event_ms": 3.2931840419769287, "kv_decode_ms": 216.84779999986858, "kv_decode_cuda_event_ms": 216.80015563964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 220.20290000000386, "cuda_event_ms": 220.09333968162537, "tokens_total": 81, "tokens_per_s": 367.8425670143244}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545780.9778647, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.229799999848183, "prefill_cuda_event_ms": 4.178944110870361, "kv_decode_ms": 218.40960000008636, "kv_decode_cuda_event_ms": 218.355712890625, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 4.229799999848183, "cuda_event_ms": 4.178944110870361, "tokens_total": 17, "tokens_per_s": 4019.102558184824}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545780.9778647, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.229799999848183, "prefill_cuda_event_ms": 4.178944110870361, "kv_decode_ms": 218.40960000008636, "kv_decode_cuda_event_ms": 218.355712890625, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 218.40960000008636, "cuda_event_ms": 218.355712890625, "tokens_total": 64, "tokens_per_s": 293.0274127143436}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545780.9778647, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.229799999848183, "prefill_cuda_event_ms": 4.178944110870361, "kv_decode_ms": 218.40960000008636, "kv_decode_cuda_event_ms": 218.355712890625, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 222.63939999993454, "cuda_event_ms": 222.53465700149536, "tokens_total": 81, "tokens_per_s": 363.8170063341161}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545781.2012467, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.762999999707972, "prefill_cuda_event_ms": 3.69868803024292, "kv_decode_ms": 216.23419999968974, "kv_decode_cuda_event_ms": 216.18687438964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 3.762999999707972, "cuda_event_ms": 3.69868803024292, "tokens_total": 17, "tokens_per_s": 4517.672070507384}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545781.2012467, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.762999999707972, "prefill_cuda_event_ms": 3.69868803024292, "kv_decode_ms": 216.23419999968974, "kv_decode_cuda_event_ms": 216.18687438964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 216.23419999968974, "cuda_event_ms": 216.18687438964844, "tokens_total": 64, "tokens_per_s": 295.9753822480062}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545781.2012467, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.762999999707972, "prefill_cuda_event_ms": 3.69868803024292, "kv_decode_ms": 216.23419999968974, "kv_decode_cuda_event_ms": 216.18687438964844, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 219.9971999993977, "cuda_event_ms": 219.88556241989136, "tokens_total": 81, "tokens_per_s": 368.1865041928795}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545781.4219277, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.305400000248483, "prefill_cuda_event_ms": 4.2342400550842285, "kv_decode_ms": 221.2189000001672, "kv_decode_cuda_event_ms": 221.16351318359375, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 4.305400000248483, "cuda_event_ms": 4.2342400550842285, "tokens_total": 1, "tokens_per_s": 232.26645606500807}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545781.4219277, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.305400000248483, "prefill_cuda_event_ms": 4.2342400550842285, "kv_decode_ms": 221.2189000001672, "kv_decode_cuda_event_ms": 221.16351318359375, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 221.2189000001672, "cuda_event_ms": 221.16351318359375, "tokens_total": 64, "tokens_per_s": 289.3062030412032}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545781.4219277, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.305400000248483, "prefill_cuda_event_ms": 4.2342400550842285, "kv_decode_ms": 221.2189000001672, "kv_decode_cuda_event_ms": 221.16351318359375, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 225.52430000041568, "cuda_event_ms": 225.39775323867798, "tokens_total": 65, "tokens_per_s": 288.2172785809786}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545781.648078, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 3.94189999997252, "prefill_cuda_event_ms": 3.892224073410034, "kv_decode_ms": 228.25490000013815, "kv_decode_cuda_event_ms": 228.21273803710938, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.94189999997252, "cuda_event_ms": 3.892224073410034, "tokens_total": 1, "tokens_per_s": 253.68477130494716}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545781.648078, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.94189999997252, "prefill_cuda_event_ms": 3.892224073410034, "kv_decode_ms": 228.25490000013815, "kv_decode_cuda_event_ms": 228.21273803710938, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 228.25490000013815, "cuda_event_ms": 228.21273803710938, "tokens_total": 64, "tokens_per_s": 280.38828520203185}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545781.648078, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 3.94189999997252, "prefill_cuda_event_ms": 3.892224073410034, "kv_decode_ms": 228.25490000013815, "kv_decode_cuda_event_ms": 228.21273803710938, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 232.19680000011067, "cuda_event_ms": 232.1049621105194, "tokens_total": 65, "tokens_per_s": 279.9349517304675}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545781.8808513, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.020999999738706, "prefill_cuda_event_ms": 3.9690239429473877, "kv_decode_ms": 224.10620000027848, "kv_decode_cuda_event_ms": 224.06040954589844, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 4.020999999738706, "cuda_event_ms": 3.9690239429473877, "tokens_total": 1, "tokens_per_s": 248.69435465431047}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545781.8808513, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.020999999738706, "prefill_cuda_event_ms": 3.9690239429473877, "kv_decode_ms": 224.10620000027848, "kv_decode_cuda_event_ms": 224.06040954589844, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 224.10620000027848, "cuda_event_ms": 224.06040954589844, "tokens_total": 64, "tokens_per_s": 285.57889072199015}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545781.8808513, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.020999999738706, "prefill_cuda_event_ms": 3.9690239429473877, "kv_decode_ms": 224.10620000027848, "kv_decode_cuda_event_ms": 224.06040954589844, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 228.12720000001718, "cuda_event_ms": 228.02943348884583, "tokens_total": 65, "tokens_per_s": 284.928759043179}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545782.1097293, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.461600000013277, "prefill_cuda_event_ms": 4.399104118347168, "kv_decode_ms": 212.90730000009717, "kv_decode_cuda_event_ms": 212.86195373535156, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 4.461600000013277, "cuda_event_ms": 4.399104118347168, "tokens_total": 1, "tokens_per_s": 224.1348395187879}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545782.1097293, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.461600000013277, "prefill_cuda_event_ms": 4.399104118347168, "kv_decode_ms": 212.90730000009717, "kv_decode_cuda_event_ms": 212.86195373535156, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 212.90730000009717, "cuda_event_ms": 212.86195373535156, "tokens_total": 64, "tokens_per_s": 300.60030820911635}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545782.1097293, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.461600000013277, "prefill_cuda_event_ms": 4.399104118347168, "kv_decode_ms": 212.90730000009717, "kv_decode_cuda_event_ms": 212.86195373535156, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 217.36890000011044, "cuda_event_ms": 217.26105785369873, "tokens_total": 65, "tokens_per_s": 299.0308181159631}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545782.3278835, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 35.57839999984935, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.9462999996504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 137.11109999985638, "params_millions_measured": 25.016064, "latency_ms": 35.57839999984935, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 252.9624716130604}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545782.3278835, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.57839999984935, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.9462999996504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 137.11109999985638, "params_millions_measured": 25.016064, "latency_ms": 645.9462999996504, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.0794436008607}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545782.3278835, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 35.57839999984935, "prefill_cuda_event_ms": null, "kv_decode_ms": 645.9462999996504, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 137.11109999985638, "params_millions_measured": 25.016064, "latency_ms": 681.5246999994997, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 107.1127722884491}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545783.1470351, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.387100000272767, "prefill_cuda_event_ms": null, "kv_decode_ms": 590.2328000001944, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.387100000272767, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 672.2889946154598}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545783.1470351, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.387100000272767, "prefill_cuda_event_ms": null, "kv_decode_ms": 590.2328000001944, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 590.2328000001944, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 108.4317916591198}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545783.1470351, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.387100000272767, "prefill_cuda_event_ms": null, "kv_decode_ms": 590.2328000001944, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 603.6199000004672, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 120.93703338797064}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545783.751176, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 13.52790000009918, "prefill_cuda_event_ms": null, "kv_decode_ms": 664.4860000001245, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.52790000009918, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 665.2917304189133}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545783.751176, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.52790000009918, "prefill_cuda_event_ms": null, "kv_decode_ms": 664.4860000001245, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 664.4860000001245, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.31504651713958}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545783.751176, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 13.52790000009918, "prefill_cuda_event_ms": null, "kv_decode_ms": 664.4860000001245, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 678.0139000002237, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 107.66740917844888}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545784.4298735, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.954000000325323, "prefill_cuda_event_ms": null, "kv_decode_ms": 629.0189000001192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.954000000325323, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 451.0373859804183}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545784.4298735, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.954000000325323, "prefill_cuda_event_ms": null, "kv_decode_ms": 629.0189000001192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 629.0189000001192, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 101.74575040589062}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545784.4298735, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.954000000325323, "prefill_cuda_event_ms": null, "kv_decode_ms": 629.0189000001192, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 648.9729000004445, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 112.48543660289975}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545785.079569, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 726.7101000002185, "prefill_cuda_event_ms": null, "kv_decode_ms": 2056.014100000084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 281.4957000000504, "params_millions_measured": 96.08832, "latency_ms": 726.7101000002185, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 23.393097192394723}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545785.079569, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 726.7101000002185, "prefill_cuda_event_ms": null, "kv_decode_ms": 2056.014100000084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 281.4957000000504, "params_millions_measured": 96.08832, "latency_ms": 2056.014100000084, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.128191192850956}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545785.079569, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 726.7101000002185, "prefill_cuda_event_ms": null, "kv_decode_ms": 2056.014100000084, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 281.4957000000504, "params_millions_measured": 96.08832, "latency_ms": 2782.7242000003025, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 29.10816673818814}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545788.1445851, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 47.97660000031101, "prefill_cuda_event_ms": null, "kv_decode_ms": 1824.3066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 47.97660000031101, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 354.3394071253444}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545788.1445851, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.97660000031101, "prefill_cuda_event_ms": null, "kv_decode_ms": 1824.3066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1824.3066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 35.08182232087523}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545788.1445851, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 47.97660000031101, "prefill_cuda_event_ms": null, "kv_decode_ms": 1824.3066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1872.283200000311, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 43.262685901356456}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545790.0173774, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.92319999971005, "prefill_cuda_event_ms": null, "kv_decode_ms": 1554.7910999998749, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 31.92319999971005, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 532.5280673665048}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545790.0173774, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.92319999971005, "prefill_cuda_event_ms": null, "kv_decode_ms": 1554.7910999998749, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1554.7910999998749, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.163086153506505}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545790.0173774, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.92319999971005, "prefill_cuda_event_ms": null, "kv_decode_ms": 1554.7910999998749, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1586.714299999585, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 51.04888762899609}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545791.604554, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 34.96140000015657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.913800000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 34.96140000015657, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 486.2505506050635}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545791.604554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.96140000015657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.913800000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1511.913800000002, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 42.33045561195348}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545791.604554, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 34.96140000015657, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.913800000002, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1546.8752000001587, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 52.36362959338393}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545793.151932, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 21.170600000004924, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.8278000003629, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 21.170600000004924, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 47.23531690173011}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545793.151932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.170600000004924, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.8278000003629, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 803.8278000003629, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.61904278499836}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545793.151932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 21.170600000004924, "prefill_cuda_event_ms": null, "kv_decode_ms": 803.8278000003629, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 824.9984000003678, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 78.78803158887462}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545793.9774942, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.914200000319397, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.7216000001317, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 11.914200000319397, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 83.93345755260042}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545793.9774942, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.914200000319397, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.7216000001317, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 827.7216000001317, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.3206836694727}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545793.9774942, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.914200000319397, "prefill_cuda_event_ms": null, "kv_decode_ms": 827.7216000001317, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 839.6358000004511, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 77.41451710368362}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545794.8175523, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 10.81270000031509, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.8229999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 10.81270000031509, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 92.48383844653596}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545794.8175523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.81270000031509, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.8229999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1029.8229999998512, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.14660189179038}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545794.8175523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 10.81270000031509, "prefill_cuda_event_ms": null, "kv_decode_ms": 1029.8229999998512, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1040.6357000001663, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 62.46182021238519}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545795.858585, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 16.022000000248227, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.9684999998026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.022000000248227, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 62.41418050084304}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545795.858585, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.022000000248227, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.9684999998026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 941.9684999998026, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 67.94282399041307}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545795.858585, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 16.022000000248227, "prefill_cuda_event_ms": null, "kv_decode_ms": 941.9684999998026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 957.9905000000508, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 67.8503596851916}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545796.8170407, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 29.227800000171555, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0676000001258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 29.227800000171555, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 307.9260156408342}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545796.8170407, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.227800000171555, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0676000001258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1349.0676000001258, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.44017275338466}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545796.8170407, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.227800000171555, "prefill_cuda_event_ms": null, "kv_decode_ms": 1349.0676000001258, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1378.2954000002974, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.9639727448733}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545798.1958373, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.02039999990302, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.446300000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 25.02039999990302, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 359.70647951411183}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545798.1958373, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.02039999990302, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.446300000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1346.446300000025, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 47.532530632672696}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545798.1958373, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.02039999990302, "prefill_cuda_event_ms": null, "kv_decode_ms": 1346.446300000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1371.466699999928, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 53.22768682608468}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545799.567994, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 25.027800000316347, "prefill_cuda_event_ms": null, "kv_decode_ms": 1231.692700000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 25.027800000316347, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 359.60012465683127}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545799.567994, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.027800000316347, "prefill_cuda_event_ms": null, "kv_decode_ms": 1231.692700000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1231.692700000167, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.961012677911725}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545799.567994, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 25.027800000316347, "prefill_cuda_event_ms": null, "kv_decode_ms": 1231.692700000167, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1256.7205000004833, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.08769730419129}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545800.825131, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 22.21670000017184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1363.1672999999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 22.21670000017184, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 405.10066751274434}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545800.825131, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.21670000017184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1363.1672999999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1363.1672999999864, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.949483016501816}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545800.825131, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 22.21670000017184, "prefill_cuda_event_ms": null, "kv_decode_ms": 1363.1672999999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1385.3840000001583, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 52.692971768110255}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545802.2110255, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 19.271600000138278, "prefill_cuda_event_ms": null, "kv_decode_ms": 1225.8291000002828, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 19.271600000138278, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 51.889827517841006}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545802.2110255, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.271600000138278, "prefill_cuda_event_ms": null, "kv_decode_ms": 1225.8291000002828, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1225.8291000002828, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.20956167542868}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545802.2110255, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 19.271600000138278, "prefill_cuda_event_ms": null, "kv_decode_ms": 1225.8291000002828, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1245.100700000421, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 52.20461284776245}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545803.4566207, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.725599999721453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1218.7579999999798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 17.725599999721453, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 56.41557972738381}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545803.4566207, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.725599999721453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1218.7579999999798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1218.7579999999798, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.512475815544235}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545803.4566207, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.725599999721453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1218.7579999999798, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1236.4835999997013, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 52.56842872806053}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545804.6934564, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.903399999999237, "prefill_cuda_event_ms": null, "kv_decode_ms": 1213.8707999997678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.903399999999237, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 52.900536411441344}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545804.6934564, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.903399999999237, "prefill_cuda_event_ms": null, "kv_decode_ms": 1213.8707999997678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1213.8707999997678, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.7238977986885}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545804.6934564, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.903399999999237, "prefill_cuda_event_ms": null, "kv_decode_ms": 1213.8707999997678, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1232.774199999767, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 52.726606380967645}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545805.9265394, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.242299999656097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1219.6062999996684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.242299999656097, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.81764909133453}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545805.9265394, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.242299999656097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1219.6062999996684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1219.6062999996684, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.47595064080712}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545805.9265394, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.242299999656097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1219.6062999996684, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1237.8485999993245, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 52.51046048768441}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545807.16483, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 15.879300000051444, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.2845999998208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 15.879300000051444, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1070.5761588952237}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545807.16483, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.879300000051444, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.2845999998208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 757.2845999998208, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 84.51248051263045}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545807.16483, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.879300000051444, "prefill_cuda_event_ms": null, "kv_decode_ms": 757.2845999998208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 773.1638999998722, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 104.76433263375772}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545807.9385056, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 17.400400000042282, "prefill_cuda_event_ms": null, "kv_decode_ms": 748.9869999999428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 17.400400000042282, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 976.9890347324597}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545807.9385056, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.400400000042282, "prefill_cuda_event_ms": null, "kv_decode_ms": 748.9869999999428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 748.9869999999428, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 85.44874610641425}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545807.9385056, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.400400000042282, "prefill_cuda_event_ms": null, "kv_decode_ms": 748.9869999999428, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 766.3873999999851, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 105.69067288945719}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545808.7052276, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 17.207800000051066, "prefill_cuda_event_ms": null, "kv_decode_ms": 774.6300999997402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 17.207800000051066, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 987.9240809371071}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545808.7052276, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.207800000051066, "prefill_cuda_event_ms": null, "kv_decode_ms": 774.6300999997402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 774.6300999997402, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.62007892543998}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545808.7052276, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 17.207800000051066, "prefill_cuda_event_ms": null, "kv_decode_ms": 774.6300999997402, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 791.8378999997913, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 102.29366389259891}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545809.4975562, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 14.593299999887677, "prefill_cuda_event_ms": null, "kv_decode_ms": 841.117899999972, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 14.593299999887677, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1164.9181473779643}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545809.4975562, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.593299999887677, "prefill_cuda_event_ms": null, "kv_decode_ms": 841.117899999972, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 841.117899999972, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.08921412801003}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545809.4975562, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 14.593299999887677, "prefill_cuda_event_ms": null, "kv_decode_ms": 841.117899999972, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 855.7111999998597, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 94.65810427631808}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545810.353719, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.045200000040495, "prefill_cuda_event_ms": 4.973567962646484, "kv_decode_ms": 192.30749999996988, "kv_decode_cuda_event_ms": 192.17715454101562, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 5.045200000040495, "cuda_event_ms": 4.973567962646484, "tokens_total": 9, "tokens_per_s": 1783.8737810052648}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545810.353719, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.045200000040495, "prefill_cuda_event_ms": 4.973567962646484, "kv_decode_ms": 192.30749999996988, "kv_decode_cuda_event_ms": 192.17715454101562, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 192.30749999996988, "cuda_event_ms": 192.17715454101562, "tokens_total": 64, "tokens_per_s": 332.8003328003849}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545810.353719, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.045200000040495, "prefill_cuda_event_ms": 4.973567962646484, "kv_decode_ms": 192.30749999996988, "kv_decode_cuda_event_ms": 192.17715454101562, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 197.35270000001037, "cuda_event_ms": 197.1507225036621, "tokens_total": 73, "tokens_per_s": 369.8961301263989}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545810.553017, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.650899999684043, "prefill_cuda_event_ms": 3.6024320125579834, "kv_decode_ms": 134.31649999984074, "kv_decode_cuda_event_ms": 134.28224182128906, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 3.650899999684043, "cuda_event_ms": 3.6024320125579834, "tokens_total": 9, "tokens_per_s": 2465.1455807551233}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545810.553017, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.650899999684043, "prefill_cuda_event_ms": 3.6024320125579834, "kv_decode_ms": 134.31649999984074, "kv_decode_cuda_event_ms": 134.28224182128906, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 134.31649999984074, "cuda_event_ms": 134.28224182128906, "tokens_total": 64, "tokens_per_s": 476.4865076150427}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545810.553017, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.650899999684043, "prefill_cuda_event_ms": 3.6024320125579834, "kv_decode_ms": 134.31649999984074, "kv_decode_cuda_event_ms": 134.28224182128906, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 137.9673999995248, "cuda_event_ms": 137.88467383384705, "tokens_total": 73, "tokens_per_s": 529.1105000184931}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545810.6921523, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.233899999941059, "prefill_cuda_event_ms": 3.1897599697113037, "kv_decode_ms": 112.58700000007593, "kv_decode_cuda_event_ms": 112.54083251953125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 3.233899999941059, "cuda_event_ms": 3.1897599697113037, "tokens_total": 9, "tokens_per_s": 2783.017409370739}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545810.6921523, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.233899999941059, "prefill_cuda_event_ms": 3.1897599697113037, "kv_decode_ms": 112.58700000007593, "kv_decode_cuda_event_ms": 112.54083251953125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 112.58700000007593, "cuda_event_ms": 112.54083251953125, "tokens_total": 64, "tokens_per_s": 568.4492881057034}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545810.6921523, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.233899999941059, "prefill_cuda_event_ms": 3.1897599697113037, "kv_decode_ms": 112.58700000007593, "kv_decode_cuda_event_ms": 112.54083251953125, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 115.82090000001699, "cuda_event_ms": 115.73059248924255, "tokens_total": 73, "tokens_per_s": 630.2834807879173}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545810.8084922, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.289900000050693, "prefill_cuda_event_ms": 2.244607925415039, "kv_decode_ms": 114.28849999992963, "kv_decode_cuda_event_ms": 114.2456283569336, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 2.289900000050693, "cuda_event_ms": 2.244607925415039, "tokens_total": 9, "tokens_per_s": 3930.3026332157565}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545810.8084922, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.289900000050693, "prefill_cuda_event_ms": 2.244607925415039, "kv_decode_ms": 114.28849999992963, "kv_decode_cuda_event_ms": 114.2456283569336, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 114.28849999992963, "cuda_event_ms": 114.2456283569336, "tokens_total": 64, "tokens_per_s": 559.9863503330555}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545810.8084922, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.289900000050693, "prefill_cuda_event_ms": 2.244607925415039, "kv_decode_ms": 114.28849999992963, "kv_decode_cuda_event_ms": 114.2456283569336, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 116.57839999998032, "cuda_event_ms": 116.49023628234863, "tokens_total": 73, "tokens_per_s": 626.1880416956513}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545810.9255722, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 9.33500000019194, "prefill_cuda_event_ms": 9.200575828552246, "kv_decode_ms": 401.8191999998635, "kv_decode_cuda_event_ms": 401.72344970703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 9.33500000019194, "cuda_event_ms": 9.200575828552246, "tokens_total": 17, "tokens_per_s": 1821.1033743599849}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545810.9255722, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.33500000019194, "prefill_cuda_event_ms": 9.200575828552246, "kv_decode_ms": 401.8191999998635, "kv_decode_cuda_event_ms": 401.72344970703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 401.8191999998635, "cuda_event_ms": 401.72344970703125, "tokens_total": 64, "tokens_per_s": 159.27561450528432}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545810.9255722, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 9.33500000019194, "prefill_cuda_event_ms": 9.200575828552246, "kv_decode_ms": 401.8191999998635, "kv_decode_cuda_event_ms": 401.72344970703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 411.1542000000554, "cuda_event_ms": 410.9240255355835, "tokens_total": 81, "tokens_per_s": 197.00637862872148}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545811.337366, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.271099999798025, "prefill_cuda_event_ms": 7.217152118682861, "kv_decode_ms": 400.47579999964, "kv_decode_cuda_event_ms": 400.4363098144531, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 7.271099999798025, "cuda_event_ms": 7.217152118682861, "tokens_total": 17, "tokens_per_s": 2338.0231327408815}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545811.337366, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.271099999798025, "prefill_cuda_event_ms": 7.217152118682861, "kv_decode_ms": 400.47579999964, "kv_decode_cuda_event_ms": 400.4363098144531, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 400.47579999964, "cuda_event_ms": 400.4363098144531, "tokens_total": 64, "tokens_per_s": 159.80990611681787}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545811.337366, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.271099999798025, "prefill_cuda_event_ms": 7.217152118682861, "kv_decode_ms": 400.47579999964, "kv_decode_cuda_event_ms": 400.4363098144531, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 407.746899999438, "cuda_event_ms": 407.653461933136, "tokens_total": 81, "tokens_per_s": 198.65264456973588}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545811.7457345, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.1236999997381645, "prefill_cuda_event_ms": 7.075136184692383, "kv_decode_ms": 410.8946000001197, "kv_decode_cuda_event_ms": 410.8584899902344, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 7.1236999997381645, "cuda_event_ms": 7.075136184692383, "tokens_total": 17, "tokens_per_s": 2386.40032576117}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545811.7457345, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.1236999997381645, "prefill_cuda_event_ms": 7.075136184692383, "kv_decode_ms": 410.8946000001197, "kv_decode_cuda_event_ms": 410.8584899902344, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 410.8946000001197, "cuda_event_ms": 410.8584899902344, "tokens_total": 64, "tokens_per_s": 155.75770526062246}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545811.7457345, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.1236999997381645, "prefill_cuda_event_ms": 7.075136184692383, "kv_decode_ms": 410.8946000001197, "kv_decode_cuda_event_ms": 410.8584899902344, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 418.01829999985785, "cuda_event_ms": 417.93362617492676, "tokens_total": 81, "tokens_per_s": 193.77142101201682}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545812.1642761, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.066799999847717, "prefill_cuda_event_ms": 7.019519805908203, "kv_decode_ms": 400.0458999998955, "kv_decode_cuda_event_ms": 400.01025390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 7.066799999847717, "cuda_event_ms": 7.019519805908203, "tokens_total": 17, "tokens_per_s": 2405.6149884482843}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545812.1642761, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.066799999847717, "prefill_cuda_event_ms": 7.019519805908203, "kv_decode_ms": 400.0458999998955, "kv_decode_cuda_event_ms": 400.01025390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 400.0458999998955, "cuda_event_ms": 400.01025390625, "tokens_total": 64, "tokens_per_s": 159.98164210661008}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545812.1642761, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.066799999847717, "prefill_cuda_event_ms": 7.019519805908203, "kv_decode_ms": 400.0458999998955, "kv_decode_cuda_event_ms": 400.01025390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 407.1126999997432, "cuda_event_ms": 407.0297737121582, "tokens_total": 81, "tokens_per_s": 198.96210557924402}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545812.5719106, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.639600000245991, "prefill_cuda_event_ms": 4.588479995727539, "kv_decode_ms": 206.22720000028494, "kv_decode_cuda_event_ms": 206.1925811767578, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 181.4079999999194, "params_millions_measured": 45.1712, "latency_ms": 4.639600000245991, "cuda_event_ms": 4.588479995727539, "tokens_total": 9, "tokens_per_s": 1939.8223983797784}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545812.5719106, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.639600000245991, "prefill_cuda_event_ms": 4.588479995727539, "kv_decode_ms": 206.22720000028494, "kv_decode_cuda_event_ms": 206.1925811767578, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 181.4079999999194, "params_millions_measured": 45.1712, "latency_ms": 206.22720000028494, "cuda_event_ms": 206.1925811767578, "tokens_total": 64, "tokens_per_s": 310.3373366845478}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545812.5719106, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.639600000245991, "prefill_cuda_event_ms": 4.588479995727539, "kv_decode_ms": 206.22720000028494, "kv_decode_cuda_event_ms": 206.1925811767578, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 181.4079999999194, "params_millions_measured": 45.1712, "latency_ms": 210.86680000053093, "cuda_event_ms": 210.78106117248535, "tokens_total": 73, "tokens_per_s": 346.19010673949714}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545812.9650495, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.8690999999744236, "prefill_cuda_event_ms": 3.826688051223755, "kv_decode_ms": 200.18380000010438, "kv_decode_cuda_event_ms": 200.1571807861328, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.8690999999744236, "cuda_event_ms": 3.826688051223755, "tokens_total": 9, "tokens_per_s": 2326.122354051199}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545812.9650495, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8690999999744236, "prefill_cuda_event_ms": 3.826688051223755, "kv_decode_ms": 200.18380000010438, "kv_decode_cuda_event_ms": 200.1571807861328, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 200.18380000010438, "cuda_event_ms": 200.1571807861328, "tokens_total": 64, "tokens_per_s": 319.7061900112128}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545812.9650495, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.8690999999744236, "prefill_cuda_event_ms": 3.826688051223755, "kv_decode_ms": 200.18380000010438, "kv_decode_cuda_event_ms": 200.1571807861328, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 204.0529000000788, "cuda_event_ms": 203.98386883735657, "tokens_total": 73, "tokens_per_s": 357.75036767412666}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545813.1696243, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.7771000002067012, "prefill_cuda_event_ms": 3.737600088119507, "kv_decode_ms": 197.9647000002842, "kv_decode_cuda_event_ms": 197.92076110839844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.7771000002067012, "cuda_event_ms": 3.737600088119507, "tokens_total": 9, "tokens_per_s": 2382.7804398897238}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545813.1696243, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7771000002067012, "prefill_cuda_event_ms": 3.737600088119507, "kv_decode_ms": 197.9647000002842, "kv_decode_cuda_event_ms": 197.92076110839844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 197.9647000002842, "cuda_event_ms": 197.92076110839844, "tokens_total": 64, "tokens_per_s": 323.2899602803334}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545813.1696243, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.7771000002067012, "prefill_cuda_event_ms": 3.737600088119507, "kv_decode_ms": 197.9647000002842, "kv_decode_cuda_event_ms": 197.92076110839844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 201.7418000004909, "cuda_event_ms": 201.65836119651794, "tokens_total": 73, "tokens_per_s": 361.8486600190063}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545813.3719454, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 3.953899999942223, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 200.65450000038254, "kv_decode_cuda_event_ms": 200.62506103515625, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 3.953899999942223, "cuda_event_ms": 3.9075839519500732, "tokens_total": 9, "tokens_per_s": 2276.233592182785}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545813.3719454, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.953899999942223, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 200.65450000038254, "kv_decode_cuda_event_ms": 200.62506103515625, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 200.65450000038254, "cuda_event_ms": 200.62506103515625, "tokens_total": 64, "tokens_per_s": 318.95621578323926}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545813.3719454, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 3.953899999942223, "prefill_cuda_event_ms": 3.9075839519500732, "kv_decode_ms": 200.65450000038254, "kv_decode_cuda_event_ms": 200.62506103515625, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 204.60840000032476, "cuda_event_ms": 204.53264498710632, "tokens_total": 73, "tokens_per_s": 356.77909606782583}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545813.577305, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.38050000014846, "prefill_cuda_event_ms": null, "kv_decode_ms": 750.6219000001693, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 18.38050000014846, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.40548407235511}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545813.577305, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.38050000014846, "prefill_cuda_event_ms": null, "kv_decode_ms": 750.6219000001693, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 750.6219000001693, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 85.26263355756815}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545813.577305, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.38050000014846, "prefill_cuda_event_ms": null, "kv_decode_ms": 750.6219000001693, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 769.0024000003177, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 84.52509380981535}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545814.3466282, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.003400000139663, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.8335000002589, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.003400000139663, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 83.30972890917279}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545814.3466282, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.003400000139663, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.8335000002589, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 739.8335000002589, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 86.5059503252794}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545814.3466282, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.003400000139663, "prefill_cuda_event_ms": null, "kv_decode_ms": 739.8335000002589, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 751.8369000003986, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 86.45492127343782}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545815.0988073, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 11.805500000264146, "prefill_cuda_event_ms": null, "kv_decode_ms": 763.079899999866, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 11.805500000264146, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 84.70628096883868}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545815.0988073, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.805500000264146, "prefill_cuda_event_ms": null, "kv_decode_ms": 763.079899999866, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 763.079899999866, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 83.87064054499567}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545815.0988073, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 11.805500000264146, "prefill_cuda_event_ms": null, "kv_decode_ms": 763.079899999866, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 774.8854000001302, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 83.88337165726581}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545815.8741395, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.580300000081479, "prefill_cuda_event_ms": null, "kv_decode_ms": 787.3878000000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.580300000081479, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 79.48936034860245}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545815.8741395, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.580300000081479, "prefill_cuda_event_ms": null, "kv_decode_ms": 787.3878000000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 787.3878000000332, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 81.28142193719195}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545815.8741395, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.580300000081479, "prefill_cuda_event_ms": null, "kv_decode_ms": 787.3878000000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 799.9681000001146, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 81.25323997293228}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545816.674624, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 20.054999999956635, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.711399999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 20.054999999956635, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 847.6689104979686}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545816.674624, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.054999999956635, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.711399999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 732.711399999971, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.34680530424738}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545816.674624, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 20.054999999956635, "prefill_cuda_event_ms": null, "kv_decode_ms": 732.711399999971, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 752.7663999999277, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 107.60310237014801}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545817.428079, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 18.659700000171142, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.9188999997168, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 18.659700000171142, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 911.054304187317}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545817.428079, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.659700000171142, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.9188999997168, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 723.9188999997168, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.4076931822405}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545817.428079, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.659700000171142, "prefill_cuda_event_ms": null, "kv_decode_ms": 723.9188999997168, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 742.578599999888, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 109.07936210390687}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545818.1711018, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 18.69449999958306, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.3517999998367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 18.69449999958306, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 909.3583674545534}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545818.1711018, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.69449999958306, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.3517999998367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 703.3517999998367, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.99287156159245}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545818.1711018, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 18.69449999958306, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.3517999998367, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 722.0462999994197, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 112.18117176151321}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545818.8937159, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 16.301799999837385, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.5204000001686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 16.301799999837385, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1042.829626186653}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545818.8937159, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.301799999837385, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.5204000001686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 720.5204000001686, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.82468837799044}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545818.8937159, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 16.301799999837385, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.5204000001686, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 736.822200000006, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 109.93154115063219}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545819.630988, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 96.37350000002698, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.4245000001793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.21319999991465, "params_millions_measured": 51.475968, "latency_ms": 96.37350000002698, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 93.38666749674424}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545819.630988, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 96.37350000002698, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.4245000001793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.21319999991465, "params_millions_measured": 51.475968, "latency_ms": 887.4245000001793, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.11881123406788}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545819.630988, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 96.37350000002698, "prefill_cuda_event_ms": null, "kv_decode_ms": 887.4245000001793, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 127.21319999991465, "params_millions_measured": 51.475968, "latency_ms": 983.7980000002062, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 74.20222444036753}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545820.744933, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.364300000077492, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.0459000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.364300000077492, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 549.9776953464176}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545820.744933, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.364300000077492, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.0459000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 857.0459000002302, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 74.67511366658754}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545820.744933, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.364300000077492, "prefill_cuda_event_ms": null, "kv_decode_ms": 857.0459000002302, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 873.4102000003077, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 83.58042990564374}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545821.6187913, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.28069999969739, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.4492000001374, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.28069999969739, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 552.8017837173637}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545821.6187913, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.28069999969739, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.4492000001374, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 852.4492000001374, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.07778762651157}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545821.6187913, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.28069999969739, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.4492000001374, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 868.7298999998347, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 84.03072117123388}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545822.4882445, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 16.39160000013362, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.436000000409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.39160000013362, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 549.061714532238}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545822.4882445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.39160000013362, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.436000000409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 840.436000000409, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.15095022103867}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545822.4882445, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 16.39160000013362, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.436000000409, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 856.8276000005426, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 85.19800249192927}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545823.345535, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 46.29179999983535, "prefill_cuda_event_ms": 46.20083236694336, "kv_decode_ms": 486.34210000000166, "kv_decode_cuda_event_ms": 486.2618408203125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 46.29179999983535, "cuda_event_ms": 46.20083236694336, "tokens_total": 17, "tokens_per_s": 367.2356659291811}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545823.345535, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 46.29179999983535, "prefill_cuda_event_ms": 46.20083236694336, "kv_decode_ms": 486.34210000000166, "kv_decode_cuda_event_ms": 486.2618408203125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 486.34210000000166, "cuda_event_ms": 486.2618408203125, "tokens_total": 64, "tokens_per_s": 131.59461210534678}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545823.345535, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 46.29179999983535, "prefill_cuda_event_ms": 46.20083236694336, "kv_decode_ms": 486.34210000000166, "kv_decode_cuda_event_ms": 486.2618408203125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 532.633899999837, "cuda_event_ms": 532.4626731872559, "tokens_total": 81, "tokens_per_s": 152.07443611836345}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545823.8801935, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.895399999895744, "prefill_cuda_event_ms": 4.856832027435303, "kv_decode_ms": 243.92330000000584, "kv_decode_cuda_event_ms": 243.89634704589844, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.895399999895744, "cuda_event_ms": 4.856832027435303, "tokens_total": 17, "tokens_per_s": 3472.647791878507}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545823.8801935, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.895399999895744, "prefill_cuda_event_ms": 4.856832027435303, "kv_decode_ms": 243.92330000000584, "kv_decode_cuda_event_ms": 243.89634704589844, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 243.92330000000584, "cuda_event_ms": 243.89634704589844, "tokens_total": 64, "tokens_per_s": 262.3775588473855}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545823.8801935, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.895399999895744, "prefill_cuda_event_ms": 4.856832027435303, "kv_decode_ms": 243.92330000000584, "kv_decode_cuda_event_ms": 243.89634704589844, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 248.81869999990158, "cuda_event_ms": 248.75317907333374, "tokens_total": 81, "tokens_per_s": 325.53823325992795}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545824.1297143, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.474999999729334, "prefill_cuda_event_ms": 4.440063953399658, "kv_decode_ms": 232.70190000039292, "kv_decode_cuda_event_ms": 232.67840576171875, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.474999999729334, "cuda_event_ms": 4.440063953399658, "tokens_total": 17, "tokens_per_s": 3798.882681794017}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545824.1297143, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.474999999729334, "prefill_cuda_event_ms": 4.440063953399658, "kv_decode_ms": 232.70190000039292, "kv_decode_cuda_event_ms": 232.67840576171875, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 232.70190000039292, "cuda_event_ms": 232.67840576171875, "tokens_total": 64, "tokens_per_s": 275.02998471388474}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545824.1297143, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.474999999729334, "prefill_cuda_event_ms": 4.440063953399658, "kv_decode_ms": 232.70190000039292, "kv_decode_cuda_event_ms": 232.67840576171875, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 237.17690000012226, "cuda_event_ms": 237.1184697151184, "tokens_total": 81, "tokens_per_s": 341.5172388202993}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545824.3673885, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.4394999999894935, "prefill_cuda_event_ms": 4.4039998054504395, "kv_decode_ms": 237.1314000001803, "kv_decode_cuda_event_ms": 237.1082305908203, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.4394999999894935, "cuda_event_ms": 4.4039998054504395, "tokens_total": 17, "tokens_per_s": 3829.2600518166983}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545824.3673885, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.4394999999894935, "prefill_cuda_event_ms": 4.4039998054504395, "kv_decode_ms": 237.1314000001803, "kv_decode_cuda_event_ms": 237.1082305908203, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 237.1314000001803, "cuda_event_ms": 237.1082305908203, "tokens_total": 64, "tokens_per_s": 269.89255745949856}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545824.3673885, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.4394999999894935, "prefill_cuda_event_ms": 4.4039998054504395, "kv_decode_ms": 237.1314000001803, "kv_decode_cuda_event_ms": 237.1082305908203, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 241.5709000001698, "cuda_event_ms": 241.51223039627075, "tokens_total": 81, "tokens_per_s": 335.30528718460323}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 360, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545824.6094363, "prompt_tokens": 84, "prefill_ms": 731.1652, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.0771, "kv_decode_ms_equiv": 2314.2351157894736, "kv_decode_ms_per_token": 36.159923684210526, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 13143.182700000125, "ollama_total_duration_ms": 13029.7325, "ollama_load_ms": 10855.5231, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 731.1652, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 114.8851176177422}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 361, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545824.6094363, "prompt_tokens": 84, "prefill_ms": 731.1652, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.0771, "kv_decode_ms_equiv": 2314.2351157894736, "kv_decode_ms_per_token": 36.159923684210526, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13143.182700000125, "ollama_total_duration_ms": 13029.7325, "ollama_load_ms": 10855.5231, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2314.2351157894736, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.65492562244142}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 120, "sample_idx": 362, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545824.6094363, "prompt_tokens": 84, "prefill_ms": 731.1652, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.0771, "kv_decode_ms_equiv": 2314.2351157894736, "kv_decode_ms_per_token": 36.159923684210526, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13143.182700000125, "ollama_total_duration_ms": 13029.7325, "ollama_load_ms": 10855.5231, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 3045.4003157894736, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 48.59788029595487}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 363, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545837.7528067, "prompt_tokens": 84, "prefill_ms": 41.6855, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.4751, "kv_decode_ms_equiv": 2527.011952941177, "kv_decode_ms_per_token": 39.48456176470589, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1643.0405000000974, "ollama_total_duration_ms": 1617.2952, "ollama_load_ms": 218.0987, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 41.6855, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 2015.0891796907797}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 364, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545837.7528067, "prompt_tokens": 84, "prefill_ms": 41.6855, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.4751, "kv_decode_ms_equiv": 2527.011952941177, "kv_decode_ms_per_token": 39.48456176470589, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1643.0405000000974, "ollama_total_duration_ms": 1617.2952, "ollama_load_ms": 218.0987, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2527.011952941177, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.326354283963994}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 121, "sample_idx": 365, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545837.7528067, "prompt_tokens": 84, "prefill_ms": 41.6855, "prefill_cuda_event_ms": null, "kv_decode_ms": 1342.4751, "kv_decode_ms_equiv": 2527.011952941177, "kv_decode_ms_per_token": 39.48456176470589, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1643.0405000000974, "ollama_total_duration_ms": 1617.2952, "ollama_load_ms": 218.0987, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2568.697452941177, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 57.616750400300724}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 366, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545839.3959804, "prompt_tokens": 84, "prefill_ms": 47.051, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.4477, "kv_decode_ms_equiv": 2402.725082352941, "kv_decode_ms_per_token": 37.542579411764706, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1583.948099999816, "ollama_total_duration_ms": 1568.2376, "ollama_load_ms": 211.5318, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 47.051, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1785.29680559393}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 367, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545839.3959804, "prompt_tokens": 84, "prefill_ms": 47.051, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.4477, "kv_decode_ms_equiv": 2402.725082352941, "kv_decode_ms_per_token": 37.542579411764706, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1583.948099999816, "ollama_total_duration_ms": 1568.2376, "ollama_load_ms": 211.5318, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2402.725082352941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.636422314835148}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 122, "sample_idx": 368, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545839.3959804, "prompt_tokens": 84, "prefill_ms": 47.051, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.4477, "kv_decode_ms_equiv": 2402.725082352941, "kv_decode_ms_per_token": 37.542579411764706, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1583.948099999816, "ollama_total_duration_ms": 1568.2376, "ollama_load_ms": 211.5318, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2449.776082352941, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 60.41368477148742}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 369, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545840.9800627, "prompt_tokens": 84, "prefill_ms": 45.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1390.1523, "kv_decode_ms_equiv": 2616.7572705882353, "kv_decode_ms_per_token": 40.88683235294118, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1668.8779999999497, "ollama_total_duration_ms": 1653.7417, "ollama_load_ms": 203.8663, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 45.3955, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1850.4036743730105}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 370, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545840.9800627, "prompt_tokens": 84, "prefill_ms": 45.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1390.1523, "kv_decode_ms_equiv": 2616.7572705882353, "kv_decode_ms_per_token": 40.88683235294118, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1668.8779999999497, "ollama_total_duration_ms": 1653.7417, "ollama_load_ms": 203.8663, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2616.7572705882353, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.4577518592747}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 123, "sample_idx": 371, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545840.9800627, "prompt_tokens": 84, "prefill_ms": 45.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 1390.1523, "kv_decode_ms_equiv": 2616.7572705882353, "kv_decode_ms_per_token": 40.88683235294118, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1668.8779999999497, "ollama_total_duration_ms": 1653.7417, "ollama_load_ms": 203.8663, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2662.1527705882354, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 55.594104754287855}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 372, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545842.6491451, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.962800000314019, "prefill_cuda_event_ms": 7.864319801330566, "kv_decode_ms": 337.6490000000558, "kv_decode_cuda_event_ms": 337.59027099609375, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 7.962800000314019, "cuda_event_ms": 7.864319801330566, "tokens_total": 9, "tokens_per_s": 1130.255688909062}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 373, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545842.6491451, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.962800000314019, "prefill_cuda_event_ms": 7.864319801330566, "kv_decode_ms": 337.6490000000558, "kv_decode_cuda_event_ms": 337.59027099609375, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 337.6490000000558, "cuda_event_ms": 337.59027099609375, "tokens_total": 64, "tokens_per_s": 189.54594860340003}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 124, "sample_idx": 374, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545842.6491451, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.962800000314019, "prefill_cuda_event_ms": 7.864319801330566, "kv_decode_ms": 337.6490000000558, "kv_decode_cuda_event_ms": 337.59027099609375, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 345.61180000036984, "cuda_event_ms": 345.4545907974243, "tokens_total": 73, "tokens_per_s": 211.2196400699336}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 375, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545842.996471, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 7.415800000217132, "prefill_cuda_event_ms": 7.34822416305542, "kv_decode_ms": 347.2385000000031, "kv_decode_cuda_event_ms": 347.1759338378906, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 7.415800000217132, "cuda_event_ms": 7.34822416305542, "tokens_total": 9, "tokens_per_s": 1213.6249628814805}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 376, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545842.996471, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.415800000217132, "prefill_cuda_event_ms": 7.34822416305542, "kv_decode_ms": 347.2385000000031, "kv_decode_cuda_event_ms": 347.1759338378906, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 347.2385000000031, "cuda_event_ms": 347.1759338378906, "tokens_total": 64, "tokens_per_s": 184.31135948346576}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 125, "sample_idx": 377, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545842.996471, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 7.415800000217132, "prefill_cuda_event_ms": 7.34822416305542, "kv_decode_ms": 347.2385000000031, "kv_decode_cuda_event_ms": 347.1759338378906, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 354.65430000022025, "cuda_event_ms": 354.52415800094604, "tokens_total": 73, "tokens_per_s": 205.8342447841593}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 378, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545843.3521688, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.911400000059075, "prefill_cuda_event_ms": 5.8572797775268555, "kv_decode_ms": 342.53460000036284, "kv_decode_cuda_event_ms": 342.4286804199219, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 5.911400000059075, "cuda_event_ms": 5.8572797775268555, "tokens_total": 9, "tokens_per_s": 1522.481983947975}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 379, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545843.3521688, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.911400000059075, "prefill_cuda_event_ms": 5.8572797775268555, "kv_decode_ms": 342.53460000036284, "kv_decode_cuda_event_ms": 342.4286804199219, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 342.53460000036284, "cuda_event_ms": 342.4286804199219, "tokens_total": 64, "tokens_per_s": 186.84243869066717}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 126, "sample_idx": 380, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545843.3521688, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.911400000059075, "prefill_cuda_event_ms": 5.8572797775268555, "kv_decode_ms": 342.53460000036284, "kv_decode_cuda_event_ms": 342.4286804199219, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 348.4460000004219, "cuda_event_ms": 348.28596019744873, "tokens_total": 73, "tokens_per_s": 209.5016157450842}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 381, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545843.7013962, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 6.126399999629939, "prefill_cuda_event_ms": 6.06822395324707, "kv_decode_ms": 348.45439999980954, "kv_decode_cuda_event_ms": 348.4088439941406, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 6.126399999629939, "cuda_event_ms": 6.06822395324707, "tokens_total": 9, "tokens_per_s": 1469.0519718829391}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 382, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545843.7013962, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.126399999629939, "prefill_cuda_event_ms": 6.06822395324707, "kv_decode_ms": 348.45439999980954, "kv_decode_cuda_event_ms": 348.4088439941406, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 348.45439999980954, "cuda_event_ms": 348.4088439941406, "tokens_total": 64, "tokens_per_s": 183.66822172437764}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 127, "sample_idx": 383, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545843.7013962, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 6.126399999629939, "prefill_cuda_event_ms": 6.06822395324707, "kv_decode_ms": 348.45439999980954, "kv_decode_cuda_event_ms": 348.4088439941406, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 354.5807999994395, "cuda_event_ms": 354.4770679473877, "tokens_total": 73, "tokens_per_s": 205.87691155334807}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 384, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545844.0565813, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 510.2765999999974, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.0966999997217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 262.9696999997577, "params_millions_measured": 74.824704, "latency_ms": 510.2765999999974, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 33.31526470153655}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 385, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545844.0565813, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 510.2765999999974, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.0966999997217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 262.9696999997577, "params_millions_measured": 74.824704, "latency_ms": 1275.0966999997217, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.19227169203243}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 128, "sample_idx": 386, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545844.0565813, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 510.2765999999974, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.0966999997217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 262.9696999997577, "params_millions_measured": 74.824704, "latency_ms": 1785.3732999997192, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 45.36866323698956}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 387, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545846.1055877, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 27.77040000000852, "prefill_cuda_event_ms": null, "kv_decode_ms": 1124.7780000003331, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 27.77040000000852, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 612.1625903838182}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 388, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545846.1055877, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.77040000000852, "prefill_cuda_event_ms": null, "kv_decode_ms": 1124.7780000003331, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1124.7780000003331, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.900117178661965}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 129, "sample_idx": 389, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545846.1055877, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 27.77040000000852, "prefill_cuda_event_ms": null, "kv_decode_ms": 1124.7780000003331, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1152.5484000003416, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 70.27904424662425}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 390, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545847.2585824, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 21.340199999940523, "prefill_cuda_event_ms": null, "kv_decode_ms": 911.2632999999732, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.340199999940523, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 796.6185883940816}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 391, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545847.2585824, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.340199999940523, "prefill_cuda_event_ms": null, "kv_decode_ms": 911.2632999999732, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 911.2632999999732, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.23217109698358}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 130, "sample_idx": 392, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545847.2585824, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 21.340199999940523, "prefill_cuda_event_ms": null, "kv_decode_ms": 911.2632999999732, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 932.6034999999138, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 86.85363072303234}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 393, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545848.1916997, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 24.82549999967887, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.9987000002438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 24.82549999967887, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 684.7797627528108}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 394, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545848.1916997, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.82549999967887, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.9987000002438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 897.9987000002438, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.2695909247782}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 131, "sample_idx": 395, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545848.1916997, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.82549999967887, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.9987000002438, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 922.8241999999227, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 87.77403106681292}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 396, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545849.11498, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 11.345100000198727, "prefill_cuda_event_ms": 11.271167755126953, "kv_decode_ms": 359.2115000001286, "kv_decode_cuda_event_ms": 359.1362609863281, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 11.345100000198727, "cuda_event_ms": 11.271167755126953, "tokens_total": 9, "tokens_per_s": 793.2940211934978}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 397, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545849.11498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.345100000198727, "prefill_cuda_event_ms": 11.271167755126953, "kv_decode_ms": 359.2115000001286, "kv_decode_cuda_event_ms": 359.1362609863281, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 359.2115000001286, "cuda_event_ms": 359.1362609863281, "tokens_total": 64, "tokens_per_s": 178.16801522216602}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 132, "sample_idx": 398, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545849.11498, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 11.345100000198727, "prefill_cuda_event_ms": 11.271167755126953, "kv_decode_ms": 359.2115000001286, "kv_decode_cuda_event_ms": 359.1362609863281, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 370.5566000003273, "cuda_event_ms": 370.4074287414551, "tokens_total": 73, "tokens_per_s": 197.0009439851713}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 399, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545849.4871764, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.287599999974191, "prefill_cuda_event_ms": 5.244639873504639, "kv_decode_ms": 274.10550000013245, "kv_decode_cuda_event_ms": 274.0548400878906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 5.287599999974191, "cuda_event_ms": 5.244639873504639, "tokens_total": 9, "tokens_per_s": 1702.0954686519271}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 400, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545849.4871764, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.287599999974191, "prefill_cuda_event_ms": 5.244639873504639, "kv_decode_ms": 274.10550000013245, "kv_decode_cuda_event_ms": 274.0548400878906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 274.10550000013245, "cuda_event_ms": 274.0548400878906, "tokens_total": 64, "tokens_per_s": 233.4867414187934}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 133, "sample_idx": 401, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545849.4871764, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.287599999974191, "prefill_cuda_event_ms": 5.244639873504639, "kv_decode_ms": 274.10550000013245, "kv_decode_cuda_event_ms": 274.0548400878906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 279.39310000010664, "cuda_event_ms": 279.29947996139526, "tokens_total": 73, "tokens_per_s": 261.2806114394813}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 402, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545849.7672486, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 4.2582999999467575, "prefill_cuda_event_ms": 4.217855930328369, "kv_decode_ms": 283.34610000001703, "kv_decode_cuda_event_ms": 283.3233947753906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 4.2582999999467575, "cuda_event_ms": 4.217855930328369, "tokens_total": 9, "tokens_per_s": 2113.519479630963}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 403, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545849.7672486, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.2582999999467575, "prefill_cuda_event_ms": 4.217855930328369, "kv_decode_ms": 283.34610000001703, "kv_decode_cuda_event_ms": 283.3233947753906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 283.34610000001703, "cuda_event_ms": 283.3233947753906, "tokens_total": 64, "tokens_per_s": 225.87217540667103}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 134, "sample_idx": 404, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545849.7672486, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 4.2582999999467575, "prefill_cuda_event_ms": 4.217855930328369, "kv_decode_ms": 283.34610000001703, "kv_decode_cuda_event_ms": 283.3233947753906, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 287.6043999999638, "cuda_event_ms": 287.541250705719, "tokens_total": 73, "tokens_per_s": 253.8208733941803}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 405, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.0554867, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 5.017299999963143, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 256.11209999988205, "kv_decode_cuda_event_ms": 256.04888916015625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 5.017299999963143, "cuda_event_ms": 4.979712009429932, "tokens_total": 9, "tokens_per_s": 1793.7934745911375}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 406, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.0554867, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.017299999963143, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 256.11209999988205, "kv_decode_cuda_event_ms": 256.04888916015625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 256.11209999988205, "cuda_event_ms": 256.04888916015625, "tokens_total": 64, "tokens_per_s": 249.89057525993294}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 135, "sample_idx": 407, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.0554867, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 5.017299999963143, "prefill_cuda_event_ms": 4.979712009429932, "kv_decode_ms": 256.11209999988205, "kv_decode_cuda_event_ms": 256.04888916015625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 261.1293999998452, "cuda_event_ms": 261.0286011695862, "tokens_total": 73, "tokens_per_s": 279.5548873472052}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 408, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.3171983, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.8355999997984327, "prefill_cuda_event_ms": null, "kv_decode_ms": 156.3920999997208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.8355999997984327, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 352.6590492562719}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 409, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.3171983, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.8355999997984327, "prefill_cuda_event_ms": null, "kv_decode_ms": 156.3920999997208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 156.3920999997208, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 409.2278318413414}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 136, "sample_idx": 410, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.3171983, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.8355999997984327, "prefill_cuda_event_ms": null, "kv_decode_ms": 156.3920999997208, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 159.22769999951925, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 408.2204289843806}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 411, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.4769046, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.2707000002810673, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.40269999991506, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.2707000002810673, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 440.3928303502092}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 412, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.4769046, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.2707000002810673, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.40269999991506, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 138.40269999991506, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 462.4187244904852}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 137, "sample_idx": 413, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.4769046, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.2707000002810673, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.40269999991506, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 140.67340000019612, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 462.06319033953383}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 414, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.6179523, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.15459999981249, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.98919999986902, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.15459999981249, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 464.12327118120663}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 415, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.6179523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.15459999981249, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.98919999986902, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 123.98919999986902, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 516.1739893480046}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 138, "sample_idx": 416, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.6179523, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.15459999981249, "prefill_cuda_event_ms": null, "kv_decode_ms": 123.98919999986902, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 126.1437999996815, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 515.2849367163833}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 417, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.7444782, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.4012000001457636, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.20350000017788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 1.4012000001457636, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 713.673993645427}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 418, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.7444782, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.4012000001457636, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.20350000017788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 127.20350000017788, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 503.1308100792077}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 139, "sample_idx": 419, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.7444782, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.4012000001457636, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.20350000017788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 128.60470000032365, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 505.4247628573172}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 420, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545850.8733807, "prompt_tokens": 99, "prefill_ms": 234.6691, "prefill_cuda_event_ms": null, "kv_decode_ms": 2393.6039, "kv_decode_ms_equiv": 2393.6039, "kv_decode_ms_per_token": 37.4000609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2876.086799999939, "ollama_total_duration_ms": 2863.7636, "ollama_load_ms": 220.4793, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 234.6691, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 421.87062548925275}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 421, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545850.8733807, "prompt_tokens": 99, "prefill_ms": 234.6691, "prefill_cuda_event_ms": null, "kv_decode_ms": 2393.6039, "kv_decode_ms_equiv": 2393.6039, "kv_decode_ms_per_token": 37.4000609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2876.086799999939, "ollama_total_duration_ms": 2863.7636, "ollama_load_ms": 220.4793, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2393.6039, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.737924349137298}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 140, "sample_idx": 422, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545850.8733807, "prompt_tokens": 99, "prefill_ms": 234.6691, "prefill_cuda_event_ms": null, "kv_decode_ms": 2393.6039, "kv_decode_ms_equiv": 2393.6039, "kv_decode_ms_per_token": 37.4000609375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2876.086799999939, "ollama_total_duration_ms": 2863.7636, "ollama_load_ms": 220.4793, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2628.273, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 62.01791062039597}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 423, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545853.7497082, "prompt_tokens": 99, "prefill_ms": 49.8563, "prefill_cuda_event_ms": null, "kv_decode_ms": 2563.2947, "kv_decode_ms_equiv": 2563.2947, "kv_decode_ms_per_token": 40.0514796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2825.4689999998845, "ollama_total_duration_ms": 2822.7462, "ollama_load_ms": 207.0423, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 49.8563, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1985.7069216929456}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 424, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545853.7497082, "prompt_tokens": 99, "prefill_ms": 49.8563, "prefill_cuda_event_ms": null, "kv_decode_ms": 2563.2947, "kv_decode_ms_equiv": 2563.2947, "kv_decode_ms_per_token": 40.0514796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2825.4689999998845, "ollama_total_duration_ms": 2822.7462, "ollama_load_ms": 207.0423, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2563.2947, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.967866550810566}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 141, "sample_idx": 425, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545853.7497082, "prompt_tokens": 99, "prefill_ms": 49.8563, "prefill_cuda_event_ms": null, "kv_decode_ms": 2563.2947, "kv_decode_ms_equiv": 2563.2947, "kv_decode_ms_per_token": 40.0514796875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2825.4689999998845, "ollama_total_duration_ms": 2822.7462, "ollama_load_ms": 207.0423, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2613.151, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 62.37680103445994}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 426, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545856.5753298, "prompt_tokens": 99, "prefill_ms": 48.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2862.8533, "kv_decode_ms_equiv": 2862.8533, "kv_decode_ms_per_token": 44.7320828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3141.482399999859, "ollama_total_duration_ms": 3138.8438, "ollama_load_ms": 206.8881, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 48.2823, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2050.440844781628}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 427, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545856.5753298, "prompt_tokens": 99, "prefill_ms": 48.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2862.8533, "kv_decode_ms_equiv": 2862.8533, "kv_decode_ms_per_token": 44.7320828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3141.482399999859, "ollama_total_duration_ms": 3138.8438, "ollama_load_ms": 206.8881, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2862.8533, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 22.35531942904654}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 142, "sample_idx": 428, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545856.5753298, "prompt_tokens": 99, "prefill_ms": 48.2823, "prefill_cuda_event_ms": null, "kv_decode_ms": 2862.8533, "kv_decode_ms_equiv": 2862.8533, "kv_decode_ms_per_token": 44.7320828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3141.482399999859, "ollama_total_duration_ms": 3138.8438, "ollama_load_ms": 206.8881, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2911.1356, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 55.99189539642193}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 429, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545859.7169628, "prompt_tokens": 99, "prefill_ms": 55.3192, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8601, "kv_decode_ms_equiv": 2962.8601, "kv_decode_ms_per_token": 46.2946890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3230.2670999997645, "ollama_total_duration_ms": 3217.016, "ollama_load_ms": 191.2331, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.3192, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1789.6137326642468}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 430, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545859.7169628, "prompt_tokens": 99, "prefill_ms": 55.3192, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8601, "kv_decode_ms_equiv": 2962.8601, "kv_decode_ms_per_token": 46.2946890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3230.2670999997645, "ollama_total_duration_ms": 3217.016, "ollama_load_ms": 191.2331, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2962.8601, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.600749897033612}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 143, "sample_idx": 431, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545859.7169628, "prompt_tokens": 99, "prefill_ms": 55.3192, "prefill_cuda_event_ms": null, "kv_decode_ms": 2962.8601, "kv_decode_ms_equiv": 2962.8601, "kv_decode_ms_per_token": 46.2946890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3230.2670999997645, "ollama_total_duration_ms": 3217.016, "ollama_load_ms": 191.2331, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3018.1793, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 54.00606915566613}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 432, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545862.9474597, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 12.182899999970687, "prefill_cuda_event_ms": 11.64799976348877, "kv_decode_ms": 174.13929999975153, "kv_decode_cuda_event_ms": 174.060546875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 12.182899999970687, "cuda_event_ms": 11.64799976348877, "tokens_total": 9, "tokens_per_s": 738.7403655961762}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 433, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545862.9474597, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.182899999970687, "prefill_cuda_event_ms": 11.64799976348877, "kv_decode_ms": 174.13929999975153, "kv_decode_cuda_event_ms": 174.060546875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 174.13929999975153, "cuda_event_ms": 174.060546875, "tokens_total": 64, "tokens_per_s": 367.5218632444906}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 144, "sample_idx": 434, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545862.9474597, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 12.182899999970687, "prefill_cuda_event_ms": 11.64799976348877, "kv_decode_ms": 174.13929999975153, "kv_decode_cuda_event_ms": 174.060546875, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 186.32219999972222, "cuda_event_ms": 185.70854663848877, "tokens_total": 73, "tokens_per_s": 391.7944292205053}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 435, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.135434, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.314700000169978, "prefill_cuda_event_ms": 2.268160104751587, "kv_decode_ms": 94.00210000012521, "kv_decode_cuda_event_ms": 93.97350311279297, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 2.314700000169978, "cuda_event_ms": 2.268160104751587, "tokens_total": 9, "tokens_per_s": 3888.1928540800504}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 436, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.135434, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.314700000169978, "prefill_cuda_event_ms": 2.268160104751587, "kv_decode_ms": 94.00210000012521, "kv_decode_cuda_event_ms": 93.97350311279297, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 94.00210000012521, "cuda_event_ms": 93.97350311279297, "tokens_total": 64, "tokens_per_s": 680.8358536661921}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 145, "sample_idx": 437, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.135434, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.314700000169978, "prefill_cuda_event_ms": 2.268160104751587, "kv_decode_ms": 94.00210000012521, "kv_decode_cuda_event_ms": 93.97350311279297, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 96.31680000029519, "cuda_event_ms": 96.24166321754456, "tokens_total": 73, "tokens_per_s": 757.9155453646329}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 438, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.2325318, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.0744999997077684, "prefill_cuda_event_ms": 2.037760019302368, "kv_decode_ms": 92.16590000005453, "kv_decode_cuda_event_ms": 92.13030242919922, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 2.0744999997077684, "cuda_event_ms": 2.037760019302368, "tokens_total": 9, "tokens_per_s": 4338.39479453739}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 439, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.2325318, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.0744999997077684, "prefill_cuda_event_ms": 2.037760019302368, "kv_decode_ms": 92.16590000005453, "kv_decode_cuda_event_ms": 92.13030242919922, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 92.16590000005453, "cuda_event_ms": 92.13030242919922, "tokens_total": 64, "tokens_per_s": 694.3999895835893}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 146, "sample_idx": 440, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.2325318, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.0744999997077684, "prefill_cuda_event_ms": 2.037760019302368, "kv_decode_ms": 92.16590000005453, "kv_decode_cuda_event_ms": 92.13030242919922, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 94.2403999997623, "cuda_event_ms": 94.16806244850159, "tokens_total": 73, "tokens_per_s": 774.6147087680456}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 441, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.3272536, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 2.070399999865913, "prefill_cuda_event_ms": 2.0357120037078857, "kv_decode_ms": 91.30579999964539, "kv_decode_cuda_event_ms": 91.26297760009766, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 2.070399999865913, "cuda_event_ms": 2.0357120037078857, "tokens_total": 9, "tokens_per_s": 4346.986089926041}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 442, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.3272536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.070399999865913, "prefill_cuda_event_ms": 2.0357120037078857, "kv_decode_ms": 91.30579999964539, "kv_decode_cuda_event_ms": 91.26297760009766, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 91.30579999964539, "cuda_event_ms": 91.26297760009766, "tokens_total": 64, "tokens_per_s": 700.9412326516888}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 147, "sample_idx": 443, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.3272536, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 2.070399999865913, "prefill_cuda_event_ms": 2.0357120037078857, "kv_decode_ms": 91.30579999964539, "kv_decode_cuda_event_ms": 91.26297760009766, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 93.3761999995113, "cuda_event_ms": 93.29868960380554, "tokens_total": 73, "tokens_per_s": 781.7837950182387}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 444, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.4210677, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 5.56849999975384, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.11309999991863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 5.56849999975384, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3052.886773951962}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 445, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.4210677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.56849999975384, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.11309999991863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 163.11309999991863, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 392.3657879105475}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 148, "sample_idx": 446, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.4210677, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 5.56849999975384, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.11309999991863, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 168.68159999967247, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 480.1946388945639}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 447, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.5908084, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.407899999729125, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.16900000005262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.407899999729125, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4988.409284706487}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 448, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.5908084, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.407899999729125, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.16900000005262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 148.16900000005262, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 431.93920455680524}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 149, "sample_idx": 449, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.5908084, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.407899999729125, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.16900000005262, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 151.57689999978174, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 534.3822178716983}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 450, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.7429702, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 4.042300000037358, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.74940000009883, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.042300000037358, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4205.52655662442}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 451, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.7429702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.042300000037358, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.74940000009883, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 136.74940000009883, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 468.0093660370996}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 150, "sample_idx": 452, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.7429702, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 4.042300000037358, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.74940000009883, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 140.7917000001362, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 575.3180052511735}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 453, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545863.88464, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.9242000002559507, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6333000000377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.9242000002559507, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4332.093165203404}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 454, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545863.88464, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.9242000002559507, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6333000000377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 138.6333000000377, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 461.6495459603327}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 151, "sample_idx": 455, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545863.88464, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.9242000002559507, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.6333000000377, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 142.55750000029366, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 568.1917822621268}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 456, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545864.0276315, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.539799999747629, "prefill_cuda_event_ms": 4.4462080001831055, "kv_decode_ms": 102.5033999999323, "kv_decode_cuda_event_ms": 102.46348571777344, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.539799999747629, "cuda_event_ms": 4.4462080001831055, "tokens_total": 1, "tokens_per_s": 220.2740208942224}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 457, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545864.0276315, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.539799999747629, "prefill_cuda_event_ms": 4.4462080001831055, "kv_decode_ms": 102.5033999999323, "kv_decode_cuda_event_ms": 102.46348571777344, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 102.5033999999323, "cuda_event_ms": 102.46348571777344, "tokens_total": 64, "tokens_per_s": 624.3695331085825}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 152, "sample_idx": 458, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545864.0276315, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.539799999747629, "prefill_cuda_event_ms": 4.4462080001831055, "kv_decode_ms": 102.5033999999323, "kv_decode_cuda_event_ms": 102.46348571777344, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 107.04319999967993, "cuda_event_ms": 106.90969371795654, "tokens_total": 65, "tokens_per_s": 607.2314729024764}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 459, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545864.1355464, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.8217000001641281, "prefill_cuda_event_ms": 1.7663999795913696, "kv_decode_ms": 103.72109999980239, "kv_decode_cuda_event_ms": 103.68204498291016, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 1.8217000001641281, "cuda_event_ms": 1.7663999795913696, "tokens_total": 1, "tokens_per_s": 548.9378052971971}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 460, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545864.1355464, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.8217000001641281, "prefill_cuda_event_ms": 1.7663999795913696, "kv_decode_ms": 103.72109999980239, "kv_decode_cuda_event_ms": 103.68204498291016, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 103.72109999980239, "cuda_event_ms": 103.68204498291016, "tokens_total": 64, "tokens_per_s": 617.0393487932729}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 153, "sample_idx": 461, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545864.1355464, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.8217000001641281, "prefill_cuda_event_ms": 1.7663999795913696, "kv_decode_ms": 103.72109999980239, "kv_decode_cuda_event_ms": 103.68204498291016, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 105.54279999996652, "cuda_event_ms": 105.44844496250153, "tokens_total": 65, "tokens_per_s": 615.863895974151}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 462, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545864.2419925, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 1.9730999997591425, "prefill_cuda_event_ms": 1.927135944366455, "kv_decode_ms": 103.02649999994173, "kv_decode_cuda_event_ms": 102.98675537109375, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 1.9730999997591425, "cuda_event_ms": 1.927135944366455, "tokens_total": 1, "tokens_per_s": 506.816684467118}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 463, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545864.2419925, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9730999997591425, "prefill_cuda_event_ms": 1.927135944366455, "kv_decode_ms": 103.02649999994173, "kv_decode_cuda_event_ms": 102.98675537109375, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 103.02649999994173, "cuda_event_ms": 102.98675537109375, "tokens_total": 64, "tokens_per_s": 621.1994001546806}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 154, "sample_idx": 464, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545864.2419925, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 1.9730999997591425, "prefill_cuda_event_ms": 1.927135944366455, "kv_decode_ms": 103.02649999994173, "kv_decode_cuda_event_ms": 102.98675537109375, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 104.99959999970088, "cuda_event_ms": 104.9138913154602, "tokens_total": 65, "tokens_per_s": 619.0499773350106}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 465, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545864.3478544, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 2.0072000002073764, "prefill_cuda_event_ms": 1.9445760250091553, "kv_decode_ms": 101.78989999985788, "kv_decode_cuda_event_ms": 101.7518081665039, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 2.0072000002073764, "cuda_event_ms": 1.9445760250091553, "tokens_total": 1, "tokens_per_s": 498.20645670420674}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 466, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545864.3478544, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0072000002073764, "prefill_cuda_event_ms": 1.9445760250091553, "kv_decode_ms": 101.78989999985788, "kv_decode_cuda_event_ms": 101.7518081665039, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 101.78989999985788, "cuda_event_ms": 101.7518081665039, "tokens_total": 64, "tokens_per_s": 628.7460740219742}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 155, "sample_idx": 467, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545864.3478544, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 2.0072000002073764, "prefill_cuda_event_ms": 1.9445760250091553, "kv_decode_ms": 101.78989999985788, "kv_decode_cuda_event_ms": 101.7518081665039, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 103.79710000006526, "cuda_event_ms": 103.69638419151306, "tokens_total": 65, "tokens_per_s": 626.2217345182007}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 468, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545864.4525008, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 31.757899999774963, "prefill_cuda_event_ms": null, "kv_decode_ms": 914.0659999998206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 31.757899999774963, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 535.2998781443503}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 469, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545864.4525008, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.757899999774963, "prefill_cuda_event_ms": null, "kv_decode_ms": 914.0659999998206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 914.0659999998206, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.01682591849227}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 156, "sample_idx": 470, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545864.4525008, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 31.757899999774963, "prefill_cuda_event_ms": null, "kv_decode_ms": 914.0659999998206, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 945.8238999995956, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 85.63962065246461}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 471, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545865.3989, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 24.874499999896216, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.8085000001556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 24.874499999896216, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 683.4308227329566}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 472, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545865.3989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.874499999896216, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.8085000001556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 847.8085000001556, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.48874539473036}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 157, "sample_idx": 473, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545865.3989, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 24.874499999896216, "prefill_cuda_event_ms": null, "kv_decode_ms": 847.8085000001556, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 872.6830000000518, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 92.81720853963603}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 474, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545866.2720914, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.585999999795604, "prefill_cuda_event_ms": null, "kv_decode_ms": 878.7138000002415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 23.585999999795604, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 720.7665564380277}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 475, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545866.2720914, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.585999999795604, "prefill_cuda_event_ms": null, "kv_decode_ms": 878.7138000002415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 878.7138000002415, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.83372583881398}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 158, "sample_idx": 476, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545866.2720914, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.585999999795604, "prefill_cuda_event_ms": null, "kv_decode_ms": 878.7138000002415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 902.2998000000371, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 89.77060617767694}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 477, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545867.1748946, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 22.37650000006397, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.3452000003199, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 22.37650000006397, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 759.7256049852033}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 478, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545867.1748946, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.37650000006397, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.3452000003199, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 865.3452000003199, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.95892413799295}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 159, "sample_idx": 479, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545867.1748946, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 22.37650000006397, "prefill_cuda_event_ms": null, "kv_decode_ms": 865.3452000003199, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 887.7217000003839, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 91.24481242259255}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 480, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545868.0632162, "prompt_tokens": 46, "prefill_ms": 22.3849, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7393, "kv_decode_ms_equiv": 722.7393, "kv_decode_ms_per_token": 11.2928015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 5983.469499999956, "ollama_total_duration_ms": 5964.9935, "ollama_load_ms": 5170.8242, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 22.3849, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2054.9566895541193}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 481, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545868.0632162, "prompt_tokens": 46, "prefill_ms": 22.3849, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7393, "kv_decode_ms_equiv": 722.7393, "kv_decode_ms_per_token": 11.2928015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5983.469499999956, "ollama_total_duration_ms": 5964.9935, "ollama_load_ms": 5170.8242, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 722.7393, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.55198548079508}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 160, "sample_idx": 482, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545868.0632162, "prompt_tokens": 46, "prefill_ms": 22.3849, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.7393, "kv_decode_ms_equiv": 722.7393, "kv_decode_ms_per_token": 11.2928015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 5983.469499999956, "ollama_total_duration_ms": 5964.9935, "ollama_load_ms": 5170.8242, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 745.1242, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 147.62639570691704}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 483, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545874.0468192, "prompt_tokens": 46, "prefill_ms": 11.5062, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.9909, "kv_decode_ms_equiv": 720.9909, "kv_decode_ms_per_token": 11.2654828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 918.2452999998532, "ollama_total_duration_ms": 915.2736, "ollama_load_ms": 137.572, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.5062, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3997.8446402808922}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 484, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545874.0468192, "prompt_tokens": 46, "prefill_ms": 11.5062, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.9909, "kv_decode_ms_equiv": 720.9909, "kv_decode_ms_per_token": 11.2654828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 918.2452999998532, "ollama_total_duration_ms": 915.2736, "ollama_load_ms": 137.572, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 720.9909, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.76672368541684}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 161, "sample_idx": 485, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545874.0468192, "prompt_tokens": 46, "prefill_ms": 11.5062, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.9909, "kv_decode_ms_equiv": 720.9909, "kv_decode_ms_per_token": 11.2654828125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 918.2452999998532, "ollama_total_duration_ms": 915.2736, "ollama_load_ms": 137.572, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 732.4971, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 150.17124299877773}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 486, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545874.9652262, "prompt_tokens": 46, "prefill_ms": 11.6616, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.1431, "kv_decode_ms_equiv": 719.1431, "kv_decode_ms_per_token": 11.2366109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 895.3157000000829, "ollama_total_duration_ms": 892.1055, "ollama_load_ms": 113.0245, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6616, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3944.570213349798}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 487, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545874.9652262, "prompt_tokens": 46, "prefill_ms": 11.6616, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.1431, "kv_decode_ms_equiv": 719.1431, "kv_decode_ms_per_token": 11.2366109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 895.3157000000829, "ollama_total_duration_ms": 892.1055, "ollama_load_ms": 113.0245, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 719.1431, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.99480506730858}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 162, "sample_idx": 488, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545874.9652262, "prompt_tokens": 46, "prefill_ms": 11.6616, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.1431, "kv_decode_ms_equiv": 719.1431, "kv_decode_ms_per_token": 11.2366109375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 895.3157000000829, "ollama_total_duration_ms": 892.1055, "ollama_load_ms": 113.0245, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 730.8047, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 150.519010072048}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 489, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545875.86082, "prompt_tokens": 46, "prefill_ms": 12.3976, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.8508, "kv_decode_ms_equiv": 721.8508, "kv_decode_ms_per_token": 11.27891875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 910.5794999995851, "ollama_total_duration_ms": 886.0994, "ollama_load_ms": 104.74, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 12.3976, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3710.3955604310513}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 490, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545875.86082, "prompt_tokens": 46, "prefill_ms": 12.3976, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.8508, "kv_decode_ms_equiv": 721.8508, "kv_decode_ms_per_token": 11.27891875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 910.5794999995851, "ollama_total_duration_ms": 886.0994, "ollama_load_ms": 104.74, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 721.8508, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.66098091184494}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 163, "sample_idx": 491, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545875.86082, "prompt_tokens": 46, "prefill_ms": 12.3976, "prefill_cuda_event_ms": null, "kv_decode_ms": 721.8508, "kv_decode_ms_equiv": 721.8508, "kv_decode_ms_per_token": 11.27891875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 910.5794999995851, "ollama_total_duration_ms": 886.0994, "ollama_load_ms": 104.74, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 734.2484000000001, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 149.81306053918536}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 492, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545876.771534, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.036999999916588, "prefill_cuda_event_ms": null, "kv_decode_ms": 609.1685999999754, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 14.036999999916588, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 71.2402935104326}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 493, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545876.771534, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.036999999916588, "prefill_cuda_event_ms": null, "kv_decode_ms": 609.1685999999754, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 609.1685999999754, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 105.06122607107882}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 164, "sample_idx": 494, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545876.771534, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.036999999916588, "prefill_cuda_event_ms": null, "kv_decode_ms": 609.1685999999754, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 623.205599999892, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 104.29944788687916}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 495, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545877.3951795, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.958099999745173, "prefill_cuda_event_ms": null, "kv_decode_ms": 592.1191000002182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.958099999745173, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 111.63081457322943}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 496, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545877.3951795, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.958099999745173, "prefill_cuda_event_ms": null, "kv_decode_ms": 592.1191000002182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 592.1191000002182, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 108.086363030641}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 165, "sample_idx": 497, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545877.3951795, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.958099999745173, "prefill_cuda_event_ms": null, "kv_decode_ms": 592.1191000002182, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 601.0771999999633, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 108.1391874454795}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 498, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545877.996693, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.728900000278372, "prefill_cuda_event_ms": null, "kv_decode_ms": 557.4991000003138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.728900000278372, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 114.56197229526161}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 499, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545877.996693, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.728900000278372, "prefill_cuda_event_ms": null, "kv_decode_ms": 557.4991000003138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 557.4991000003138, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 114.79839160272005}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 166, "sample_idx": 500, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545877.996693, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.728900000278372, "prefill_cuda_event_ms": null, "kv_decode_ms": 557.4991000003138, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 566.2280000005921, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 114.79474699225759}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 501, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545878.5632932, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.314400000017486, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.8627000001143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 8.314400000017486, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 120.27326084839518}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 502, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545878.5632932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.314400000017486, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.8627000001143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 542.8627000001143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 117.89352998462876}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 167, "sample_idx": 503, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545878.5632932, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.314400000017486, "prefill_cuda_event_ms": null, "kv_decode_ms": 542.8627000001143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 551.1771000001318, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 117.92942776465942}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 504, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545879.1148458, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 29.14889999965453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1246.711299999788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 29.14889999965453, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 308.75950722348585}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 505, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545879.1148458, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.14889999965453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1246.711299999788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1246.711299999788, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.33506049075747}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 168, "sample_idx": 506, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545879.1148458, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 29.14889999965453, "prefill_cuda_event_ms": null, "kv_decode_ms": 1246.711299999788, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1275.8601999994426, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.21630003038883}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 507, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545880.3911343, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 21.13980000012816, "prefill_cuda_event_ms": null, "kv_decode_ms": 1196.8751999997949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 21.13980000012816, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 425.73723497599013}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 508, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545880.3911343, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.13980000012816, "prefill_cuda_event_ms": null, "kv_decode_ms": 1196.8751999997949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1196.8751999997949, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 53.472575921040864}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 169, "sample_idx": 509, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545880.3911343, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 21.13980000012816, "prefill_cuda_event_ms": null, "kv_decode_ms": 1196.8751999997949, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1218.014999999923, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 59.93358045673051}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 510, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545881.609742, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 33.45730000000913, "prefill_cuda_event_ms": null, "kv_decode_ms": 1062.4636999996255, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 33.45730000000913, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 268.99959052277217}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 511, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545881.609742, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.45730000000913, "prefill_cuda_event_ms": null, "kv_decode_ms": 1062.4636999996255, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1062.4636999996255, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.237352109086224}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 170, "sample_idx": 512, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545881.609742, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 33.45730000000913, "prefill_cuda_event_ms": null, "kv_decode_ms": 1062.4636999996255, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1095.9209999996347, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 66.61064073051281}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 513, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545882.706142, "prompt_tokens": 9, "gen_tokens": 0, "prefill_ms": 19.212700000025507, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.3307999999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 19.212700000025507, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 468.4401463608994}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 514, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545882.706142, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.212700000025507, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.3307999999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1232.3307999999997, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.93410730300664}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 171, "sample_idx": 515, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545882.706142, "prompt_tokens": 9, "gen_tokens": 64, "prefill_ms": 19.212700000025507, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.3307999999997, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1251.5435000000252, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 58.32797661447527}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 516, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545883.9579651, "prompt_tokens": 92, "prefill_ms": 808.747, "prefill_cuda_event_ms": null, "kv_decode_ms": 2328.7209, "kv_decode_ms_equiv": 2328.7209, "kv_decode_ms_per_token": 36.3862640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 13052.265299999817, "ollama_total_duration_ms": 12954.7349, "ollama_load_ms": 9749.0483, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 808.747, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 113.75621795196767}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 517, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545883.9579651, "prompt_tokens": 92, "prefill_ms": 808.747, "prefill_cuda_event_ms": null, "kv_decode_ms": 2328.7209, "kv_decode_ms_equiv": 2328.7209, "kv_decode_ms_per_token": 36.3862640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13052.265299999817, "ollama_total_duration_ms": 12954.7349, "ollama_load_ms": 9749.0483, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2328.7209, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.48289844437777}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 172, "sample_idx": 518, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545883.9579651, "prompt_tokens": 92, "prefill_ms": 808.747, "prefill_cuda_event_ms": null, "kv_decode_ms": 2328.7209, "kv_decode_ms_equiv": 2328.7209, "kv_decode_ms_per_token": 36.3862640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 13052.265299999817, "ollama_total_duration_ms": 12954.7349, "ollama_load_ms": 9749.0483, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3137.4678999999996, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 49.72162424355003}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 519, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545897.010914, "prompt_tokens": 92, "prefill_ms": 42.628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2317.9235, "kv_decode_ms_equiv": 2317.9235, "kv_decode_ms_per_token": 36.2175546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2643.3962999999494, "ollama_total_duration_ms": 2616.142, "ollama_load_ms": 236.6354, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 42.628, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 2158.205874073379}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 520, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545897.010914, "prompt_tokens": 92, "prefill_ms": 42.628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2317.9235, "kv_decode_ms_equiv": 2317.9235, "kv_decode_ms_per_token": 36.2175546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2643.3962999999494, "ollama_total_duration_ms": 2616.142, "ollama_load_ms": 236.6354, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2317.9235, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 27.610919859952237}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 173, "sample_idx": 521, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545897.010914, "prompt_tokens": 92, "prefill_ms": 42.628, "prefill_cuda_event_ms": null, "kv_decode_ms": 2317.9235, "kv_decode_ms_equiv": 2317.9235, "kv_decode_ms_per_token": 36.2175546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2643.3962999999494, "ollama_total_duration_ms": 2616.142, "ollama_load_ms": 236.6354, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2360.5515, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 66.0862514543741}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 522, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545899.6544726, "prompt_tokens": 92, "prefill_ms": 49.9951, "prefill_cuda_event_ms": null, "kv_decode_ms": 2643.4828, "kv_decode_ms_equiv": 2643.4828, "kv_decode_ms_per_token": 41.30441875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2945.32369999979, "ollama_total_duration_ms": 2942.213, "ollama_load_ms": 226.8832, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 49.9951, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1840.180337673092}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 523, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545899.6544726, "prompt_tokens": 92, "prefill_ms": 49.9951, "prefill_cuda_event_ms": null, "kv_decode_ms": 2643.4828, "kv_decode_ms_equiv": 2643.4828, "kv_decode_ms_per_token": 41.30441875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2945.32369999979, "ollama_total_duration_ms": 2942.213, "ollama_load_ms": 226.8832, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2643.4828, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.210484743838695}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 174, "sample_idx": 524, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545899.6544726, "prompt_tokens": 92, "prefill_ms": 49.9951, "prefill_cuda_event_ms": null, "kv_decode_ms": 2643.4828, "kv_decode_ms_equiv": 2643.4828, "kv_decode_ms_per_token": 41.30441875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2945.32369999979, "ollama_total_duration_ms": 2942.213, "ollama_load_ms": 226.8832, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2693.4779000000003, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 57.91768330454836}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 525, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545902.5999398, "prompt_tokens": 92, "prefill_ms": 54.9736, "prefill_cuda_event_ms": null, "kv_decode_ms": 2910.6196, "kv_decode_ms_equiv": 2910.6196, "kv_decode_ms_per_token": 45.47843125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3215.9844999996494, "ollama_total_duration_ms": 3193.5082, "ollama_load_ms": 209.3747, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 54.9736, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1673.5305673996247}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 526, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545902.5999398, "prompt_tokens": 92, "prefill_ms": 54.9736, "prefill_cuda_event_ms": null, "kv_decode_ms": 2910.6196, "kv_decode_ms_equiv": 2910.6196, "kv_decode_ms_per_token": 45.47843125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3215.9844999996494, "ollama_total_duration_ms": 3193.5082, "ollama_load_ms": 209.3747, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2910.6196, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.98844534682581}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 175, "sample_idx": 527, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545902.5999398, "prompt_tokens": 92, "prefill_ms": 54.9736, "prefill_cuda_event_ms": null, "kv_decode_ms": 2910.6196, "kv_decode_ms_equiv": 2910.6196, "kv_decode_ms_per_token": 45.47843125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3215.9844999996494, "ollama_total_duration_ms": 3193.5082, "ollama_load_ms": 209.3747, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2965.5932, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 52.60330378421424}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 528, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545905.8167477, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 9.571900000082678, "prefill_cuda_event_ms": 9.091072082519531, "kv_decode_ms": 410.64410000035423, "kv_decode_cuda_event_ms": 410.59429931640625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 9.571900000082678, "cuda_event_ms": 9.091072082519531, "tokens_total": 1, "tokens_per_s": 104.47246628060911}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 529, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545905.8167477, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.571900000082678, "prefill_cuda_event_ms": 9.091072082519531, "kv_decode_ms": 410.64410000035423, "kv_decode_cuda_event_ms": 410.59429931640625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 410.64410000035423, "cuda_event_ms": 410.59429931640625, "tokens_total": 64, "tokens_per_s": 155.85272015339996}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 176, "sample_idx": 530, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545905.8167477, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 9.571900000082678, "prefill_cuda_event_ms": 9.091072082519531, "kv_decode_ms": 410.64410000035423, "kv_decode_cuda_event_ms": 410.59429931640625, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 420.2160000004369, "cuda_event_ms": 419.6853713989258, "tokens_total": 65, "tokens_per_s": 154.68235383691345}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 531, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545906.2387373, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 412.34669999994367, "kv_decode_cuda_event_ms": 412.3105163574219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 5.046700000093551, "cuda_event_ms": 4.927487850189209, "tokens_total": 1, "tokens_per_s": 198.14928566815203}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 532, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545906.2387373, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 412.34669999994367, "kv_decode_cuda_event_ms": 412.3105163574219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 412.34669999994367, "cuda_event_ms": 412.3105163574219, "tokens_total": 64, "tokens_per_s": 155.20919653293876}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 177, "sample_idx": 533, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545906.2387373, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 5.046700000093551, "prefill_cuda_event_ms": 4.927487850189209, "kv_decode_ms": 412.34669999994367, "kv_decode_cuda_event_ms": 412.3105163574219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 417.3934000000372, "cuda_event_ms": 417.2380042076111, "tokens_total": 65, "tokens_per_s": 155.72838478038753}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 534, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545906.6567166, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.361399999605055, "prefill_cuda_event_ms": 6.312960147857666, "kv_decode_ms": 408.11459999986255, "kv_decode_cuda_event_ms": 408.0680847167969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 6.361399999605055, "cuda_event_ms": 6.312960147857666, "tokens_total": 1, "tokens_per_s": 157.19810105669893}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 535, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545906.6567166, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.361399999605055, "prefill_cuda_event_ms": 6.312960147857666, "kv_decode_ms": 408.11459999986255, "kv_decode_cuda_event_ms": 408.0680847167969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 408.11459999986255, "cuda_event_ms": 408.0680847167969, "tokens_total": 64, "tokens_per_s": 156.81869749335493}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 178, "sample_idx": 536, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545906.6567166, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.361399999605055, "prefill_cuda_event_ms": 6.312960147857666, "kv_decode_ms": 408.11459999986255, "kv_decode_cuda_event_ms": 408.0680847167969, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 414.4759999994676, "cuda_event_ms": 414.38104486465454, "tokens_total": 65, "tokens_per_s": 156.82452059970538}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 537, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545907.0717733, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 6.929200000286073, "prefill_cuda_event_ms": 6.875135898590088, "kv_decode_ms": 388.91960000000836, "kv_decode_cuda_event_ms": 388.8752746582031, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 6.929200000286073, "cuda_event_ms": 6.875135898590088, "tokens_total": 1, "tokens_per_s": 144.31680424272858}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 538, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545907.0717733, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.929200000286073, "prefill_cuda_event_ms": 6.875135898590088, "kv_decode_ms": 388.91960000000836, "kv_decode_cuda_event_ms": 388.8752746582031, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 388.91960000000836, "cuda_event_ms": 388.8752746582031, "tokens_total": 64, "tokens_per_s": 164.55843315687517}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 179, "sample_idx": 539, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545907.0717733, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 6.929200000286073, "prefill_cuda_event_ms": 6.875135898590088, "kv_decode_ms": 388.91960000000836, "kv_decode_cuda_event_ms": 388.8752746582031, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 395.84880000029443, "cuda_event_ms": 395.7504105567932, "tokens_total": 65, "tokens_per_s": 164.2041102561171}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 540, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545907.4683242, "prompt_tokens": 18, "prefill_ms": 11.4687, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.0158, "kv_decode_ms_equiv": 256.0297411764706, "kv_decode_ms_per_token": 4.0004647058823535, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 2838.270199999897, "ollama_total_duration_ms": 2780.3352, "ollama_load_ms": 2599.5035, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 11.4687, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1569.4891312877658}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 541, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545907.4683242, "prompt_tokens": 18, "prefill_ms": 11.4687, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.0158, "kv_decode_ms_equiv": 256.0297411764706, "kv_decode_ms_per_token": 4.0004647058823535, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2838.270199999897, "ollama_total_duration_ms": 2780.3352, "ollama_load_ms": 2599.5035, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 256.0297411764706, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 249.970959256204}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 180, "sample_idx": 542, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545907.4683242, "prompt_tokens": 18, "prefill_ms": 11.4687, "prefill_cuda_event_ms": null, "kv_decode_ms": 136.0158, "kv_decode_ms_equiv": 256.0297411764706, "kv_decode_ms_per_token": 4.0004647058823535, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2838.270199999897, "ollama_total_duration_ms": 2780.3352, "ollama_load_ms": 2599.5035, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 267.49844117647064, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 306.54384242151156}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 543, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545910.3067331, "prompt_tokens": 18, "prefill_ms": 4.6421, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.6731, "kv_decode_ms_equiv": 259.14936470588236, "kv_decode_ms_per_token": 4.049208823529412, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 309.53820000013366, "ollama_total_duration_ms": 303.5835, "ollama_load_ms": 142.9629, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.6421, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3877.5554167294977}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 544, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545910.3067331, "prompt_tokens": 18, "prefill_ms": 4.6421, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.6731, "kv_decode_ms_equiv": 259.14936470588236, "kv_decode_ms_per_token": 4.049208823529412, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 309.53820000013366, "ollama_total_duration_ms": 303.5835, "ollama_load_ms": 142.9629, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 259.14936470588236, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 246.9618247863962}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 181, "sample_idx": 545, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545910.3067331, "prompt_tokens": 18, "prefill_ms": 4.6421, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.6731, "kv_decode_ms_equiv": 259.14936470588236, "kv_decode_ms_per_token": 4.049208823529412, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 309.53820000013366, "ollama_total_duration_ms": 303.5835, "ollama_load_ms": 142.9629, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 263.7914647058824, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 310.8516042830534}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 546, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545910.6163926, "prompt_tokens": 18, "prefill_ms": 4.625, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.891, "kv_decode_ms_equiv": 246.38305882352938, "kv_decode_ms_per_token": 3.8497352941176466, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.04039999989254, "ollama_total_duration_ms": 305.559, "ollama_load_ms": 147.1061, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.625, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3891.891891891892}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 547, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545910.6163926, "prompt_tokens": 18, "prefill_ms": 4.625, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.891, "kv_decode_ms_equiv": 246.38305882352938, "kv_decode_ms_per_token": 3.8497352941176466, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.04039999989254, "ollama_total_duration_ms": 305.559, "ollama_load_ms": 147.1061, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 246.38305882352938, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 259.75811935121595}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 182, "sample_idx": 548, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545910.6163926, "prompt_tokens": 18, "prefill_ms": 4.625, "prefill_cuda_event_ms": null, "kv_decode_ms": 130.891, "kv_decode_ms_equiv": 246.38305882352938, "kv_decode_ms_per_token": 3.8497352941176466, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 331.04039999989254, "ollama_total_duration_ms": 305.559, "ollama_load_ms": 147.1061, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 251.00805882352938, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 326.68273833251664}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 549, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545910.9475472, "prompt_tokens": 18, "prefill_ms": 4.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8389, "kv_decode_ms_equiv": 251.9320470588235, "kv_decode_ms_per_token": 3.9364382352941174, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.02519999973447, "ollama_total_duration_ms": 326.2543, "ollama_load_ms": 165.8903, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.6491, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3871.7171065367493}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 550, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545910.9475472, "prompt_tokens": 18, "prefill_ms": 4.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8389, "kv_decode_ms_equiv": 251.9320470588235, "kv_decode_ms_per_token": 3.9364382352941174, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.02519999973447, "ollama_total_duration_ms": 326.2543, "ollama_load_ms": 165.8903, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 251.9320470588235, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 254.0367561299443}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 183, "sample_idx": 551, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545910.9475472, "prompt_tokens": 18, "prefill_ms": 4.6491, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.8389, "kv_decode_ms_equiv": 251.9320470588235, "kv_decode_ms_per_token": 3.9364382352941174, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.02519999973447, "ollama_total_duration_ms": 326.2543, "ollama_load_ms": 165.8903, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 256.5811470588235, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 319.5870037216755}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 552, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545911.2947032, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 13.045300000158022, "prefill_cuda_event_ms": 12.918880462646484, "kv_decode_ms": 445.8506999999372, "kv_decode_cuda_event_ms": 445.77996826171875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 13.045300000158022, "cuda_event_ms": 12.918880462646484, "tokens_total": 17, "tokens_per_s": 1303.1513265156088}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 553, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545911.2947032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.045300000158022, "prefill_cuda_event_ms": 12.918880462646484, "kv_decode_ms": 445.8506999999372, "kv_decode_cuda_event_ms": 445.77996826171875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 445.8506999999372, "cuda_event_ms": 445.77996826171875, "tokens_total": 64, "tokens_per_s": 143.54581029032593}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 184, "sample_idx": 554, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545911.2947032, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 13.045300000158022, "prefill_cuda_event_ms": 12.918880462646484, "kv_decode_ms": 445.8506999999372, "kv_decode_cuda_event_ms": 445.77996826171875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 458.8960000000952, "cuda_event_ms": 458.69884872436523, "tokens_total": 81, "tokens_per_s": 176.5105819183065}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 555, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545911.7546363, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.222400000046036, "prefill_cuda_event_ms": 7.1690239906311035, "kv_decode_ms": 457.4958999996852, "kv_decode_cuda_event_ms": 457.4228515625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 7.222400000046036, "cuda_event_ms": 7.1690239906311035, "tokens_total": 17, "tokens_per_s": 2353.788214428949}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 556, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545911.7546363, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.222400000046036, "prefill_cuda_event_ms": 7.1690239906311035, "kv_decode_ms": 457.4958999996852, "kv_decode_cuda_event_ms": 457.4228515625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 457.4958999996852, "cuda_event_ms": 457.4228515625, "tokens_total": 64, "tokens_per_s": 139.89196405922772}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 185, "sample_idx": 557, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545911.7546363, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.222400000046036, "prefill_cuda_event_ms": 7.1690239906311035, "kv_decode_ms": 457.4958999996852, "kv_decode_cuda_event_ms": 457.4228515625, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 464.71829999973124, "cuda_event_ms": 464.5918755531311, "tokens_total": 81, "tokens_per_s": 174.2991399306781}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 558, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545912.2202737, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 7.673200000226643, "prefill_cuda_event_ms": 7.614463806152344, "kv_decode_ms": 471.99399999999514, "kv_decode_cuda_event_ms": 471.9216613769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 7.673200000226643, "cuda_event_ms": 7.614463806152344, "tokens_total": 17, "tokens_per_s": 2215.5033101571535}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 559, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545912.2202737, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.673200000226643, "prefill_cuda_event_ms": 7.614463806152344, "kv_decode_ms": 471.99399999999514, "kv_decode_cuda_event_ms": 471.9216613769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 471.99399999999514, "cuda_event_ms": 471.9216613769531, "tokens_total": 64, "tokens_per_s": 135.59494400352688}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 186, "sample_idx": 560, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545912.2202737, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 7.673200000226643, "prefill_cuda_event_ms": 7.614463806152344, "kv_decode_ms": 471.99399999999514, "kv_decode_cuda_event_ms": 471.9216613769531, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 479.6672000002218, "cuda_event_ms": 479.53612518310547, "tokens_total": 81, "tokens_per_s": 168.86708117620415}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 561, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545912.7006962, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 8.1132999998772, "prefill_cuda_event_ms": 8.049663543701172, "kv_decode_ms": 483.03690000011557, "kv_decode_cuda_event_ms": 482.9593505859375, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 8.1132999998772, "cuda_event_ms": 8.049663543701172, "tokens_total": 17, "tokens_per_s": 2095.324960282167}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 562, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545912.7006962, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.1132999998772, "prefill_cuda_event_ms": 8.049663543701172, "kv_decode_ms": 483.03690000011557, "kv_decode_cuda_event_ms": 482.9593505859375, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 483.03690000011557, "cuda_event_ms": 482.9593505859375, "tokens_total": 64, "tokens_per_s": 132.49505369048345}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 187, "sample_idx": 563, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545912.7006962, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 8.1132999998772, "prefill_cuda_event_ms": 8.049663543701172, "kv_decode_ms": 483.03690000011557, "kv_decode_cuda_event_ms": 482.9593505859375, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 491.15019999999276, "cuda_event_ms": 491.0090141296387, "tokens_total": 81, "tokens_per_s": 164.91900033839178}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 564, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545913.1926425, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 37.313499999982014, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.5803000000014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 37.313499999982014, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 26.79995176009975}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 565, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545913.1926425, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 37.313499999982014, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.5803000000014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1275.5803000000014, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.17324271941165}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 188, "sample_idx": 566, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545913.1926425, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 37.313499999982014, "prefill_cuda_event_ms": null, "kv_decode_ms": 1275.5803000000014, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1312.8937999999835, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 49.50895495126934}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 567, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545914.5060592, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 26.352699999733886, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.8320999998214, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 26.352699999733886, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 37.94677585257291}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 568, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545914.5060592, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 26.352699999733886, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.8320999998214, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1374.8320999998214, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.5511388627079}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 189, "sample_idx": 569, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545914.5060592, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 26.352699999733886, "prefill_cuda_event_ms": null, "kv_decode_ms": 1374.8320999998214, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1401.1847999995553, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 46.389312815854574}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 570, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545915.9078348, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 30.73599999970611, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.3737000000692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.73599999970611, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 32.535137949295994}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 571, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545915.9078348, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 30.73599999970611, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.3737000000692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1524.3737000000692, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.98445564889836}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 190, "sample_idx": 572, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545915.9078348, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 30.73599999970611, "prefill_cuda_event_ms": null, "kv_decode_ms": 1524.3737000000692, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1555.1096999997753, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 41.79769440060041}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 573, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545917.4633422, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 20.756900000378664, "prefill_cuda_event_ms": null, "kv_decode_ms": 1086.1645999998473, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.756900000378664, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 48.17675086268938}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 574, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545917.4633422, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.756900000378664, "prefill_cuda_event_ms": null, "kv_decode_ms": 1086.1645999998473, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1086.1645999998473, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 58.922929360806826}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 191, "sample_idx": 575, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545917.4633422, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 20.756900000378664, "prefill_cuda_event_ms": null, "kv_decode_ms": 1086.1645999998473, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1106.921500000226, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 58.72141791444717}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 576, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545918.57059, "prompt_tokens": 26, "prefill_ms": 116.0714, "prefill_cuda_event_ms": null, "kv_decode_ms": 500.9569, "kv_decode_ms_equiv": 500.9569, "kv_decode_ms_per_token": 7.8274515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 843.8375999999153, "ollama_total_duration_ms": 806.7942, "ollama_load_ms": 153.9353, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 116.0714, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 224.00005513847512}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 577, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545918.57059, "prompt_tokens": 26, "prefill_ms": 116.0714, "prefill_cuda_event_ms": null, "kv_decode_ms": 500.9569, "kv_decode_ms_equiv": 500.9569, "kv_decode_ms_per_token": 7.8274515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 843.8375999999153, "ollama_total_duration_ms": 806.7942, "ollama_load_ms": 153.9353, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 500.9569, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 127.75550152118873}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 192, "sample_idx": 578, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545918.57059, "prompt_tokens": 26, "prefill_ms": 116.0714, "prefill_cuda_event_ms": null, "kv_decode_ms": 500.9569, "kv_decode_ms_equiv": 500.9569, "kv_decode_ms_per_token": 7.8274515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 843.8375999999153, "ollama_total_duration_ms": 806.7942, "ollama_load_ms": 153.9353, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 617.0283000000001, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 145.860408671693}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 579, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545919.4146266, "prompt_tokens": 26, "prefill_ms": 4.6092, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.947, "kv_decode_ms_equiv": 252.947, "kv_decode_ms_per_token": 3.952296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.42719999972905, "ollama_total_duration_ms": 452.5471, "ollama_load_ms": 159.6943, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6092, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5640.892128785906}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 580, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545919.4146266, "prompt_tokens": 26, "prefill_ms": 4.6092, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.947, "kv_decode_ms_equiv": 252.947, "kv_decode_ms_per_token": 3.952296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.42719999972905, "ollama_total_duration_ms": 452.5471, "ollama_load_ms": 159.6943, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 252.947, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 253.01743052892505}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 193, "sample_idx": 581, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545919.4146266, "prompt_tokens": 26, "prefill_ms": 4.6092, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.947, "kv_decode_ms_equiv": 252.947, "kv_decode_ms_per_token": 3.952296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.42719999972905, "ollama_total_duration_ms": 452.5471, "ollama_load_ms": 159.6943, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 257.5562, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 349.4382973502482}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 582, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545919.9021988, "prompt_tokens": 26, "prefill_ms": 4.462, "prefill_cuda_event_ms": null, "kv_decode_ms": 255.1086, "kv_decode_ms_equiv": 255.1086, "kv_decode_ms_per_token": 3.986071875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.2998000000116, "ollama_total_duration_ms": 451.525, "ollama_load_ms": 159.9725, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.462, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5826.983415508741}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 583, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545919.9021988, "prompt_tokens": 26, "prefill_ms": 4.462, "prefill_cuda_event_ms": null, "kv_decode_ms": 255.1086, "kv_decode_ms_equiv": 255.1086, "kv_decode_ms_per_token": 3.986071875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.2998000000116, "ollama_total_duration_ms": 451.525, "ollama_load_ms": 159.9725, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 255.1086, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 250.87354953929423}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 194, "sample_idx": 584, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545919.9021988, "prompt_tokens": 26, "prefill_ms": 4.462, "prefill_cuda_event_ms": null, "kv_decode_ms": 255.1086, "kv_decode_ms_equiv": 255.1086, "kv_decode_ms_per_token": 3.986071875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 480.2998000000116, "ollama_total_duration_ms": 451.525, "ollama_load_ms": 159.9725, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 259.5706, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 346.72647826834003}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 585, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545920.3826215, "prompt_tokens": 26, "prefill_ms": 4.6031, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.3223, "kv_decode_ms_equiv": 252.3223, "kv_decode_ms_per_token": 3.9425359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.19720000017696, "ollama_total_duration_ms": 447.0501, "ollama_load_ms": 155.9528, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6031, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5648.367404575177}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 586, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545920.3826215, "prompt_tokens": 26, "prefill_ms": 4.6031, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.3223, "kv_decode_ms_equiv": 252.3223, "kv_decode_ms_per_token": 3.9425359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.19720000017696, "ollama_total_duration_ms": 447.0501, "ollama_load_ms": 155.9528, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 252.3223, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 253.64385153432733}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 195, "sample_idx": 587, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545920.3826215, "prompt_tokens": 26, "prefill_ms": 4.6031, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.3223, "kv_decode_ms_equiv": 252.3223, "kv_decode_ms_per_token": 3.9425359375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 483.19720000017696, "ollama_total_duration_ms": 447.0501, "ollama_load_ms": 155.9528, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 256.9254, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 350.29623384842444}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 588, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545920.8659935, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 28.460999999879277, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.2249000002157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 28.460999999879277, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 35.13579986663299}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 589, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545920.8659935, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 28.460999999879277, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.2249000002157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1276.2249000002157, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.14790104783975}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 196, "sample_idx": 590, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545920.8659935, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 28.460999999879277, "prefill_cuda_event_ms": null, "kv_decode_ms": 1276.2249000002157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1304.685900000095, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 49.82042037857178}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 591, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545922.1710958, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 17.058800000086194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1732.0293000002493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 17.058800000086194, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 58.6207705111114}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 592, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545922.1710958, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.058800000086194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1732.0293000002493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1732.0293000002493, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 36.9508760619643}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 197, "sample_idx": 593, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545922.1710958, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 17.058800000086194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1732.0293000002493, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1749.0881000003355, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 37.16222184576496}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 594, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545923.92065, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 25.667300000350224, "prefill_cuda_event_ms": null, "kv_decode_ms": 1531.2008000000787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 25.667300000350224, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 38.960077607943}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 595, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545923.92065, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 25.667300000350224, "prefill_cuda_event_ms": null, "kv_decode_ms": 1531.2008000000787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1531.2008000000787, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 41.79726133894177}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 198, "sample_idx": 596, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545923.92065, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 25.667300000350224, "prefill_cuda_event_ms": null, "kv_decode_ms": 1531.2008000000787, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1556.868100000429, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 41.75048612016785}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 597, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545925.4781432, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 18.329999999878055, "prefill_cuda_event_ms": null, "kv_decode_ms": 1387.6421999998456, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 18.329999999878055, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.55537370467282}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 598, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545925.4781432, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.329999999878055, "prefill_cuda_event_ms": null, "kv_decode_ms": 1387.6421999998456, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1387.6421999998456, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 46.12139930596455}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 199, "sample_idx": 599, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545925.4781432, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 18.329999999878055, "prefill_cuda_event_ms": null, "kv_decode_ms": 1387.6421999998456, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1405.9721999997237, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 46.23135507232133}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 600, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545926.8845935, "prompt_tokens": 10, "prefill_ms": 10.3243, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.575, "kv_decode_ms_equiv": 119.7090909090909, "kv_decode_ms_per_token": 1.8704545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1323.8995000001523, "ollama_total_duration_ms": 1265.4779, "ollama_load_ms": 1208.9223, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 10.3243, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 968.5886694497448}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 601, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545926.8845935, "prompt_tokens": 10, "prefill_ms": 10.3243, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.575, "kv_decode_ms_equiv": 119.7090909090909, "kv_decode_ms_per_token": 1.8704545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1323.8995000001523, "ollama_total_duration_ms": 1265.4779, "ollama_load_ms": 1208.9223, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 119.7090909090909, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 534.629404617254}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 200, "sample_idx": 602, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545926.8845935, "prompt_tokens": 10, "prefill_ms": 10.3243, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.575, "kv_decode_ms_equiv": 119.7090909090909, "kv_decode_ms_per_token": 1.8704545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1323.8995000001523, "ollama_total_duration_ms": 1265.4779, "ollama_load_ms": 1208.9223, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 130.0333909090909, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 569.0845980609316}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 603, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545928.2085788, "prompt_tokens": 10, "prefill_ms": 3.2538, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.4398, "kv_decode_ms_equiv": 113.10429090909092, "kv_decode_ms_per_token": 1.7672545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 200.42869999997492, "ollama_total_duration_ms": 164.1341, "ollama_load_ms": 134.0111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.2538, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3073.3296453377593}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 604, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545928.2085788, "prompt_tokens": 10, "prefill_ms": 3.2538, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.4398, "kv_decode_ms_equiv": 113.10429090909092, "kv_decode_ms_per_token": 1.7672545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 200.42869999997492, "ollama_total_duration_ms": 164.1341, "ollama_load_ms": 134.0111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 113.10429090909092, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 565.849442895503}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 201, "sample_idx": 605, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545928.2085788, "prompt_tokens": 10, "prefill_ms": 3.2538, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.4398, "kv_decode_ms_equiv": 113.10429090909092, "kv_decode_ms_per_token": 1.7672545454545456, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 200.42869999997492, "ollama_total_duration_ms": 164.1341, "ollama_load_ms": 134.0111, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 116.35809090909092, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 635.9678078408423}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 606, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545928.4093082, "prompt_tokens": 10, "prefill_ms": 3.206, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5185, "kv_decode_ms_equiv": 131.01672727272728, "kv_decode_ms_per_token": 2.0471363636363638, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.7782999999581, "ollama_total_duration_ms": 161.5733, "ollama_load_ms": 127.0419, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.206, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3119.151590767311}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 607, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545928.4093082, "prompt_tokens": 10, "prefill_ms": 3.206, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5185, "kv_decode_ms_equiv": 131.01672727272728, "kv_decode_ms_per_token": 2.0471363636363638, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.7782999999581, "ollama_total_duration_ms": 161.5733, "ollama_load_ms": 127.0419, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 131.01672727272728, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 488.4872438217465}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 202, "sample_idx": 608, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545928.4093082, "prompt_tokens": 10, "prefill_ms": 3.206, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.5185, "kv_decode_ms_equiv": 131.01672727272728, "kv_decode_ms_per_token": 2.0471363636363638, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.7782999999581, "ollama_total_duration_ms": 161.5733, "ollama_load_ms": 127.0419, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 134.22272727272727, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 551.322428798808}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 609, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545928.6082156, "prompt_tokens": 10, "prefill_ms": 2.1846, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.2564, "kv_decode_ms_equiv": 112.03723636363635, "kv_decode_ms_per_token": 1.750581818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.1527999996615, "ollama_total_duration_ms": 157.6116, "ollama_load_ms": 125.4036, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1846, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4577.497024626934}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 610, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545928.6082156, "prompt_tokens": 10, "prefill_ms": 2.1846, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.2564, "kv_decode_ms_equiv": 112.03723636363635, "kv_decode_ms_per_token": 1.750581818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.1527999996615, "ollama_total_duration_ms": 157.6116, "ollama_load_ms": 125.4036, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 112.03723636363635, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 571.2386531231175}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 203, "sample_idx": 611, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545928.6082156, "prompt_tokens": 10, "prefill_ms": 2.1846, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.2564, "kv_decode_ms_equiv": 112.03723636363635, "kv_decode_ms_per_token": 1.750581818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 198.1527999996615, "ollama_total_duration_ms": 157.6116, "ollama_load_ms": 125.4036, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 114.22183636363636, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 647.8621107474913}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 612, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545928.8066025, "prompt_tokens": 11, "prefill_ms": 91.4227, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.5912, "kv_decode_ms_equiv": 258.5912, "kv_decode_ms_per_token": 4.0404875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 580.6400000001304, "ollama_total_duration_ms": 534.2122, "ollama_load_ms": 147.4386, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 91.4227, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 120.32022681456573}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 613, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545928.8066025, "prompt_tokens": 11, "prefill_ms": 91.4227, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.5912, "kv_decode_ms_equiv": 258.5912, "kv_decode_ms_per_token": 4.0404875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 580.6400000001304, "ollama_total_duration_ms": 534.2122, "ollama_load_ms": 147.4386, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 258.5912, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 247.49488768372626}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 204, "sample_idx": 614, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545928.8066025, "prompt_tokens": 11, "prefill_ms": 91.4227, "prefill_cuda_event_ms": null, "kv_decode_ms": 258.5912, "kv_decode_ms_equiv": 258.5912, "kv_decode_ms_per_token": 4.0404875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 580.6400000001304, "ollama_total_duration_ms": 534.2122, "ollama_load_ms": 147.4386, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 350.01390000000004, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 214.27720441959588}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 615, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545929.3874085, "prompt_tokens": 11, "prefill_ms": 4.4983, "prefill_cuda_event_ms": null, "kv_decode_ms": 251.0115, "kv_decode_ms_equiv": 251.0115, "kv_decode_ms_per_token": 3.9220546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 490.7490000000507, "ollama_total_duration_ms": 436.4414, "ollama_load_ms": 145.9722, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.4983, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2445.368250227864}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 616, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545929.3874085, "prompt_tokens": 11, "prefill_ms": 4.4983, "prefill_cuda_event_ms": null, "kv_decode_ms": 251.0115, "kv_decode_ms_equiv": 251.0115, "kv_decode_ms_per_token": 3.9220546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 490.7490000000507, "ollama_total_duration_ms": 436.4414, "ollama_load_ms": 145.9722, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 251.0115, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 254.96839786224933}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 205, "sample_idx": 617, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545929.3874085, "prompt_tokens": 11, "prefill_ms": 4.4983, "prefill_cuda_event_ms": null, "kv_decode_ms": 251.0115, "kv_decode_ms_equiv": 251.0115, "kv_decode_ms_per_token": 3.9220546875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 490.7490000000507, "ollama_total_duration_ms": 436.4414, "ollama_load_ms": 145.9722, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 255.5098, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 293.5308156477755}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 618, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545929.8783102, "prompt_tokens": 11, "prefill_ms": 5.1323, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.9626, "kv_decode_ms_equiv": 252.9626, "kv_decode_ms_per_token": 3.952540625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.3994000000712, "ollama_total_duration_ms": 433.3468, "ollama_load_ms": 137.8027, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.1323, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2143.2885840656236}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 619, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545929.8783102, "prompt_tokens": 11, "prefill_ms": 5.1323, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.9626, "kv_decode_ms_equiv": 252.9626, "kv_decode_ms_per_token": 3.952540625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.3994000000712, "ollama_total_duration_ms": 433.3468, "ollama_load_ms": 137.8027, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 252.9626, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 253.00182714757045}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 206, "sample_idx": 620, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545929.8783102, "prompt_tokens": 11, "prefill_ms": 5.1323, "prefill_cuda_event_ms": null, "kv_decode_ms": 252.9626, "kv_decode_ms_equiv": 252.9626, "kv_decode_ms_per_token": 3.952540625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.3994000000712, "ollama_total_duration_ms": 433.3468, "ollama_load_ms": 137.8027, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 258.0949, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 290.5907865672665}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 621, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545930.3418362, "prompt_tokens": 11, "prefill_ms": 4.5948, "prefill_cuda_event_ms": null, "kv_decode_ms": 248.9432, "kv_decode_ms_equiv": 248.9432, "kv_decode_ms_per_token": 3.8897375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.70609999985027, "ollama_total_duration_ms": 439.5272, "ollama_load_ms": 154.7639, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5948, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2394.0106207016624}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 622, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545930.3418362, "prompt_tokens": 11, "prefill_ms": 4.5948, "prefill_cuda_event_ms": null, "kv_decode_ms": 248.9432, "kv_decode_ms_equiv": 248.9432, "kv_decode_ms_per_token": 3.8897375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.70609999985027, "ollama_total_duration_ms": 439.5272, "ollama_load_ms": 154.7639, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 248.9432, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 257.0867571397813}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 207, "sample_idx": 623, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545930.3418362, "prompt_tokens": 11, "prefill_ms": 4.5948, "prefill_cuda_event_ms": null, "kv_decode_ms": 248.9432, "kv_decode_ms_equiv": 248.9432, "kv_decode_ms_per_token": 3.8897375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.70609999985027, "ollama_total_duration_ms": 439.5272, "ollama_load_ms": 154.7639, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 253.53799999999998, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 295.81364529183}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 624, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545930.8347616, "prompt_tokens": 11, "prefill_ms": 18.7037, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.5352, "kv_decode_ms_equiv": 681.82528, "kv_decode_ms_per_token": 10.65352, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 4658.788799999911, "ollama_total_duration_ms": 4656.0054, "ollama_load_ms": 4515.6094, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 18.7037, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 588.1189283403819}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 625, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545930.8347616, "prompt_tokens": 11, "prefill_ms": 18.7037, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.5352, "kv_decode_ms_equiv": 681.82528, "kv_decode_ms_per_token": 10.65352, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4658.788799999911, "ollama_total_duration_ms": 4656.0054, "ollama_load_ms": 4515.6094, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 681.82528, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 93.86568946226224}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 208, "sample_idx": 626, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545930.8347616, "prompt_tokens": 11, "prefill_ms": 18.7037, "prefill_cuda_event_ms": null, "kv_decode_ms": 106.5352, "kv_decode_ms_equiv": 681.82528, "kv_decode_ms_per_token": 10.65352, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 4658.788799999911, "ollama_total_duration_ms": 4656.0054, "ollama_load_ms": 4515.6094, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 700.52898, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 107.06195195522103}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 627, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545935.4936771, "prompt_tokens": 11, "prefill_ms": 11.844, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4026, "kv_decode_ms_equiv": 655.3766400000001, "kv_decode_ms_per_token": 10.240260000000001, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 267.6083999999719, "ollama_total_duration_ms": 243.0489, "ollama_load_ms": 119.4882, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.844, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 928.7402904424182}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 628, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545935.4936771, "prompt_tokens": 11, "prefill_ms": 11.844, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4026, "kv_decode_ms_equiv": 655.3766400000001, "kv_decode_ms_per_token": 10.240260000000001, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 267.6083999999719, "ollama_total_duration_ms": 243.0489, "ollama_load_ms": 119.4882, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 655.3766400000001, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 97.65377050973314}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 209, "sample_idx": 629, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545935.4936771, "prompt_tokens": 11, "prefill_ms": 11.844, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.4026, "kv_decode_ms_equiv": 655.3766400000001, "kv_decode_ms_per_token": 10.240260000000001, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 267.6083999999719, "ollama_total_duration_ms": 243.0489, "ollama_load_ms": 119.4882, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 667.2206400000001, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 112.40659461613775}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 630, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545935.7613926, "prompt_tokens": 11, "prefill_ms": 11.2126, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.2667, "kv_decode_ms_equiv": 660.90688, "kv_decode_ms_per_token": 10.32667, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 272.11320000014894, "ollama_total_duration_ms": 249.605, "ollama_load_ms": 125.1553, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.2126, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 981.0391880562938}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 631, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545935.7613926, "prompt_tokens": 11, "prefill_ms": 11.2126, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.2667, "kv_decode_ms_equiv": 660.90688, "kv_decode_ms_per_token": 10.32667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 272.11320000014894, "ollama_total_duration_ms": 249.605, "ollama_load_ms": 125.1553, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 660.90688, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.83663756080131}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 210, "sample_idx": 632, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545935.7613926, "prompt_tokens": 11, "prefill_ms": 11.2126, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.2667, "kv_decode_ms_equiv": 660.90688, "kv_decode_ms_per_token": 10.32667, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 272.11320000014894, "ollama_total_duration_ms": 249.605, "ollama_load_ms": 125.1553, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 672.11948, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 111.58730290037123}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 633, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545936.0336952, "prompt_tokens": 11, "prefill_ms": 15.0486, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.517, "kv_decode_ms_equiv": 662.5088, "kv_decode_ms_per_token": 10.3517, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.8816999998671, "ollama_total_duration_ms": 273.3854, "ollama_load_ms": 146.3518, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 15.0486, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 730.9650067115878}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 634, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545936.0336952, "prompt_tokens": 11, "prefill_ms": 15.0486, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.517, "kv_decode_ms_equiv": 662.5088, "kv_decode_ms_per_token": 10.3517, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.8816999998671, "ollama_total_duration_ms": 273.3854, "ollama_load_ms": 146.3518, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 662.5088, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.60249041220284}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 211, "sample_idx": 635, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545936.0336952, "prompt_tokens": 11, "prefill_ms": 15.0486, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.517, "kv_decode_ms_equiv": 662.5088, "kv_decode_ms_per_token": 10.3517, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 286.8816999998671, "ollama_total_duration_ms": 273.3854, "ollama_load_ms": 146.3518, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 677.5573999999999, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 110.69172884836033}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 636, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545936.3206606, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 3.7357999999585445, "prefill_cuda_event_ms": 3.689471960067749, "kv_decode_ms": 358.06460000003426, "kv_decode_cuda_event_ms": 358.04364013671875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 3.7357999999585445, "cuda_event_ms": 3.689471960067749, "tokens_total": 17, "tokens_per_s": 4550.564805446931}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 637, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545936.3206606, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.7357999999585445, "prefill_cuda_event_ms": 3.689471960067749, "kv_decode_ms": 358.06460000003426, "kv_decode_cuda_event_ms": 358.04364013671875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 358.06460000003426, "cuda_event_ms": 358.04364013671875, "tokens_total": 64, "tokens_per_s": 178.73869687199985}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 212, "sample_idx": 638, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545936.3206606, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 3.7357999999585445, "prefill_cuda_event_ms": 3.689471960067749, "kv_decode_ms": 358.06460000003426, "kv_decode_cuda_event_ms": 358.04364013671875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 361.8003999999928, "cuda_event_ms": 361.7331120967865, "tokens_total": 81, "tokens_per_s": 223.8803494965777}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 639, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545936.6833463, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.702199999836012, "prefill_cuda_event_ms": 6.651904106140137, "kv_decode_ms": 332.0498000002772, "kv_decode_cuda_event_ms": 332.0176696777344, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 6.702199999836012, "cuda_event_ms": 6.651904106140137, "tokens_total": 17, "tokens_per_s": 2536.480558684604}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 640, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545936.6833463, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.702199999836012, "prefill_cuda_event_ms": 6.651904106140137, "kv_decode_ms": 332.0498000002772, "kv_decode_cuda_event_ms": 332.0176696777344, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 332.0498000002772, "cuda_event_ms": 332.0176696777344, "tokens_total": 64, "tokens_per_s": 192.7421730112368}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 213, "sample_idx": 641, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545936.6833463, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.702199999836012, "prefill_cuda_event_ms": 6.651904106140137, "kv_decode_ms": 332.0498000002772, "kv_decode_cuda_event_ms": 332.0176696777344, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 338.7520000001132, "cuda_event_ms": 338.6695737838745, "tokens_total": 81, "tokens_per_s": 239.11297940668376}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 642, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545937.0227985, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.408400000054826, "prefill_cuda_event_ms": 6.356991767883301, "kv_decode_ms": 299.01539999991655, "kv_decode_cuda_event_ms": 298.96588134765625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 6.408400000054826, "cuda_event_ms": 6.356991767883301, "tokens_total": 17, "tokens_per_s": 2652.768241660096}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 643, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545937.0227985, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.408400000054826, "prefill_cuda_event_ms": 6.356991767883301, "kv_decode_ms": 299.01539999991655, "kv_decode_cuda_event_ms": 298.96588134765625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 299.01539999991655, "cuda_event_ms": 298.96588134765625, "tokens_total": 64, "tokens_per_s": 214.03579882513696}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 214, "sample_idx": 644, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545937.0227985, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.408400000054826, "prefill_cuda_event_ms": 6.356991767883301, "kv_decode_ms": 299.01539999991655, "kv_decode_cuda_event_ms": 298.96588134765625, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 305.4237999999714, "cuda_event_ms": 305.32287311553955, "tokens_total": 81, "tokens_per_s": 265.2052656014613}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 645, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545937.3289318, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 6.5679000003910915, "prefill_cuda_event_ms": 6.509535789489746, "kv_decode_ms": 277.49690000018745, "kv_decode_cuda_event_ms": 277.4588623046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 6.5679000003910915, "cuda_event_ms": 6.509535789489746, "tokens_total": 17, "tokens_per_s": 2588.346351038798}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 646, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545937.3289318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.5679000003910915, "prefill_cuda_event_ms": 6.509535789489746, "kv_decode_ms": 277.49690000018745, "kv_decode_cuda_event_ms": 277.4588623046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 277.49690000018745, "cuda_event_ms": 277.4588623046875, "tokens_total": 64, "tokens_per_s": 230.6332070735088}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 215, "sample_idx": 647, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545937.3289318, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 6.5679000003910915, "prefill_cuda_event_ms": 6.509535789489746, "kv_decode_ms": 277.49690000018745, "kv_decode_cuda_event_ms": 277.4588623046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 284.06480000057854, "cuda_event_ms": 283.96839809417725, "tokens_total": 81, "tokens_per_s": 285.14620607634254}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 648, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545937.6135814, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.245900000270922, "prefill_cuda_event_ms": 7.199520111083984, "kv_decode_ms": 594.2869999998948, "kv_decode_cuda_event_ms": 594.239501953125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 7.245900000270922, "cuda_event_ms": 7.199520111083984, "tokens_total": 1, "tokens_per_s": 138.00908099236952}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 649, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545937.6135814, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.245900000270922, "prefill_cuda_event_ms": 7.199520111083984, "kv_decode_ms": 594.2869999998948, "kv_decode_cuda_event_ms": 594.239501953125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 594.2869999998948, "cuda_event_ms": 594.239501953125, "tokens_total": 64, "tokens_per_s": 107.69207470466515}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 216, "sample_idx": 650, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545937.6135814, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.245900000270922, "prefill_cuda_event_ms": 7.199520111083984, "kv_decode_ms": 594.2869999998948, "kv_decode_cuda_event_ms": 594.239501953125, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 601.5329000001657, "cuda_event_ms": 601.439022064209, "tokens_total": 65, "tokens_per_s": 108.05726503069424}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 651, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545938.2157838, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 14.671200000066165, "prefill_cuda_event_ms": 14.519392013549805, "kv_decode_ms": 641.1650999998528, "kv_decode_cuda_event_ms": 641.0946655273438, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 14.671200000066165, "cuda_event_ms": 14.519392013549805, "tokens_total": 1, "tokens_per_s": 68.16075031323206}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 652, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545938.2157838, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.671200000066165, "prefill_cuda_event_ms": 14.519392013549805, "kv_decode_ms": 641.1650999998528, "kv_decode_cuda_event_ms": 641.0946655273438, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 641.1650999998528, "cuda_event_ms": 641.0946655273438, "tokens_total": 64, "tokens_per_s": 99.8182839334435}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 217, "sample_idx": 653, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545938.2157838, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 14.671200000066165, "prefill_cuda_event_ms": 14.519392013549805, "kv_decode_ms": 641.1650999998528, "kv_decode_cuda_event_ms": 641.0946655273438, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 655.836299999919, "cuda_event_ms": 655.6140575408936, "tokens_total": 65, "tokens_per_s": 99.11009805344418}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 654, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545938.8723338, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 12.14639999989231, "prefill_cuda_event_ms": 12.07801628112793, "kv_decode_ms": 661.642899999606, "kv_decode_cuda_event_ms": 661.6187133789062, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 12.14639999989231, "cuda_event_ms": 12.07801628112793, "tokens_total": 1, "tokens_per_s": 82.3289205039243}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 655, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545938.8723338, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.14639999989231, "prefill_cuda_event_ms": 12.07801628112793, "kv_decode_ms": 661.642899999606, "kv_decode_cuda_event_ms": 661.6187133789062, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 661.642899999606, "cuda_event_ms": 661.6187133789062, "tokens_total": 64, "tokens_per_s": 96.7289152502628}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 218, "sample_idx": 656, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545938.8723338, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 12.14639999989231, "prefill_cuda_event_ms": 12.07801628112793, "kv_decode_ms": 661.642899999606, "kv_decode_cuda_event_ms": 661.6187133789062, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 673.7892999994983, "cuda_event_ms": 673.6967296600342, "tokens_total": 65, "tokens_per_s": 96.4693265387984}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 657, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545939.5466392, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.309999999961292, "prefill_cuda_event_ms": 7.2570881843566895, "kv_decode_ms": 505.7937999999922, "kv_decode_cuda_event_ms": 505.7290344238281, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 7.309999999961292, "cuda_event_ms": 7.2570881843566895, "tokens_total": 1, "tokens_per_s": 136.7989056094795}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 658, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545939.5466392, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.309999999961292, "prefill_cuda_event_ms": 7.2570881843566895, "kv_decode_ms": 505.7937999999922, "kv_decode_cuda_event_ms": 505.7290344238281, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 505.7937999999922, "cuda_event_ms": 505.7290344238281, "tokens_total": 64, "tokens_per_s": 126.53377720328123}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 219, "sample_idx": 659, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545939.5466392, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.309999999961292, "prefill_cuda_event_ms": 7.2570881843566895, "kv_decode_ms": 505.7937999999922, "kv_decode_cuda_event_ms": 505.7290344238281, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 513.1037999999535, "cuda_event_ms": 512.9861226081848, "tokens_total": 65, "tokens_per_s": 126.68002068978225}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 660, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545940.0603557, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 7.937500000025466, "prefill_cuda_event_ms": 7.840767860412598, "kv_decode_ms": 347.9325999996945, "kv_decode_cuda_event_ms": 347.894775390625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 7.937500000025466, "cuda_event_ms": 7.840767860412598, "tokens_total": 1, "tokens_per_s": 125.98425196809974}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 661, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545940.0603557, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.937500000025466, "prefill_cuda_event_ms": 7.840767860412598, "kv_decode_ms": 347.9325999996945, "kv_decode_cuda_event_ms": 347.894775390625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 347.9325999996945, "cuda_event_ms": 347.894775390625, "tokens_total": 64, "tokens_per_s": 183.9436718492495}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 220, "sample_idx": 662, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545940.0603557, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 7.937500000025466, "prefill_cuda_event_ms": 7.840767860412598, "kv_decode_ms": 347.9325999996945, "kv_decode_cuda_event_ms": 347.894775390625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 355.87009999971997, "cuda_event_ms": 355.7355432510376, "tokens_total": 65, "tokens_per_s": 182.65091672509476}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 663, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545940.4169197, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.213799999888579, "prefill_cuda_event_ms": 4.1748480796813965, "kv_decode_ms": 423.3945999999378, "kv_decode_cuda_event_ms": 423.36358642578125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.213799999888579, "cuda_event_ms": 4.1748480796813965, "tokens_total": 1, "tokens_per_s": 237.31548721497032}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 664, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545940.4169197, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.213799999888579, "prefill_cuda_event_ms": 4.1748480796813965, "kv_decode_ms": 423.3945999999378, "kv_decode_cuda_event_ms": 423.36358642578125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 423.3945999999378, "cuda_event_ms": 423.36358642578125, "tokens_total": 64, "tokens_per_s": 151.1592259325211}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 221, "sample_idx": 665, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545940.4169197, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.213799999888579, "prefill_cuda_event_ms": 4.1748480796813965, "kv_decode_ms": 423.3945999999378, "kv_decode_cuda_event_ms": 423.36358642578125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 427.60839999982636, "cuda_event_ms": 427.53843450546265, "tokens_total": 65, "tokens_per_s": 152.00823931435022}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 666, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545940.8450441, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 8.119499999793334, "prefill_cuda_event_ms": 8.072192192077637, "kv_decode_ms": 388.8110000002598, "kv_decode_cuda_event_ms": 388.7882385253906, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 8.119499999793334, "cuda_event_ms": 8.072192192077637, "tokens_total": 1, "tokens_per_s": 123.16029312463243}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 667, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545940.8450441, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.119499999793334, "prefill_cuda_event_ms": 8.072192192077637, "kv_decode_ms": 388.8110000002598, "kv_decode_cuda_event_ms": 388.7882385253906, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 388.8110000002598, "cuda_event_ms": 388.7882385253906, "tokens_total": 64, "tokens_per_s": 164.60439648044226}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 222, "sample_idx": 668, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545940.8450441, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 8.119499999793334, "prefill_cuda_event_ms": 8.072192192077637, "kv_decode_ms": 388.8110000002598, "kv_decode_cuda_event_ms": 388.7882385253906, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 396.93050000005314, "cuda_event_ms": 396.86043071746826, "tokens_total": 65, "tokens_per_s": 163.7566274196397}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 669, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545941.2424793, "prompt_tokens": 1, "gen_tokens": 0, "prefill_ms": 4.224000000249362, "prefill_cuda_event_ms": 4.1902079582214355, "kv_decode_ms": 440.94929999982924, "kv_decode_cuda_event_ms": 440.922119140625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.224000000249362, "cuda_event_ms": 4.1902079582214355, "tokens_total": 1, "tokens_per_s": 236.74242422844827}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 670, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545941.2424793, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.224000000249362, "prefill_cuda_event_ms": 4.1902079582214355, "kv_decode_ms": 440.94929999982924, "kv_decode_cuda_event_ms": 440.922119140625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 440.94929999982924, "cuda_event_ms": 440.922119140625, "tokens_total": 64, "tokens_per_s": 145.14140287789272}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 223, "sample_idx": 671, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545941.2424793, "prompt_tokens": 1, "gen_tokens": 64, "prefill_ms": 4.224000000249362, "prefill_cuda_event_ms": 4.1902079582214355, "kv_decode_ms": 440.94929999982924, "kv_decode_cuda_event_ms": 440.922119140625, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 445.1733000000786, "cuda_event_ms": 445.11232709884644, "tokens_total": 65, "tokens_per_s": 146.01055364279154}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 672, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545941.6882412, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 23.103699999865057, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.296599999976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 23.103699999865057, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 735.8128784609951}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 673, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545941.6882412, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.103699999865057, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.296599999976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 653.296599999976, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 97.96469168828116}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 224, "sample_idx": 674, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545941.6882412, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 23.103699999865057, "prefill_cuda_event_ms": null, "kv_decode_ms": 653.296599999976, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 676.4002999998411, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 119.7515731439194}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 675, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545942.3650773, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 11.299900000267371, "prefill_cuda_event_ms": null, "kv_decode_ms": 637.6030999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.299900000267371, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1504.4380923369017}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 676, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545942.3650773, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.299900000267371, "prefill_cuda_event_ms": null, "kv_decode_ms": 637.6030999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 637.6030999999784, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 100.37592351731377}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 225, "sample_idx": 677, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545942.3650773, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 11.299900000267371, "prefill_cuda_event_ms": null, "kv_decode_ms": 637.6030999999784, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 648.9030000002458, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 124.82605258408316}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 678, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545943.014352, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 15.404599999783386, "prefill_cuda_event_ms": null, "kv_decode_ms": 784.32550000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.404599999783386, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1103.566467174678}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 679, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545943.014352, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.404599999783386, "prefill_cuda_event_ms": null, "kv_decode_ms": 784.32550000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 784.32550000025, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 81.59877499836432}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 226, "sample_idx": 680, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545943.014352, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 15.404599999783386, "prefill_cuda_event_ms": null, "kv_decode_ms": 784.32550000025, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 799.7301000000334, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 101.28417074710157}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 681, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766545943.8147004, "prompt_tokens": 17, "gen_tokens": 0, "prefill_ms": 19.318999999995867, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.833399999734, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.318999999995867, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 879.9627309904051}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 682, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766545943.8147004, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.318999999995867, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.833399999734, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 729.833399999734, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.69124570076312}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 227, "sample_idx": 683, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766545943.8147004, "prompt_tokens": 17, "gen_tokens": 64, "prefill_ms": 19.318999999995867, "prefill_cuda_event_ms": null, "kv_decode_ms": 729.833399999734, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 749.1523999997298, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 108.12219249384933}
