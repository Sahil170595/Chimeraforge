{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548382.4367294, "prompt_tokens": 18, "prefill_ms": 13.6272, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.8726, "kv_decode_ms_equiv": 280.23077647058824, "kv_decode_ms_per_token": 4.378605882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 3019.216300000153, "ollama_total_duration_ms": 2947.9571, "ollama_load_ms": 2733.4335, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 13.6272, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1320.8876364917223}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548382.4367294, "prompt_tokens": 18, "prefill_ms": 13.6272, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.8726, "kv_decode_ms_equiv": 280.23077647058824, "kv_decode_ms_per_token": 4.378605882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3019.216300000153, "ollama_total_duration_ms": 2947.9571, "ollama_load_ms": 2733.4335, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 280.23077647058824, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.38319475847132}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548382.4367294, "prompt_tokens": 18, "prefill_ms": 13.6272, "prefill_cuda_event_ms": null, "kv_decode_ms": 148.8726, "kv_decode_ms_equiv": 280.23077647058824, "kv_decode_ms_per_token": 4.378605882352941, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3019.216300000153, "ollama_total_duration_ms": 2947.9571, "ollama_load_ms": 2733.4335, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 293.85797647058826, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 279.04636445424933}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548385.4561386, "prompt_tokens": 18, "prefill_ms": 4.7455, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.8584, "kv_decode_ms_equiv": 282.08639999999997, "kv_decode_ms_per_token": 4.4075999999999995, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 365.73999999927764, "ollama_total_duration_ms": 345.2418, "ollama_load_ms": 164.315, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.7455, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3793.067116215362}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548385.4561386, "prompt_tokens": 18, "prefill_ms": 4.7455, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.8584, "kv_decode_ms_equiv": 282.08639999999997, "kv_decode_ms_per_token": 4.4075999999999995, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 365.73999999927764, "ollama_total_duration_ms": 345.2418, "ollama_load_ms": 164.315, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 282.08639999999997, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 226.8808421816862}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548385.4561386, "prompt_tokens": 18, "prefill_ms": 4.7455, "prefill_cuda_event_ms": null, "kv_decode_ms": 149.8584, "kv_decode_ms_equiv": 282.08639999999997, "kv_decode_ms_per_token": 4.4075999999999995, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 365.73999999927764, "ollama_total_duration_ms": 345.2418, "ollama_load_ms": 164.315, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 286.83189999999996, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 285.8817307279979}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548385.8220172, "prompt_tokens": 18, "prefill_ms": 4.9005, "prefill_cuda_event_ms": null, "kv_decode_ms": 150.0673, "kv_decode_ms_equiv": 282.47962352941175, "kv_decode_ms_per_token": 4.413744117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7166999994952, "ollama_total_duration_ms": 343.3315, "ollama_load_ms": 156.9593, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9005, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3673.094582185491}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548385.8220172, "prompt_tokens": 18, "prefill_ms": 4.9005, "prefill_cuda_event_ms": null, "kv_decode_ms": 150.0673, "kv_decode_ms_equiv": 282.47962352941175, "kv_decode_ms_per_token": 4.413744117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7166999994952, "ollama_total_duration_ms": 343.3315, "ollama_load_ms": 156.9593, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 282.47962352941175, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 226.5650144968291}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548385.8220172, "prompt_tokens": 18, "prefill_ms": 4.9005, "prefill_cuda_event_ms": null, "kv_decode_ms": 150.0673, "kv_decode_ms_equiv": 282.47962352941175, "kv_decode_ms_per_token": 4.413744117647059, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7166999994952, "ollama_total_duration_ms": 343.3315, "ollama_load_ms": 156.9593, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 287.3801235294118, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 285.33636562240446}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548386.1760375, "prompt_tokens": 18, "prefill_ms": 4.955, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.1173, "kv_decode_ms_equiv": 275.0443294117647, "kv_decode_ms_per_token": 4.297567647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.4002000002656, "ollama_total_duration_ms": 318.6506, "ollama_load_ms": 142.7882, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.955, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3632.694248234107}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548386.1760375, "prompt_tokens": 18, "prefill_ms": 4.955, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.1173, "kv_decode_ms_equiv": 275.0443294117647, "kv_decode_ms_per_token": 4.297567647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.4002000002656, "ollama_total_duration_ms": 318.6506, "ollama_load_ms": 142.7882, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 275.0443294117647, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.68976363510686}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548386.1760375, "prompt_tokens": 18, "prefill_ms": 4.955, "prefill_cuda_event_ms": null, "kv_decode_ms": 146.1173, "kv_decode_ms_equiv": 275.0443294117647, "kv_decode_ms_per_token": 4.297567647058823, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 347.4002000002656, "ollama_total_duration_ms": 318.6506, "ollama_load_ms": 142.7882, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 279.9993294117647, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 292.85784423937486}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548386.5235653, "prompt_tokens": 18, "prefill_ms": 4.9245, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.0447, "kv_decode_ms_equiv": 269.2606117647059, "kv_decode_ms_per_token": 4.20719705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 343.0268000001888, "ollama_total_duration_ms": 322.4053, "ollama_load_ms": 146.9116, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9245, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3655.1934206518426}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548386.5235653, "prompt_tokens": 18, "prefill_ms": 4.9245, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.0447, "kv_decode_ms_equiv": 269.2606117647059, "kv_decode_ms_per_token": 4.20719705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 343.0268000001888, "ollama_total_duration_ms": 322.4053, "ollama_load_ms": 146.9116, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 269.2606117647059, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 237.6879395042249}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548386.5235653, "prompt_tokens": 18, "prefill_ms": 4.9245, "prefill_cuda_event_ms": null, "kv_decode_ms": 143.0447, "kv_decode_ms_equiv": 269.2606117647059, "kv_decode_ms_per_token": 4.20719705882353, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 343.0268000001888, "ollama_total_duration_ms": 322.4053, "ollama_load_ms": 146.9116, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 274.18511176470594, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 299.0680255110603}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548386.8667188, "prompt_tokens": 18, "prefill_ms": 4.8114, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.4253, "kv_decode_ms_equiv": 271.8593882352941, "kv_decode_ms_per_token": 4.24780294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 357.2726000002149, "ollama_total_duration_ms": 318.9755, "ollama_load_ms": 139.7168, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.8114, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3741.1148522259637}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548386.8667188, "prompt_tokens": 18, "prefill_ms": 4.8114, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.4253, "kv_decode_ms_equiv": 271.8593882352941, "kv_decode_ms_per_token": 4.24780294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 357.2726000002149, "ollama_total_duration_ms": 318.9755, "ollama_load_ms": 139.7168, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 271.8593882352941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 235.41581703482703}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548386.8667188, "prompt_tokens": 18, "prefill_ms": 4.8114, "prefill_cuda_event_ms": null, "kv_decode_ms": 144.4253, "kv_decode_ms_equiv": 271.8593882352941, "kv_decode_ms_per_token": 4.24780294117647, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 357.2726000002149, "ollama_total_duration_ms": 318.9755, "ollama_load_ms": 139.7168, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 276.6707882352941, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 296.3811269090804}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548387.2242668, "prompt_tokens": 11, "prefill_ms": 87.3542, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.0486, "kv_decode_ms_equiv": 275.0486, "kv_decode_ms_per_token": 4.297634375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 576.4634999995906, "ollama_total_duration_ms": 545.8268, "ollama_load_ms": 145.6069, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 87.3542, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 125.92411126196565}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548387.2242668, "prompt_tokens": 11, "prefill_ms": 87.3542, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.0486, "kv_decode_ms_equiv": 275.0486, "kv_decode_ms_per_token": 4.297634375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 576.4634999995906, "ollama_total_duration_ms": 545.8268, "ollama_load_ms": 145.6069, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 275.0486, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.6861507384513}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548387.2242668, "prompt_tokens": 11, "prefill_ms": 87.3542, "prefill_cuda_event_ms": null, "kv_decode_ms": 275.0486, "kv_decode_ms_equiv": 275.0486, "kv_decode_ms_per_token": 4.297634375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 576.4634999995906, "ollama_total_duration_ms": 545.8268, "ollama_load_ms": 145.6069, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 362.4028, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 206.95204341688307}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548387.800897, "prompt_tokens": 11, "prefill_ms": 4.6714, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.7636, "kv_decode_ms_equiv": 266.7636, "kv_decode_ms_per_token": 4.16818125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.57780000044295, "ollama_total_duration_ms": 462.1526, "ollama_load_ms": 147.3995, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6714, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2354.754463330051}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548387.800897, "prompt_tokens": 11, "prefill_ms": 4.6714, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.7636, "kv_decode_ms_equiv": 266.7636, "kv_decode_ms_per_token": 4.16818125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.57780000044295, "ollama_total_duration_ms": 462.1526, "ollama_load_ms": 147.3995, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 266.7636, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.91279170021699}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548387.800897, "prompt_tokens": 11, "prefill_ms": 4.6714, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.7636, "kv_decode_ms_equiv": 266.7636, "kv_decode_ms_per_token": 4.16818125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.57780000044295, "ollama_total_duration_ms": 462.1526, "ollama_load_ms": 147.3995, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.435, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 276.309245307348}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548388.3106327, "prompt_tokens": 11, "prefill_ms": 5.0001, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.9705, "kv_decode_ms_equiv": 269.9705, "kv_decode_ms_per_token": 4.2182890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 505.77720000001136, "ollama_total_duration_ms": 457.3641, "ollama_load_ms": 142.3416, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.0001, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2199.9560008799826}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548388.3106327, "prompt_tokens": 11, "prefill_ms": 5.0001, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.9705, "kv_decode_ms_equiv": 269.9705, "kv_decode_ms_per_token": 4.2182890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 505.77720000001136, "ollama_total_duration_ms": 457.3641, "ollama_load_ms": 142.3416, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 269.9705, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 237.06293835807986}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548388.3106327, "prompt_tokens": 11, "prefill_ms": 5.0001, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.9705, "kv_decode_ms_equiv": 269.9705, "kv_decode_ms_per_token": 4.2182890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 505.77720000001136, "ollama_total_duration_ms": 457.3641, "ollama_load_ms": 142.3416, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.9706, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 272.7564328695504}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548388.8166237, "prompt_tokens": 11, "prefill_ms": 5.1856, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.6985, "kv_decode_ms_equiv": 270.6985, "kv_decode_ms_per_token": 4.2296640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2555000000939, "ollama_total_duration_ms": 464.3617, "ollama_load_ms": 143.2413, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.1856, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2121.258870718914}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548388.8166237, "prompt_tokens": 11, "prefill_ms": 5.1856, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.6985, "kv_decode_ms_equiv": 270.6985, "kv_decode_ms_per_token": 4.2296640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2555000000939, "ollama_total_duration_ms": 464.3617, "ollama_load_ms": 143.2413, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 270.6985, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 236.42539578165375}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548388.8166237, "prompt_tokens": 11, "prefill_ms": 5.1856, "prefill_cuda_event_ms": null, "kv_decode_ms": 270.6985, "kv_decode_ms_equiv": 270.6985, "kv_decode_ms_per_token": 4.2296640625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.2555000000939, "ollama_total_duration_ms": 464.3617, "ollama_load_ms": 143.2413, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 275.88410000000005, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 271.8532891166979}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548389.3170028, "prompt_tokens": 11, "prefill_ms": 4.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.1462, "kv_decode_ms_equiv": 267.1462, "kv_decode_ms_per_token": 4.174159375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.76469999938854, "ollama_total_duration_ms": 457.5409, "ollama_load_ms": 145.4378, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.3955, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2502.5594357866}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548389.3170028, "prompt_tokens": 11, "prefill_ms": 4.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.1462, "kv_decode_ms_equiv": 267.1462, "kv_decode_ms_per_token": 4.174159375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.76469999938854, "ollama_total_duration_ms": 457.5409, "ollama_load_ms": 145.4378, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.1462, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.56919469563857}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548389.3170028, "prompt_tokens": 11, "prefill_ms": 4.3955, "prefill_cuda_event_ms": null, "kv_decode_ms": 267.1462, "kv_decode_ms_equiv": 267.1462, "kv_decode_ms_per_token": 4.174159375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 487.76469999938854, "ollama_total_duration_ms": 457.5409, "ollama_load_ms": 145.4378, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.54170000000005, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 276.20067194099465}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548389.8049161, "prompt_tokens": 11, "prefill_ms": 4.5558, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.1845, "kv_decode_ms_equiv": 265.1845, "kv_decode_ms_per_token": 4.1435078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.83149999997113, "ollama_total_duration_ms": 455.0523, "ollama_load_ms": 140.62, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5558, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2414.5045875587166}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548389.8049161, "prompt_tokens": 11, "prefill_ms": 4.5558, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.1845, "kv_decode_ms_equiv": 265.1845, "kv_decode_ms_per_token": 4.1435078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.83149999997113, "ollama_total_duration_ms": 455.0523, "ollama_load_ms": 140.62, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 265.1845, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 241.34140570055942}
{"task_idx": 1, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548389.8049161, "prompt_tokens": 11, "prefill_ms": 4.5558, "prefill_cuda_event_ms": null, "kv_decode_ms": 265.1845, "kv_decode_ms_equiv": 265.1845, "kv_decode_ms_per_token": 4.1435078125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 511.83149999997113, "ollama_total_duration_ms": 455.0523, "ollama_load_ms": 140.62, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 269.7403, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 278.0452160837665}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548390.3171444, "prompt_tokens": 17, "prefill_ms": 8.0329, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6018, "kv_decode_ms_equiv": 139.41932972972972, "kv_decode_ms_per_token": 2.178427027027027, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 1473.3382000003985, "ollama_total_duration_ms": 1411.1402, "ollama_load_ms": 1264.4873, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.0329, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2116.296729699113}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548390.3171444, "prompt_tokens": 17, "prefill_ms": 8.0329, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6018, "kv_decode_ms_equiv": 139.41932972972972, "kv_decode_ms_per_token": 2.178427027027027, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1473.3382000003985, "ollama_total_duration_ms": 1411.1402, "ollama_load_ms": 1264.4873, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.41932972972972, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 459.0468202943359}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548390.3171444, "prompt_tokens": 17, "prefill_ms": 8.0329, "prefill_cuda_event_ms": null, "kv_decode_ms": 80.6018, "kv_decode_ms_equiv": 139.41932972972972, "kv_decode_ms_per_token": 2.178427027027027, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1473.3382000003985, "ollama_total_duration_ms": 1411.1402, "ollama_load_ms": 1264.4873, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 147.45222972972974, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 549.3304519603921}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548391.790649, "prompt_tokens": 17, "prefill_ms": 2.4207, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3813, "kv_decode_ms_equiv": 133.84173333333334, "kv_decode_ms_per_token": 2.0912770833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 320.6544999993639, "ollama_total_duration_ms": 293.8826, "ollama_load_ms": 151.171, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4207, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7022.7620109885565}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548391.790649, "prompt_tokens": 17, "prefill_ms": 2.4207, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3813, "kv_decode_ms_equiv": 133.84173333333334, "kv_decode_ms_per_token": 2.0912770833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 320.6544999993639, "ollama_total_duration_ms": 293.8826, "ollama_load_ms": 151.171, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 133.84173333333334, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 478.17671219639516}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548391.790649, "prompt_tokens": 17, "prefill_ms": 2.4207, "prefill_cuda_event_ms": null, "kv_decode_ms": 100.3813, "kv_decode_ms_equiv": 133.84173333333334, "kv_decode_ms_per_token": 2.0912770833333334, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 320.6544999993639, "ollama_total_duration_ms": 293.8826, "ollama_load_ms": 151.171, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 136.26243333333335, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 594.4411678151449}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548392.111478, "prompt_tokens": 17, "prefill_ms": 2.9042, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9923, "kv_decode_ms_equiv": 139.98973333333333, "kv_decode_ms_per_token": 2.1873395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.4635999996026, "ollama_total_duration_ms": 288.9137, "ollama_load_ms": 149.3357, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.9042, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 5853.591350457958}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548392.111478, "prompt_tokens": 17, "prefill_ms": 2.9042, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9923, "kv_decode_ms_equiv": 139.98973333333333, "kv_decode_ms_per_token": 2.1873395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.4635999996026, "ollama_total_duration_ms": 288.9137, "ollama_load_ms": 149.3357, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.98973333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 457.1763834109739}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548392.111478, "prompt_tokens": 17, "prefill_ms": 2.9042, "prefill_cuda_event_ms": null, "kv_decode_ms": 104.9923, "kv_decode_ms_equiv": 139.98973333333333, "kv_decode_ms_per_token": 2.1873395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 300.4635999996026, "ollama_total_duration_ms": 288.9137, "ollama_load_ms": 149.3357, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.89393333333334, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 566.8540161956957}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548392.412147, "prompt_tokens": 17, "prefill_ms": 2.6337, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.3534, "kv_decode_ms_equiv": 136.47119999999998, "kv_decode_ms_per_token": 2.1323624999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 312.1255999994901, "ollama_total_duration_ms": 289.3442, "ollama_load_ms": 146.3236, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6337, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6454.797433268785}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548392.412147, "prompt_tokens": 17, "prefill_ms": 2.6337, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.3534, "kv_decode_ms_equiv": 136.47119999999998, "kv_decode_ms_per_token": 2.1323624999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 312.1255999994901, "ollama_total_duration_ms": 289.3442, "ollama_load_ms": 146.3236, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 136.47119999999998, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 468.963414991588}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548392.412147, "prompt_tokens": 17, "prefill_ms": 2.6337, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.3534, "kv_decode_ms_equiv": 136.47119999999998, "kv_decode_ms_per_token": 2.1323624999999997, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 312.1255999994901, "ollama_total_duration_ms": 289.3442, "ollama_load_ms": 146.3236, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 139.1049, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 582.2943692134498}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548392.724397, "prompt_tokens": 17, "prefill_ms": 2.583, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.183, "kv_decode_ms_equiv": 140.244, "kv_decode_ms_per_token": 2.1913125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 296.75439999937225, "ollama_total_duration_ms": 287.0372, "ollama_load_ms": 144.4492, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.583, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6581.494386372435}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548392.724397, "prompt_tokens": 17, "prefill_ms": 2.583, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.183, "kv_decode_ms_equiv": 140.244, "kv_decode_ms_per_token": 2.1913125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 296.75439999937225, "ollama_total_duration_ms": 287.0372, "ollama_load_ms": 144.4492, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 140.244, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 456.34750862782005}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548392.724397, "prompt_tokens": 17, "prefill_ms": 2.583, "prefill_cuda_event_ms": null, "kv_decode_ms": 105.183, "kv_decode_ms_equiv": 140.244, "kv_decode_ms_per_token": 2.1913125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 296.75439999937225, "ollama_total_duration_ms": 287.0372, "ollama_load_ms": 144.4492, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 142.827, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 567.1196622487345}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548393.0212858, "prompt_tokens": 17, "prefill_ms": 2.576, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.6179, "kv_decode_ms_equiv": 135.49053333333333, "kv_decode_ms_per_token": 2.1170395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 327.88170000003447, "ollama_total_duration_ms": 296.5633, "ollama_load_ms": 153.246, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.576, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6599.378881987577}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548393.0212858, "prompt_tokens": 17, "prefill_ms": 2.576, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.6179, "kv_decode_ms_equiv": 135.49053333333333, "kv_decode_ms_per_token": 2.1170395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 327.88170000003447, "ollama_total_duration_ms": 296.5633, "ollama_load_ms": 153.246, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.49053333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 472.3577243772997}
{"task_idx": 2, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548393.0212858, "prompt_tokens": 17, "prefill_ms": 2.576, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.6179, "kv_decode_ms_equiv": 135.49053333333333, "kv_decode_ms_per_token": 2.1170395833333333, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 327.88170000003447, "ollama_total_duration_ms": 296.5633, "ollama_load_ms": 153.246, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 138.06653333333333, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 586.6736713410636}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548393.349435, "prompt_tokens": 25, "prefill_ms": 6.6322, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.0003, "kv_decode_ms_equiv": 129.0003, "kv_decode_ms_per_token": 2.0156296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.88839999994525, "ollama_total_duration_ms": 329.0732, "ollama_load_ms": 145.3928, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 6.6322, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 3769.4882542745995}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548393.349435, "prompt_tokens": 25, "prefill_ms": 6.6322, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.0003, "kv_decode_ms_equiv": 129.0003, "kv_decode_ms_per_token": 2.0156296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.88839999994525, "ollama_total_duration_ms": 329.0732, "ollama_load_ms": 145.3928, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.0003, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 496.1228772336188}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548393.349435, "prompt_tokens": 25, "prefill_ms": 6.6322, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.0003, "kv_decode_ms_equiv": 129.0003, "kv_decode_ms_per_token": 2.0156296875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.88839999994525, "ollama_total_duration_ms": 329.0732, "ollama_load_ms": 145.3928, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.63250000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 656.184911433469}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548393.6944737, "prompt_tokens": 25, "prefill_ms": 4.0385, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.7748, "kv_decode_ms_equiv": 127.7748, "kv_decode_ms_per_token": 1.99648125, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.8625000002139, "ollama_total_duration_ms": 332.2462, "ollama_load_ms": 150.2483, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 4.0385, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 6190.417234121581}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548393.6944737, "prompt_tokens": 25, "prefill_ms": 4.0385, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.7748, "kv_decode_ms_equiv": 127.7748, "kv_decode_ms_per_token": 1.99648125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.8625000002139, "ollama_total_duration_ms": 332.2462, "ollama_load_ms": 150.2483, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 127.7748, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 500.88123792797956}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548393.6944737, "prompt_tokens": 25, "prefill_ms": 4.0385, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.7748, "kv_decode_ms_equiv": 127.7748, "kv_decode_ms_per_token": 1.99648125, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 358.8625000002139, "ollama_total_duration_ms": 332.2462, "ollama_load_ms": 150.2483, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.8133, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 675.1974193802902}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548394.0534558, "prompt_tokens": 25, "prefill_ms": 2.837, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2975, "kv_decode_ms_equiv": 129.2975, "kv_decode_ms_per_token": 2.0202734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 367.8240999997797, "ollama_total_duration_ms": 329.7975, "ollama_load_ms": 151.1295, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.837, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 8812.125484666902}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548394.0534558, "prompt_tokens": 25, "prefill_ms": 2.837, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2975, "kv_decode_ms_equiv": 129.2975, "kv_decode_ms_per_token": 2.0202734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 367.8240999997797, "ollama_total_duration_ms": 329.7975, "ollama_load_ms": 151.1295, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.2975, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 494.9825015951584}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548394.0534558, "prompt_tokens": 25, "prefill_ms": 2.837, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.2975, "kv_decode_ms_equiv": 129.2975, "kv_decode_ms_per_token": 2.0202734375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 367.8240999997797, "ollama_total_duration_ms": 329.7975, "ollama_load_ms": 151.1295, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 132.1345, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 673.5561113865039}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548394.4214926, "prompt_tokens": 25, "prefill_ms": 4.5459, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.1385, "kv_decode_ms_equiv": 129.1385, "kv_decode_ms_per_token": 2.0177890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 350.43599999971775, "ollama_total_duration_ms": 321.9546, "ollama_load_ms": 143.2098, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 4.5459, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 5499.461052816825}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548394.4214926, "prompt_tokens": 25, "prefill_ms": 4.5459, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.1385, "kv_decode_ms_equiv": 129.1385, "kv_decode_ms_per_token": 2.0177890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 350.43599999971775, "ollama_total_duration_ms": 321.9546, "ollama_load_ms": 143.2098, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.1385, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 495.5919419847683}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548394.4214926, "prompt_tokens": 25, "prefill_ms": 4.5459, "prefill_cuda_event_ms": null, "kv_decode_ms": 129.1385, "kv_decode_ms_equiv": 129.1385, "kv_decode_ms_per_token": 2.0177890625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 350.43599999971775, "ollama_total_duration_ms": 321.9546, "ollama_load_ms": 143.2098, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 133.68439999999998, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 665.7470879175132}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548394.772098, "prompt_tokens": 25, "prefill_ms": 2.5449, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.5498, "kv_decode_ms_equiv": 133.5498, "kv_decode_ms_per_token": 2.086715625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.8810999994603, "ollama_total_duration_ms": 335.9302, "ollama_load_ms": 148.3116, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.5449, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 9823.56870603953}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548394.772098, "prompt_tokens": 25, "prefill_ms": 2.5449, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.5498, "kv_decode_ms_equiv": 133.5498, "kv_decode_ms_per_token": 2.086715625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.8810999994603, "ollama_total_duration_ms": 335.9302, "ollama_load_ms": 148.3116, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 133.5498, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 479.22198311042024}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548394.772098, "prompt_tokens": 25, "prefill_ms": 2.5449, "prefill_cuda_event_ms": null, "kv_decode_ms": 133.5498, "kv_decode_ms_equiv": 133.5498, "kv_decode_ms_per_token": 2.086715625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 360.8810999994603, "ollama_total_duration_ms": 335.9302, "ollama_load_ms": 148.3116, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 136.09470000000002, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 653.9563994777166}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548395.1331022, "prompt_tokens": 25, "prefill_ms": 2.2918, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7583, "kv_decode_ms_equiv": 128.7583, "kv_decode_ms_per_token": 2.0118484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 339.0669999998863, "ollama_total_duration_ms": 317.5574, "ollama_load_ms": 140.7437, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2918, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10908.456235273587}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548395.1331022, "prompt_tokens": 25, "prefill_ms": 2.2918, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7583, "kv_decode_ms_equiv": 128.7583, "kv_decode_ms_per_token": 2.0118484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 339.0669999998863, "ollama_total_duration_ms": 317.5574, "ollama_load_ms": 140.7437, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 128.7583, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 497.0553354618693}
{"task_idx": 3, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548395.1331022, "prompt_tokens": 25, "prefill_ms": 2.2918, "prefill_cuda_event_ms": null, "kv_decode_ms": 128.7583, "kv_decode_ms_equiv": 128.7583, "kv_decode_ms_per_token": 2.0118484375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 339.0669999998863, "ollama_total_duration_ms": 317.5574, "ollama_load_ms": 140.7437, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 131.0501, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 679.129584792381}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548395.472491, "prompt_tokens": 10, "prefill_ms": 51.4006, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.6298, "kv_decode_ms_equiv": 131.6642909090909, "kv_decode_ms_per_token": 2.0572545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 239.7623000006206, "ollama_total_duration_ms": 229.2361, "ollama_load_ms": 139.353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 51.4006, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 194.55025816819258}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548395.472491, "prompt_tokens": 10, "prefill_ms": 51.4006, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.6298, "kv_decode_ms_equiv": 131.6642909090909, "kv_decode_ms_per_token": 2.0572545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 239.7623000006206, "ollama_total_duration_ms": 229.2361, "ollama_load_ms": 139.353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 131.6642909090909, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 486.08472014776976}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548395.472491, "prompt_tokens": 10, "prefill_ms": 51.4006, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.6298, "kv_decode_ms_equiv": 131.6642909090909, "kv_decode_ms_per_token": 2.0572545454545454, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 239.7623000006206, "ollama_total_duration_ms": 229.2361, "ollama_load_ms": 139.353, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 183.0648909090909, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 404.22824733087697}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548395.7125542, "prompt_tokens": 10, "prefill_ms": 3.0317, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.8596, "kv_decode_ms_equiv": 121.36494545454546, "kv_decode_ms_per_token": 1.8963272727272729, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.87120000052528, "ollama_total_duration_ms": 179.8167, "ollama_load_ms": 144.1941, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.0317, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3298.4794009961406}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548395.7125542, "prompt_tokens": 10, "prefill_ms": 3.0317, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.8596, "kv_decode_ms_equiv": 121.36494545454546, "kv_decode_ms_per_token": 1.8963272727272729, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.87120000052528, "ollama_total_duration_ms": 179.8167, "ollama_load_ms": 144.1941, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 121.36494545454546, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 527.3351358607068}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548395.7125542, "prompt_tokens": 10, "prefill_ms": 3.0317, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.8596, "kv_decode_ms_equiv": 121.36494545454546, "kv_decode_ms_per_token": 1.8963272727272729, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.87120000052528, "ollama_total_duration_ms": 179.8167, "ollama_load_ms": 144.1941, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 124.39664545454546, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 594.8713466476844}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548395.917564, "prompt_tokens": 10, "prefill_ms": 2.3812, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2178, "kv_decode_ms_equiv": 117.63083636363636, "kv_decode_ms_per_token": 1.8379818181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.70380000006116, "ollama_total_duration_ms": 182.8541, "ollama_load_ms": 145.0821, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.3812, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4199.563245422475}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548395.917564, "prompt_tokens": 10, "prefill_ms": 2.3812, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2178, "kv_decode_ms_equiv": 117.63083636363636, "kv_decode_ms_per_token": 1.8379818181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.70380000006116, "ollama_total_duration_ms": 182.8541, "ollama_load_ms": 145.0821, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.63083636363636, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 544.0750229995351}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548395.917564, "prompt_tokens": 10, "prefill_ms": 2.3812, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2178, "kv_decode_ms_equiv": 117.63083636363636, "kv_decode_ms_per_token": 1.8379818181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 204.70380000006116, "ollama_total_duration_ms": 182.8541, "ollama_load_ms": 145.0821, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 120.01203636363637, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 616.6048193347879}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548396.1225202, "prompt_tokens": 10, "prefill_ms": 2.3368, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7856, "kv_decode_ms_equiv": 132.57076363636364, "kv_decode_ms_per_token": 2.071418181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 190.62229999963165, "ollama_total_duration_ms": 182.508, "ollama_load_ms": 147.2559, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.3368, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4279.356384799726}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548396.1225202, "prompt_tokens": 10, "prefill_ms": 2.3368, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7856, "kv_decode_ms_equiv": 132.57076363636364, "kv_decode_ms_per_token": 2.071418181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 190.62229999963165, "ollama_total_duration_ms": 182.508, "ollama_load_ms": 147.2559, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 132.57076363636364, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 482.761042061653}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548396.1225202, "prompt_tokens": 10, "prefill_ms": 2.3368, "prefill_cuda_event_ms": null, "kv_decode_ms": 22.7856, "kv_decode_ms_equiv": 132.57076363636364, "kv_decode_ms_per_token": 2.071418181818182, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 190.62229999963165, "ollama_total_duration_ms": 182.508, "ollama_load_ms": 147.2559, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 134.90756363636365, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 548.5237299182363}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 84, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548396.3132646, "prompt_tokens": 10, "prefill_ms": 2.6504, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.9485, "kv_decode_ms_equiv": 127.70036363636363, "kv_decode_ms_per_token": 1.9953181818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 208.59230000041862, "ollama_total_duration_ms": 179.2072, "ollama_load_ms": 138.6686, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.6504, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3773.0153939028073}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 85, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548396.3132646, "prompt_tokens": 10, "prefill_ms": 2.6504, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.9485, "kv_decode_ms_equiv": 127.70036363636363, "kv_decode_ms_per_token": 1.9953181818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 208.59230000041862, "ollama_total_duration_ms": 179.2072, "ollama_load_ms": 138.6686, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 127.70036363636363, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 501.1732009021118}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 28, "sample_idx": 86, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548396.3132646, "prompt_tokens": 10, "prefill_ms": 2.6504, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.9485, "kv_decode_ms_equiv": 127.70036363636363, "kv_decode_ms_per_token": 1.9953181818181818, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 208.59230000041862, "ollama_total_duration_ms": 179.2072, "ollama_load_ms": 138.6686, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 130.35076363636364, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 567.6990140727983}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548396.5219986, "prompt_tokens": 10, "prefill_ms": 3.1406, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2612, "kv_decode_ms_equiv": 117.88334545454545, "kv_decode_ms_per_token": 1.8419272727272726, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 211.6059000009045, "ollama_total_duration_ms": 177.2103, "ollama_load_ms": 142.6067, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.1406, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3184.1049480990896}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548396.5219986, "prompt_tokens": 10, "prefill_ms": 3.1406, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2612, "kv_decode_ms_equiv": 117.88334545454545, "kv_decode_ms_per_token": 1.8419272727272726, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 211.6059000009045, "ollama_total_duration_ms": 177.2103, "ollama_load_ms": 142.6067, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.88334545454545, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 542.9096006159557}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548396.5219986, "prompt_tokens": 10, "prefill_ms": 3.1406, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.2612, "kv_decode_ms_equiv": 117.88334545454545, "kv_decode_ms_per_token": 1.8419272727272726, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 211.6059000009045, "ollama_total_duration_ms": 177.2103, "ollama_load_ms": 142.6067, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 121.02394545454545, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 611.4492443794369}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 90, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548396.7338257, "prompt_tokens": 26, "prefill_ms": 10.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.9294, "kv_decode_ms_equiv": 279.9294, "kv_decode_ms_per_token": 4.373896875, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 512.4526999998125, "ollama_total_duration_ms": 471.3668, "ollama_load_ms": 138.5883, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 10.9968, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 2364.3241670304087}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 91, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548396.7338257, "prompt_tokens": 26, "prefill_ms": 10.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.9294, "kv_decode_ms_equiv": 279.9294, "kv_decode_ms_per_token": 4.373896875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 512.4526999998125, "ollama_total_duration_ms": 471.3668, "ollama_load_ms": 138.5883, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 279.9294, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 228.62907575981657}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 30, "sample_idx": 92, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548396.7338257, "prompt_tokens": 26, "prefill_ms": 10.9968, "prefill_cuda_event_ms": null, "kv_decode_ms": 279.9294, "kv_decode_ms_equiv": 279.9294, "kv_decode_ms_per_token": 4.373896875, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 512.4526999998125, "ollama_total_duration_ms": 471.3668, "ollama_load_ms": 138.5883, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 290.9262, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 309.3568059528499}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548397.2464175, "prompt_tokens": 26, "prefill_ms": 4.7807, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6732, "kv_decode_ms_equiv": 263.6732, "kv_decode_ms_per_token": 4.11989375, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.3772000001918, "ollama_total_duration_ms": 459.5537, "ollama_load_ms": 139.4613, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.7807, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5438.534105884075}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548397.2464175, "prompt_tokens": 26, "prefill_ms": 4.7807, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6732, "kv_decode_ms_equiv": 263.6732, "kv_decode_ms_per_token": 4.11989375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.3772000001918, "ollama_total_duration_ms": 459.5537, "ollama_load_ms": 139.4613, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 263.6732, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 242.72470618932832}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548397.2464175, "prompt_tokens": 26, "prefill_ms": 4.7807, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6732, "kv_decode_ms_equiv": 263.6732, "kv_decode_ms_per_token": 4.11989375, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 492.3772000001918, "ollama_total_duration_ms": 459.5537, "ollama_load_ms": 139.4613, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 268.45390000000003, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 335.25309187163975}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 96, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548397.7389314, "prompt_tokens": 26, "prefill_ms": 4.5687, "prefill_cuda_event_ms": null, "kv_decode_ms": 268.6849, "kv_decode_ms_equiv": 268.6849, "kv_decode_ms_per_token": 4.1982015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.0384000001359, "ollama_total_duration_ms": 457.7074, "ollama_load_ms": 140.6246, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.5687, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5690.896754000044}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 97, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548397.7389314, "prompt_tokens": 26, "prefill_ms": 4.5687, "prefill_cuda_event_ms": null, "kv_decode_ms": 268.6849, "kv_decode_ms_equiv": 268.6849, "kv_decode_ms_per_token": 4.1982015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.0384000001359, "ollama_total_duration_ms": 457.7074, "ollama_load_ms": 140.6246, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 268.6849, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 238.1972340090567}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 32, "sample_idx": 98, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548397.7389314, "prompt_tokens": 26, "prefill_ms": 4.5687, "prefill_cuda_event_ms": null, "kv_decode_ms": 268.6849, "kv_decode_ms_equiv": 268.6849, "kv_decode_ms_per_token": 4.1982015625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 494.0384000001359, "ollama_total_duration_ms": 457.7074, "ollama_load_ms": 140.6246, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 273.2536, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 329.36437067983735}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548398.233152, "prompt_tokens": 26, "prefill_ms": 5.0209, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8314, "kv_decode_ms_equiv": 269.8314, "kv_decode_ms_per_token": 4.216115625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 481.9471999999223, "ollama_total_duration_ms": 457.5303, "ollama_load_ms": 137.965, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.0209, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5178.354478280787}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548398.233152, "prompt_tokens": 26, "prefill_ms": 5.0209, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8314, "kv_decode_ms_equiv": 269.8314, "kv_decode_ms_per_token": 4.216115625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 481.9471999999223, "ollama_total_duration_ms": 457.5303, "ollama_load_ms": 137.965, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 269.8314, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 237.18514598375134}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548398.233152, "prompt_tokens": 26, "prefill_ms": 5.0209, "prefill_cuda_event_ms": null, "kv_decode_ms": 269.8314, "kv_decode_ms_equiv": 269.8314, "kv_decode_ms_per_token": 4.216115625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 481.9471999999223, "ollama_total_duration_ms": 457.5303, "ollama_load_ms": 137.965, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.85229999999996, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 327.44859693733696}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548398.7152271, "prompt_tokens": 26, "prefill_ms": 4.6571, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.0241, "kv_decode_ms_equiv": 271.0241, "kv_decode_ms_per_token": 4.2347515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.1989999998041, "ollama_total_duration_ms": 452.4731, "ollama_load_ms": 137.307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.6571, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5582.873462025725}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548398.7152271, "prompt_tokens": 26, "prefill_ms": 4.6571, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.0241, "kv_decode_ms_equiv": 271.0241, "kv_decode_ms_per_token": 4.2347515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.1989999998041, "ollama_total_duration_ms": 452.4731, "ollama_load_ms": 137.307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.0241, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 236.1413615984704}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548398.7152271, "prompt_tokens": 26, "prefill_ms": 4.6571, "prefill_cuda_event_ms": null, "kv_decode_ms": 271.0241, "kv_decode_ms_equiv": 271.0241, "kv_decode_ms_per_token": 4.2347515625, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 463.1989999998041, "ollama_total_duration_ms": 452.4731, "ollama_load_ms": 137.307, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 275.6812, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 326.46404615185946}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766548399.1786077, "prompt_tokens": 26, "prefill_ms": 4.4175, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.688, "kv_decode_ms_equiv": 274.688, "kv_decode_ms_per_token": 4.292, "kv_decode_cuda_event_ms": null, "gen_tokens": 0, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7083999999886, "ollama_total_duration_ms": 458.0044, "ollama_load_ms": 141.3791, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.4175, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5885.68194680249}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766548399.1786077, "prompt_tokens": 26, "prefill_ms": 4.4175, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.688, "kv_decode_ms_equiv": 274.688, "kv_decode_ms_per_token": 4.292, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7083999999886, "ollama_total_duration_ms": 458.0044, "ollama_load_ms": 141.3791, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 274.688, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 232.99161230195713}
{"task_idx": 5, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766548399.1786077, "prompt_tokens": 26, "prefill_ms": 4.4175, "prefill_cuda_event_ms": null, "kv_decode_ms": 274.688, "kv_decode_ms_equiv": 274.688, "kv_decode_ms_per_token": 4.292, "kv_decode_cuda_event_ms": null, "gen_tokens": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 496.7083999999886, "ollama_total_duration_ms": 458.0044, "ollama_load_ms": 141.3791, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 279.1055, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 322.4587118491036}
