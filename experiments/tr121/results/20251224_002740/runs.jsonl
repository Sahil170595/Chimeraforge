{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554069.7853622, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 644.26579999963, "prefill_cuda_event_ms": 643.523193359375, "kv_decode_ms": 463.4375999994518, "kv_decode_cuda_event_ms": 463.39154052734375, "gpu_peak_mb": 119.23681640625, "hf_load_ms": 9566.628600001422, "params_millions_measured": 51.475968, "latency_ms": 644.26579999963, "cuda_event_ms": 643.523193359375, "tokens_total": 17, "tokens_per_s": 26.386624899241532, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554069.7853622, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 644.26579999963, "prefill_cuda_event_ms": 643.523193359375, "kv_decode_ms": 463.4375999994518, "kv_decode_cuda_event_ms": 463.39154052734375, "gpu_peak_mb": 119.23681640625, "hf_load_ms": 9566.628600001422, "params_millions_measured": 51.475968, "latency_ms": 463.4375999994518, "cuda_event_ms": 463.39154052734375, "tokens_total": 64, "tokens_per_s": 138.09841929113156, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554069.7853622, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 644.26579999963, "prefill_cuda_event_ms": 643.523193359375, "kv_decode_ms": 463.4375999994518, "kv_decode_cuda_event_ms": 463.39154052734375, "gpu_peak_mb": 119.23681640625, "hf_load_ms": 9566.628600001422, "params_millions_measured": 51.475968, "latency_ms": 1107.7033999990817, "cuda_event_ms": 1106.9147338867188, "tokens_total": 81, "tokens_per_s": 73.12426774176838, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554080.6864357, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.286700001102872, "prefill_cuda_event_ms": 4.250847816467285, "kv_decode_ms": 379.40809999963676, "kv_decode_cuda_event_ms": 379.3836975097656, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 4.286700001102872, "cuda_event_ms": 4.250847816467285, "tokens_total": 17, "tokens_per_s": 3965.7545421014484, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554080.6864357, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.286700001102872, "prefill_cuda_event_ms": 4.250847816467285, "kv_decode_ms": 379.40809999963676, "kv_decode_cuda_event_ms": 379.3836975097656, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 379.40809999963676, "cuda_event_ms": 379.3836975097656, "tokens_total": 64, "tokens_per_s": 168.68379984523597, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554080.6864357, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.286700001102872, "prefill_cuda_event_ms": 4.250847816467285, "kv_decode_ms": 379.40809999963676, "kv_decode_cuda_event_ms": 379.3836975097656, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 383.69480000073963, "cuda_event_ms": 383.6345453262329, "tokens_total": 81, "tokens_per_s": 211.10528472067867, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554081.0708494, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.120199999306351, "prefill_cuda_event_ms": 4.086495876312256, "kv_decode_ms": 412.99959999923885, "kv_decode_cuda_event_ms": 412.9566650390625, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 4.120199999306351, "cuda_event_ms": 4.086495876312256, "tokens_total": 17, "tokens_per_s": 4126.013301019854, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554081.0708494, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.120199999306351, "prefill_cuda_event_ms": 4.086495876312256, "kv_decode_ms": 412.99959999923885, "kv_decode_cuda_event_ms": 412.9566650390625, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 412.99959999923885, "cuda_event_ms": 412.9566650390625, "tokens_total": 64, "tokens_per_s": 154.96383047372916, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554081.0708494, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.120199999306351, "prefill_cuda_event_ms": 4.086495876312256, "kv_decode_ms": 412.99959999923885, "kv_decode_cuda_event_ms": 412.9566650390625, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 417.1197999985452, "cuda_event_ms": 417.04316091537476, "tokens_total": 81, "tokens_per_s": 194.18881577974122, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554081.4887388, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.6091999993223, "prefill_cuda_event_ms": 4.572512149810791, "kv_decode_ms": 319.74040000022796, "kv_decode_cuda_event_ms": 319.7081604003906, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 4.6091999993223, "cuda_event_ms": 4.572512149810791, "tokens_total": 17, "tokens_per_s": 3688.275623210003, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554081.4887388, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.6091999993223, "prefill_cuda_event_ms": 4.572512149810791, "kv_decode_ms": 319.74040000022796, "kv_decode_cuda_event_ms": 319.7081604003906, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 319.74040000022796, "cuda_event_ms": 319.7081604003906, "tokens_total": 64, "tokens_per_s": 200.16238173203752, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554081.4887388, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.6091999993223, "prefill_cuda_event_ms": 4.572512149810791, "kv_decode_ms": 319.74040000022796, "kv_decode_cuda_event_ms": 319.7081604003906, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 324.34959999955026, "cuda_event_ms": 324.2806725502014, "tokens_total": 81, "tokens_per_s": 249.73053766711078, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554081.8138587, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.50689999909082, "prefill_cuda_event_ms": 5.454495906829834, "kv_decode_ms": 277.31560000029276, "kv_decode_cuda_event_ms": 277.285888671875, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 5.50689999909082, "cuda_event_ms": 5.454495906829834, "tokens_total": 17, "tokens_per_s": 3087.0362641062434, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554081.8138587, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.50689999909082, "prefill_cuda_event_ms": 5.454495906829834, "kv_decode_ms": 277.31560000029276, "kv_decode_cuda_event_ms": 277.285888671875, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 277.31560000029276, "cuda_event_ms": 277.285888671875, "tokens_total": 64, "tokens_per_s": 230.7839876297346, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554081.8138587, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.50689999909082, "prefill_cuda_event_ms": 5.454495906829834, "kv_decode_ms": 277.31560000029276, "kv_decode_cuda_event_ms": 277.285888671875, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 282.8224999993836, "cuda_event_ms": 282.74038457870483, "tokens_total": 81, "tokens_per_s": 286.39871297430915, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554082.0976515, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.914800001643016, "prefill_cuda_event_ms": 4.8712639808654785, "kv_decode_ms": 325.287899999239, "kv_decode_cuda_event_ms": 325.2214050292969, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 4.914800001643016, "cuda_event_ms": 4.8712639808654785, "tokens_total": 17, "tokens_per_s": 3458.940342296107, "gen_tokens": 0}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554082.0976515, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.914800001643016, "prefill_cuda_event_ms": 4.8712639808654785, "kv_decode_ms": 325.287899999239, "kv_decode_cuda_event_ms": 325.2214050292969, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 325.287899999239, "cuda_event_ms": 325.2214050292969, "tokens_total": 64, "tokens_per_s": 196.74878776662067, "gen_tokens": 64}
{"task_idx": 0, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554082.0976515, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.914800001643016, "prefill_cuda_event_ms": 4.8712639808654785, "kv_decode_ms": 325.287899999239, "kv_decode_cuda_event_ms": 325.2214050292969, "gpu_peak_mb": 119.23681640625, "params_millions_measured": 51.475968, "latency_ms": 330.20270000088203, "cuda_event_ms": 330.09266901016235, "tokens_total": 81, "tokens_per_s": 245.30386941046706, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554082.4288328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.091999999654945, "prefill_cuda_event_ms": 24.014623641967773, "kv_decode_ms": 487.8720999986399, "kv_decode_cuda_event_ms": 487.8202819824219, "gpu_peak_mb": 315.7841796875, "hf_load_ms": 439.9403000006714, "params_millions_measured": 96.08832, "latency_ms": 24.091999999654945, "cuda_event_ms": 24.014623641967773, "tokens_total": 17, "tokens_per_s": 705.6284243833422, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554082.4288328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.091999999654945, "prefill_cuda_event_ms": 24.014623641967773, "kv_decode_ms": 487.8720999986399, "kv_decode_cuda_event_ms": 487.8202819824219, "gpu_peak_mb": 315.7841796875, "hf_load_ms": 439.9403000006714, "params_millions_measured": 96.08832, "latency_ms": 487.8720999986399, "cuda_event_ms": 487.8202819824219, "tokens_total": 64, "tokens_per_s": 131.181922475539, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554082.4288328, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.091999999654945, "prefill_cuda_event_ms": 24.014623641967773, "kv_decode_ms": 487.8720999986399, "kv_decode_cuda_event_ms": 487.8202819824219, "gpu_peak_mb": 315.7841796875, "hf_load_ms": 439.9403000006714, "params_millions_measured": 96.08832, "latency_ms": 511.96409999829484, "cuda_event_ms": 511.83490562438965, "tokens_total": 81, "tokens_per_s": 158.2142185365532, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554083.3819916, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.496999999901163, "prefill_cuda_event_ms": 4.46127986907959, "kv_decode_ms": 326.61860000007437, "kv_decode_cuda_event_ms": 326.5945739746094, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 4.496999999901163, "cuda_event_ms": 4.46127986907959, "tokens_total": 17, "tokens_per_s": 3780.297976511815, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554083.3819916, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.496999999901163, "prefill_cuda_event_ms": 4.46127986907959, "kv_decode_ms": 326.61860000007437, "kv_decode_cuda_event_ms": 326.5945739746094, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 326.61860000007437, "cuda_event_ms": 326.5945739746094, "tokens_total": 64, "tokens_per_s": 195.94719957768916, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554083.3819916, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.496999999901163, "prefill_cuda_event_ms": 4.46127986907959, "kv_decode_ms": 326.61860000007437, "kv_decode_cuda_event_ms": 326.5945739746094, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 331.11559999997553, "cuda_event_ms": 331.05585384368896, "tokens_total": 81, "tokens_per_s": 244.62755605596954, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554083.7137, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.75180000103137, "prefill_cuda_event_ms": 4.708608150482178, "kv_decode_ms": 587.4853000004805, "kv_decode_cuda_event_ms": 587.4595947265625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 4.75180000103137, "cuda_event_ms": 4.708608150482178, "tokens_total": 17, "tokens_per_s": 3577.591648703687, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554083.7137, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.75180000103137, "prefill_cuda_event_ms": 4.708608150482178, "kv_decode_ms": 587.4853000004805, "kv_decode_cuda_event_ms": 587.4595947265625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 587.4853000004805, "cuda_event_ms": 587.4595947265625, "tokens_total": 64, "tokens_per_s": 108.93889600292579, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554083.7137, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.75180000103137, "prefill_cuda_event_ms": 4.708608150482178, "kv_decode_ms": 587.4853000004805, "kv_decode_cuda_event_ms": 587.4595947265625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 592.2371000015119, "cuda_event_ms": 592.1682028770447, "tokens_total": 81, "tokens_per_s": 136.7695471962044, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554084.3067212, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.707299999921815, "prefill_cuda_event_ms": 10.646080017089844, "kv_decode_ms": 518.6874000009993, "kv_decode_cuda_event_ms": 518.6447143554688, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 10.707299999921815, "cuda_event_ms": 10.646080017089844, "tokens_total": 17, "tokens_per_s": 1587.7018482833332, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554084.3067212, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.707299999921815, "prefill_cuda_event_ms": 10.646080017089844, "kv_decode_ms": 518.6874000009993, "kv_decode_cuda_event_ms": 518.6447143554688, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 518.6874000009993, "cuda_event_ms": 518.6447143554688, "tokens_total": 64, "tokens_per_s": 123.38838383171964, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554084.3067212, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.707299999921815, "prefill_cuda_event_ms": 10.646080017089844, "kv_decode_ms": 518.6874000009993, "kv_decode_cuda_event_ms": 518.6447143554688, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 529.3947000009211, "cuda_event_ms": 529.2907943725586, "tokens_total": 81, "tokens_per_s": 153.00493185870403, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554084.8369806, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.618599999565049, "prefill_cuda_event_ms": 11.562784194946289, "kv_decode_ms": 485.50580000119226, "kv_decode_cuda_event_ms": 485.465087890625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 11.618599999565049, "cuda_event_ms": 11.562784194946289, "tokens_total": 17, "tokens_per_s": 1463.1711222209567, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554084.8369806, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.618599999565049, "prefill_cuda_event_ms": 11.562784194946289, "kv_decode_ms": 485.50580000119226, "kv_decode_cuda_event_ms": 485.465087890625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 485.50580000119226, "cuda_event_ms": 485.465087890625, "tokens_total": 64, "tokens_per_s": 131.82128823145436, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554084.8369806, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.618599999565049, "prefill_cuda_event_ms": 11.562784194946289, "kv_decode_ms": 485.50580000119226, "kv_decode_cuda_event_ms": 485.465087890625, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 497.1244000007573, "cuda_event_ms": 497.0278720855713, "tokens_total": 81, "tokens_per_s": 162.93708375584987, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554085.3348517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.165000000488362, "prefill_cuda_event_ms": 5.12502384185791, "kv_decode_ms": 406.52330000011716, "kv_decode_cuda_event_ms": 406.50341796875, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 5.165000000488362, "cuda_event_ms": 5.12502384185791, "tokens_total": 17, "tokens_per_s": 3291.3843172105735, "gen_tokens": 0}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554085.3348517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.165000000488362, "prefill_cuda_event_ms": 5.12502384185791, "kv_decode_ms": 406.52330000011716, "kv_decode_cuda_event_ms": 406.50341796875, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 406.52330000011716, "cuda_event_ms": 406.50341796875, "tokens_total": 64, "tokens_per_s": 157.432550606525, "gen_tokens": 64}
{"task_idx": 1, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554085.3348517, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.165000000488362, "prefill_cuda_event_ms": 5.12502384185791, "kv_decode_ms": 406.52330000011716, "kv_decode_cuda_event_ms": 406.50341796875, "gpu_peak_mb": 315.7841796875, "params_millions_measured": 96.08832, "latency_ms": 411.6883000006055, "cuda_event_ms": 411.6284418106079, "tokens_total": 81, "tokens_per_s": 196.75079422922843, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554085.7473035, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 17.948599999726866, "prefill_cuda_event_ms": 17.890335083007812, "kv_decode_ms": 577.566799998749, "kv_decode_cuda_event_ms": 577.5145263671875, "gpu_peak_mb": 428.79150390625, "hf_load_ms": 308.8712000007945, "params_millions_measured": 45.1712, "latency_ms": 17.948599999726866, "cuda_event_ms": 17.890335083007812, "tokens_total": 136, "tokens_per_s": 7577.192650238436, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554085.7473035, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 17.948599999726866, "prefill_cuda_event_ms": 17.890335083007812, "kv_decode_ms": 577.566799998749, "kv_decode_cuda_event_ms": 577.5145263671875, "gpu_peak_mb": 428.79150390625, "hf_load_ms": 308.8712000007945, "params_millions_measured": 45.1712, "latency_ms": 577.566799998749, "cuda_event_ms": 577.5145263671875, "tokens_total": 512, "tokens_per_s": 886.4775468415237, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554085.7473035, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 17.948599999726866, "prefill_cuda_event_ms": 17.890335083007812, "kv_decode_ms": 577.566799998749, "kv_decode_cuda_event_ms": 577.5145263671875, "gpu_peak_mb": 428.79150390625, "hf_load_ms": 308.8712000007945, "params_millions_measured": 45.1712, "latency_ms": 595.5153999984759, "cuda_event_ms": 595.4048614501953, "tokens_total": 648, "tokens_per_s": 1088.1330692735376, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554086.6563735, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 11.368199999196804, "prefill_cuda_event_ms": 11.27513599395752, "kv_decode_ms": 414.4472999996651, "kv_decode_cuda_event_ms": 414.3800354003906, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 11.368199999196804, "cuda_event_ms": 11.27513599395752, "tokens_total": 136, "tokens_per_s": 11963.195581500044, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554086.6563735, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 11.368199999196804, "prefill_cuda_event_ms": 11.27513599395752, "kv_decode_ms": 414.4472999996651, "kv_decode_cuda_event_ms": 414.3800354003906, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 414.4472999996651, "cuda_event_ms": 414.3800354003906, "tokens_total": 512, "tokens_per_s": 1235.380228078247, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554086.6563735, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 11.368199999196804, "prefill_cuda_event_ms": 11.27513599395752, "kv_decode_ms": 414.4472999996651, "kv_decode_cuda_event_ms": 414.3800354003906, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 425.8154999988619, "cuda_event_ms": 425.65517139434814, "tokens_total": 648, "tokens_per_s": 1521.7858438730668, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554087.0847218, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.82469999892055, "prefill_cuda_event_ms": 10.761695861816406, "kv_decode_ms": 258.250599999883, "kv_decode_cuda_event_ms": 258.1862487792969, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 10.82469999892055, "cuda_event_ms": 10.761695861816406, "tokens_total": 136, "tokens_per_s": 12563.858583938774, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554087.0847218, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.82469999892055, "prefill_cuda_event_ms": 10.761695861816406, "kv_decode_ms": 258.250599999883, "kv_decode_cuda_event_ms": 258.1862487792969, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 258.250599999883, "cuda_event_ms": 258.1862487792969, "tokens_total": 512, "tokens_per_s": 1982.5704180367131, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554087.0847218, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.82469999892055, "prefill_cuda_event_ms": 10.761695861816406, "kv_decode_ms": 258.250599999883, "kv_decode_cuda_event_ms": 258.1862487792969, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 269.07529999880353, "cuda_event_ms": 268.9479446411133, "tokens_total": 648, "tokens_per_s": 2408.247802763321, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554087.3557272, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.932800001071882, "prefill_cuda_event_ms": 7.8640642166137695, "kv_decode_ms": 422.93180000160646, "kv_decode_cuda_event_ms": 422.8822937011719, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 7.932800001071882, "cuda_event_ms": 7.8640642166137695, "tokens_total": 136, "tokens_per_s": 17144.00967900661, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554087.3557272, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.932800001071882, "prefill_cuda_event_ms": 7.8640642166137695, "kv_decode_ms": 422.93180000160646, "kv_decode_cuda_event_ms": 422.8822937011719, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 422.93180000160646, "cuda_event_ms": 422.8822937011719, "tokens_total": 512, "tokens_per_s": 1210.597074984797, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554087.3557272, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.932800001071882, "prefill_cuda_event_ms": 7.8640642166137695, "kv_decode_ms": 422.93180000160646, "kv_decode_cuda_event_ms": 422.8822937011719, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 430.86460000267834, "cuda_event_ms": 430.74635791778564, "tokens_total": 648, "tokens_per_s": 1503.9527498800596, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 16, "sample_idx": 48, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554087.7886357, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.22949999908451, "prefill_cuda_event_ms": 7.155680179595947, "kv_decode_ms": 392.18199999959324, "kv_decode_cuda_event_ms": 392.13568115234375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 7.22949999908451, "cuda_event_ms": 7.155680179595947, "tokens_total": 136, "tokens_per_s": 18811.812714187985, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 16, "sample_idx": 49, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554087.7886357, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.22949999908451, "prefill_cuda_event_ms": 7.155680179595947, "kv_decode_ms": 392.18199999959324, "kv_decode_cuda_event_ms": 392.13568115234375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 392.18199999959324, "cuda_event_ms": 392.13568115234375, "tokens_total": 512, "tokens_per_s": 1305.5163164054725, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 16, "sample_idx": 50, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554087.7886357, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.22949999908451, "prefill_cuda_event_ms": 7.155680179595947, "kv_decode_ms": 392.18199999959324, "kv_decode_cuda_event_ms": 392.13568115234375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 399.41149999867775, "cuda_event_ms": 399.2913613319397, "tokens_total": 648, "tokens_per_s": 1622.3869367861096, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554088.1904085, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.412800000063726, "prefill_cuda_event_ms": 9.343775749206543, "kv_decode_ms": 416.8189000010898, "kv_decode_cuda_event_ms": 416.7454833984375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 9.412800000063726, "cuda_event_ms": 9.343775749206543, "tokens_total": 136, "tokens_per_s": 14448.410674727951, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554088.1904085, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.412800000063726, "prefill_cuda_event_ms": 9.343775749206543, "kv_decode_ms": 416.8189000010898, "kv_decode_cuda_event_ms": 416.7454833984375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 416.8189000010898, "cuda_event_ms": 416.7454833984375, "tokens_total": 512, "tokens_per_s": 1228.3512095988483, "gen_tokens": 512}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554088.1904085, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.412800000063726, "prefill_cuda_event_ms": 9.343775749206543, "kv_decode_ms": 416.8189000010898, "kv_decode_cuda_event_ms": 416.7454833984375, "gpu_peak_mb": 428.79150390625, "params_millions_measured": 45.1712, "latency_ms": 426.2317000011535, "cuda_event_ms": 426.08925914764404, "tokens_total": 648, "tokens_per_s": 1520.2998744538388, "gen_tokens": 512}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554088.6189897, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.972899998800131, "prefill_cuda_event_ms": 6.913248062133789, "kv_decode_ms": 390.95199999974284, "kv_decode_cuda_event_ms": 390.9058532714844, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.972899998800131, "cuda_event_ms": 6.913248062133789, "tokens_total": 17, "tokens_per_s": 2438.0100106018003, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554088.6189897, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.972899998800131, "prefill_cuda_event_ms": 6.913248062133789, "kv_decode_ms": 390.95199999974284, "kv_decode_cuda_event_ms": 390.9058532714844, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 390.95199999974284, "cuda_event_ms": 390.9058532714844, "tokens_total": 64, "tokens_per_s": 163.70296097741436, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554088.6189897, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.972899998800131, "prefill_cuda_event_ms": 6.913248062133789, "kv_decode_ms": 390.95199999974284, "kv_decode_cuda_event_ms": 390.9058532714844, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 397.92489999854297, "cuda_event_ms": 397.81910133361816, "tokens_total": 81, "tokens_per_s": 203.5559976274332, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554089.0177782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.871199999295641, "prefill_cuda_event_ms": 6.810336112976074, "kv_decode_ms": 395.0294999995094, "kv_decode_cuda_event_ms": 394.9854736328125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.871199999295641, "cuda_event_ms": 6.810336112976074, "tokens_total": 17, "tokens_per_s": 2474.0947726368977, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554089.0177782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.871199999295641, "prefill_cuda_event_ms": 6.810336112976074, "kv_decode_ms": 395.0294999995094, "kv_decode_cuda_event_ms": 394.9854736328125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 395.0294999995094, "cuda_event_ms": 394.9854736328125, "tokens_total": 64, "tokens_per_s": 162.0132167346476, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554089.0177782, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.871199999295641, "prefill_cuda_event_ms": 6.810336112976074, "kv_decode_ms": 395.0294999995094, "kv_decode_cuda_event_ms": 394.9854736328125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 401.90069999880507, "cuda_event_ms": 401.7958097457886, "tokens_total": 81, "tokens_per_s": 201.5423212754813, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554089.4205246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.942500000150176, "prefill_cuda_event_ms": 6.879199981689453, "kv_decode_ms": 383.0292999991798, "kv_decode_cuda_event_ms": 382.98419189453125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.942500000150176, "cuda_event_ms": 6.879199981689453, "tokens_total": 17, "tokens_per_s": 2448.685631923985, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554089.4205246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.942500000150176, "prefill_cuda_event_ms": 6.879199981689453, "kv_decode_ms": 383.0292999991798, "kv_decode_cuda_event_ms": 382.98419189453125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 383.0292999991798, "cuda_event_ms": 382.98419189453125, "tokens_total": 64, "tokens_per_s": 167.0890451465124, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554089.4205246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.942500000150176, "prefill_cuda_event_ms": 6.879199981689453, "kv_decode_ms": 383.0292999991798, "kv_decode_cuda_event_ms": 382.98419189453125, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 389.97179999932996, "cuda_event_ms": 389.8633918762207, "tokens_total": 81, "tokens_per_s": 207.70732653012135, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554089.8115332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.818699999712408, "prefill_cuda_event_ms": 6.767360210418701, "kv_decode_ms": 380.4168999995454, "kv_decode_cuda_event_ms": 380.3740234375, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.818699999712408, "cuda_event_ms": 6.767360210418701, "tokens_total": 17, "tokens_per_s": 2493.143854505552, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554089.8115332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.818699999712408, "prefill_cuda_event_ms": 6.767360210418701, "kv_decode_ms": 380.4168999995454, "kv_decode_cuda_event_ms": 380.3740234375, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 380.4168999995454, "cuda_event_ms": 380.3740234375, "tokens_total": 64, "tokens_per_s": 168.23647950466048, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554089.8115332, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.818699999712408, "prefill_cuda_event_ms": 6.767360210418701, "kv_decode_ms": 380.4168999995454, "kv_decode_cuda_event_ms": 380.3740234375, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 387.23559999925783, "cuda_event_ms": 387.1413836479187, "tokens_total": 81, "tokens_per_s": 209.17498287904118, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554090.199625, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.915600000866107, "prefill_cuda_event_ms": 6.860447883605957, "kv_decode_ms": 383.20329999987734, "kv_decode_cuda_event_ms": 383.1336975097656, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.915600000866107, "cuda_event_ms": 6.860447883605957, "tokens_total": 17, "tokens_per_s": 2458.2104225043277, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554090.199625, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.915600000866107, "prefill_cuda_event_ms": 6.860447883605957, "kv_decode_ms": 383.20329999987734, "kv_decode_cuda_event_ms": 383.1336975097656, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 383.20329999987734, "cuda_event_ms": 383.1336975097656, "tokens_total": 64, "tokens_per_s": 167.0131755128948, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554090.199625, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.915600000866107, "prefill_cuda_event_ms": 6.860447883605957, "kv_decode_ms": 383.20329999987734, "kv_decode_cuda_event_ms": 383.1336975097656, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 390.11890000074345, "cuda_event_ms": 389.9941453933716, "tokens_total": 81, "tokens_per_s": 207.629007463739, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554090.590703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.597600000532111, "prefill_cuda_event_ms": 6.5404157638549805, "kv_decode_ms": 387.0043999995687, "kv_decode_cuda_event_ms": 386.95526123046875, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 6.597600000532111, "cuda_event_ms": 6.5404157638549805, "tokens_total": 17, "tokens_per_s": 2576.6945553881583, "gen_tokens": 0}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554090.590703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.597600000532111, "prefill_cuda_event_ms": 6.5404157638549805, "kv_decode_ms": 387.0043999995687, "kv_decode_cuda_event_ms": 386.95526123046875, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 387.0043999995687, "cuda_event_ms": 386.95526123046875, "tokens_total": 64, "tokens_per_s": 165.3727967952595, "gen_tokens": 64}
{"task_idx": 3, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554090.590703, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.597600000532111, "prefill_cuda_event_ms": 6.5404157638549805, "kv_decode_ms": 387.0043999995687, "kv_decode_cuda_event_ms": 386.95526123046875, "gpu_peak_mb": 409.03564453125, "params_millions_measured": 45.1712, "latency_ms": 393.6020000001008, "cuda_event_ms": 393.49567699432373, "tokens_total": 81, "tokens_per_s": 205.79163723756295, "gen_tokens": 64}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554090.9854908, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.11850000092818, "prefill_cuda_event_ms": 13.046879768371582, "kv_decode_ms": 514.4896999991033, "kv_decode_cuda_event_ms": 514.4412231445312, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 13.11850000092818, "cuda_event_ms": 13.046879768371582, "tokens_total": 136, "tokens_per_s": 10367.03891377654, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554090.9854908, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.11850000092818, "prefill_cuda_event_ms": 13.046879768371582, "kv_decode_ms": 514.4896999991033, "kv_decode_cuda_event_ms": 514.4412231445312, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 514.4896999991033, "cuda_event_ms": 514.4412231445312, "tokens_total": 512, "tokens_per_s": 995.1608360690066, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554090.9854908, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.11850000092818, "prefill_cuda_event_ms": 13.046879768371582, "kv_decode_ms": 514.4896999991033, "kv_decode_cuda_event_ms": 514.4412231445312, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 527.6082000000315, "cuda_event_ms": 527.4881029129028, "tokens_total": 648, "tokens_per_s": 1228.1840956982119, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554091.515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.675600000467966, "prefill_cuda_event_ms": 10.608223915100098, "kv_decode_ms": 505.284799999572, "kv_decode_cuda_event_ms": 505.2262268066406, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 10.675600000467966, "cuda_event_ms": 10.608223915100098, "tokens_total": 136, "tokens_per_s": 12739.33080988782, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554091.515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.675600000467966, "prefill_cuda_event_ms": 10.608223915100098, "kv_decode_ms": 505.284799999572, "kv_decode_cuda_event_ms": 505.2262268066406, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 505.284799999572, "cuda_event_ms": 505.2262268066406, "tokens_total": 512, "tokens_per_s": 1013.2899307488246, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554091.515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.675600000467966, "prefill_cuda_event_ms": 10.608223915100098, "kv_decode_ms": 505.284799999572, "kv_decode_cuda_event_ms": 505.2262268066406, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 515.96040000004, "cuda_event_ms": 515.8344507217407, "tokens_total": 648, "tokens_per_s": 1255.9103373048588, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554092.0332198, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.679299999537761, "prefill_cuda_event_ms": 10.605376243591309, "kv_decode_ms": 504.5358000006672, "kv_decode_cuda_event_ms": 504.474609375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 10.679299999537761, "cuda_event_ms": 10.605376243591309, "tokens_total": 136, "tokens_per_s": 12734.917083131531, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554092.0332198, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.679299999537761, "prefill_cuda_event_ms": 10.605376243591309, "kv_decode_ms": 504.5358000006672, "kv_decode_cuda_event_ms": 504.474609375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 504.5358000006672, "cuda_event_ms": 504.474609375, "tokens_total": 512, "tokens_per_s": 1014.7941929974502, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554092.0332198, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.679299999537761, "prefill_cuda_event_ms": 10.605376243591309, "kv_decode_ms": 504.5358000006672, "kv_decode_cuda_event_ms": 504.474609375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 515.215100000205, "cuda_event_ms": 515.0799856185913, "tokens_total": 648, "tokens_per_s": 1257.7271124230292, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554092.5507982, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.00770000043849, "prefill_cuda_event_ms": 8.95580768585205, "kv_decode_ms": 505.5787999990571, "kv_decode_cuda_event_ms": 505.5314025878906, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 9.00770000043849, "cuda_event_ms": 8.95580768585205, "tokens_total": 136, "tokens_per_s": 15098.193766819453, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554092.5507982, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.00770000043849, "prefill_cuda_event_ms": 8.95580768585205, "kv_decode_ms": 505.5787999990571, "kv_decode_cuda_event_ms": 505.5314025878906, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 505.5787999990571, "cuda_event_ms": 505.5314025878906, "tokens_total": 512, "tokens_per_s": 1012.700690774524, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554092.5507982, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.00770000043849, "prefill_cuda_event_ms": 8.95580768585205, "kv_decode_ms": 505.5787999990571, "kv_decode_cuda_event_ms": 505.5314025878906, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 514.5864999994956, "cuda_event_ms": 514.4872102737427, "tokens_total": 648, "tokens_per_s": 1259.2635057480816, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 28, "sample_idx": 84, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554093.0670767, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.558300000411691, "prefill_cuda_event_ms": 10.490752220153809, "kv_decode_ms": 502.41339999956836, "kv_decode_cuda_event_ms": 502.36517333984375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 10.558300000411691, "cuda_event_ms": 10.490752220153809, "tokens_total": 136, "tokens_per_s": 12880.861501822932, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 28, "sample_idx": 85, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554093.0670767, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.558300000411691, "prefill_cuda_event_ms": 10.490752220153809, "kv_decode_ms": 502.41339999956836, "kv_decode_cuda_event_ms": 502.36517333984375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 502.41339999956836, "cuda_event_ms": 502.36517333984375, "tokens_total": 512, "tokens_per_s": 1019.0810993505346, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 28, "sample_idx": 86, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554093.0670767, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.558300000411691, "prefill_cuda_event_ms": 10.490752220153809, "kv_decode_ms": 502.41339999956836, "kv_decode_cuda_event_ms": 502.36517333984375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 512.97169999998, "cuda_event_ms": 512.8559255599976, "tokens_total": 648, "tokens_per_s": 1263.2275815605913, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554093.5823505, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.431299999254406, "prefill_cuda_event_ms": 10.365216255187988, "kv_decode_ms": 501.9611999996414, "kv_decode_cuda_event_ms": 501.8890380859375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 10.431299999254406, "cuda_event_ms": 10.365216255187988, "tokens_total": 136, "tokens_per_s": 13037.684661520694, "gen_tokens": 0}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554093.5823505, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.431299999254406, "prefill_cuda_event_ms": 10.365216255187988, "kv_decode_ms": 501.9611999996414, "kv_decode_cuda_event_ms": 501.8890380859375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 501.9611999996414, "cuda_event_ms": 501.8890380859375, "tokens_total": 512, "tokens_per_s": 1019.9991553139282, "gen_tokens": 512}
{"task_idx": 4, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554093.5823505, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.431299999254406, "prefill_cuda_event_ms": 10.365216255187988, "kv_decode_ms": 501.9611999996414, "kv_decode_cuda_event_ms": 501.8890380859375, "gpu_peak_mb": 436.4248046875, "params_millions_measured": 96.08832, "latency_ms": 512.3924999988958, "cuda_event_ms": 512.2542543411255, "tokens_total": 648, "tokens_per_s": 1264.6555131103528, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 30, "sample_idx": 90, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554094.0966926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.050599999682163, "prefill_cuda_event_ms": 8.94480037689209, "kv_decode_ms": 329.19690000017, "kv_decode_cuda_event_ms": 329.101318359375, "gpu_peak_mb": 580.705078125, "hf_load_ms": 318.3679000012489, "params_millions_measured": 74.824704, "latency_ms": 9.050599999682163, "cuda_event_ms": 8.94480037689209, "tokens_total": 136, "tokens_per_s": 15026.628069385015, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 30, "sample_idx": 91, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554094.0966926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.050599999682163, "prefill_cuda_event_ms": 8.94480037689209, "kv_decode_ms": 329.19690000017, "kv_decode_cuda_event_ms": 329.101318359375, "gpu_peak_mb": 580.705078125, "hf_load_ms": 318.3679000012489, "params_millions_measured": 74.824704, "latency_ms": 329.19690000017, "cuda_event_ms": 329.101318359375, "tokens_total": 512, "tokens_per_s": 1555.3001866048423, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 30, "sample_idx": 92, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554094.0966926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.050599999682163, "prefill_cuda_event_ms": 8.94480037689209, "kv_decode_ms": 329.19690000017, "kv_decode_cuda_event_ms": 329.101318359375, "gpu_peak_mb": 580.705078125, "hf_load_ms": 318.3679000012489, "params_millions_measured": 74.824704, "latency_ms": 338.24749999985215, "cuda_event_ms": 338.0461187362671, "tokens_total": 648, "tokens_per_s": 1915.7569531194856, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554094.7557783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.7615000009245705, "prefill_cuda_event_ms": 7.686079978942871, "kv_decode_ms": 314.4929999998567, "kv_decode_cuda_event_ms": 314.4407043457031, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 7.7615000009245705, "cuda_event_ms": 7.686079978942871, "tokens_total": 136, "tokens_per_s": 17522.386134613065, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554094.7557783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.7615000009245705, "prefill_cuda_event_ms": 7.686079978942871, "kv_decode_ms": 314.4929999998567, "kv_decode_cuda_event_ms": 314.4407043457031, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 314.4929999998567, "cuda_event_ms": 314.4407043457031, "tokens_total": 512, "tokens_per_s": 1628.0171577753188, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554094.7557783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.7615000009245705, "prefill_cuda_event_ms": 7.686079978942871, "kv_decode_ms": 314.4929999998567, "kv_decode_cuda_event_ms": 314.4407043457031, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 322.25450000078126, "cuda_event_ms": 322.126784324646, "tokens_total": 648, "tokens_per_s": 2010.833052753116, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 32, "sample_idx": 96, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554095.080144, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.895599999348633, "prefill_cuda_event_ms": 5.8152642250061035, "kv_decode_ms": 331.57910000045376, "kv_decode_cuda_event_ms": 331.50054931640625, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 5.895599999348633, "cuda_event_ms": 5.8152642250061035, "tokens_total": 136, "tokens_per_s": 23068.05075226029, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 32, "sample_idx": 97, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554095.080144, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.895599999348633, "prefill_cuda_event_ms": 5.8152642250061035, "kv_decode_ms": 331.57910000045376, "kv_decode_cuda_event_ms": 331.50054931640625, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 331.57910000045376, "cuda_event_ms": 331.50054931640625, "tokens_total": 512, "tokens_per_s": 1544.1262733365863, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 32, "sample_idx": 98, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554095.080144, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.895599999348633, "prefill_cuda_event_ms": 5.8152642250061035, "kv_decode_ms": 331.57910000045376, "kv_decode_cuda_event_ms": 331.50054931640625, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 337.4746999998024, "cuda_event_ms": 337.31581354141235, "tokens_total": 648, "tokens_per_s": 1920.143939680158, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554095.4203062, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.789900000716443, "prefill_cuda_event_ms": 7.719615936279297, "kv_decode_ms": 322.195599999759, "kv_decode_cuda_event_ms": 322.1299133300781, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 7.789900000716443, "cuda_event_ms": 7.719615936279297, "tokens_total": 136, "tokens_per_s": 17458.503958650555, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554095.4203062, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.789900000716443, "prefill_cuda_event_ms": 7.719615936279297, "kv_decode_ms": 322.195599999759, "kv_decode_cuda_event_ms": 322.1299133300781, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 322.195599999759, "cuda_event_ms": 322.1299133300781, "tokens_total": 512, "tokens_per_s": 1589.096809516899, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554095.4203062, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.789900000716443, "prefill_cuda_event_ms": 7.719615936279297, "kv_decode_ms": 322.195599999759, "kv_decode_cuda_event_ms": 322.1299133300781, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 329.98550000047544, "cuda_event_ms": 329.8495292663574, "tokens_total": 648, "tokens_per_s": 1963.722648416571, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554095.7524076, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.206300000892952, "prefill_cuda_event_ms": 8.128864288330078, "kv_decode_ms": 324.41689999905066, "kv_decode_cuda_event_ms": 324.3540344238281, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 8.206300000892952, "cuda_event_ms": 8.128864288330078, "tokens_total": 136, "tokens_per_s": 16572.633219014835, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554095.7524076, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.206300000892952, "prefill_cuda_event_ms": 8.128864288330078, "kv_decode_ms": 324.41689999905066, "kv_decode_cuda_event_ms": 324.3540344238281, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 324.41689999905066, "cuda_event_ms": 324.3540344238281, "tokens_total": 512, "tokens_per_s": 1578.2161780150734, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554095.7524076, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.206300000892952, "prefill_cuda_event_ms": 8.128864288330078, "kv_decode_ms": 324.41689999905066, "kv_decode_cuda_event_ms": 324.3540344238281, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 332.6231999999436, "cuda_event_ms": 332.4828987121582, "tokens_total": 648, "tokens_per_s": 1948.1503394835654, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554096.0870814, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.632400000147754, "prefill_cuda_event_ms": 7.5609917640686035, "kv_decode_ms": 323.89359999979206, "kv_decode_cuda_event_ms": 323.83282470703125, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 7.632400000147754, "cuda_event_ms": 7.5609917640686035, "tokens_total": 136, "tokens_per_s": 17818.772600671768, "gen_tokens": 0}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554096.0870814, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.632400000147754, "prefill_cuda_event_ms": 7.5609917640686035, "kv_decode_ms": 323.89359999979206, "kv_decode_cuda_event_ms": 323.83282470703125, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 323.89359999979206, "cuda_event_ms": 323.83282470703125, "tokens_total": 512, "tokens_per_s": 1580.7660293390443, "gen_tokens": 512}
{"task_idx": 5, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554096.0870814, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 7.632400000147754, "prefill_cuda_event_ms": 7.5609917640686035, "kv_decode_ms": 323.89359999979206, "kv_decode_cuda_event_ms": 323.83282470703125, "gpu_peak_mb": 580.705078125, "params_millions_measured": 74.824704, "latency_ms": 331.5259999999398, "cuda_event_ms": 331.39381647109985, "tokens_total": 648, "tokens_per_s": 1954.5978294315307, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554096.421111, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.767099999706261, "prefill_cuda_event_ms": 6.691232204437256, "kv_decode_ms": 218.00450000046112, "kv_decode_cuda_event_ms": 217.9287109375, "gpu_peak_mb": 625.66357421875, "hf_load_ms": 207.7833000003011, "params_millions_measured": 25.016064, "latency_ms": 6.767099999706261, "cuda_event_ms": 6.691232204437256, "tokens_total": 136, "tokens_per_s": 20097.235153301022, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554096.421111, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.767099999706261, "prefill_cuda_event_ms": 6.691232204437256, "kv_decode_ms": 218.00450000046112, "kv_decode_cuda_event_ms": 217.9287109375, "gpu_peak_mb": 625.66357421875, "hf_load_ms": 207.7833000003011, "params_millions_measured": 25.016064, "latency_ms": 218.00450000046112, "cuda_event_ms": 217.9287109375, "tokens_total": 512, "tokens_per_s": 2348.5753734391583, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554096.421111, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.767099999706261, "prefill_cuda_event_ms": 6.691232204437256, "kv_decode_ms": 218.00450000046112, "kv_decode_cuda_event_ms": 217.9287109375, "gpu_peak_mb": 625.66357421875, "hf_load_ms": 207.7833000003011, "params_millions_measured": 25.016064, "latency_ms": 224.77160000016738, "cuda_event_ms": 224.61994314193726, "tokens_total": 648, "tokens_per_s": 2882.926490711093, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554096.8559878, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.57990000177233, "prefill_cuda_event_ms": 4.520768165588379, "kv_decode_ms": 211.78949999921315, "kv_decode_cuda_event_ms": 211.73146057128906, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 4.57990000177233, "cuda_event_ms": 4.520768165588379, "tokens_total": 136, "tokens_per_s": 29694.971494436715, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554096.8559878, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.57990000177233, "prefill_cuda_event_ms": 4.520768165588379, "kv_decode_ms": 211.78949999921315, "kv_decode_cuda_event_ms": 211.73146057128906, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 211.78949999921315, "cuda_event_ms": 211.73146057128906, "tokens_total": 512, "tokens_per_s": 2417.494729445521, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554096.8559878, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.57990000177233, "prefill_cuda_event_ms": 4.520768165588379, "kv_decode_ms": 211.78949999921315, "kv_decode_cuda_event_ms": 211.73146057128906, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 216.36940000098548, "cuda_event_ms": 216.25222873687744, "tokens_total": 648, "tokens_per_s": 2994.8782036510183, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554097.074783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.8217000001168344, "prefill_cuda_event_ms": 3.778048038482666, "kv_decode_ms": 216.93659999982629, "kv_decode_cuda_event_ms": 216.8606719970703, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 3.8217000001168344, "cuda_event_ms": 3.778048038482666, "tokens_total": 136, "tokens_per_s": 35586.257423618365, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554097.074783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.8217000001168344, "prefill_cuda_event_ms": 3.778048038482666, "kv_decode_ms": 216.93659999982629, "kv_decode_cuda_event_ms": 216.8606719970703, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 216.93659999982629, "cuda_event_ms": 216.8606719970703, "tokens_total": 512, "tokens_per_s": 2360.1365560279364, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554097.074783, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.8217000001168344, "prefill_cuda_event_ms": 3.778048038482666, "kv_decode_ms": 216.93659999982629, "kv_decode_cuda_event_ms": 216.8606719970703, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 220.75829999994312, "cuda_event_ms": 220.63872003555298, "tokens_total": 648, "tokens_per_s": 2935.3369726083547, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554097.2976933, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.354900000384077, "prefill_cuda_event_ms": 5.274112224578857, "kv_decode_ms": 210.29670000098122, "kv_decode_cuda_event_ms": 210.24972534179688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 5.354900000384077, "cuda_event_ms": 5.274112224578857, "tokens_total": 136, "tokens_per_s": 25397.299667640007, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554097.2976933, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.354900000384077, "prefill_cuda_event_ms": 5.274112224578857, "kv_decode_ms": 210.29670000098122, "kv_decode_cuda_event_ms": 210.24972534179688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 210.29670000098122, "cuda_event_ms": 210.24972534179688, "tokens_total": 512, "tokens_per_s": 2434.655417786447, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554097.2976933, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.354900000384077, "prefill_cuda_event_ms": 5.274112224578857, "kv_decode_ms": 210.29670000098122, "kv_decode_cuda_event_ms": 210.24972534179688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 215.6516000013653, "cuda_event_ms": 215.52383756637573, "tokens_total": 648, "tokens_per_s": 3004.846706427856, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 40, "sample_idx": 120, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554097.5159009, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.295900000419351, "prefill_cuda_event_ms": 5.228928089141846, "kv_decode_ms": 214.42859999842767, "kv_decode_cuda_event_ms": 214.38156127929688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 5.295900000419351, "cuda_event_ms": 5.228928089141846, "tokens_total": 136, "tokens_per_s": 25680.243204975726, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 40, "sample_idx": 121, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554097.5159009, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.295900000419351, "prefill_cuda_event_ms": 5.228928089141846, "kv_decode_ms": 214.42859999842767, "kv_decode_cuda_event_ms": 214.38156127929688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 214.42859999842767, "cuda_event_ms": 214.38156127929688, "tokens_total": 512, "tokens_per_s": 2387.7411875270104, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 40, "sample_idx": 122, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554097.5159009, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.295900000419351, "prefill_cuda_event_ms": 5.228928089141846, "kv_decode_ms": 214.42859999842767, "kv_decode_cuda_event_ms": 214.38156127929688, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 219.72449999884702, "cuda_event_ms": 219.61048936843872, "tokens_total": 648, "tokens_per_s": 2949.1476826817234, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554097.7377515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.379500000344706, "prefill_cuda_event_ms": 5.305471897125244, "kv_decode_ms": 219.2003999989538, "kv_decode_cuda_event_ms": 219.14215087890625, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 5.379500000344706, "cuda_event_ms": 5.305471897125244, "tokens_total": 136, "tokens_per_s": 25281.159957484047, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554097.7377515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.379500000344706, "prefill_cuda_event_ms": 5.305471897125244, "kv_decode_ms": 219.2003999989538, "kv_decode_cuda_event_ms": 219.14215087890625, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 219.2003999989538, "cuda_event_ms": 219.14215087890625, "tokens_total": 512, "tokens_per_s": 2335.7621610291026, "gen_tokens": 512}
{"task_idx": 6, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554097.7377515, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 5.379500000344706, "prefill_cuda_event_ms": 5.305471897125244, "kv_decode_ms": 219.2003999989538, "kv_decode_cuda_event_ms": 219.14215087890625, "gpu_peak_mb": 625.66357421875, "params_millions_measured": 25.016064, "latency_ms": 224.5798999992985, "cuda_event_ms": 224.4476227760315, "tokens_total": 648, "tokens_per_s": 2885.387338769071, "gen_tokens": 512}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554097.9650166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.33600000059232, "prefill_cuda_event_ms": 16.249631881713867, "kv_decode_ms": 716.6146999988996, "kv_decode_cuda_event_ms": 716.5603637695312, "gpu_peak_mb": 631.6650390625, "hf_load_ms": 229.26139999981388, "params_millions_measured": 5.03672, "latency_ms": 16.33600000059232, "cuda_event_ms": 16.249631881713867, "tokens_total": 17, "tokens_per_s": 1040.6464250357249, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554097.9650166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.33600000059232, "prefill_cuda_event_ms": 16.249631881713867, "kv_decode_ms": 716.6146999988996, "kv_decode_cuda_event_ms": 716.5603637695312, "gpu_peak_mb": 631.6650390625, "hf_load_ms": 229.26139999981388, "params_millions_measured": 5.03672, "latency_ms": 716.6146999988996, "cuda_event_ms": 716.5603637695312, "tokens_total": 64, "tokens_per_s": 89.30880150811625, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554097.9650166, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.33600000059232, "prefill_cuda_event_ms": 16.249631881713867, "kv_decode_ms": 716.6146999988996, "kv_decode_cuda_event_ms": 716.5603637695312, "gpu_peak_mb": 631.6650390625, "hf_load_ms": 229.26139999981388, "params_millions_measured": 5.03672, "latency_ms": 732.9506999994919, "cuda_event_ms": 732.8099956512451, "tokens_total": 81, "tokens_per_s": 110.51220771063613, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554098.9285526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.580099999468075, "prefill_cuda_event_ms": 12.520832061767578, "kv_decode_ms": 734.460499999841, "kv_decode_cuda_event_ms": 734.403564453125, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 12.580099999468075, "cuda_event_ms": 12.520832061767578, "tokens_total": 17, "tokens_per_s": 1351.340609432263, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554098.9285526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.580099999468075, "prefill_cuda_event_ms": 12.520832061767578, "kv_decode_ms": 734.460499999841, "kv_decode_cuda_event_ms": 734.403564453125, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 734.460499999841, "cuda_event_ms": 734.403564453125, "tokens_total": 64, "tokens_per_s": 87.1387909901402, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554098.9285526, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.580099999468075, "prefill_cuda_event_ms": 12.520832061767578, "kv_decode_ms": 734.460499999841, "kv_decode_cuda_event_ms": 734.403564453125, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 747.040599999309, "cuda_event_ms": 746.9243965148926, "tokens_total": 81, "tokens_per_s": 108.42784180682405, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554099.6764503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.197699999887845, "prefill_cuda_event_ms": 12.136223793029785, "kv_decode_ms": 727.2202000003745, "kv_decode_cuda_event_ms": 727.1710815429688, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 12.197699999887845, "cuda_event_ms": 12.136223793029785, "tokens_total": 17, "tokens_per_s": 1393.705370697452, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554099.6764503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.197699999887845, "prefill_cuda_event_ms": 12.136223793029785, "kv_decode_ms": 727.2202000003745, "kv_decode_cuda_event_ms": 727.1710815429688, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 727.2202000003745, "cuda_event_ms": 727.1710815429688, "tokens_total": 64, "tokens_per_s": 88.0063562590355, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554099.6764503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.197699999887845, "prefill_cuda_event_ms": 12.136223793029785, "kv_decode_ms": 727.2202000003745, "kv_decode_cuda_event_ms": 727.1710815429688, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 739.4179000002623, "cuda_event_ms": 739.3073053359985, "tokens_total": 81, "tokens_per_s": 109.54563042086386, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554100.418725, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.390500000037719, "prefill_cuda_event_ms": 12.329631805419922, "kv_decode_ms": 734.9783000008756, "kv_decode_cuda_event_ms": 734.9370727539062, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 12.390500000037719, "cuda_event_ms": 12.329631805419922, "tokens_total": 17, "tokens_per_s": 1372.0188854322464, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554100.418725, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.390500000037719, "prefill_cuda_event_ms": 12.329631805419922, "kv_decode_ms": 734.9783000008756, "kv_decode_cuda_event_ms": 734.9370727539062, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 734.9783000008756, "cuda_event_ms": 734.9370727539062, "tokens_total": 64, "tokens_per_s": 87.07740078846376, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554100.418725, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.390500000037719, "prefill_cuda_event_ms": 12.329631805419922, "kv_decode_ms": 734.9783000008756, "kv_decode_cuda_event_ms": 734.9370727539062, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 747.3688000009133, "cuda_event_ms": 747.2667045593262, "tokens_total": 81, "tokens_per_s": 108.380226736654, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554101.167027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.004599999272614, "prefill_cuda_event_ms": 11.9486083984375, "kv_decode_ms": 697.9751000017131, "kv_decode_cuda_event_ms": 697.9174194335938, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 12.004599999272614, "cuda_event_ms": 11.9486083984375, "tokens_total": 17, "tokens_per_s": 1416.1238192884452, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554101.167027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.004599999272614, "prefill_cuda_event_ms": 11.9486083984375, "kv_decode_ms": 697.9751000017131, "kv_decode_cuda_event_ms": 697.9174194335938, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 697.9751000017131, "cuda_event_ms": 697.9174194335938, "tokens_total": 64, "tokens_per_s": 91.69381543817669, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554101.167027, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.004599999272614, "prefill_cuda_event_ms": 11.9486083984375, "kv_decode_ms": 697.9751000017131, "kv_decode_cuda_event_ms": 697.9174194335938, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 709.9797000009858, "cuda_event_ms": 709.8660278320312, "tokens_total": 81, "tokens_per_s": 114.08776898816619, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554101.877864, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.022300001466647, "prefill_cuda_event_ms": 12.955007553100586, "kv_decode_ms": 753.2940000000963, "kv_decode_cuda_event_ms": 753.2308349609375, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 13.022300001466647, "cuda_event_ms": 12.955007553100586, "tokens_total": 17, "tokens_per_s": 1305.4529536322586, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554101.877864, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.022300001466647, "prefill_cuda_event_ms": 12.955007553100586, "kv_decode_ms": 753.2940000000963, "kv_decode_cuda_event_ms": 753.2308349609375, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 753.2940000000963, "cuda_event_ms": 753.2308349609375, "tokens_total": 64, "tokens_per_s": 84.96018818680598, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554101.877864, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 13.022300001466647, "prefill_cuda_event_ms": 12.955007553100586, "kv_decode_ms": 753.2940000000963, "kv_decode_cuda_event_ms": 753.2308349609375, "gpu_peak_mb": 631.6650390625, "params_millions_measured": 5.03672, "latency_ms": 766.3163000015629, "cuda_event_ms": 766.1858425140381, "tokens_total": 81, "tokens_per_s": 105.7004790317455, "gen_tokens": 64}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554102.645195, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 15.358300001025782, "prefill_cuda_event_ms": 15.264287948608398, "kv_decode_ms": 555.8845999985351, "kv_decode_cuda_event_ms": 555.8046875, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 15.358300001025782, "cuda_event_ms": 15.264287948608398, "tokens_total": 136, "tokens_per_s": 8855.146727887628, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554102.645195, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 15.358300001025782, "prefill_cuda_event_ms": 15.264287948608398, "kv_decode_ms": 555.8845999985351, "kv_decode_cuda_event_ms": 555.8046875, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 555.8845999985351, "cuda_event_ms": 555.8046875, "tokens_total": 512, "tokens_per_s": 921.0544778562839, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554102.645195, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 15.358300001025782, "prefill_cuda_event_ms": 15.264287948608398, "kv_decode_ms": 555.8845999985351, "kv_decode_cuda_event_ms": 555.8046875, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 571.2428999995609, "cuda_event_ms": 571.0689754486084, "tokens_total": 648, "tokens_per_s": 1134.3685847132597, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554103.2184906, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.858600000370643, "prefill_cuda_event_ms": 9.777791976928711, "kv_decode_ms": 489.07969999891066, "kv_decode_cuda_event_ms": 489.0142822265625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 9.858600000370643, "cuda_event_ms": 9.777791976928711, "tokens_total": 136, "tokens_per_s": 13795.062178695449, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554103.2184906, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.858600000370643, "prefill_cuda_event_ms": 9.777791976928711, "kv_decode_ms": 489.07969999891066, "kv_decode_cuda_event_ms": 489.0142822265625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 489.07969999891066, "cuda_event_ms": 489.0142822265625, "tokens_total": 512, "tokens_per_s": 1046.864140959317, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554103.2184906, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.858600000370643, "prefill_cuda_event_ms": 9.777791976928711, "kv_decode_ms": 489.07969999891066, "kv_decode_cuda_event_ms": 489.0142822265625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 498.9382999992813, "cuda_event_ms": 498.7920742034912, "tokens_total": 648, "tokens_per_s": 1298.7577822767532, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554103.7195017, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.417799999937415, "prefill_cuda_event_ms": 9.351967811584473, "kv_decode_ms": 531.1178000010841, "kv_decode_cuda_event_ms": 531.0485229492188, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 9.417799999937415, "cuda_event_ms": 9.351967811584473, "tokens_total": 136, "tokens_per_s": 14440.739875650765, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554103.7195017, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.417799999937415, "prefill_cuda_event_ms": 9.351967811584473, "kv_decode_ms": 531.1178000010841, "kv_decode_cuda_event_ms": 531.0485229492188, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 531.1178000010841, "cuda_event_ms": 531.0485229492188, "tokens_total": 512, "tokens_per_s": 964.0045955886903, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554103.7195017, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.417799999937415, "prefill_cuda_event_ms": 9.351967811584473, "kv_decode_ms": 531.1178000010841, "kv_decode_cuda_event_ms": 531.0485229492188, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 540.5356000010215, "cuda_event_ms": 540.4004907608032, "tokens_total": 648, "tokens_per_s": 1198.8109571298826, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554104.2625108, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.276500000851229, "prefill_cuda_event_ms": 9.205408096313477, "kv_decode_ms": 524.3955999994796, "kv_decode_cuda_event_ms": 524.3441772460938, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 9.276500000851229, "cuda_event_ms": 9.205408096313477, "tokens_total": 136, "tokens_per_s": 14660.701771952825, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554104.2625108, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.276500000851229, "prefill_cuda_event_ms": 9.205408096313477, "kv_decode_ms": 524.3955999994796, "kv_decode_cuda_event_ms": 524.3441772460938, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 524.3955999994796, "cuda_event_ms": 524.3441772460938, "tokens_total": 512, "tokens_per_s": 976.362120506938, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554104.2625108, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 9.276500000851229, "prefill_cuda_event_ms": 9.205408096313477, "kv_decode_ms": 524.3955999994796, "kv_decode_cuda_event_ms": 524.3441772460938, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 533.6721000003308, "cuda_event_ms": 533.5495853424072, "tokens_total": 648, "tokens_per_s": 1214.2287370833108, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 52, "sample_idx": 156, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554104.7984912, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.186699998987024, "prefill_cuda_event_ms": 8.136063575744629, "kv_decode_ms": 513.7880000002042, "kv_decode_cuda_event_ms": 513.728515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 8.186699998987024, "cuda_event_ms": 8.136063575744629, "tokens_total": 136, "tokens_per_s": 16612.310212518827, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 52, "sample_idx": 157, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554104.7984912, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.186699998987024, "prefill_cuda_event_ms": 8.136063575744629, "kv_decode_ms": 513.7880000002042, "kv_decode_cuda_event_ms": 513.728515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 513.7880000002042, "cuda_event_ms": 513.728515625, "tokens_total": 512, "tokens_per_s": 996.5199654328177, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 52, "sample_idx": 158, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554104.7984912, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.186699998987024, "prefill_cuda_event_ms": 8.136063575744629, "kv_decode_ms": 513.7880000002042, "kv_decode_cuda_event_ms": 513.728515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 521.9746999991912, "cuda_event_ms": 521.8645792007446, "tokens_total": 648, "tokens_per_s": 1241.4394797314967, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554105.322283, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.624799999277457, "prefill_cuda_event_ms": 10.534239768981934, "kv_decode_ms": 515.0475999998889, "kv_decode_cuda_event_ms": 514.9849853515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 10.624799999277457, "cuda_event_ms": 10.534239768981934, "tokens_total": 136, "tokens_per_s": 12800.240946582404, "gen_tokens": 0}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554105.322283, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.624799999277457, "prefill_cuda_event_ms": 10.534239768981934, "kv_decode_ms": 515.0475999998889, "kv_decode_cuda_event_ms": 514.9849853515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 515.0475999998889, "cuda_event_ms": 514.9849853515625, "tokens_total": 512, "tokens_per_s": 994.0828770003208, "gen_tokens": 512}
{"task_idx": 8, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554105.322283, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 10.624799999277457, "prefill_cuda_event_ms": 10.534239768981934, "kv_decode_ms": 515.0475999998889, "kv_decode_cuda_event_ms": 514.9849853515625, "gpu_peak_mb": 654.60302734375, "params_millions_measured": 51.475968, "latency_ms": 525.6723999991664, "cuda_event_ms": 525.5192251205444, "tokens_total": 648, "tokens_per_s": 1232.706910237303, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 54, "sample_idx": 162, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554105.8503585, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 19.801800001005176, "prefill_cuda_event_ms": 19.722719192504883, "kv_decode_ms": 740.2086000001873, "kv_decode_cuda_event_ms": 740.1533203125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 19.801800001005176, "cuda_event_ms": 19.722719192504883, "tokens_total": 136, "tokens_per_s": 6868.062499020109, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 54, "sample_idx": 163, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554105.8503585, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 19.801800001005176, "prefill_cuda_event_ms": 19.722719192504883, "kv_decode_ms": 740.2086000001873, "kv_decode_cuda_event_ms": 740.1533203125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 740.2086000001873, "cuda_event_ms": 740.1533203125, "tokens_total": 512, "tokens_per_s": 691.696908141665, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 54, "sample_idx": 164, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554105.8503585, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 19.801800001005176, "prefill_cuda_event_ms": 19.722719192504883, "kv_decode_ms": 740.2086000001873, "kv_decode_cuda_event_ms": 740.1533203125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 760.0104000011925, "cuda_event_ms": 759.8760395050049, "tokens_total": 648, "tokens_per_s": 852.6199115156626, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554106.6123044, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.843800000249757, "prefill_cuda_event_ms": 8.804224014282227, "kv_decode_ms": 699.9412999994092, "kv_decode_cuda_event_ms": 699.883544921875, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 8.843800000249757, "cuda_event_ms": 8.804224014282227, "tokens_total": 136, "tokens_per_s": 15378.004929573173, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554106.6123044, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.843800000249757, "prefill_cuda_event_ms": 8.804224014282227, "kv_decode_ms": 699.9412999994092, "kv_decode_cuda_event_ms": 699.883544921875, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 699.9412999994092, "cuda_event_ms": 699.883544921875, "tokens_total": 512, "tokens_per_s": 731.4899120832449, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554106.6123044, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 8.843800000249757, "prefill_cuda_event_ms": 8.804224014282227, "kv_decode_ms": 699.9412999994092, "kv_decode_cuda_event_ms": 699.883544921875, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 708.785099999659, "cuda_event_ms": 708.6877689361572, "tokens_total": 648, "tokens_per_s": 914.2404376168627, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 56, "sample_idx": 168, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554107.322958, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.933899999552523, "prefill_cuda_event_ms": 12.867520332336426, "kv_decode_ms": 724.688000000242, "kv_decode_cuda_event_ms": 724.611083984375, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 12.933899999552523, "cuda_event_ms": 12.867520332336426, "tokens_total": 136, "tokens_per_s": 10515.003208986092, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 56, "sample_idx": 169, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554107.322958, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.933899999552523, "prefill_cuda_event_ms": 12.867520332336426, "kv_decode_ms": 724.688000000242, "kv_decode_cuda_event_ms": 724.611083984375, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 724.688000000242, "cuda_event_ms": 724.611083984375, "tokens_total": 512, "tokens_per_s": 706.5109398800987, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 56, "sample_idx": 170, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554107.322958, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.933899999552523, "prefill_cuda_event_ms": 12.867520332336426, "kv_decode_ms": 724.688000000242, "kv_decode_cuda_event_ms": 724.611083984375, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 737.6218999997946, "cuda_event_ms": 737.4786043167114, "tokens_total": 648, "tokens_per_s": 878.498862357775, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554108.062977, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.945099999342347, "prefill_cuda_event_ms": 12.870623588562012, "kv_decode_ms": 725.8516999991116, "kv_decode_cuda_event_ms": 725.7999267578125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 12.945099999342347, "cuda_event_ms": 12.870623588562012, "tokens_total": 136, "tokens_per_s": 10505.905710029992, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554108.062977, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.945099999342347, "prefill_cuda_event_ms": 12.870623588562012, "kv_decode_ms": 725.8516999991116, "kv_decode_cuda_event_ms": 725.7999267578125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 725.8516999991116, "cuda_event_ms": 725.7999267578125, "tokens_total": 512, "tokens_per_s": 705.378247375637, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554108.062977, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.945099999342347, "prefill_cuda_event_ms": 12.870623588562012, "kv_decode_ms": 725.8516999991116, "kv_decode_cuda_event_ms": 725.7999267578125, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 738.7967999984539, "cuda_event_ms": 738.6705503463745, "tokens_total": 648, "tokens_per_s": 877.1017957865492, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554108.804214, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.345999999524793, "prefill_cuda_event_ms": 12.288703918457031, "kv_decode_ms": 745.8004999989498, "kv_decode_cuda_event_ms": 745.7402954101562, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 12.345999999524793, "cuda_event_ms": 12.288703918457031, "tokens_total": 136, "tokens_per_s": 11015.713591870626, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554108.804214, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.345999999524793, "prefill_cuda_event_ms": 12.288703918457031, "kv_decode_ms": 745.8004999989498, "kv_decode_cuda_event_ms": 745.7402954101562, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 745.8004999989498, "cuda_event_ms": 745.7402954101562, "tokens_total": 512, "tokens_per_s": 686.510668738786, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554108.804214, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 12.345999999524793, "prefill_cuda_event_ms": 12.288703918457031, "kv_decode_ms": 745.8004999989498, "kv_decode_cuda_event_ms": 745.7402954101562, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 758.1464999984746, "cuda_event_ms": 758.0289993286133, "tokens_total": 648, "tokens_per_s": 854.7160740058864, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554109.5644042, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.016500000958331, "prefill_cuda_event_ms": 12.947903633117676, "kv_decode_ms": 740.1545000011538, "kv_decode_cuda_event_ms": 740.0929565429688, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 13.016500000958331, "cuda_event_ms": 12.947903633117676, "tokens_total": 136, "tokens_per_s": 10448.277185878469, "gen_tokens": 0}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554109.5644042, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.016500000958331, "prefill_cuda_event_ms": 12.947903633117676, "kv_decode_ms": 740.1545000011538, "kv_decode_cuda_event_ms": 740.0929565429688, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 740.1545000011538, "cuda_event_ms": 740.0929565429688, "tokens_total": 512, "tokens_per_s": 691.7474662373894, "gen_tokens": 512}
{"task_idx": 9, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554109.5644042, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 13.016500000958331, "prefill_cuda_event_ms": 12.947903633117676, "kv_decode_ms": 740.1545000011538, "kv_decode_cuda_event_ms": 740.0929565429688, "gpu_peak_mb": 646.7919921875, "params_millions_measured": 5.03672, "latency_ms": 753.1710000021121, "cuda_event_ms": 753.0408601760864, "tokens_total": 648, "tokens_per_s": 860.3623878218663, "gen_tokens": 512}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554110.3200283, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.366100000785082, "prefill_cuda_event_ms": 8.273664474487305, "kv_decode_ms": 203.66810000086844, "kv_decode_cuda_event_ms": 203.55410766601562, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 8.366100000785082, "cuda_event_ms": 8.273664474487305, "tokens_total": 17, "tokens_per_s": 2032.0101359539935, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554110.3200283, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.366100000785082, "prefill_cuda_event_ms": 8.273664474487305, "kv_decode_ms": 203.66810000086844, "kv_decode_cuda_event_ms": 203.55410766601562, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 203.66810000086844, "cuda_event_ms": 203.55410766601562, "tokens_total": 64, "tokens_per_s": 314.23674104941864, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554110.3200283, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.366100000785082, "prefill_cuda_event_ms": 8.273664474487305, "kv_decode_ms": 203.66810000086844, "kv_decode_cuda_event_ms": 203.55410766601562, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 212.03420000165352, "cuda_event_ms": 211.82777214050293, "tokens_total": 81, "tokens_per_s": 382.01384493335667, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554110.533025, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8604000001214445, "prefill_cuda_event_ms": 3.8114559650421143, "kv_decode_ms": 217.1780000007857, "kv_decode_cuda_event_ms": 217.10105895996094, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 3.8604000001214445, "cuda_event_ms": 3.8114559650421143, "tokens_total": 17, "tokens_per_s": 4403.688736779918, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554110.533025, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8604000001214445, "prefill_cuda_event_ms": 3.8114559650421143, "kv_decode_ms": 217.1780000007857, "kv_decode_cuda_event_ms": 217.10105895996094, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 217.1780000007857, "cuda_event_ms": 217.10105895996094, "tokens_total": 64, "tokens_per_s": 294.68914899192583, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554110.533025, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8604000001214445, "prefill_cuda_event_ms": 3.8114559650421143, "kv_decode_ms": 217.1780000007857, "kv_decode_cuda_event_ms": 217.10105895996094, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 221.03840000090713, "cuda_event_ms": 220.91251492500305, "tokens_total": 81, "tokens_per_s": 366.45216396638585, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554110.7549922, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8936000000830973, "prefill_cuda_event_ms": 3.8128960132598877, "kv_decode_ms": 215.32589999878837, "kv_decode_cuda_event_ms": 215.2611846923828, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 3.8936000000830973, "cuda_event_ms": 3.8128960132598877, "tokens_total": 17, "tokens_per_s": 4366.139305433837, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554110.7549922, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8936000000830973, "prefill_cuda_event_ms": 3.8128960132598877, "kv_decode_ms": 215.32589999878837, "kv_decode_cuda_event_ms": 215.2611846923828, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 215.32589999878837, "cuda_event_ms": 215.2611846923828, "tokens_total": 64, "tokens_per_s": 297.22388249792584, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554110.7549922, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8936000000830973, "prefill_cuda_event_ms": 3.8128960132598877, "kv_decode_ms": 215.32589999878837, "kv_decode_cuda_event_ms": 215.2611846923828, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 219.21949999887147, "cuda_event_ms": 219.0740807056427, "tokens_total": 81, "tokens_per_s": 369.4926774325139, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554110.9752386, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.568100001051789, "prefill_cuda_event_ms": 4.4549760818481445, "kv_decode_ms": 214.74659999876167, "kv_decode_cuda_event_ms": 214.67532348632812, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 4.568100001051789, "cuda_event_ms": 4.4549760818481445, "tokens_total": 17, "tokens_per_s": 3721.459686978352, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554110.9752386, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.568100001051789, "prefill_cuda_event_ms": 4.4549760818481445, "kv_decode_ms": 214.74659999876167, "kv_decode_cuda_event_ms": 214.67532348632812, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 214.74659999876167, "cuda_event_ms": 214.67532348632812, "tokens_total": 64, "tokens_per_s": 298.0256730507913, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554110.9752386, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.568100001051789, "prefill_cuda_event_ms": 4.4549760818481445, "kv_decode_ms": 214.74659999876167, "kv_decode_cuda_event_ms": 214.67532348632812, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 219.31469999981346, "cuda_event_ms": 219.13029956817627, "tokens_total": 81, "tokens_per_s": 369.33228826006143, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554111.1954107, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.7207999994279817, "prefill_cuda_event_ms": 3.6582720279693604, "kv_decode_ms": 209.90159999928437, "kv_decode_cuda_event_ms": 209.82774353027344, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 3.7207999994279817, "cuda_event_ms": 3.6582720279693604, "tokens_total": 17, "tokens_per_s": 4568.909912549318, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554111.1954107, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.7207999994279817, "prefill_cuda_event_ms": 3.6582720279693604, "kv_decode_ms": 209.90159999928437, "kv_decode_cuda_event_ms": 209.82774353027344, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 209.90159999928437, "cuda_event_ms": 209.82774353027344, "tokens_total": 64, "tokens_per_s": 304.9047744286761, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554111.1954107, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.7207999994279817, "prefill_cuda_event_ms": 3.6582720279693604, "kv_decode_ms": 209.90159999928437, "kv_decode_cuda_event_ms": 209.82774353027344, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 213.62239999871235, "cuda_event_ms": 213.4860155582428, "tokens_total": 81, "tokens_per_s": 379.1737196122141, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554111.409917, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.792299999986426, "prefill_cuda_event_ms": 3.7288639545440674, "kv_decode_ms": 212.95619999909832, "kv_decode_cuda_event_ms": 212.8896026611328, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 3.792299999986426, "cuda_event_ms": 3.7288639545440674, "tokens_total": 17, "tokens_per_s": 4482.767713540819, "gen_tokens": 0}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554111.409917, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.792299999986426, "prefill_cuda_event_ms": 3.7288639545440674, "kv_decode_ms": 212.95619999909832, "kv_decode_cuda_event_ms": 212.8896026611328, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 212.95619999909832, "cuda_event_ms": 212.8896026611328, "tokens_total": 64, "tokens_per_s": 300.5312829599278, "gen_tokens": 64}
{"task_idx": 10, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554111.409917, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.792299999986426, "prefill_cuda_event_ms": 3.7288639545440674, "kv_decode_ms": 212.95619999909832, "kv_decode_cuda_event_ms": 212.8896026611328, "gpu_peak_mb": 631.72314453125, "params_millions_measured": 25.016064, "latency_ms": 216.74849999908474, "cuda_event_ms": 216.61846661567688, "tokens_total": 81, "tokens_per_s": 373.7050083407361, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554111.6276264, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 281.9639000008465, "prefill_cuda_event_ms": 281.8785400390625, "kv_decode_ms": 214.36530000028142, "kv_decode_cuda_event_ms": 214.30169677734375, "gpu_peak_mb": 633.5712890625, "hf_load_ms": 167.06920000069658, "params_millions_measured": 0.102714, "latency_ms": 281.9639000008465, "cuda_event_ms": 281.8785400390625, "tokens_total": 17, "tokens_per_s": 60.29140609825926, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554111.6276264, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 281.9639000008465, "prefill_cuda_event_ms": 281.8785400390625, "kv_decode_ms": 214.36530000028142, "kv_decode_cuda_event_ms": 214.30169677734375, "gpu_peak_mb": 633.5712890625, "hf_load_ms": 167.06920000069658, "params_millions_measured": 0.102714, "latency_ms": 214.36530000028142, "cuda_event_ms": 214.30169677734375, "tokens_total": 64, "tokens_per_s": 298.5557830484504, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554111.6276264, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 281.9639000008465, "prefill_cuda_event_ms": 281.8785400390625, "kv_decode_ms": 214.36530000028142, "kv_decode_cuda_event_ms": 214.30169677734375, "gpu_peak_mb": 633.5712890625, "hf_load_ms": 167.06920000069658, "params_millions_measured": 0.102714, "latency_ms": 496.3292000011279, "cuda_event_ms": 496.18023681640625, "tokens_total": 81, "tokens_per_s": 163.19813543070995, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554112.2920504, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8507000008394243, "prefill_cuda_event_ms": 3.7839999198913574, "kv_decode_ms": 191.57910000103584, "kv_decode_cuda_event_ms": 191.51052856445312, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 3.8507000008394243, "cuda_event_ms": 3.7839999198913574, "tokens_total": 17, "tokens_per_s": 4414.781727035117, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554112.2920504, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8507000008394243, "prefill_cuda_event_ms": 3.7839999198913574, "kv_decode_ms": 191.57910000103584, "kv_decode_cuda_event_ms": 191.51052856445312, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 191.57910000103584, "cuda_event_ms": 191.51052856445312, "tokens_total": 64, "tokens_per_s": 334.06566791290885, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554112.2920504, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.8507000008394243, "prefill_cuda_event_ms": 3.7839999198913574, "kv_decode_ms": 191.57910000103584, "kv_decode_cuda_event_ms": 191.51052856445312, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 195.42980000187526, "cuda_event_ms": 195.29452848434448, "tokens_total": 81, "tokens_per_s": 414.4710786135111, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554112.4883664, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8583359718322754, "kv_decode_ms": 191.5977999997267, "kv_decode_cuda_event_ms": 191.52896118164062, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 3.9319999996223487, "cuda_event_ms": 3.8583359718322754, "tokens_total": 17, "tokens_per_s": 4323.499491768254, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554112.4883664, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8583359718322754, "kv_decode_ms": 191.5977999997267, "kv_decode_cuda_event_ms": 191.52896118164062, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 191.5977999997267, "cuda_event_ms": 191.52896118164062, "tokens_total": 64, "tokens_per_s": 334.03306301059456, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554112.4883664, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8583359718322754, "kv_decode_ms": 191.5977999997267, "kv_decode_cuda_event_ms": 191.52896118164062, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 195.52979999934905, "cuda_event_ms": 195.3872971534729, "tokens_total": 81, "tokens_per_s": 414.2591052630835, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554112.6867008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.027699998914613, "prefill_cuda_event_ms": 3.9550719261169434, "kv_decode_ms": 191.53860000005807, "kv_decode_cuda_event_ms": 191.4746856689453, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 4.027699998914613, "cuda_event_ms": 3.9550719261169434, "tokens_total": 17, "tokens_per_s": 4220.771160856362, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554112.6867008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.027699998914613, "prefill_cuda_event_ms": 3.9550719261169434, "kv_decode_ms": 191.53860000005807, "kv_decode_cuda_event_ms": 191.4746856689453, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 191.53860000005807, "cuda_event_ms": 191.4746856689453, "tokens_total": 64, "tokens_per_s": 334.13630464032104, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554112.6867008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.027699998914613, "prefill_cuda_event_ms": 3.9550719261169434, "kv_decode_ms": 191.53860000005807, "kv_decode_cuda_event_ms": 191.4746856689453, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 195.56629999897268, "cuda_event_ms": 195.42975759506226, "tokens_total": 81, "tokens_per_s": 414.181788991383, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554112.8832438, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9121000008890405, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 192.42840000151773, "kv_decode_cuda_event_ms": 192.35430908203125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 3.9121000008890405, "cuda_event_ms": 3.8377599716186523, "tokens_total": 17, "tokens_per_s": 4345.492189907385, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554112.8832438, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9121000008890405, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 192.42840000151773, "kv_decode_cuda_event_ms": 192.35430908203125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 192.42840000151773, "cuda_event_ms": 192.35430908203125, "tokens_total": 64, "tokens_per_s": 332.591239128399, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554112.8832438, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.9121000008890405, "prefill_cuda_event_ms": 3.8377599716186523, "kv_decode_ms": 192.42840000151773, "kv_decode_cuda_event_ms": 192.35430908203125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 196.34050000240677, "cuda_event_ms": 196.1920690536499, "tokens_total": 81, "tokens_per_s": 412.5486081527097, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554113.0804405, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.002599998784717, "prefill_cuda_event_ms": 3.9191999435424805, "kv_decode_ms": 188.63790000068548, "kv_decode_cuda_event_ms": 188.57061767578125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 4.002599998784717, "cuda_event_ms": 3.9191999435424805, "tokens_total": 17, "tokens_per_s": 4247.239295748163, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554113.0804405, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.002599998784717, "prefill_cuda_event_ms": 3.9191999435424805, "kv_decode_ms": 188.63790000068548, "kv_decode_cuda_event_ms": 188.57061767578125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 188.63790000068548, "cuda_event_ms": 188.57061767578125, "tokens_total": 64, "tokens_per_s": 339.27434518602803, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554113.0804405, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.002599998784717, "prefill_cuda_event_ms": 3.9191999435424805, "kv_decode_ms": 188.63790000068548, "kv_decode_cuda_event_ms": 188.57061767578125, "gpu_peak_mb": 633.5712890625, "params_millions_measured": 0.102714, "latency_ms": 192.6404999994702, "cuda_event_ms": 192.48981761932373, "tokens_total": 81, "tokens_per_s": 420.4723305858465, "gen_tokens": 64}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554113.274026, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.098199999541976, "prefill_cuda_event_ms": 6.0193281173706055, "kv_decode_ms": 197.80999999966298, "kv_decode_cuda_event_ms": 197.7364501953125, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 6.098199999541976, "cuda_event_ms": 6.0193281173706055, "tokens_total": 136, "tokens_per_s": 22301.66278741509, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554113.274026, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.098199999541976, "prefill_cuda_event_ms": 6.0193281173706055, "kv_decode_ms": 197.80999999966298, "kv_decode_cuda_event_ms": 197.7364501953125, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 197.80999999966298, "cuda_event_ms": 197.7364501953125, "tokens_total": 512, "tokens_per_s": 2588.342348722877, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554113.274026, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 6.098199999541976, "prefill_cuda_event_ms": 6.0193281173706055, "kv_decode_ms": 197.80999999966298, "kv_decode_cuda_event_ms": 197.7364501953125, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 203.90819999920495, "cuda_event_ms": 203.7557783126831, "tokens_total": 648, "tokens_per_s": 3177.900643537271, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554113.4798844, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.197000000203843, "prefill_cuda_event_ms": 4.128191947937012, "kv_decode_ms": 193.21400000080757, "kv_decode_cuda_event_ms": 193.1499481201172, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 4.197000000203843, "cuda_event_ms": 4.128191947937012, "tokens_total": 136, "tokens_per_s": 32404.09816378238, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554113.4798844, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.197000000203843, "prefill_cuda_event_ms": 4.128191947937012, "kv_decode_ms": 193.21400000080757, "kv_decode_cuda_event_ms": 193.1499481201172, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 193.21400000080757, "cuda_event_ms": 193.1499481201172, "tokens_total": 512, "tokens_per_s": 2649.911497085408, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554113.4798844, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.197000000203843, "prefill_cuda_event_ms": 4.128191947937012, "kv_decode_ms": 193.21400000080757, "kv_decode_cuda_event_ms": 193.1499481201172, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 197.41100000101142, "cuda_event_ms": 197.2781400680542, "tokens_total": 648, "tokens_per_s": 3282.491857073213, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554113.6799, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.129200000534183, "prefill_cuda_event_ms": 4.069536209106445, "kv_decode_ms": 197.18470000043453, "kv_decode_cuda_event_ms": 197.1189727783203, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 4.129200000534183, "cuda_event_ms": 4.069536209106445, "tokens_total": 136, "tokens_per_s": 32936.16196415917, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554113.6799, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.129200000534183, "prefill_cuda_event_ms": 4.069536209106445, "kv_decode_ms": 197.18470000043453, "kv_decode_cuda_event_ms": 197.1189727783203, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 197.18470000043453, "cuda_event_ms": 197.1189727783203, "tokens_total": 512, "tokens_per_s": 2596.550340867581, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554113.6799, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.129200000534183, "prefill_cuda_event_ms": 4.069536209106445, "kv_decode_ms": 197.18470000043453, "kv_decode_cuda_event_ms": 197.1189727783203, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 201.31390000096872, "cuda_event_ms": 201.18850898742676, "tokens_total": 648, "tokens_per_s": 3218.8537403372634, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554113.8835926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.172699998889584, "prefill_cuda_event_ms": 4.1102399826049805, "kv_decode_ms": 191.2358999998105, "kv_decode_cuda_event_ms": 191.172607421875, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 4.172699998889584, "cuda_event_ms": 4.1102399826049805, "tokens_total": 136, "tokens_per_s": 32592.80562613933, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554113.8835926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.172699998889584, "prefill_cuda_event_ms": 4.1102399826049805, "kv_decode_ms": 191.2358999998105, "kv_decode_cuda_event_ms": 191.172607421875, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 191.2358999998105, "cuda_event_ms": 191.172607421875, "tokens_total": 512, "tokens_per_s": 2677.321569854339, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554113.8835926, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.172699998889584, "prefill_cuda_event_ms": 4.1102399826049805, "kv_decode_ms": 191.2358999998105, "kv_decode_cuda_event_ms": 191.172607421875, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 195.40859999870008, "cuda_event_ms": 195.28284740447998, "tokens_total": 648, "tokens_per_s": 3316.1283587534567, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 76, "sample_idx": 228, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554114.0810378, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.979399998570443, "prefill_cuda_event_ms": 3.919327974319458, "kv_decode_ms": 188.7055999995937, "kv_decode_cuda_event_ms": 188.6341094970703, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 3.979399998570443, "cuda_event_ms": 3.919327974319458, "tokens_total": 136, "tokens_per_s": 34176.006445407984, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 76, "sample_idx": 229, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554114.0810378, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.979399998570443, "prefill_cuda_event_ms": 3.919327974319458, "kv_decode_ms": 188.7055999995937, "kv_decode_cuda_event_ms": 188.6341094970703, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 188.7055999995937, "cuda_event_ms": 188.6341094970703, "tokens_total": 512, "tokens_per_s": 2713.2210172941473, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 76, "sample_idx": 230, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554114.0810378, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 3.979399998570443, "prefill_cuda_event_ms": 3.919327974319458, "kv_decode_ms": 188.7055999995937, "kv_decode_cuda_event_ms": 188.6341094970703, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 192.68499999816413, "cuda_event_ms": 192.55343747138977, "tokens_total": 648, "tokens_per_s": 3363.0017905191066, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554114.2758183, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.049200000736164, "prefill_cuda_event_ms": 3.98688006401062, "kv_decode_ms": 189.84180000006745, "kv_decode_cuda_event_ms": 189.76051330566406, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 4.049200000736164, "cuda_event_ms": 3.98688006401062, "tokens_total": 136, "tokens_per_s": 33586.88135317459, "gen_tokens": 0}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554114.2758183, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.049200000736164, "prefill_cuda_event_ms": 3.98688006401062, "kv_decode_ms": 189.84180000006745, "kv_decode_cuda_event_ms": 189.76051330566406, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 189.84180000006745, "cuda_event_ms": 189.76051330566406, "tokens_total": 512, "tokens_per_s": 2696.9824348474262, "gen_tokens": 512}
{"task_idx": 12, "scenario": "medium", "backend": "hf_gpu_fp16_bs8", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 8, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554114.2758183, "batch_size": 8, "seq_len": 17, "prompt_tokens_raw": 136, "gen_tokens_raw": 512, "prefill_ms": 4.049200000736164, "prefill_cuda_event_ms": 3.98688006401062, "kv_decode_ms": 189.84180000006745, "kv_decode_cuda_event_ms": 189.76051330566406, "gpu_peak_mb": 646.6240234375, "params_millions_measured": 0.102714, "latency_ms": 193.89100000080361, "cuda_event_ms": 193.74739336967468, "tokens_total": 648, "tokens_per_s": 3342.083954372891, "gen_tokens": 512}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554114.4722452, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.889399999839952, "prefill_cuda_event_ms": 7.828000068664551, "kv_decode_ms": 311.5932000000612, "kv_decode_cuda_event_ms": 311.5212707519531, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 7.889399999839952, "cuda_event_ms": 7.828000068664551, "tokens_total": 17, "tokens_per_s": 2154.7899713976817, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554114.4722452, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.889399999839952, "prefill_cuda_event_ms": 7.828000068664551, "kv_decode_ms": 311.5932000000612, "kv_decode_cuda_event_ms": 311.5212707519531, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 311.5932000000612, "cuda_event_ms": 311.5212707519531, "tokens_total": 64, "tokens_per_s": 205.39600992572184, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554114.4722452, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.889399999839952, "prefill_cuda_event_ms": 7.828000068664551, "kv_decode_ms": 311.5932000000612, "kv_decode_cuda_event_ms": 311.5212707519531, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 319.48259999990114, "cuda_event_ms": 319.3492708206177, "tokens_total": 81, "tokens_per_s": 253.53493429696974, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554114.7927146, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.071100000553997, "prefill_cuda_event_ms": 6.016448020935059, "kv_decode_ms": 323.7448999989283, "kv_decode_cuda_event_ms": 323.7024841308594, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 6.071100000553997, "cuda_event_ms": 6.016448020935059, "tokens_total": 17, "tokens_per_s": 2800.1515373571056, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554114.7927146, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.071100000553997, "prefill_cuda_event_ms": 6.016448020935059, "kv_decode_ms": 323.7448999989283, "kv_decode_cuda_event_ms": 323.7024841308594, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 323.7448999989283, "cuda_event_ms": 323.7024841308594, "tokens_total": 64, "tokens_per_s": 197.68651181906452, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554114.7927146, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.071100000553997, "prefill_cuda_event_ms": 6.016448020935059, "kv_decode_ms": 323.7448999989283, "kv_decode_cuda_event_ms": 323.7024841308594, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 329.8159999994823, "cuda_event_ms": 329.71893215179443, "tokens_total": 81, "tokens_per_s": 245.59148131117698, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554115.1234083, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.995700001221849, "prefill_cuda_event_ms": 5.9389119148254395, "kv_decode_ms": 336.8460000001505, "kv_decode_cuda_event_ms": 336.8049011230469, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 5.995700001221849, "cuda_event_ms": 5.9389119148254395, "tokens_total": 17, "tokens_per_s": 2835.3653445862224, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554115.1234083, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.995700001221849, "prefill_cuda_event_ms": 5.9389119148254395, "kv_decode_ms": 336.8460000001505, "kv_decode_cuda_event_ms": 336.8049011230469, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 336.8460000001505, "cuda_event_ms": 336.8049011230469, "tokens_total": 64, "tokens_per_s": 189.99780315031617, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554115.1234083, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.995700001221849, "prefill_cuda_event_ms": 5.9389119148254395, "kv_decode_ms": 336.8460000001505, "kv_decode_cuda_event_ms": 336.8049011230469, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 342.8417000013724, "cuda_event_ms": 342.7438130378723, "tokens_total": 81, "tokens_per_s": 236.26064157211846, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554115.4671757, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.974700001388555, "prefill_cuda_event_ms": 5.919968128204346, "kv_decode_ms": 330.7525000000169, "kv_decode_cuda_event_ms": 330.70880126953125, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 5.974700001388555, "cuda_event_ms": 5.919968128204346, "tokens_total": 17, "tokens_per_s": 2845.3311456724355, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554115.4671757, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.974700001388555, "prefill_cuda_event_ms": 5.919968128204346, "kv_decode_ms": 330.7525000000169, "kv_decode_cuda_event_ms": 330.70880126953125, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 330.7525000000169, "cuda_event_ms": 330.70880126953125, "tokens_total": 64, "tokens_per_s": 193.49815949991833, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554115.4671757, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.974700001388555, "prefill_cuda_event_ms": 5.919968128204346, "kv_decode_ms": 330.7525000000169, "kv_decode_cuda_event_ms": 330.70880126953125, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 336.72720000140544, "cuda_event_ms": 336.6287693977356, "tokens_total": 81, "tokens_per_s": 240.55080789333894, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554115.804756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.18890000077954, "prefill_cuda_event_ms": 6.132448196411133, "kv_decode_ms": 334.1266000006726, "kv_decode_cuda_event_ms": 334.07977294921875, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 6.18890000077954, "cuda_event_ms": 6.132448196411133, "tokens_total": 17, "tokens_per_s": 2746.8532369013424, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554115.804756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.18890000077954, "prefill_cuda_event_ms": 6.132448196411133, "kv_decode_ms": 334.1266000006726, "kv_decode_cuda_event_ms": 334.07977294921875, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 334.1266000006726, "cuda_event_ms": 334.07977294921875, "tokens_total": 64, "tokens_per_s": 191.54416320003006, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 3, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554115.804756, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.18890000077954, "prefill_cuda_event_ms": 6.132448196411133, "kv_decode_ms": 334.1266000006726, "kv_decode_cuda_event_ms": 334.07977294921875, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 340.31550000145216, "cuda_event_ms": 340.2122211456299, "tokens_total": 81, "tokens_per_s": 238.01443072576583, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766554116.146118, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.910900001254049, "prefill_cuda_event_ms": 5.849664211273193, "kv_decode_ms": 332.840799999758, "kv_decode_cuda_event_ms": 332.7979431152344, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 5.910900001254049, "cuda_event_ms": 5.849664211273193, "tokens_total": 17, "tokens_per_s": 2876.0425648197906, "gen_tokens": 0}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766554116.146118, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.910900001254049, "prefill_cuda_event_ms": 5.849664211273193, "kv_decode_ms": 332.840799999758, "kv_decode_cuda_event_ms": 332.7979431152344, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 332.840799999758, "cuda_event_ms": 332.7979431152344, "tokens_total": 64, "tokens_per_s": 192.28411901439526, "gen_tokens": 64}
{"task_idx": 13, "scenario": "medium", "backend": "hf_gpu_fp16_bs1", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 4, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766554116.146118, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.910900001254049, "prefill_cuda_event_ms": 5.849664211273193, "kv_decode_ms": 332.840799999758, "kv_decode_cuda_event_ms": 332.7979431152344, "gpu_peak_mb": 634.76806640625, "params_millions_measured": 74.824704, "latency_ms": 338.75170000101207, "cuda_event_ms": 338.64760732650757, "tokens_total": 81, "tokens_per_s": 239.11319116555873, "gen_tokens": 64}
