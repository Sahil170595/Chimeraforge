{"scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766455819.6281397, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 18.909900000153357, "cuda_event_ms": null, "prefill_ms": 18.909900000153357, "prefill_cuda_event_ms": null, "kv_decode_ms": 336.80519999984426, "kv_decode_cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 52.88235262967494, "gpu_peak_mb": null}
{"scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766455819.9841065, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 304.9553000000742, "cuda_event_ms": null, "prefill_ms": 23.369700000102966, "prefill_cuda_event_ms": null, "kv_decode_ms": 304.9553000000742, "kv_decode_cuda_event_ms": null, "tokens_total": 16, "tokens_per_s": 52.46670577621083, "gpu_peak_mb": null}
{"scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766455820.3127327, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 340.42249999993146, "cuda_event_ms": null, "prefill_ms": 18.484599999965212, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.93789999996625, "kv_decode_cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 49.93794475983057, "gpu_peak_mb": null}
{"scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766455823.384879, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 7.43119999992814, "cuda_event_ms": 7.3758721351623535, "prefill_ms": 7.43119999992814, "prefill_cuda_event_ms": 7.3758721351623535, "kv_decode_ms": 112.10809999988669, "kv_decode_cuda_event_ms": 112.06655883789062, "tokens_total": 1, "tokens_per_s": 134.56776832943132, "gpu_peak_mb": 31.03662109375}
{"scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766455823.5049782, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 114.91890000002059, "cuda_event_ms": 114.8743667602539, "prefill_ms": 6.427499999972497, "prefill_cuda_event_ms": 6.385663986206055, "kv_decode_ms": 114.91890000002059, "kv_decode_cuda_event_ms": 114.8743667602539, "tokens_total": 16, "tokens_per_s": 139.22862122764084, "gpu_peak_mb": 31.03662109375}
{"scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766455823.6268666, "prompt_tokens": 1, "gen_tokens": 16, "latency_ms": 108.91850000007253, "cuda_event_ms": 108.82354879379272, "prefill_ms": 6.653800000094634, "prefill_cuda_event_ms": 6.594560146331787, "kv_decode_ms": 102.26469999997789, "kv_decode_cuda_event_ms": 102.22898864746094, "tokens_total": 17, "tokens_per_s": 156.08000477410798, "gpu_peak_mb": 31.03662109375}
{"scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766455825.0144064, "prompt_tokens": 10, "gen_tokens": 11, "latency_ms": 7.6924, "cuda_event_ms": null, "prefill_ms": 7.6924, "prefill_cuda_event_ms": null, "kv_decode_ms": 67.0479, "kv_decode_cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1299.9844001871977, "ollama_wall_ms": 192.7787999998145, "ollama_total_duration_ms": 170.6614, "ollama_load_ms": 84.251, "ollama_done_reason": "stop"}
{"scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766455825.2072346, "prompt_tokens": 10, "gen_tokens": 11, "latency_ms": 63.3251, "cuda_event_ms": null, "prefill_ms": 7.268, "prefill_cuda_event_ms": null, "kv_decode_ms": 63.3251, "kv_decode_cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 173.706792409329, "ollama_wall_ms": 224.91869999998926, "ollama_total_duration_ms": 200.9397, "ollama_load_ms": 120.1399, "ollama_done_reason": "stop"}
{"scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 270.0, "prompt": "Hello", "gen_tokens_target": 16, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766455825.4322217, "prompt_tokens": 10, "gen_tokens": 11, "latency_ms": 56.3889, "cuda_event_ms": null, "prefill_ms": 5.7641, "prefill_cuda_event_ms": null, "kv_decode_ms": 50.6248, "kv_decode_cuda_event_ms": null, "tokens_total": 21, "tokens_per_s": 372.4137197214345, "ollama_wall_ms": 198.33840000001146, "ollama_total_duration_ms": 179.5433, "ollama_load_ms": 110.8523, "ollama_done_reason": "stop"}
