{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 0, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553740.9401057, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 19.9786, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.9594, "kv_decode_ms_equiv": 724.9207172413793, "kv_decode_ms_per_token": 11.326886206896551, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 58, "gen_tokens_equiv": 64, "ollama_wall_ms": 8411.729299999934, "ollama_total_duration_ms": 8409.3339, "ollama_load_ms": 7691.5968, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 19.9786, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 951.0175888200375, "gen_tokens": 0}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 1, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553740.9401057, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 19.9786, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.9594, "kv_decode_ms_equiv": 724.9207172413793, "kv_decode_ms_per_token": 11.326886206896551, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 58, "gen_tokens_equiv": 64, "ollama_wall_ms": 8411.729299999934, "ollama_total_duration_ms": 8409.3339, "ollama_load_ms": 7691.5968, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 724.9207172413793, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.28551657834564, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 0, "sample_idx": 2, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553740.9401057, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 19.9786, "prefill_cuda_event_ms": null, "kv_decode_ms": 656.9594, "kv_decode_ms_equiv": 724.9207172413793, "kv_decode_ms_per_token": 11.326886206896551, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 58, "gen_tokens_equiv": 64, "ollama_wall_ms": 8411.729299999934, "ollama_total_duration_ms": 8409.3339, "ollama_load_ms": 7691.5968, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 744.8993172413793, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 111.42445439120256, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 3, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553749.3519733, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.517, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.3656, "kv_decode_ms_equiv": 717.8841824561404, "kv_decode_ms_per_token": 11.216940350877193, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 864.43700000018, "ollama_total_duration_ms": 838.7538, "ollama_load_ms": 147.5084, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.517, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1649.735174090475, "gen_tokens": 0}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 4, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553749.3519733, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.517, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.3656, "kv_decode_ms_equiv": 717.8841824561404, "kv_decode_ms_per_token": 11.216940350877193, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 864.43700000018, "ollama_total_duration_ms": 838.7538, "ollama_load_ms": 147.5084, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 717.8841824561404, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.15087080068118, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 1, "sample_idx": 5, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553749.3519733, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.517, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.3656, "kv_decode_ms_equiv": 717.8841824561404, "kv_decode_ms_per_token": 11.216940350877193, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 864.43700000018, "ollama_total_duration_ms": 838.7538, "ollama_load_ms": 147.5084, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 729.4011824561404, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.79197346583801, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 6, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553750.21654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.6671, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.6446, "kv_decode_ms_equiv": 719.3202526315789, "kv_decode_ms_per_token": 11.23937894736842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 860.8426000009786, "ollama_total_duration_ms": 834.443, "ollama_load_ms": 138.7494, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.6671, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1628.5109410221905, "gen_tokens": 0}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 7, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553750.21654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.6671, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.6446, "kv_decode_ms_equiv": 719.3202526315789, "kv_decode_ms_per_token": 11.23937894736842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 860.8426000009786, "ollama_total_duration_ms": 834.443, "ollama_load_ms": 138.7494, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 719.3202526315789, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.97288761975048, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 2, "sample_idx": 8, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553750.21654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.6671, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.6446, "kv_decode_ms_equiv": 719.3202526315789, "kv_decode_ms_per_token": 11.23937894736842, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 860.8426000009786, "ollama_total_duration_ms": 834.443, "ollama_load_ms": 138.7494, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 730.9873526315789, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.54505615069431, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 9, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553751.0777519, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.5936, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.1509, "kv_decode_ms_equiv": 717.6431157894737, "kv_decode_ms_per_token": 11.213173684210526, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 845.7298000012088, "ollama_total_duration_ms": 831.2309, "ollama_load_ms": 140.0565, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.5936, "cuda_event_ms": null, "tokens_total": 19, "tokens_per_s": 1638.8352194314102, "gen_tokens": 0}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 10, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553751.0777519, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.5936, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.1509, "kv_decode_ms_equiv": 717.6431157894737, "kv_decode_ms_per_token": 11.213173684210526, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 845.7298000012088, "ollama_total_duration_ms": 831.2309, "ollama_load_ms": 140.0565, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 717.6431157894737, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.18081786319944, "gen_tokens": 64}
{"task_idx": 0, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 3, "sample_idx": 11, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553751.0777519, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 19, "prefill_ms": 11.5936, "prefill_cuda_event_ms": null, "kv_decode_ms": 639.1509, "kv_decode_ms_equiv": 717.6431157894737, "kv_decode_ms_per_token": 11.213173684210526, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 57, "gen_tokens_equiv": 64, "ollama_wall_ms": 845.7298000012088, "ollama_total_duration_ms": 831.2309, "ollama_load_ms": 140.0565, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 729.2367157894737, "cuda_event_ms": null, "tokens_total": 83, "tokens_per_s": 113.81763726767922, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 12, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553751.92361, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 346.1416999998619, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8882999997877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 13233.942499999102, "params_millions_measured": 45.1712, "latency_ms": 346.1416999998619, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 26.000912343134594, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 13, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553751.92361, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 346.1416999998619, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8882999997877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 13233.942499999102, "params_millions_measured": 45.1712, "latency_ms": 1155.8882999997877, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 55.36867186908264, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 4, "sample_idx": 14, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553751.92361, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 346.1416999998619, "prefill_cuda_event_ms": null, "kv_decode_ms": 1155.8882999997877, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 13233.942499999102, "params_millions_measured": 45.1712, "latency_ms": 1502.0299999996496, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 48.60089345753216, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 15, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553766.6822293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.181599999588798, "prefill_cuda_event_ms": null, "kv_decode_ms": 966.7442999998457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 23.181599999588798, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 388.2389481381632, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 16, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553766.6822293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.181599999588798, "prefill_cuda_event_ms": null, "kv_decode_ms": 966.7442999998457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 966.7442999998457, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 66.20157988002642, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 5, "sample_idx": 17, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553766.6822293, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.181599999588798, "prefill_cuda_event_ms": null, "kv_decode_ms": 966.7442999998457, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 989.9258999994345, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.74289328124631, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 18, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553767.6728177, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.639200000208803, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.135000000635, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 21.639200000208803, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 415.91186365083536, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 19, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553767.6728177, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.639200000208803, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.135000000635, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1002.135000000635, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.86365110485059, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 6, "sample_idx": 20, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553767.6728177, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 21.639200000208803, "prefill_cuda_event_ms": null, "kv_decode_ms": 1002.135000000635, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1023.7742000008438, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 71.30478576227046, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 21, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553768.6970792, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.737599998625228, "prefill_cuda_event_ms": null, "kv_decode_ms": 867.2005999997054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 14.737599998625228, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 610.6828792231809, "gen_tokens": 0}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 22, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553768.6970792, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.737599998625228, "prefill_cuda_event_ms": null, "kv_decode_ms": 867.2005999997054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 867.2005999997054, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.80068694604425, "gen_tokens": 64}
{"task_idx": 1, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 7, "sample_idx": 23, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553768.6970792, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 14.737599998625228, "prefill_cuda_event_ms": null, "kv_decode_ms": 867.2005999997054, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 881.9381999983307, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 82.7722395969901, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 24, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553769.5795882, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 836.0825999989174, "prefill_cuda_event_ms": 834.3807983398438, "kv_decode_ms": 378.834299999653, "kv_decode_cuda_event_ms": 378.7356262207031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 405.0846999998612, "params_millions_measured": 0.102714, "latency_ms": 836.0825999989174, "cuda_event_ms": 834.3807983398438, "tokens_total": 17, "tokens_per_s": 20.332919259439212, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 25, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553769.5795882, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 836.0825999989174, "prefill_cuda_event_ms": 834.3807983398438, "kv_decode_ms": 378.834299999653, "kv_decode_cuda_event_ms": 378.7356262207031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 405.0846999998612, "params_millions_measured": 0.102714, "latency_ms": 378.834299999653, "cuda_event_ms": 378.7356262207031, "tokens_total": 64, "tokens_per_s": 168.9392961515328, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 8, "sample_idx": 26, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553769.5795882, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 836.0825999989174, "prefill_cuda_event_ms": 834.3807983398438, "kv_decode_ms": 378.834299999653, "kv_decode_cuda_event_ms": 378.7356262207031, "gpu_peak_mb": 13.068359375, "hf_load_ms": 405.0846999998612, "params_millions_measured": 0.102714, "latency_ms": 1214.9168999985704, "cuda_event_ms": 1213.1164245605469, "tokens_total": 81, "tokens_per_s": 66.6712266494073, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 27, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553771.3723474, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.2697999997471925, "prefill_cuda_event_ms": 4.194015979766846, "kv_decode_ms": 234.4039999989036, "kv_decode_cuda_event_ms": 234.30860900878906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 4.2697999997471925, "cuda_event_ms": 4.194015979766846, "tokens_total": 17, "tokens_per_s": 3981.4511220681393, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 28, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553771.3723474, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.2697999997471925, "prefill_cuda_event_ms": 4.194015979766846, "kv_decode_ms": 234.4039999989036, "kv_decode_cuda_event_ms": 234.30860900878906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 234.4039999989036, "cuda_event_ms": 234.30860900878906, "tokens_total": 64, "tokens_per_s": 273.03288339917134, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 9, "sample_idx": 29, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553771.3723474, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.2697999997471925, "prefill_cuda_event_ms": 4.194015979766846, "kv_decode_ms": 234.4039999989036, "kv_decode_cuda_event_ms": 234.30860900878906, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 238.6737999986508, "cuda_event_ms": 238.5026249885559, "tokens_total": 81, "tokens_per_s": 339.37533152133955, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 30, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553771.6120005, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": 4.22105598449707, "kv_decode_ms": 279.3609000000288, "kv_decode_cuda_event_ms": 279.2899169921875, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 4.299799998989329, "cuda_event_ms": 4.22105598449707, "tokens_total": 17, "tokens_per_s": 3953.672264755538, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 31, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553771.6120005, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": 4.22105598449707, "kv_decode_ms": 279.3609000000288, "kv_decode_cuda_event_ms": 279.2899169921875, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 279.3609000000288, "cuda_event_ms": 279.2899169921875, "tokens_total": 64, "tokens_per_s": 229.09433639422485, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 10, "sample_idx": 32, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553771.6120005, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": 4.22105598449707, "kv_decode_ms": 279.3609000000288, "kv_decode_cuda_event_ms": 279.2899169921875, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 283.6606999990181, "cuda_event_ms": 283.51097297668457, "tokens_total": 81, "tokens_per_s": 285.5524223139842, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 33, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553771.896762, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.5552000010502525, "prefill_cuda_event_ms": 4.472383975982666, "kv_decode_ms": 212.35300000080315, "kv_decode_cuda_event_ms": 212.28440856933594, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 4.5552000010502525, "cuda_event_ms": 4.472383975982666, "tokens_total": 17, "tokens_per_s": 3731.9985941518394, "gen_tokens": 0}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 34, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553771.896762, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.5552000010502525, "prefill_cuda_event_ms": 4.472383975982666, "kv_decode_ms": 212.35300000080315, "kv_decode_cuda_event_ms": 212.28440856933594, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 212.35300000080315, "cuda_event_ms": 212.28440856933594, "tokens_total": 64, "tokens_per_s": 301.38495806396867, "gen_tokens": 64}
{"task_idx": 2, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 11, "sample_idx": 35, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553771.896762, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.5552000010502525, "prefill_cuda_event_ms": 4.472383975982666, "kv_decode_ms": 212.35300000080315, "kv_decode_cuda_event_ms": 212.28440856933594, "gpu_peak_mb": 13.068359375, "params_millions_measured": 0.102714, "latency_ms": 216.9082000018534, "cuda_event_ms": 216.7567925453186, "tokens_total": 81, "tokens_per_s": 373.42986571880584, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 36, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553772.1147027, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 24.626700000226265, "prefill_cuda_event_ms": 24.551807403564453, "kv_decode_ms": 525.4856999999902, "kv_decode_cuda_event_ms": 525.4461669921875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 546.4169000006223, "params_millions_measured": 96.08832, "latency_ms": 24.626700000226265, "cuda_event_ms": 24.551807403564453, "tokens_total": 1, "tokens_per_s": 40.60633377556929, "gen_tokens": 0}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 37, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553772.1147027, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 24.626700000226265, "prefill_cuda_event_ms": 24.551807403564453, "kv_decode_ms": 525.4856999999902, "kv_decode_cuda_event_ms": 525.4461669921875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 546.4169000006223, "params_millions_measured": 96.08832, "latency_ms": 525.4856999999902, "cuda_event_ms": 525.4461669921875, "tokens_total": 64, "tokens_per_s": 121.79208682558098, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 12, "sample_idx": 38, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553772.1147027, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 24.626700000226265, "prefill_cuda_event_ms": 24.551807403564453, "kv_decode_ms": 525.4856999999902, "kv_decode_cuda_event_ms": 525.4461669921875, "gpu_peak_mb": 208.4775390625, "hf_load_ms": 546.4169000006223, "params_millions_measured": 96.08832, "latency_ms": 550.1124000002164, "cuda_event_ms": 549.997974395752, "tokens_total": 65, "tokens_per_s": 118.15767105045155, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 39, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553773.2124093, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.684200000061537, "prefill_cuda_event_ms": 8.610015869140625, "kv_decode_ms": 522.5546000001486, "kv_decode_cuda_event_ms": 522.51318359375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 8.684200000061537, "cuda_event_ms": 8.610015869140625, "tokens_total": 1, "tokens_per_s": 115.15165472846249, "gen_tokens": 0}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 40, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553773.2124093, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.684200000061537, "prefill_cuda_event_ms": 8.610015869140625, "kv_decode_ms": 522.5546000001486, "kv_decode_cuda_event_ms": 522.51318359375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 522.5546000001486, "cuda_event_ms": 522.51318359375, "tokens_total": 64, "tokens_per_s": 122.47523990790971, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 13, "sample_idx": 41, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553773.2124093, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.684200000061537, "prefill_cuda_event_ms": 8.610015869140625, "kv_decode_ms": 522.5546000001486, "kv_decode_cuda_event_ms": 522.51318359375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 531.2388000002102, "cuda_event_ms": 531.1231994628906, "tokens_total": 65, "tokens_per_s": 122.35552071869428, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 42, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553773.744463, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.78760000159673, "prefill_cuda_event_ms": 8.732319831848145, "kv_decode_ms": 511.89330000124755, "kv_decode_cuda_event_ms": 511.8504943847656, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 8.78760000159673, "cuda_event_ms": 8.732319831848145, "tokens_total": 1, "tokens_per_s": 113.7967135302355, "gen_tokens": 0}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 43, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553773.744463, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.78760000159673, "prefill_cuda_event_ms": 8.732319831848145, "kv_decode_ms": 511.89330000124755, "kv_decode_cuda_event_ms": 511.8504943847656, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 511.89330000124755, "cuda_event_ms": 511.8504943847656, "tokens_total": 64, "tokens_per_s": 125.02605523425296, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 14, "sample_idx": 44, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553773.744463, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.78760000159673, "prefill_cuda_event_ms": 8.732319831848145, "kv_decode_ms": 511.89330000124755, "kv_decode_cuda_event_ms": 511.8504943847656, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 520.6809000028443, "cuda_event_ms": 520.5828142166138, "tokens_total": 65, "tokens_per_s": 124.83653615802871, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 45, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553774.2663174, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.685000000696164, "prefill_cuda_event_ms": 8.625856399536133, "kv_decode_ms": 515.7779000001028, "kv_decode_cuda_event_ms": 515.7386474609375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 8.685000000696164, "cuda_event_ms": 8.625856399536133, "tokens_total": 1, "tokens_per_s": 115.14104777430546, "gen_tokens": 0}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 46, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553774.2663174, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.685000000696164, "prefill_cuda_event_ms": 8.625856399536133, "kv_decode_ms": 515.7779000001028, "kv_decode_cuda_event_ms": 515.7386474609375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 515.7779000001028, "cuda_event_ms": 515.7386474609375, "tokens_total": 64, "tokens_per_s": 124.08441695541286, "gen_tokens": 64}
{"task_idx": 3, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 15, "sample_idx": 47, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553774.2663174, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.685000000696164, "prefill_cuda_event_ms": 8.625856399536133, "kv_decode_ms": 515.7779000001028, "kv_decode_cuda_event_ms": 515.7386474609375, "gpu_peak_mb": 208.4775390625, "params_millions_measured": 96.08832, "latency_ms": 524.462900000799, "cuda_event_ms": 524.3645038604736, "tokens_total": 65, "tokens_per_s": 123.93631656290842, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 48, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553774.7917583, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 67.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.9362, "kv_decode_ms_equiv": 710.4798896551724, "kv_decode_ms_per_token": 11.10124827586207, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 5848.840900000141, "ollama_total_duration_ms": 5845.9538, "ollama_load_ms": 5430.0863, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 67.5811, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 443.91109348619653, "gen_tokens": 0}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 49, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553774.7917583, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 67.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.9362, "kv_decode_ms_equiv": 710.4798896551724, "kv_decode_ms_per_token": 11.10124827586207, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 5848.840900000141, "ollama_total_duration_ms": 5845.9538, "ollama_load_ms": 5430.0863, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 710.4798896551724, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.07995994237368, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 16, "sample_idx": 50, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553774.7917583, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 67.5811, "prefill_cuda_event_ms": null, "kv_decode_ms": 321.9362, "kv_decode_ms_equiv": 710.4798896551724, "kv_decode_ms_per_token": 11.10124827586207, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 5848.840900000141, "ollama_total_duration_ms": 5845.9538, "ollama_load_ms": 5430.0863, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 778.0609896551724, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 120.81315121795234, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 51, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553780.6407406, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.3945, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.4912, "kv_decode_ms_equiv": 705.0840275862068, "kv_decode_ms_per_token": 11.016937931034482, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 558.4312000009959, "ollama_total_duration_ms": 532.1161, "ollama_load_ms": 176.3442, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.3945, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2632.849181622713, "gen_tokens": 0}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 52, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553780.6407406, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.3945, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.4912, "kv_decode_ms_equiv": 705.0840275862068, "kv_decode_ms_per_token": 11.016937931034482, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 558.4312000009959, "ollama_total_duration_ms": 532.1161, "ollama_load_ms": 176.3442, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 705.0840275862068, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.76932322392605, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 17, "sample_idx": 53, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553780.6407406, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.3945, "prefill_cuda_event_ms": null, "kv_decode_ms": 319.4912, "kv_decode_ms_equiv": 705.0840275862068, "kv_decode_ms_per_token": 11.016937931034482, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 558.4312000009959, "ollama_total_duration_ms": 532.1161, "ollama_load_ms": 176.3442, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 716.4785275862068, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 131.19723254887063, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 54, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553781.209228, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 12.1587, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.1217, "kv_decode_ms_equiv": 699.8547862068965, "kv_decode_ms_per_token": 10.935231034482758, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 497.2414000003482, "ollama_total_duration_ms": 484.6476, "ollama_load_ms": 126.3874, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 12.1587, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2467.369044387969, "gen_tokens": 0}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 55, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553781.209228, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 12.1587, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.1217, "kv_decode_ms_equiv": 699.8547862068965, "kv_decode_ms_per_token": 10.935231034482758, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 497.2414000003482, "ollama_total_duration_ms": 484.6476, "ollama_load_ms": 126.3874, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 699.8547862068965, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 91.44754206350434, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 18, "sample_idx": 56, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553781.209228, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 12.1587, "prefill_cuda_event_ms": null, "kv_decode_ms": 317.1217, "kv_decode_ms_equiv": 699.8547862068965, "kv_decode_ms_per_token": 10.935231034482758, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 497.2414000003482, "ollama_total_duration_ms": 484.6476, "ollama_load_ms": 126.3874, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 712.0134862068965, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 132.01997127998996, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 57, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553781.7066092, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.5957, "kv_decode_ms_equiv": 703.107751724138, "kv_decode_ms_per_token": 10.986058620689656, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.9878000000899, "ollama_total_duration_ms": 484.5952, "ollama_load_ms": 129.975, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4804, "cuda_event_ms": null, "tokens_total": 30, "tokens_per_s": 2613.149367617853, "gen_tokens": 0}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 58, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553781.7066092, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.5957, "kv_decode_ms_equiv": 703.107751724138, "kv_decode_ms_per_token": 10.986058620689656, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.9878000000899, "ollama_total_duration_ms": 484.5952, "ollama_load_ms": 129.975, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 703.107751724138, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 91.02445513231973, "gen_tokens": 64}
{"task_idx": 4, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 19, "sample_idx": 59, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553781.7066092, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 30, "prefill_ms": 11.4804, "prefill_cuda_event_ms": null, "kv_decode_ms": 318.5957, "kv_decode_ms_equiv": 703.107751724138, "kv_decode_ms_per_token": 10.986058620689656, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 29, "gen_tokens_equiv": 64, "ollama_wall_ms": 509.9878000000899, "ollama_total_duration_ms": 484.5952, "ollama_load_ms": 129.975, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 714.588151724138, "cuda_event_ms": null, "tokens_total": 94, "tokens_per_s": 131.54430250935377, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 60, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553782.2167356, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.002799998925184, "prefill_cuda_event_ms": 23.909343719482422, "kv_decode_ms": 773.3934999996563, "kv_decode_cuda_event_ms": 773.338134765625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 286.20300000147836, "params_millions_measured": 5.03672, "latency_ms": 24.002799998925184, "cuda_event_ms": 23.909343719482422, "tokens_total": 9, "tokens_per_s": 374.95625512036133, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 61, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553782.2167356, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.002799998925184, "prefill_cuda_event_ms": 23.909343719482422, "kv_decode_ms": 773.3934999996563, "kv_decode_cuda_event_ms": 773.338134765625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 286.20300000147836, "params_millions_measured": 5.03672, "latency_ms": 773.3934999996563, "cuda_event_ms": 773.338134765625, "tokens_total": 64, "tokens_per_s": 82.75218242722293, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 20, "sample_idx": 62, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553782.2167356, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.002799998925184, "prefill_cuda_event_ms": 23.909343719482422, "kv_decode_ms": 773.3934999996563, "kv_decode_cuda_event_ms": 773.338134765625, "gpu_peak_mb": 229.62109375, "hf_load_ms": 286.20300000147836, "params_millions_measured": 5.03672, "latency_ms": 797.3962999985815, "cuda_event_ms": 797.2474784851074, "tokens_total": 73, "tokens_per_s": 91.54795426079838, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 63, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553783.3017633, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.849099999584723, "prefill_cuda_event_ms": 12.770560264587402, "kv_decode_ms": 753.7376999989647, "kv_decode_cuda_event_ms": 753.6803588867188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 12.849099999584723, "cuda_event_ms": 12.770560264587402, "tokens_total": 9, "tokens_per_s": 700.4381630068157, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 64, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553783.3017633, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.849099999584723, "prefill_cuda_event_ms": 12.770560264587402, "kv_decode_ms": 753.7376999989647, "kv_decode_cuda_event_ms": 753.6803588867188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 753.7376999989647, "cuda_event_ms": 753.6803588867188, "tokens_total": 64, "tokens_per_s": 84.91017498539334, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 21, "sample_idx": 65, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553783.3017633, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.849099999584723, "prefill_cuda_event_ms": 12.770560264587402, "kv_decode_ms": 753.7376999989647, "kv_decode_cuda_event_ms": 753.6803588867188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 766.5867999985494, "cuda_event_ms": 766.4509191513062, "tokens_total": 73, "tokens_per_s": 95.22731150619622, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 66, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553784.0692012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.866000000736676, "prefill_cuda_event_ms": 11.795999526977539, "kv_decode_ms": 698.8206999994873, "kv_decode_cuda_event_ms": 698.7653198242188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 11.866000000736676, "cuda_event_ms": 11.795999526977539, "tokens_total": 9, "tokens_per_s": 758.4695768954368, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 67, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553784.0692012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.866000000736676, "prefill_cuda_event_ms": 11.795999526977539, "kv_decode_ms": 698.8206999994873, "kv_decode_cuda_event_ms": 698.7653198242188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 698.8206999994873, "cuda_event_ms": 698.7653198242188, "tokens_total": 64, "tokens_per_s": 91.58286238522551, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 22, "sample_idx": 68, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553784.0692012, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.866000000736676, "prefill_cuda_event_ms": 11.795999526977539, "kv_decode_ms": 698.8206999994873, "kv_decode_cuda_event_ms": 698.7653198242188, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 710.6867000002239, "cuda_event_ms": 710.5613193511963, "tokens_total": 73, "tokens_per_s": 102.7175547255591, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 69, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553784.7807918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.775099999795202, "prefill_cuda_event_ms": 11.70803165435791, "kv_decode_ms": 736.7591999991419, "kv_decode_cuda_event_ms": 736.7024536132812, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 11.775099999795202, "cuda_event_ms": 11.70803165435791, "tokens_total": 9, "tokens_per_s": 764.3247191239592, "gen_tokens": 0}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 70, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553784.7807918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.775099999795202, "prefill_cuda_event_ms": 11.70803165435791, "kv_decode_ms": 736.7591999991419, "kv_decode_cuda_event_ms": 736.7024536132812, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 736.7591999991419, "cuda_event_ms": 736.7024536132812, "tokens_total": 64, "tokens_per_s": 86.86691662632043, "gen_tokens": 64}
{"task_idx": 5, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 23, "sample_idx": 71, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553784.7807918, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 11.775099999795202, "prefill_cuda_event_ms": 11.70803165435791, "kv_decode_ms": 736.7591999991419, "kv_decode_cuda_event_ms": 736.7024536132812, "gpu_peak_mb": 229.62109375, "params_millions_measured": 5.03672, "latency_ms": 748.5342999989371, "cuda_event_ms": 748.4104852676392, "tokens_total": 73, "tokens_per_s": 97.52392108164403, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 72, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553785.530225, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.3759, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.5594, "kv_decode_ms_equiv": 127.5594, "kv_decode_ms_per_token": 1.993115625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1943.6218999999255, "ollama_total_duration_ms": 1863.881, "ollama_load_ms": 1669.7131, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 8.3759, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 2984.7538771952863, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 73, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553785.530225, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.3759, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.5594, "kv_decode_ms_equiv": 127.5594, "kv_decode_ms_per_token": 1.993115625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1943.6218999999255, "ollama_total_duration_ms": 1863.881, "ollama_load_ms": 1669.7131, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 127.5594, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 501.7270385404761, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 24, "sample_idx": 74, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553785.530225, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 8.3759, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.5594, "kv_decode_ms_equiv": 127.5594, "kv_decode_ms_per_token": 1.993115625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 1943.6218999999255, "ollama_total_duration_ms": 1863.881, "ollama_load_ms": 1669.7131, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 135.93529999999998, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 654.7232396588672, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 75, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553787.473991, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.26, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.7069, "kv_decode_ms_equiv": 120.7069, "kv_decode_ms_per_token": 1.8860453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7753000000521, "ollama_total_duration_ms": 320.9448, "ollama_load_ms": 155.2111, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.26, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11061.946902654869, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 76, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553787.473991, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.26, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.7069, "kv_decode_ms_equiv": 120.7069, "kv_decode_ms_per_token": 1.8860453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7753000000521, "ollama_total_duration_ms": 320.9448, "ollama_load_ms": 155.2111, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 120.7069, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 530.2099548575931, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 25, "sample_idx": 77, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553787.473991, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.26, "prefill_cuda_event_ms": null, "kv_decode_ms": 120.7069, "kv_decode_ms_equiv": 120.7069, "kv_decode_ms_per_token": 1.8860453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 353.7753000000521, "ollama_total_duration_ms": 320.9448, "ollama_load_ms": 155.2111, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 122.96690000000001, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 723.7720069384525, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 78, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553787.8278837, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3203, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.1311, "kv_decode_ms_equiv": 127.1311, "kv_decode_ms_per_token": 1.9864234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 366.06969999957073, "ollama_total_duration_ms": 335.5269, "ollama_load_ms": 169.4003, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.3203, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 10774.468818687239, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 79, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553787.8278837, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3203, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.1311, "kv_decode_ms_equiv": 127.1311, "kv_decode_ms_per_token": 1.9864234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 366.06969999957073, "ollama_total_duration_ms": 335.5269, "ollama_load_ms": 169.4003, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 127.1311, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 503.4173384797268, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 26, "sample_idx": 80, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553787.8278837, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.3203, "prefill_cuda_event_ms": null, "kv_decode_ms": 127.1311, "kv_decode_ms_equiv": 127.1311, "kv_decode_ms_per_token": 1.9864234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 366.06969999957073, "ollama_total_duration_ms": 335.5269, "ollama_load_ms": 169.4003, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 129.4514, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 687.5167051109529, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 81, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553788.194127, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2589, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.8711, "kv_decode_ms_equiv": 119.8711, "kv_decode_ms_per_token": 1.8729859375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.58170000003884, "ollama_total_duration_ms": 297.0388, "ollama_load_ms": 139.1896, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 2.2589, "cuda_event_ms": null, "tokens_total": 25, "tokens_per_s": 11067.33365797512, "gen_tokens": 0}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 82, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553788.194127, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2589, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.8711, "kv_decode_ms_equiv": 119.8711, "kv_decode_ms_per_token": 1.8729859375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.58170000003884, "ollama_total_duration_ms": 297.0388, "ollama_load_ms": 139.1896, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 119.8711, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 533.9068382620999, "gen_tokens": 64}
{"task_idx": 6, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 27, "sample_idx": 83, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553788.194127, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 25, "prefill_ms": 2.2589, "prefill_cuda_event_ms": null, "kv_decode_ms": 119.8711, "kv_decode_ms_equiv": 119.8711, "kv_decode_ms_per_token": 1.8729859375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 330.58170000003884, "ollama_total_duration_ms": 297.0388, "ollama_load_ms": 139.1896, "ollama_done_reason": "length", "params_millions_measured": 268.1, "latency_ms": 122.13, "cuda_event_ms": null, "tokens_total": 89, "tokens_per_s": 728.7316793580611, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 84, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553788.5248356, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 18.2711, "prefill_cuda_event_ms": null, "kv_decode_ms": 725.5651, "kv_decode_ms_equiv": 725.5651, "kv_decode_ms_per_token": 11.3369546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 952.6508999988437, "ollama_total_duration_ms": 937.7844, "ollama_load_ms": 151.2852, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 18.2711, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 1477.743540345135, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 85, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553788.5248356, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 18.2711, "prefill_cuda_event_ms": null, "kv_decode_ms": 725.5651, "kv_decode_ms_equiv": 725.5651, "kv_decode_ms_per_token": 11.3369546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 952.6508999988437, "ollama_total_duration_ms": 937.7844, "ollama_load_ms": 151.2852, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 725.5651, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.20710918978875, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 28, "sample_idx": 86, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553788.5248356, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 18.2711, "prefill_cuda_event_ms": null, "kv_decode_ms": 725.5651, "kv_decode_ms_equiv": 725.5651, "kv_decode_ms_per_token": 11.3369546875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 952.6508999988437, "ollama_total_duration_ms": 937.7844, "ollama_load_ms": 151.2852, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 743.8362000000001, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.33876221673535, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 87, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553789.4776654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.9582, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.0073, "kv_decode_ms_equiv": 726.0073, "kv_decode_ms_per_token": 11.3438640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.2273000008281, "ollama_total_duration_ms": 926.5971, "ollama_load_ms": 149.9531, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.9582, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2257.86489605459, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 88, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553789.4776654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.9582, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.0073, "kv_decode_ms_equiv": 726.0073, "kv_decode_ms_per_token": 11.3438640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.2273000008281, "ollama_total_duration_ms": 926.5971, "ollama_load_ms": 149.9531, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 726.0073, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.15338358168023, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 29, "sample_idx": 89, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553789.4776654, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.9582, "prefill_cuda_event_ms": null, "kv_decode_ms": 726.0073, "kv_decode_ms_equiv": 726.0073, "kv_decode_ms_per_token": 11.3438640625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 946.2273000008281, "ollama_total_duration_ms": 926.5971, "ollama_load_ms": 149.9531, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 737.9655, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 123.3119976475865, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 90, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553790.4240139, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.486, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.3606, "kv_decode_ms_equiv": 722.3606, "kv_decode_ms_per_token": 11.286884375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.9742999994487, "ollama_total_duration_ms": 937.8324, "ollama_load_ms": 158.1898, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.486, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2350.687793835974, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 91, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553790.4240139, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.486, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.3606, "kv_decode_ms_equiv": 722.3606, "kv_decode_ms_per_token": 11.286884375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.9742999994487, "ollama_total_duration_ms": 937.8324, "ollama_load_ms": 158.1898, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 722.3606, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.59840916018952, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 30, "sample_idx": 92, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553790.4240139, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.486, "prefill_cuda_event_ms": null, "kv_decode_ms": 722.3606, "kv_decode_ms_equiv": 722.3606, "kv_decode_ms_per_token": 11.286884375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 939.9742999994487, "ollama_total_duration_ms": 937.8324, "ollama_load_ms": 158.1898, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 733.8466, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 124.00411748177343, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 93, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553791.36412, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.3086, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.6278, "kv_decode_ms_equiv": 728.6278, "kv_decode_ms_per_token": 11.384809375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.3820999994496, "ollama_total_duration_ms": 938.0739, "ollama_load_ms": 160.4111, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 11.3086, "cuda_event_ms": null, "tokens_total": 27, "tokens_per_s": 2387.5634472879046, "gen_tokens": 0}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 94, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553791.36412, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.3086, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.6278, "kv_decode_ms_equiv": 728.6278, "kv_decode_ms_per_token": 11.384809375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.3820999994496, "ollama_total_duration_ms": 938.0739, "ollama_load_ms": 160.4111, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 728.6278, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 87.83634113329192, "gen_tokens": 64}
{"task_idx": 7, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 31, "sample_idx": 95, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553791.36412, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 27, "prefill_ms": 11.3086, "prefill_cuda_event_ms": null, "kv_decode_ms": 728.6278, "kv_decode_ms_equiv": 728.6278, "kv_decode_ms_per_token": 11.384809375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 957.3820999994496, "ollama_total_duration_ms": 938.0739, "ollama_load_ms": 160.4111, "ollama_done_reason": "length", "params_millions_measured": 8000.0, "latency_ms": 739.9363999999999, "cuda_event_ms": null, "tokens_total": 91, "tokens_per_s": 122.98354290990416, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 96, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553792.3216896, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 6.4813, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.6802, "kv_decode_ms_equiv": 123.98737297297298, "kv_decode_ms_per_token": 1.9373027027027028, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 276.52429999943706, "ollama_total_duration_ms": 246.9166, "ollama_load_ms": 143.7376, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 6.4813, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 2622.9305849135208, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 97, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553792.3216896, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 6.4813, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.6802, "kv_decode_ms_equiv": 123.98737297297298, "kv_decode_ms_per_token": 1.9373027027027028, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 276.52429999943706, "ollama_total_duration_ms": 246.9166, "ollama_load_ms": 143.7376, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.98737297297298, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 516.1815954754591, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 32, "sample_idx": 98, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553792.3216896, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 6.4813, "prefill_cuda_event_ms": null, "kv_decode_ms": 71.6802, "kv_decode_ms_equiv": 123.98737297297298, "kv_decode_ms_per_token": 1.9373027027027028, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 37, "gen_tokens_equiv": 64, "ollama_wall_ms": 276.52429999943706, "ollama_total_duration_ms": 246.9166, "ollama_load_ms": 143.7376, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 130.46867297297297, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 620.8386898882571, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 99, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553792.5983596, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.7874, "prefill_cuda_event_ms": null, "kv_decode_ms": 97.9705, "kv_decode_ms_equiv": 130.62733333333333, "kv_decode_ms_per_token": 2.041052083333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.4444999999978, "ollama_total_duration_ms": 267.4712, "ollama_load_ms": 142.4624, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.7874, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 6098.8735021884195, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 100, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553792.5983596, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.7874, "prefill_cuda_event_ms": null, "kv_decode_ms": 97.9705, "kv_decode_ms_equiv": 130.62733333333333, "kv_decode_ms_per_token": 2.041052083333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.4444999999978, "ollama_total_duration_ms": 267.4712, "ollama_load_ms": 142.4624, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 130.62733333333333, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 489.94340132999224, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 33, "sample_idx": 101, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553792.5983596, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.7874, "prefill_cuda_event_ms": null, "kv_decode_ms": 97.9705, "kv_decode_ms_equiv": 130.62733333333333, "kv_decode_ms_per_token": 2.041052083333333, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 306.4444999999978, "ollama_total_duration_ms": 267.4712, "ollama_load_ms": 142.4624, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 133.41473333333332, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 607.129347533331, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 102, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553792.904947, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.3168, "kv_decode_ms_equiv": 123.08906666666667, "kv_decode_ms_per_token": 1.9232666666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 298.07720000098925, "ollama_total_duration_ms": 263.6414, "ollama_load_ms": 138.2477, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.2954, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7406.116580988063, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 103, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553792.904947, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.3168, "kv_decode_ms_equiv": 123.08906666666667, "kv_decode_ms_per_token": 1.9232666666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 298.07720000098925, "ollama_total_duration_ms": 263.6414, "ollama_load_ms": 138.2477, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.08906666666667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 519.9486983950917, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 34, "sample_idx": 104, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553792.904947, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.2954, "prefill_cuda_event_ms": null, "kv_decode_ms": 92.3168, "kv_decode_ms_equiv": 123.08906666666667, "kv_decode_ms_per_token": 1.9232666666666667, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 298.07720000098925, "ollama_total_duration_ms": 263.6414, "ollama_load_ms": 138.2477, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 125.38446666666667, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 646.0130361708814, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 105, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553793.2032073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1292, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.4096, "kv_decode_ms_equiv": 117.87946666666666, "kv_decode_ms_per_token": 1.8418666666666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 319.263299999875, "ollama_total_duration_ms": 282.388, "ollama_load_ms": 161.2447, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1292, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 7984.219425136202, "gen_tokens": 0}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 106, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553793.2032073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1292, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.4096, "kv_decode_ms_equiv": 117.87946666666666, "kv_decode_ms_per_token": 1.8418666666666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 319.263299999875, "ollama_total_duration_ms": 282.388, "ollama_load_ms": 161.2447, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.87946666666666, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 542.9274648906907, "gen_tokens": 64}
{"task_idx": 8, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 35, "sample_idx": 107, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553793.2032073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 17, "prefill_ms": 2.1292, "prefill_cuda_event_ms": null, "kv_decode_ms": 88.4096, "kv_decode_ms_equiv": 117.87946666666666, "kv_decode_ms_per_token": 1.8418666666666665, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 48, "gen_tokens_equiv": 64, "ollama_wall_ms": 319.263299999875, "ollama_total_duration_ms": 282.388, "ollama_load_ms": 161.2447, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 120.00866666666666, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 674.9512535205791, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 108, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553793.5225625, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 33.315299999230774, "prefill_cuda_event_ms": null, "kv_decode_ms": 1277.4377999994613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.45689999964088, "params_millions_measured": 5.03672, "latency_ms": 33.315299999230774, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 270.1461490728825, "gen_tokens": 0}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 109, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553793.5225625, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 33.315299999230774, "prefill_cuda_event_ms": null, "kv_decode_ms": 1277.4377999994613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.45689999964088, "params_millions_measured": 5.03672, "latency_ms": 1277.4377999994613, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 50.100286683255334, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 36, "sample_idx": 110, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553793.5225625, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 33.315299999230774, "prefill_cuda_event_ms": null, "kv_decode_ms": 1277.4377999994613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 189.45689999964088, "params_millions_measured": 5.03672, "latency_ms": 1310.753099998692, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 55.69317364198707, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 111, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553795.023837, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.02010000050359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.0724000006157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 28.02010000050359, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 321.1979971462717, "gen_tokens": 0}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 112, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553795.023837, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.02010000050359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.0724000006157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1172.0724000006157, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.60413537590884, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 37, "sample_idx": 113, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553795.023837, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.02010000050359, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.0724000006157, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1200.0925000011193, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 60.82864445859958, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 114, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553796.2244763, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.381799998896895, "prefill_cuda_event_ms": null, "kv_decode_ms": 1136.61460000003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 28.381799998896895, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 317.10462339773375, "gen_tokens": 0}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 115, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553796.2244763, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.381799998896895, "prefill_cuda_event_ms": null, "kv_decode_ms": 1136.61460000003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1136.61460000003, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.307564586974614, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 38, "sample_idx": 116, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553796.2244763, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 28.381799998896895, "prefill_cuda_event_ms": null, "kv_decode_ms": 1136.61460000003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1164.9963999989268, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 62.661137837050184, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 117, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553797.3899493, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 27.532199999768636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1139.059900000575, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 27.532199999768636, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 326.88996883923664, "gen_tokens": 0}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 118, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553797.3899493, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 27.532199999768636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1139.059900000575, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1139.059900000575, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.186685177809956, "gen_tokens": 64}
{"task_idx": 9, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 39, "sample_idx": 119, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553797.3899493, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 27.532199999768636, "prefill_cuda_event_ms": null, "kv_decode_ms": 1139.059900000575, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1166.5921000003436, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 62.575428035196275, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 120, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553798.5571096, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.770300000833231, "prefill_cuda_event_ms": 8.676480293273926, "kv_decode_ms": 336.6072999997414, "kv_decode_cuda_event_ms": 336.53863525390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 626.7999000010605, "params_millions_measured": 74.824704, "latency_ms": 8.770300000833231, "cuda_event_ms": 8.676480293273926, "tokens_total": 1, "tokens_per_s": 114.0211851253656, "gen_tokens": 0}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 121, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553798.5571096, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.770300000833231, "prefill_cuda_event_ms": 8.676480293273926, "kv_decode_ms": 336.6072999997414, "kv_decode_cuda_event_ms": 336.53863525390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 626.7999000010605, "params_millions_measured": 74.824704, "latency_ms": 336.6072999997414, "cuda_event_ms": 336.53863525390625, "tokens_total": 64, "tokens_per_s": 190.13253723270165, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 40, "sample_idx": 122, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553798.5571096, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.770300000833231, "prefill_cuda_event_ms": 8.676480293273926, "kv_decode_ms": 336.6072999997414, "kv_decode_cuda_event_ms": 336.53863525390625, "gpu_peak_mb": 379.8994140625, "hf_load_ms": 626.7999000010605, "params_millions_measured": 74.824704, "latency_ms": 345.3776000005746, "cuda_event_ms": 345.2151155471802, "tokens_total": 65, "tokens_per_s": 188.19981376873272, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 123, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553799.5311606, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.860400000528898, "prefill_cuda_event_ms": 5.805056095123291, "kv_decode_ms": 328.1150000002526, "kv_decode_cuda_event_ms": 328.0742492675781, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.860400000528898, "cuda_event_ms": 5.805056095123291, "tokens_total": 1, "tokens_per_s": 170.63681658414964, "gen_tokens": 0}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 124, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553799.5311606, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.860400000528898, "prefill_cuda_event_ms": 5.805056095123291, "kv_decode_ms": 328.1150000002526, "kv_decode_cuda_event_ms": 328.0742492675781, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 328.1150000002526, "cuda_event_ms": 328.0742492675781, "tokens_total": 64, "tokens_per_s": 195.0535635370243, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 41, "sample_idx": 125, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553799.5311606, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.860400000528898, "prefill_cuda_event_ms": 5.805056095123291, "kv_decode_ms": 328.1150000002526, "kv_decode_cuda_event_ms": 328.0742492675781, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 333.9754000007815, "cuda_event_ms": 333.8793053627014, "tokens_total": 65, "tokens_per_s": 194.62511310667762, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 126, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553799.8659542, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.82120000035502, "prefill_cuda_event_ms": 5.76361608505249, "kv_decode_ms": 337.55480000036187, "kv_decode_cuda_event_ms": 337.5093688964844, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.82120000035502, "cuda_event_ms": 5.76361608505249, "tokens_total": 1, "tokens_per_s": 171.78588606112356, "gen_tokens": 0}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 127, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553799.8659542, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.82120000035502, "prefill_cuda_event_ms": 5.76361608505249, "kv_decode_ms": 337.55480000036187, "kv_decode_cuda_event_ms": 337.5093688964844, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 337.55480000036187, "cuda_event_ms": 337.5093688964844, "tokens_total": 64, "tokens_per_s": 189.59884439484017, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 42, "sample_idx": 128, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553799.8659542, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.82120000035502, "prefill_cuda_event_ms": 5.76361608505249, "kv_decode_ms": 337.55480000036187, "kv_decode_cuda_event_ms": 337.5093688964844, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 343.3760000007169, "cuda_event_ms": 343.27298498153687, "tokens_total": 65, "tokens_per_s": 189.29686407863187, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 129, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553800.2101622, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.73669999903359, "prefill_cuda_event_ms": 5.6809282302856445, "kv_decode_ms": 328.64619999963907, "kv_decode_cuda_event_ms": 328.6077575683594, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 5.73669999903359, "cuda_event_ms": 5.6809282302856445, "tokens_total": 1, "tokens_per_s": 174.31624456019333, "gen_tokens": 0}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 130, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553800.2101622, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.73669999903359, "prefill_cuda_event_ms": 5.6809282302856445, "kv_decode_ms": 328.64619999963907, "kv_decode_cuda_event_ms": 328.6077575683594, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 328.64619999963907, "cuda_event_ms": 328.6077575683594, "tokens_total": 64, "tokens_per_s": 194.73829303387743, "gen_tokens": 64}
{"task_idx": 10, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 43, "sample_idx": 131, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553800.2101622, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 5.73669999903359, "prefill_cuda_event_ms": 5.6809282302856445, "kv_decode_ms": 328.64619999963907, "kv_decode_cuda_event_ms": 328.6077575683594, "gpu_peak_mb": 379.8994140625, "params_millions_measured": 74.824704, "latency_ms": 334.38289999867266, "cuda_event_ms": 334.288685798645, "tokens_total": 65, "tokens_per_s": 194.38793072330557, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 132, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553800.54548, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 51.710499999899184, "prefill_cuda_event_ms": 51.6044807434082, "kv_decode_ms": 531.4526000001933, "kv_decode_cuda_event_ms": 531.4017333984375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 454.6933000001445, "params_millions_measured": 51.475968, "latency_ms": 51.710499999899184, "cuda_event_ms": 51.6044807434082, "tokens_total": 17, "tokens_per_s": 328.7533479667213, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 133, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553800.54548, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 51.710499999899184, "prefill_cuda_event_ms": 51.6044807434082, "kv_decode_ms": 531.4526000001933, "kv_decode_cuda_event_ms": 531.4017333984375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 454.6933000001445, "params_millions_measured": 51.475968, "latency_ms": 531.4526000001933, "cuda_event_ms": 531.4017333984375, "tokens_total": 64, "tokens_per_s": 120.42466251924766, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 44, "sample_idx": 134, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553800.54548, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 51.710499999899184, "prefill_cuda_event_ms": 51.6044807434082, "kv_decode_ms": 531.4526000001933, "kv_decode_cuda_event_ms": 531.4017333984375, "gpu_peak_mb": 489.4736328125, "hf_load_ms": 454.6933000001445, "params_millions_measured": 51.475968, "latency_ms": 583.1631000000925, "cuda_event_ms": 583.0062141418457, "tokens_total": 81, "tokens_per_s": 138.89767716782345, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 135, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553801.5850503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.221399999660207, "prefill_cuda_event_ms": 9.161055564880371, "kv_decode_ms": 535.3402999990067, "kv_decode_cuda_event_ms": 535.2714233398438, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.221399999660207, "cuda_event_ms": 9.161055564880371, "tokens_total": 17, "tokens_per_s": 1843.537857660054, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 136, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553801.5850503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.221399999660207, "prefill_cuda_event_ms": 9.161055564880371, "kv_decode_ms": 535.3402999990067, "kv_decode_cuda_event_ms": 535.2714233398438, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 535.3402999990067, "cuda_event_ms": 535.2714233398438, "tokens_total": 64, "tokens_per_s": 119.55012540643541, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 45, "sample_idx": 137, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553801.5850503, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.221399999660207, "prefill_cuda_event_ms": 9.161055564880371, "kv_decode_ms": 535.3402999990067, "kv_decode_cuda_event_ms": 535.2714233398438, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 544.5616999986669, "cuda_event_ms": 544.4324789047241, "tokens_total": 81, "tokens_per_s": 148.74347571670629, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 138, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553802.1305811, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.356599999591708, "prefill_cuda_event_ms": 9.287232398986816, "kv_decode_ms": 541.8417999990197, "kv_decode_cuda_event_ms": 541.8045654296875, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.356599999591708, "cuda_event_ms": 9.287232398986816, "tokens_total": 17, "tokens_per_s": 1816.899301107435, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 139, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553802.1305811, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.356599999591708, "prefill_cuda_event_ms": 9.287232398986816, "kv_decode_ms": 541.8417999990197, "kv_decode_cuda_event_ms": 541.8045654296875, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 541.8417999990197, "cuda_event_ms": 541.8045654296875, "tokens_total": 64, "tokens_per_s": 118.11565663652341, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 46, "sample_idx": 140, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553802.1305811, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.356599999591708, "prefill_cuda_event_ms": 9.287232398986816, "kv_decode_ms": 541.8417999990197, "kv_decode_cuda_event_ms": 541.8045654296875, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 551.1983999986114, "cuda_event_ms": 551.0917978286743, "tokens_total": 81, "tokens_per_s": 146.95253106722382, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 141, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553802.6828783, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.278900000936119, "prefill_cuda_event_ms": 9.217920303344727, "kv_decode_ms": 533.5221999994246, "kv_decode_cuda_event_ms": 533.47021484375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 9.278900000936119, "cuda_event_ms": 9.217920303344727, "tokens_total": 17, "tokens_per_s": 1832.1137201915014, "gen_tokens": 0}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 142, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553802.6828783, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.278900000936119, "prefill_cuda_event_ms": 9.217920303344727, "kv_decode_ms": 533.5221999994246, "kv_decode_cuda_event_ms": 533.47021484375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 533.5221999994246, "cuda_event_ms": 533.47021484375, "tokens_total": 64, "tokens_per_s": 119.95752004334406, "gen_tokens": 64}
{"task_idx": 11, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 47, "sample_idx": 143, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553802.6828783, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.278900000936119, "prefill_cuda_event_ms": 9.217920303344727, "kv_decode_ms": 533.5221999994246, "kv_decode_cuda_event_ms": 533.47021484375, "gpu_peak_mb": 489.4736328125, "params_millions_measured": 51.475968, "latency_ms": 542.8011000003607, "cuda_event_ms": 542.6881351470947, "tokens_total": 81, "tokens_per_s": 149.22593192966295, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 144, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553803.226589, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.353299999726005, "prefill_cuda_event_ms": null, "kv_decode_ms": 170.87749999882362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 163.58690000015486, "params_millions_measured": 0.102714, "latency_ms": 8.353299999726005, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 1077.4185052967339, "gen_tokens": 0}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 145, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553803.226589, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.353299999726005, "prefill_cuda_event_ms": null, "kv_decode_ms": 170.87749999882362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 163.58690000015486, "params_millions_measured": 0.102714, "latency_ms": 170.87749999882362, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 374.5373147455961, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 48, "sample_idx": 146, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553803.226589, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.353299999726005, "prefill_cuda_event_ms": null, "kv_decode_ms": 170.87749999882362, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 163.58690000015486, "params_millions_measured": 0.102714, "latency_ms": 179.23079999854963, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 407.29606742028005, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 147, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553803.570054, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.41820000100415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.299799998989329, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2093.120610752932, "gen_tokens": 0}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 148, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553803.570054, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.41820000100415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 160.41820000100415, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 398.9572255492169, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 49, "sample_idx": 149, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553803.570054, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.299799998989329, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.41820000100415, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 164.71799999999348, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 443.1816802049739, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 150, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553803.7352576, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.123700000491226, "prefill_cuda_event_ms": null, "kv_decode_ms": 155.59810000013385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.123700000491226, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2182.506001631519, "gen_tokens": 0}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 151, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553803.7352576, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.123700000491226, "prefill_cuda_event_ms": null, "kv_decode_ms": 155.59810000013385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 155.59810000013385, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 411.31607648129983, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 50, "sample_idx": 152, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553803.7352576, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.123700000491226, "prefill_cuda_event_ms": null, "kv_decode_ms": 155.59810000013385, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 159.72180000062508, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 457.04468644677377, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 153, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553803.8954604, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.7063999989186414, "prefill_cuda_event_ms": null, "kv_decode_ms": 153.10540000064066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.7063999989186414, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 2428.2322476326867, "gen_tokens": 0}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 154, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553803.8954604, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.7063999989186414, "prefill_cuda_event_ms": null, "kv_decode_ms": 153.10540000064066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 153.10540000064066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 418.01268929595034, "gen_tokens": 64}
{"task_idx": 12, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 51, "sample_idx": 155, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553803.8954604, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.7063999989186414, "prefill_cuda_event_ms": null, "kv_decode_ms": 153.10540000064066, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 156.8117999995593, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 465.5261912700776, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 156, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553804.0528226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 86.5886, "prefill_cuda_event_ms": null, "kv_decode_ms": 488.9803, "kv_decode_ms_equiv": 869.2983111111112, "kv_decode_ms_per_token": 13.582786111111112, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 36, "gen_tokens_equiv": 64, "ollama_wall_ms": 726.3583000003564, "ollama_total_duration_ms": 723.7989, "ollama_load_ms": 124.3538, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 86.5886, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 438.8568472062142, "gen_tokens": 0}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 157, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553804.0528226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 86.5886, "prefill_cuda_event_ms": null, "kv_decode_ms": 488.9803, "kv_decode_ms_equiv": 869.2983111111112, "kv_decode_ms_per_token": 13.582786111111112, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 36, "gen_tokens_equiv": 64, "ollama_wall_ms": 726.3583000003564, "ollama_total_duration_ms": 723.7989, "ollama_load_ms": 124.3538, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 869.2983111111112, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.62259788380022, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 52, "sample_idx": 158, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553804.0528226, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 86.5886, "prefill_cuda_event_ms": null, "kv_decode_ms": 488.9803, "kv_decode_ms_equiv": 869.2983111111112, "kv_decode_ms_per_token": 13.582786111111112, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 36, "gen_tokens_equiv": 64, "ollama_wall_ms": 726.3583000003564, "ollama_total_duration_ms": 723.7989, "ollama_load_ms": 124.3538, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 955.8869111111112, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 106.70718346947177, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 159, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553804.779414, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5517, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.3667, "kv_decode_ms_equiv": 710.1562514285714, "kv_decode_ms_per_token": 11.096191428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 583.1252000007225, "ollama_total_duration_ms": 559.7181, "ollama_load_ms": 133.1238, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.5517, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3289.5591125115784, "gen_tokens": 0}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 160, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553804.779414, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5517, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.3667, "kv_decode_ms_equiv": 710.1562514285714, "kv_decode_ms_per_token": 11.096191428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 583.1252000007225, "ollama_total_duration_ms": 559.7181, "ollama_load_ms": 133.1238, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 710.1562514285714, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.12101191992002, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 53, "sample_idx": 161, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553804.779414, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.5517, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.3667, "kv_decode_ms_equiv": 710.1562514285714, "kv_decode_ms_per_token": 11.096191428571428, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 583.1252000007225, "ollama_total_duration_ms": 559.7181, "ollama_load_ms": 133.1238, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 721.7079514285714, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.33140669726308, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 162, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553805.3626688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4448, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.7356, "kv_decode_ms_equiv": 710.8308114285713, "kv_decode_ms_per_token": 11.106731428571427, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 565.7021000006353, "ollama_total_duration_ms": 551.7945, "ollama_load_ms": 122.0726, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.4448, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3320.285195023067, "gen_tokens": 0}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 163, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553805.3626688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4448, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.7356, "kv_decode_ms_equiv": 710.8308114285713, "kv_decode_ms_per_token": 11.106731428571427, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 565.7021000006353, "ollama_total_duration_ms": 551.7945, "ollama_load_ms": 122.0726, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 710.8308114285713, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.03548941748583, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 54, "sample_idx": 164, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553805.3626688, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.4448, "prefill_cuda_event_ms": null, "kv_decode_ms": 388.7356, "kv_decode_ms_equiv": 710.8308114285713, "kv_decode_ms_per_token": 11.106731428571427, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 565.7021000006353, "ollama_total_duration_ms": 551.7945, "ollama_load_ms": 122.0726, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 722.2756114285713, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.22032972739683, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 165, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553805.9284976, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.7914, "kv_decode_ms_equiv": 709.1042742857143, "kv_decode_ms_per_token": 11.079754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 559.9736000003759, "ollama_total_duration_ms": 538.4553, "ollama_load_ms": 109.8071, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 11.86, "cuda_event_ms": null, "tokens_total": 38, "tokens_per_s": 3204.0472175379427, "gen_tokens": 0}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 166, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553805.9284976, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.7914, "kv_decode_ms_equiv": 709.1042742857143, "kv_decode_ms_per_token": 11.079754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 559.9736000003759, "ollama_total_duration_ms": 538.4553, "ollama_load_ms": 109.8071, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 709.1042742857143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.25470910391515, "gen_tokens": 64}
{"task_idx": 13, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 55, "sample_idx": 167, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553805.9284976, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 38, "prefill_ms": 11.86, "prefill_cuda_event_ms": null, "kv_decode_ms": 387.7914, "kv_decode_ms_equiv": 709.1042742857143, "kv_decode_ms_per_token": 11.079754285714285, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 35, "gen_tokens_equiv": 64, "ollama_wall_ms": 559.9736000003759, "ollama_total_duration_ms": 538.4553, "ollama_load_ms": 109.8071, "ollama_done_reason": "stop", "params_millions_measured": 7600.0, "latency_ms": 720.9642742857143, "cuda_event_ms": null, "tokens_total": 102, "tokens_per_s": 141.47719053215937, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 168, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553806.4905062, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 70.30729999860341, "prefill_cuda_event_ms": 70.22176361083984, "kv_decode_ms": 516.2438000006659, "kv_decode_cuda_event_ms": 516.1905517578125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 70.30729999860341, "cuda_event_ms": 70.22176361083984, "tokens_total": 9, "tokens_per_s": 128.00946701379198, "gen_tokens": 0}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 169, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553806.4905062, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 70.30729999860341, "prefill_cuda_event_ms": 70.22176361083984, "kv_decode_ms": 516.2438000006659, "kv_decode_cuda_event_ms": 516.1905517578125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 516.2438000006659, "cuda_event_ms": 516.1905517578125, "tokens_total": 64, "tokens_per_s": 123.97243317966714, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 56, "sample_idx": 170, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553806.4905062, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 70.30729999860341, "prefill_cuda_event_ms": 70.22176361083984, "kv_decode_ms": 516.2438000006659, "kv_decode_cuda_event_ms": 516.1905517578125, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 586.5510999992694, "cuda_event_ms": 586.4123153686523, "tokens_total": 73, "tokens_per_s": 124.45633466562577, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 171, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553807.0784292, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.22549999995681, "prefill_cuda_event_ms": 9.165216445922852, "kv_decode_ms": 508.4956999999122, "kv_decode_cuda_event_ms": 508.45172119140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 9.22549999995681, "cuda_event_ms": 9.165216445922852, "tokens_total": 9, "tokens_per_s": 975.5568803904541, "gen_tokens": 0}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 172, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553807.0784292, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.22549999995681, "prefill_cuda_event_ms": 9.165216445922852, "kv_decode_ms": 508.4956999999122, "kv_decode_cuda_event_ms": 508.45172119140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 508.4956999999122, "cuda_event_ms": 508.45172119140625, "tokens_total": 64, "tokens_per_s": 125.8614379630173, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 57, "sample_idx": 173, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553807.0784292, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.22549999995681, "prefill_cuda_event_ms": 9.165216445922852, "kv_decode_ms": 508.4956999999122, "kv_decode_cuda_event_ms": 508.45172119140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 517.721199999869, "cuda_event_ms": 517.6169376373291, "tokens_total": 73, "tokens_per_s": 141.00253186467634, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 174, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553807.5970457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.001500000522356, "prefill_cuda_event_ms": 8.939552307128906, "kv_decode_ms": 519.3255000012869, "kv_decode_cuda_event_ms": 519.28369140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 9.001500000522356, "cuda_event_ms": 8.939552307128906, "tokens_total": 9, "tokens_per_s": 999.833361048462, "gen_tokens": 0}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 175, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553807.5970457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.001500000522356, "prefill_cuda_event_ms": 8.939552307128906, "kv_decode_ms": 519.3255000012869, "kv_decode_cuda_event_ms": 519.28369140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 519.3255000012869, "cuda_event_ms": 519.28369140625, "tokens_total": 64, "tokens_per_s": 123.23677539393196, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 58, "sample_idx": 176, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553807.5970457, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 9.001500000522356, "prefill_cuda_event_ms": 8.939552307128906, "kv_decode_ms": 519.3255000012869, "kv_decode_cuda_event_ms": 519.28369140625, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 528.3270000018092, "cuda_event_ms": 528.2232437133789, "tokens_total": 73, "tokens_per_s": 138.172003323226, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 177, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553808.126336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.044799999377574, "prefill_cuda_event_ms": 9.991392135620117, "kv_decode_ms": 511.95800000095915, "kv_decode_cuda_event_ms": 511.93548583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 10.044799999377574, "cuda_event_ms": 9.991392135620117, "tokens_total": 9, "tokens_per_s": 895.9859828525889, "gen_tokens": 0}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 178, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553808.126336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.044799999377574, "prefill_cuda_event_ms": 9.991392135620117, "kv_decode_ms": 511.95800000095915, "kv_decode_cuda_event_ms": 511.93548583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 511.95800000095915, "cuda_event_ms": 511.93548583984375, "tokens_total": 64, "tokens_per_s": 125.01025474722555, "gen_tokens": 64}
{"task_idx": 14, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 59, "sample_idx": 179, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553808.126336, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.044799999377574, "prefill_cuda_event_ms": 9.991392135620117, "kv_decode_ms": 511.95800000095915, "kv_decode_cuda_event_ms": 511.93548583984375, "gpu_peak_mb": 489.1572265625, "params_millions_measured": 96.08832, "latency_ms": 522.0028000003367, "cuda_event_ms": 521.9268779754639, "tokens_total": 73, "tokens_per_s": 139.84599316316485, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 180, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553808.6491895, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.685399999289075, "prefill_cuda_event_ms": 4.6038079261779785, "kv_decode_ms": 216.93410000079894, "kv_decode_cuda_event_ms": 216.85968017578125, "gpu_peak_mb": 539.3515625, "hf_load_ms": 316.86029999946186, "params_millions_measured": 25.016064, "latency_ms": 4.685399999289075, "cuda_event_ms": 4.6038079261779785, "tokens_total": 17, "tokens_per_s": 3628.292142096607, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 181, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553808.6491895, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.685399999289075, "prefill_cuda_event_ms": 4.6038079261779785, "kv_decode_ms": 216.93410000079894, "kv_decode_cuda_event_ms": 216.85968017578125, "gpu_peak_mb": 539.3515625, "hf_load_ms": 316.86029999946186, "params_millions_measured": 25.016064, "latency_ms": 216.93410000079894, "cuda_event_ms": 216.85968017578125, "tokens_total": 64, "tokens_per_s": 295.0204693488221, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 60, "sample_idx": 182, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553808.6491895, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.685399999289075, "prefill_cuda_event_ms": 4.6038079261779785, "kv_decode_ms": 216.93410000079894, "kv_decode_cuda_event_ms": 216.85968017578125, "gpu_peak_mb": 539.3515625, "hf_load_ms": 316.86029999946186, "params_millions_measured": 25.016064, "latency_ms": 221.619500000088, "cuda_event_ms": 221.46348810195923, "tokens_total": 81, "tokens_per_s": 365.49130378855574, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 183, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553809.1889367, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.1584999999031425, "prefill_cuda_event_ms": 4.0930562019348145, "kv_decode_ms": 216.9659999999567, "kv_decode_cuda_event_ms": 216.90982055664062, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 4.1584999999031425, "cuda_event_ms": 4.0930562019348145, "tokens_total": 17, "tokens_per_s": 4088.012504604053, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 184, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553809.1889367, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.1584999999031425, "prefill_cuda_event_ms": 4.0930562019348145, "kv_decode_ms": 216.9659999999567, "kv_decode_cuda_event_ms": 216.90982055664062, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 216.9659999999567, "cuda_event_ms": 216.90982055664062, "tokens_total": 64, "tokens_per_s": 294.9770931851662, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 61, "sample_idx": 185, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553809.1889367, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.1584999999031425, "prefill_cuda_event_ms": 4.0930562019348145, "kv_decode_ms": 216.9659999999567, "kv_decode_cuda_event_ms": 216.90982055664062, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 221.12449999985984, "cuda_event_ms": 221.00287675857544, "tokens_total": 81, "tokens_per_s": 366.30947724042943, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 186, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553809.411101, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.055899999002577, "prefill_cuda_event_ms": 3.9906558990478516, "kv_decode_ms": 215.61250000013388, "kv_decode_cuda_event_ms": 215.552001953125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 4.055899999002577, "cuda_event_ms": 3.9906558990478516, "tokens_total": 17, "tokens_per_s": 4191.424838921232, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 187, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553809.411101, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.055899999002577, "prefill_cuda_event_ms": 3.9906558990478516, "kv_decode_ms": 215.61250000013388, "kv_decode_cuda_event_ms": 215.552001953125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 215.61250000013388, "cuda_event_ms": 215.552001953125, "tokens_total": 64, "tokens_per_s": 296.8288016694777, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 62, "sample_idx": 188, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553809.411101, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.055899999002577, "prefill_cuda_event_ms": 3.9906558990478516, "kv_decode_ms": 215.61250000013388, "kv_decode_cuda_event_ms": 215.552001953125, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 219.66839999913645, "cuda_event_ms": 219.54265785217285, "tokens_total": 81, "tokens_per_s": 368.73760632079274, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 189, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553809.631666, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.180300000371062, "prefill_cuda_event_ms": 4.120160102844238, "kv_decode_ms": 222.407999999632, "kv_decode_cuda_event_ms": 222.33494567871094, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 4.180300000371062, "cuda_event_ms": 4.120160102844238, "tokens_total": 17, "tokens_per_s": 4066.6937775975416, "gen_tokens": 0}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 190, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553809.631666, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.180300000371062, "prefill_cuda_event_ms": 4.120160102844238, "kv_decode_ms": 222.407999999632, "kv_decode_cuda_event_ms": 222.33494567871094, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 222.407999999632, "cuda_event_ms": 222.33494567871094, "tokens_total": 64, "tokens_per_s": 287.7594331143929, "gen_tokens": 64}
{"task_idx": 15, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 63, "sample_idx": 191, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553809.631666, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.180300000371062, "prefill_cuda_event_ms": 4.120160102844238, "kv_decode_ms": 222.407999999632, "kv_decode_cuda_event_ms": 222.33494567871094, "gpu_peak_mb": 539.3515625, "params_millions_measured": 25.016064, "latency_ms": 226.58830000000307, "cuda_event_ms": 226.45510578155518, "tokens_total": 81, "tokens_per_s": 357.4765334308916, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 192, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553809.8591263, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8736000005883398, "prefill_cuda_event_ms": 3.8124160766601562, "kv_decode_ms": 216.98230000038166, "kv_decode_cuda_event_ms": 216.9231414794922, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.8736000005883398, "cuda_event_ms": 3.8124160766601562, "tokens_total": 1, "tokens_per_s": 258.15778599961675, "gen_tokens": 0}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 193, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553809.8591263, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8736000005883398, "prefill_cuda_event_ms": 3.8124160766601562, "kv_decode_ms": 216.98230000038166, "kv_decode_cuda_event_ms": 216.9231414794922, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 216.98230000038166, "cuda_event_ms": 216.9231414794922, "tokens_total": 64, "tokens_per_s": 294.95493411161846, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 64, "sample_idx": 194, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553809.8591263, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8736000005883398, "prefill_cuda_event_ms": 3.8124160766601562, "kv_decode_ms": 216.98230000038166, "kv_decode_cuda_event_ms": 216.9231414794922, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 220.85590000097, "cuda_event_ms": 220.73555755615234, "tokens_total": 65, "tokens_per_s": 294.3095475362647, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 195, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553810.080973, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8772480487823486, "kv_decode_ms": 213.47769999920274, "kv_decode_cuda_event_ms": 213.43026733398438, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.9319999996223487, "cuda_event_ms": 3.8772480487823486, "tokens_total": 1, "tokens_per_s": 254.32349951577964, "gen_tokens": 0}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 196, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553810.080973, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8772480487823486, "kv_decode_ms": 213.47769999920274, "kv_decode_cuda_event_ms": 213.43026733398438, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 213.47769999920274, "cuda_event_ms": 213.43026733398438, "tokens_total": 64, "tokens_per_s": 299.79712166769184, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 65, "sample_idx": 197, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553810.080973, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.9319999996223487, "prefill_cuda_event_ms": 3.8772480487823486, "kv_decode_ms": 213.47769999920274, "kv_decode_cuda_event_ms": 213.43026733398438, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 217.4096999988251, "cuda_event_ms": 217.30751538276672, "tokens_total": 65, "tokens_per_s": 298.9747007624373, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 198, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553810.2991712, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.6416000002645887, "prefill_cuda_event_ms": 3.5880320072174072, "kv_decode_ms": 205.15940000041155, "kv_decode_cuda_event_ms": 205.11334228515625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.6416000002645887, "cuda_event_ms": 3.5880320072174072, "tokens_total": 1, "tokens_per_s": 274.6045694000831, "gen_tokens": 0}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 199, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553810.2991712, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.6416000002645887, "prefill_cuda_event_ms": 3.5880320072174072, "kv_decode_ms": 205.15940000041155, "kv_decode_cuda_event_ms": 205.11334228515625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 205.15940000041155, "cuda_event_ms": 205.11334228515625, "tokens_total": 64, "tokens_per_s": 311.9525598138404, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 66, "sample_idx": 200, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553810.2991712, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.6416000002645887, "prefill_cuda_event_ms": 3.5880320072174072, "kv_decode_ms": 205.15940000041155, "kv_decode_cuda_event_ms": 205.11334228515625, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 208.80100000067614, "cuda_event_ms": 208.70137429237366, "tokens_total": 65, "tokens_per_s": 311.30119108524156, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 201, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553810.5087657, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8646999983029673, "prefill_cuda_event_ms": 3.8054399490356445, "kv_decode_ms": 223.6706999992748, "kv_decode_cuda_event_ms": 223.62112426757812, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 3.8646999983029673, "cuda_event_ms": 3.8054399490356445, "tokens_total": 1, "tokens_per_s": 258.7522965402518, "gen_tokens": 0}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 202, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553810.5087657, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8646999983029673, "prefill_cuda_event_ms": 3.8054399490356445, "kv_decode_ms": 223.6706999992748, "kv_decode_cuda_event_ms": 223.62112426757812, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 223.6706999992748, "cuda_event_ms": 223.62112426757812, "tokens_total": 64, "tokens_per_s": 286.13492960949964, "gen_tokens": 64}
{"task_idx": 16, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 67, "sample_idx": 203, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553810.5087657, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8646999983029673, "prefill_cuda_event_ms": 3.8054399490356445, "kv_decode_ms": 223.6706999992748, "kv_decode_cuda_event_ms": 223.62112426757812, "gpu_peak_mb": 537.74755859375, "params_millions_measured": 25.016064, "latency_ms": 227.53539999757777, "cuda_event_ms": 227.42656421661377, "tokens_total": 65, "tokens_per_s": 285.66983423542865, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 204, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553810.7372313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 37.32619999937015, "prefill_cuda_event_ms": null, "kv_decode_ms": 707.3916999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.52299999984098, "params_millions_measured": 25.016064, "latency_ms": 37.32619999937015, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 241.11749924052995, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 205, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553810.7372313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 37.32619999937015, "prefill_cuda_event_ms": null, "kv_decode_ms": 707.3916999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.52299999984098, "params_millions_measured": 25.016064, "latency_ms": 707.3916999997891, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 90.47321307278426, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 68, "sample_idx": 206, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553810.7372313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 37.32619999937015, "prefill_cuda_event_ms": null, "kv_decode_ms": 707.3916999997891, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.52299999984098, "params_millions_measured": 25.016064, "latency_ms": 744.7178999991593, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 98.0236946098414, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 207, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553811.6311188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.009200000553392, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.2911000005697, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 16.009200000553392, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 562.1767483502546, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 208, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553811.6311188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.009200000553392, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.2911000005697, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 703.2911000005697, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 91.00072501976516, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 69, "sample_idx": 209, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553811.6311188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.009200000553392, "prefill_cuda_event_ms": null, "kv_decode_ms": 703.2911000005697, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 719.3003000011231, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 101.4875150196462, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 210, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553812.3509476, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.96120000067458, "prefill_cuda_event_ms": null, "kv_decode_ms": 686.7735000014363, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 15.96120000067458, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 563.8673783687709, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 211, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553812.3509476, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.96120000067458, "prefill_cuda_event_ms": null, "kv_decode_ms": 686.7735000014363, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 686.7735000014363, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 93.18938485521959, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 70, "sample_idx": 212, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553812.3509476, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 15.96120000067458, "prefill_cuda_event_ms": null, "kv_decode_ms": 686.7735000014363, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 702.7347000021109, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 103.8798852536821, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 213, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553813.0542963, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.933199999883072, "prefill_cuda_event_ms": null, "kv_decode_ms": 666.5145000006305, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.933199999883072, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 645.9391955958091, "gen_tokens": 0}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 214, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553813.0542963, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.933199999883072, "prefill_cuda_event_ms": null, "kv_decode_ms": 666.5145000006305, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 666.5145000006305, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.021917002465, "gen_tokens": 64}
{"task_idx": 17, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 71, "sample_idx": 215, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553813.0542963, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 13.933199999883072, "prefill_cuda_event_ms": null, "kv_decode_ms": 666.5145000006305, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 680.4477000005136, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 107.28230839775769, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 216, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553813.7353299, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 787.9304999987653, "prefill_cuda_event_ms": null, "kv_decode_ms": 1965.0101999995968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 416.11399999965215, "params_millions_measured": 96.08832, "latency_ms": 787.9304999987653, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 21.575506976854733, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 217, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553813.7353299, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 787.9304999987653, "prefill_cuda_event_ms": null, "kv_decode_ms": 1965.0101999995968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 416.11399999965215, "params_millions_measured": 96.08832, "latency_ms": 1965.0101999995968, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.56980549007488, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 72, "sample_idx": 218, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553813.7353299, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 787.9304999987653, "prefill_cuda_event_ms": null, "kv_decode_ms": 1965.0101999995968, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 416.11399999965215, "params_millions_measured": 96.08832, "latency_ms": 2752.940699998362, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 29.423082015550932, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 219, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553816.9067516, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 47.06819999955769, "prefill_cuda_event_ms": null, "kv_decode_ms": 1767.5097999999707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 47.06819999955769, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 361.17803527986524, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 220, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553816.9067516, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 47.06819999955769, "prefill_cuda_event_ms": null, "kv_decode_ms": 1767.5097999999707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1767.5097999999707, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 36.20913445571904, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 73, "sample_idx": 221, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553816.9067516, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 47.06819999955769, "prefill_cuda_event_ms": null, "kv_decode_ms": 1767.5097999999707, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1814.5779999995284, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 44.638477927110905, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 222, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553818.721868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.418000001314795, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.6540999989957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 44.418000001314795, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 382.7277229838532, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 223, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553818.721868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.418000001314795, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.6540999989957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1601.6540999989957, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.958690206605866, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 74, "sample_idx": 224, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553818.721868, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.418000001314795, "prefill_cuda_event_ms": null, "kv_decode_ms": 1601.6540999989957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1646.0721000003105, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 49.20805109325693, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 225, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553820.3689973, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 45.402899999317015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.8211999995256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 45.402899999317015, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 374.42542217029586, "gen_tokens": 0}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 226, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553820.3689973, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 45.402899999317015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.8211999995256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1617.8211999995256, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.55937776066896, "gen_tokens": 64}
{"task_idx": 18, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 75, "sample_idx": 227, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553820.3689973, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 45.402899999317015, "prefill_cuda_event_ms": null, "kv_decode_ms": 1617.8211999995256, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1663.2240999988426, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 48.700593022946435, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 228, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553822.032769, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.01549999981944, "prefill_cuda_event_ms": null, "kv_decode_ms": 677.6397000012366, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 18.01549999981944, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 55.50775720962629, "gen_tokens": 0}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 229, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553822.032769, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.01549999981944, "prefill_cuda_event_ms": null, "kv_decode_ms": 677.6397000012366, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 677.6397000012366, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 94.44547006304856, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 76, "sample_idx": 230, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553822.032769, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.01549999981944, "prefill_cuda_event_ms": null, "kv_decode_ms": 677.6397000012366, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 695.6552000010561, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 93.43709354850121, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 231, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553822.72885, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.50809999894409, "prefill_cuda_event_ms": null, "kv_decode_ms": 716.2485000008019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 13.50809999894409, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 74.02965628609269, "gen_tokens": 0}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 232, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553822.72885, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.50809999894409, "prefill_cuda_event_ms": null, "kv_decode_ms": 716.2485000008019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 716.2485000008019, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.35446287137543, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 77, "sample_idx": 233, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553822.72885, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.50809999894409, "prefill_cuda_event_ms": null, "kv_decode_ms": 716.2485000008019, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 729.756599999746, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 89.07079428952424, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 234, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553823.4590538, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.62079999994603, "prefill_cuda_event_ms": null, "kv_decode_ms": 662.5911999999516, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 11.62079999994603, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 86.05259534667529, "gen_tokens": 0}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 235, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553823.4590538, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.62079999994603, "prefill_cuda_event_ms": null, "kv_decode_ms": 662.5911999999516, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 662.5911999999516, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.59047690341295, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 78, "sample_idx": 236, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553823.4590538, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.62079999994603, "prefill_cuda_event_ms": null, "kv_decode_ms": 662.5911999999516, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 674.2119999998977, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 96.40884469574831, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 237, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553824.1336343, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 9.984100001020124, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.8140999992611, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 9.984100001020124, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 100.15925320237429, "gen_tokens": 0}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 238, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553824.1336343, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 9.984100001020124, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.8140999992611, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 640.8140999992611, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 99.872958475904, "gen_tokens": 64}
{"task_idx": 19, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 79, "sample_idx": 239, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553824.1336343, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 9.984100001020124, "prefill_cuda_event_ms": null, "kv_decode_ms": 640.8140999992611, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 650.7982000002812, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 99.87735061340354, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 240, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553824.784781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 43.206199999985984, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.1462999991345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 43.206199999985984, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 208.30343793258652, "gen_tokens": 0}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 241, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553824.784781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 43.206199999985984, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.1462999991345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1233.1462999991345, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.89976242076461, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 80, "sample_idx": 242, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553824.784781, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 43.206199999985984, "prefill_cuda_event_ms": null, "kv_decode_ms": 1233.1462999991345, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1276.3524999991205, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.194231217512645, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 243, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553826.0616627, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.98299999893061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.9432999995624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 22.98299999893061, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 391.5937867301382, "gen_tokens": 0}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 244, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553826.0616627, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.98299999893061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.9432999995624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1250.9432999995624, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.161391567485424, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 81, "sample_idx": 245, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553826.0616627, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 22.98299999893061, "prefill_cuda_event_ms": null, "kv_decode_ms": 1250.9432999995624, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1273.926299998493, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.30315796140354, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 246, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553827.3360188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.431800000253133, "prefill_cuda_event_ms": null, "kv_decode_ms": 1253.164599998854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 23.431800000253133, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 384.09341151353175, "gen_tokens": 0}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 247, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553827.3360188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.431800000253133, "prefill_cuda_event_ms": null, "kv_decode_ms": 1253.164599998854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1253.164599998854, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.070705316810354, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 82, "sample_idx": 248, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553827.3360188, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 23.431800000253133, "prefill_cuda_event_ms": null, "kv_decode_ms": 1253.164599998854, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1276.5963999991072, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 57.183303979277284, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 249, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553828.6130235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.358499998925254, "prefill_cuda_event_ms": null, "kv_decode_ms": 1284.4140999986848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 24.358499998925254, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 369.48087938079516, "gen_tokens": 0}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 250, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553828.6130235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.358499998925254, "prefill_cuda_event_ms": null, "kv_decode_ms": 1284.4140999986848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1284.4140999986848, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.82816678831658, "gen_tokens": 64}
{"task_idx": 20, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 83, "sample_idx": 251, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553828.6130235, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 24.358499998925254, "prefill_cuda_event_ms": null, "kv_decode_ms": 1284.4140999986848, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1308.77259999761, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 55.77745133121927, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 252, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553829.9222157, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.778400000883266, "prefill_cuda_event_ms": null, "kv_decode_ms": 1282.9463999987638, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.778400000883266, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 53.25267328169406, "gen_tokens": 0}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 253, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553829.9222157, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.778400000883266, "prefill_cuda_event_ms": null, "kv_decode_ms": 1282.9463999987638, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1282.9463999987638, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.885170573035374, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 84, "sample_idx": 254, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553829.9222157, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.778400000883266, "prefill_cuda_event_ms": null, "kv_decode_ms": 1282.9463999987638, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1301.724799999647, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 49.9337494376827, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 255, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553831.2245178, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.45389999834879, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.04829999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.45389999834879, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 54.18908740642778, "gen_tokens": 0}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 256, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553831.2245178, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.45389999834879, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.04829999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1280.04829999918, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.998113352473496, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 85, "sample_idx": 257, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553831.2245178, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.45389999834879, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.04829999918, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1298.5021999975288, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 50.05767414188725, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 258, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553832.5234132, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.137199999633594, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.0534999998927, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 22.137199999633594, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 45.172831253119256, "gen_tokens": 0}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 259, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553832.5234132, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.137199999633594, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.0534999998927, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1232.0534999998927, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 51.94579618499162, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 86, "sample_idx": 260, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553832.5234132, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.137199999633594, "prefill_cuda_event_ms": null, "kv_decode_ms": 1232.0534999998927, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1254.1906999995263, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 51.82624938936683, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 261, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553833.777966, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.02990000032878, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.8546999996906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 18.02990000032878, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 55.463424643606714, "gen_tokens": 0}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 262, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553833.777966, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.02990000032878, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.8546999996906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1705.8546999996906, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 37.51784955659565, "gen_tokens": 64}
{"task_idx": 21, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 87, "sample_idx": 263, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553833.777966, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 18.02990000032878, "prefill_cuda_event_ms": null, "kv_decode_ms": 1705.8546999996906, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 96.08832, "latency_ms": 1723.8846000000194, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 37.70554015042496, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 264, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553835.5023408, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.97240000027523, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.8414000002667, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 23.97240000027523, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 709.148854507885, "gen_tokens": 0}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 265, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553835.5023408, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.97240000027523, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.8414000002667, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1071.8414000002667, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.71032654643128, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 88, "sample_idx": 266, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553835.5023408, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.97240000027523, "prefill_cuda_event_ms": null, "kv_decode_ms": 1071.8414000002667, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1095.813800000542, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 73.91766739929716, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 267, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553836.5992873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.951200001349207, "prefill_cuda_event_ms": null, "kv_decode_ms": 1135.2891000005911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 26.951200001349207, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 630.7696874034908, "gen_tokens": 0}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 268, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553836.5992873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.951200001349207, "prefill_cuda_event_ms": null, "kv_decode_ms": 1135.2891000005911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1135.2891000005911, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.37330614727709, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 89, "sample_idx": 269, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553836.5992873, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.951200001349207, "prefill_cuda_event_ms": null, "kv_decode_ms": 1135.2891000005911, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1162.2403000019403, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 69.69298861850237, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 270, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553837.7621455, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.877199999333243, "prefill_cuda_event_ms": null, "kv_decode_ms": 1108.325600000171, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 23.877199999333243, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 711.976278645516, "gen_tokens": 0}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 271, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553837.7621455, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.877199999333243, "prefill_cuda_event_ms": null, "kv_decode_ms": 1108.325600000171, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1108.325600000171, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 57.74476381307995, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 90, "sample_idx": 272, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553837.7621455, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.877199999333243, "prefill_cuda_event_ms": null, "kv_decode_ms": 1108.325600000171, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1132.2027999995044, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 71.54195343805496, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 273, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553838.8954246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.59449999978824, "prefill_cuda_event_ms": null, "kv_decode_ms": 931.7025000000285, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 20.59449999978824, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 825.4631090910098, "gen_tokens": 0}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 274, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553838.8954246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.59449999978824, "prefill_cuda_event_ms": null, "kv_decode_ms": 931.7025000000285, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 931.7025000000285, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 68.69145462204732, "gen_tokens": 64}
{"task_idx": 22, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 91, "sample_idx": 275, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553838.8954246, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 20.59449999978824, "prefill_cuda_event_ms": null, "kv_decode_ms": 931.7025000000285, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 952.2969999998168, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 85.05749781844906, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 276, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553839.8481803, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.753600000592996, "prefill_cuda_event_ms": 6.639103889465332, "kv_decode_ms": 218.86760000052163, "kv_decode_cuda_event_ms": 218.6710968017578, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 6.753600000592996, "cuda_event_ms": 6.639103889465332, "tokens_total": 9, "tokens_per_s": 1332.6226011623075, "gen_tokens": 0}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 277, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553839.8481803, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.753600000592996, "prefill_cuda_event_ms": 6.639103889465332, "kv_decode_ms": 218.86760000052163, "kv_decode_cuda_event_ms": 218.6710968017578, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 218.86760000052163, "cuda_event_ms": 218.6710968017578, "tokens_total": 64, "tokens_per_s": 292.4142266824668, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 92, "sample_idx": 278, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553839.8481803, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.753600000592996, "prefill_cuda_event_ms": 6.639103889465332, "kv_decode_ms": 218.86760000052163, "kv_decode_cuda_event_ms": 218.6710968017578, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 225.62120000111463, "cuda_event_ms": 225.31020069122314, "tokens_total": 73, "tokens_per_s": 323.5511556522142, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 279, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553840.0757651, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.1335000003309688, "prefill_cuda_event_ms": 3.070784091949463, "kv_decode_ms": 190.9814999999071, "kv_decode_cuda_event_ms": 190.87564086914062, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 3.1335000003309688, "cuda_event_ms": 3.070784091949463, "tokens_total": 9, "tokens_per_s": 2872.1876492897386, "gen_tokens": 0}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 280, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553840.0757651, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.1335000003309688, "prefill_cuda_event_ms": 3.070784091949463, "kv_decode_ms": 190.9814999999071, "kv_decode_cuda_event_ms": 190.87564086914062, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 190.9814999999071, "cuda_event_ms": 190.87564086914062, "tokens_total": 64, "tokens_per_s": 335.11099242613096, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 93, "sample_idx": 281, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553840.0757651, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.1335000003309688, "prefill_cuda_event_ms": 3.070784091949463, "kv_decode_ms": 190.9814999999071, "kv_decode_cuda_event_ms": 190.87564086914062, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 194.11500000023807, "cuda_event_ms": 193.9464249610901, "tokens_total": 73, "tokens_per_s": 376.065734229248, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 282, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553840.2705717, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 1.7886999994516373, "prefill_cuda_event_ms": 1.7541439533233643, "kv_decode_ms": 108.06339999908232, "kv_decode_cuda_event_ms": 108.02454376220703, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 1.7886999994516373, "cuda_event_ms": 1.7541439533233643, "tokens_total": 9, "tokens_per_s": 5031.587187767169, "gen_tokens": 0}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 283, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553840.2705717, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 1.7886999994516373, "prefill_cuda_event_ms": 1.7541439533233643, "kv_decode_ms": 108.06339999908232, "kv_decode_cuda_event_ms": 108.02454376220703, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 108.06339999908232, "cuda_event_ms": 108.02454376220703, "tokens_total": 64, "tokens_per_s": 592.2449228928897, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 94, "sample_idx": 284, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553840.2705717, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 1.7886999994516373, "prefill_cuda_event_ms": 1.7541439533233643, "kv_decode_ms": 108.06339999908232, "kv_decode_cuda_event_ms": 108.02454376220703, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 109.85209999853396, "cuda_event_ms": 109.7786877155304, "tokens_total": 73, "tokens_per_s": 664.5298542401486, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 285, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553840.3809826, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.1178999995754566, "prefill_cuda_event_ms": 2.073728084564209, "kv_decode_ms": 112.03660000137461, "kv_decode_cuda_event_ms": 111.98880004882812, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 2.1178999995754566, "cuda_event_ms": 2.073728084564209, "tokens_total": 9, "tokens_per_s": 4249.492422590346, "gen_tokens": 0}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 286, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553840.3809826, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.1178999995754566, "prefill_cuda_event_ms": 2.073728084564209, "kv_decode_ms": 112.03660000137461, "kv_decode_cuda_event_ms": 111.98880004882812, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 112.03660000137461, "cuda_event_ms": 111.98880004882812, "tokens_total": 64, "tokens_per_s": 571.2418977299808, "gen_tokens": 64}
{"task_idx": 23, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 95, "sample_idx": 287, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553840.3809826, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 2.1178999995754566, "prefill_cuda_event_ms": 2.073728084564209, "kv_decode_ms": 112.03660000137461, "kv_decode_cuda_event_ms": 111.98880004882812, "gpu_peak_mb": 538.54931640625, "params_millions_measured": 25.016064, "latency_ms": 114.15450000095007, "cuda_event_ms": 114.06252813339233, "tokens_total": 73, "tokens_per_s": 639.4842078007651, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 288, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553840.4958062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.92319999911706, "prefill_cuda_event_ms": 8.867584228515625, "kv_decode_ms": 798.7929999999324, "kv_decode_cuda_event_ms": 798.7548217773438, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 8.92319999911706, "cuda_event_ms": 8.867584228515625, "tokens_total": 17, "tokens_per_s": 1905.1461361038787, "gen_tokens": 0}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 289, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553840.4958062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.92319999911706, "prefill_cuda_event_ms": 8.867584228515625, "kv_decode_ms": 798.7929999999324, "kv_decode_cuda_event_ms": 798.7548217773438, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 798.7929999999324, "cuda_event_ms": 798.7548217773438, "tokens_total": 64, "tokens_per_s": 80.12088238129957, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 96, "sample_idx": 290, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553840.4958062, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.92319999911706, "prefill_cuda_event_ms": 8.867584228515625, "kv_decode_ms": 798.7929999999324, "kv_decode_cuda_event_ms": 798.7548217773438, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 807.7161999990494, "cuda_event_ms": 807.6224060058594, "tokens_total": 81, "tokens_per_s": 100.28274782664421, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 291, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553841.3047428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.492399999857298, "prefill_cuda_event_ms": 15.415519714355469, "kv_decode_ms": 843.6266999997315, "kv_decode_cuda_event_ms": 843.579345703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 15.492399999857298, "cuda_event_ms": 15.415519714355469, "tokens_total": 17, "tokens_per_s": 1097.3122305231332, "gen_tokens": 0}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 292, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553841.3047428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.492399999857298, "prefill_cuda_event_ms": 15.415519714355469, "kv_decode_ms": 843.6266999997315, "kv_decode_cuda_event_ms": 843.579345703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 843.6266999997315, "cuda_event_ms": 843.579345703125, "tokens_total": 64, "tokens_per_s": 75.86293795587595, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 97, "sample_idx": 293, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553841.3047428, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 15.492399999857298, "prefill_cuda_event_ms": 15.415519714355469, "kv_decode_ms": 843.6266999997315, "kv_decode_cuda_event_ms": 843.579345703125, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 859.1190999995888, "cuda_event_ms": 858.9948654174805, "tokens_total": 81, "tokens_per_s": 94.28262041903011, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 294, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553842.164802, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.412000000855187, "prefill_cuda_event_ms": 16.331071853637695, "kv_decode_ms": 805.4307000002154, "kv_decode_cuda_event_ms": 805.3790893554688, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 16.412000000855187, "cuda_event_ms": 16.331071853637695, "tokens_total": 17, "tokens_per_s": 1035.8274432801716, "gen_tokens": 0}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 295, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553842.164802, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.412000000855187, "prefill_cuda_event_ms": 16.331071853637695, "kv_decode_ms": 805.4307000002154, "kv_decode_cuda_event_ms": 805.3790893554688, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 805.4307000002154, "cuda_event_ms": 805.3790893554688, "tokens_total": 64, "tokens_per_s": 79.46059170575802, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 98, "sample_idx": 296, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553842.164802, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 16.412000000855187, "prefill_cuda_event_ms": 16.331071853637695, "kv_decode_ms": 805.4307000002154, "kv_decode_cuda_event_ms": 805.3790893554688, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 821.8427000010706, "cuda_event_ms": 821.7101612091064, "tokens_total": 81, "tokens_per_s": 98.55900648614933, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 297, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553842.9875183, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.678199998641503, "prefill_cuda_event_ms": 11.575872421264648, "kv_decode_ms": 832.9087999991316, "kv_decode_cuda_event_ms": 832.85400390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 11.678199998641503, "cuda_event_ms": 11.575872421264648, "tokens_total": 17, "tokens_per_s": 1455.7037901369706, "gen_tokens": 0}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 298, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553842.9875183, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.678199998641503, "prefill_cuda_event_ms": 11.575872421264648, "kv_decode_ms": 832.9087999991316, "kv_decode_cuda_event_ms": 832.85400390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 832.9087999991316, "cuda_event_ms": 832.85400390625, "tokens_total": 64, "tokens_per_s": 76.83914493407529, "gen_tokens": 64}
{"task_idx": 24, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 99, "sample_idx": 299, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553842.9875183, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 11.678199998641503, "prefill_cuda_event_ms": 11.575872421264648, "kv_decode_ms": 832.9087999991316, "kv_decode_cuda_event_ms": 832.85400390625, "gpu_peak_mb": 539.29345703125, "params_millions_measured": 5.03672, "latency_ms": 844.5869999977731, "cuda_event_ms": 844.4298763275146, "tokens_total": 81, "tokens_per_s": 95.90486237677536, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 300, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553843.8330355, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.351400000217836, "prefill_cuda_event_ms": 10.243712425231934, "kv_decode_ms": 523.4319000010146, "kv_decode_cuda_event_ms": 523.315185546875, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 275.2355000011448, "params_millions_measured": 45.1712, "latency_ms": 10.351400000217836, "cuda_event_ms": 10.243712425231934, "tokens_total": 9, "tokens_per_s": 869.4476109328789, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 301, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553843.8330355, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.351400000217836, "prefill_cuda_event_ms": 10.243712425231934, "kv_decode_ms": 523.4319000010146, "kv_decode_cuda_event_ms": 523.315185546875, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 275.2355000011448, "params_millions_measured": 45.1712, "latency_ms": 523.4319000010146, "cuda_event_ms": 523.315185546875, "tokens_total": 64, "tokens_per_s": 122.26996482231203, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 100, "sample_idx": 302, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553843.8330355, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.351400000217836, "prefill_cuda_event_ms": 10.243712425231934, "kv_decode_ms": 523.4319000010146, "kv_decode_cuda_event_ms": 523.315185546875, "gpu_peak_mb": 632.16455078125, "hf_load_ms": 275.2355000011448, "params_millions_measured": 45.1712, "latency_ms": 533.7833000012324, "cuda_event_ms": 533.5588979721069, "tokens_total": 73, "tokens_per_s": 136.75961761979337, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 303, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553844.6432743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33379999898898, "prefill_cuda_event_ms": 10.211263656616211, "kv_decode_ms": 459.32879999963916, "kv_decode_cuda_event_ms": 459.1851501464844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 10.33379999898898, "cuda_event_ms": 10.211263656616211, "tokens_total": 9, "tokens_per_s": 870.9284097699323, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 304, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553844.6432743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33379999898898, "prefill_cuda_event_ms": 10.211263656616211, "kv_decode_ms": 459.32879999963916, "kv_decode_cuda_event_ms": 459.1851501464844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 459.32879999963916, "cuda_event_ms": 459.1851501464844, "tokens_total": 64, "tokens_per_s": 139.3337408846349, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 101, "sample_idx": 305, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553844.6432743, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33379999898898, "prefill_cuda_event_ms": 10.211263656616211, "kv_decode_ms": 459.32879999963916, "kv_decode_cuda_event_ms": 459.1851501464844, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 469.66259999862814, "cuda_event_ms": 469.3964138031006, "tokens_total": 73, "tokens_per_s": 155.43072835736385, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 306, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553845.1138313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.710999999791966, "prefill_cuda_event_ms": 5.665791988372803, "kv_decode_ms": 218.1466999991244, "kv_decode_cuda_event_ms": 218.10585021972656, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 5.710999999791966, "cuda_event_ms": 5.665791988372803, "tokens_total": 9, "tokens_per_s": 1575.906146091375, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 307, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553845.1138313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.710999999791966, "prefill_cuda_event_ms": 5.665791988372803, "kv_decode_ms": 218.1466999991244, "kv_decode_cuda_event_ms": 218.10585021972656, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 218.1466999991244, "cuda_event_ms": 218.10585021972656, "tokens_total": 64, "tokens_per_s": 293.3805553797371, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 102, "sample_idx": 308, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553845.1138313, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.710999999791966, "prefill_cuda_event_ms": 5.665791988372803, "kv_decode_ms": 218.1466999991244, "kv_decode_cuda_event_ms": 218.10585021972656, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 223.85769999891636, "cuda_event_ms": 223.77164220809937, "tokens_total": 73, "tokens_per_s": 326.1000180040864, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 309, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553845.3385196, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.380399999921792, "prefill_cuda_event_ms": 5.333312034606934, "kv_decode_ms": 207.65340000070864, "kv_decode_cuda_event_ms": 207.61497497558594, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 5.380399999921792, "cuda_event_ms": 5.333312034606934, "tokens_total": 9, "tokens_per_s": 1672.7380864119436, "gen_tokens": 0}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 310, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553845.3385196, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.380399999921792, "prefill_cuda_event_ms": 5.333312034606934, "kv_decode_ms": 207.65340000070864, "kv_decode_cuda_event_ms": 207.61497497558594, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 207.65340000070864, "cuda_event_ms": 207.61497497558594, "tokens_total": 64, "tokens_per_s": 308.2058853829583, "gen_tokens": 64}
{"task_idx": 25, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 103, "sample_idx": 311, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553845.3385196, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.380399999921792, "prefill_cuda_event_ms": 5.333312034606934, "kv_decode_ms": 207.65340000070864, "kv_decode_cuda_event_ms": 207.61497497558594, "gpu_peak_mb": 632.16455078125, "params_millions_measured": 45.1712, "latency_ms": 213.03380000063044, "cuda_event_ms": 212.94828701019287, "tokens_total": 73, "tokens_per_s": 342.6686281697269, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 312, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553845.5523083, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 19.974399998318404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1024.6614000006957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 19.974399998318404, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 50.064082029206766, "gen_tokens": 0}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 313, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553845.5523083, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 19.974399998318404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1024.6614000006957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1024.6614000006957, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 62.459657404832996, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 104, "sample_idx": 314, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553845.5523083, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 19.974399998318404, "prefill_cuda_event_ms": null, "kv_decode_ms": 1024.6614000006957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 1044.6357999990141, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 62.22264257079964, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 315, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553846.597299, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.297199999215081, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3026000003301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.297199999215081, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.20380230868369, "gen_tokens": 0}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 316, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553846.597299, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.297199999215081, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3026000003301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 897.3026000003301, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 71.32487970053408, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 105, "sample_idx": 317, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553846.597299, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.297199999215081, "prefill_cuda_event_ms": null, "kv_decode_ms": 897.3026000003301, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 910.5997999995452, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 71.3815223768251, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 318, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553847.5083456, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.328999999226653, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.8351000013063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 12.328999999226653, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 81.10957904637245, "gen_tokens": 0}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 319, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553847.5083456, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.328999999226653, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.8351000013063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 883.8351000013063, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.41169761181176, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 106, "sample_idx": 320, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553847.5083456, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.328999999226653, "prefill_cuda_event_ms": null, "kv_decode_ms": 883.8351000013063, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 896.1641000005329, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 72.53135893299157, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 321, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553848.405017, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.281999999890104, "prefill_cuda_event_ms": null, "kv_decode_ms": 909.3740000007529, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 13.281999999890104, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.2898659846615, "gen_tokens": 0}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 322, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553848.405017, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.281999999890104, "prefill_cuda_event_ms": null, "kv_decode_ms": 909.3740000007529, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 909.3740000007529, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.37808426450175, "gen_tokens": 64}
{"task_idx": 26, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 107, "sample_idx": 323, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553848.405017, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.281999999890104, "prefill_cuda_event_ms": null, "kv_decode_ms": 909.3740000007529, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 5.03672, "latency_ms": 922.656000000643, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 70.44879131545744, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 324, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553849.3282833, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.6700000007113, "prefill_cuda_event_ms": null, "kv_decode_ms": 1081.3175000002957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 33.6700000007113, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 504.9005048898386, "gen_tokens": 0}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 325, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553849.3282833, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.6700000007113, "prefill_cuda_event_ms": null, "kv_decode_ms": 1081.3175000002957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1081.3175000002957, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 59.187056530558785, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 108, "sample_idx": 326, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553849.3282833, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 33.6700000007113, "prefill_cuda_event_ms": null, "kv_decode_ms": 1081.3175000002957, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 1114.987500001007, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 72.64655433350315, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 327, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553850.4439168, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.076799998714705, "prefill_cuda_event_ms": null, "kv_decode_ms": 874.6503000002122, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 25.076799998714705, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 677.9174376663419, "gen_tokens": 0}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 328, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553850.4439168, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.076799998714705, "prefill_cuda_event_ms": null, "kv_decode_ms": 874.6503000002122, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 874.6503000002122, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.1721008956202, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 109, "sample_idx": 329, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553850.4439168, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.076799998714705, "prefill_cuda_event_ms": null, "kv_decode_ms": 874.6503000002122, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 899.7270999989269, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 90.02729827755172, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 330, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553851.3447063, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.513599999205326, "prefill_cuda_event_ms": null, "kv_decode_ms": 798.7739000000147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 23.513599999205326, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 722.9858465132749, "gen_tokens": 0}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 331, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553851.3447063, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.513599999205326, "prefill_cuda_event_ms": null, "kv_decode_ms": 798.7739000000147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 798.7739000000147, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 80.12279820359531, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 110, "sample_idx": 332, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553851.3447063, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 23.513599999205326, "prefill_cuda_event_ms": null, "kv_decode_ms": 798.7739000000147, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 822.28749999922, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 98.50569296028073, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 333, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553852.1677198, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.831100000374136, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.4128000015044, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 19.831100000374136, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 857.2393866038332, "gen_tokens": 0}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 334, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553852.1677198, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.831100000374136, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.4128000015044, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 775.4128000015044, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 82.5366823966226, "gen_tokens": 64}
{"task_idx": 27, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 111, "sample_idx": 335, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553852.1677198, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.831100000374136, "prefill_cuda_event_ms": null, "kv_decode_ms": 775.4128000015044, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 45.1712, "latency_ms": 795.2439000018785, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 101.85554394042968, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 336, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553852.9638944, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 337.48150000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.1992000005994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.9149000008183, "params_millions_measured": 51.475968, "latency_ms": 337.48150000064925, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 26.668128475139188, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 337, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553852.9638944, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 337.48150000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.1992000005994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.9149000008183, "params_millions_measured": 51.475968, "latency_ms": 1013.1992000005994, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 63.16625595436923, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 112, "sample_idx": 338, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553852.9638944, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 337.48150000064925, "prefill_cuda_event_ms": null, "kv_decode_ms": 1013.1992000005994, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 148.9149000008183, "params_millions_measured": 51.475968, "latency_ms": 1350.6807000012486, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 54.04682246509668, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 339, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553854.4669695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.515000000377768, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.4237999995821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.515000000377768, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 294.9369162670353, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 340, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553854.4669695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.515000000377768, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.4237999995821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1033.4237999995821, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.930061993952414, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 113, "sample_idx": 341, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553854.4669695, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 30.515000000377768, "prefill_cuda_event_ms": null, "kv_decode_ms": 1033.4237999995821, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1063.9387999999599, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 68.6129690918338, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 342, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553855.5314133, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.788200000519282, "prefill_cuda_event_ms": null, "kv_decode_ms": 912.233400000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 16.788200000519282, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 536.0908256824208, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 343, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553855.5314133, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.788200000519282, "prefill_cuda_event_ms": null, "kv_decode_ms": 912.233400000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 912.233400000332, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 70.15748381935666, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 114, "sample_idx": 344, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553855.5314133, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 16.788200000519282, "prefill_cuda_event_ms": null, "kv_decode_ms": 912.233400000332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 929.0216000008513, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 78.57729034495334, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 345, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553856.461117, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 18.14869999907387, "prefill_cuda_event_ms": null, "kv_decode_ms": 977.5454999999056, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 18.14869999907387, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 495.90328786410436, "gen_tokens": 0}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 346, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553856.461117, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 18.14869999907387, "prefill_cuda_event_ms": null, "kv_decode_ms": 977.5454999999056, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 977.5454999999056, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 65.4700983227954, "gen_tokens": 64}
{"task_idx": 28, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 115, "sample_idx": 347, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553856.461117, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 18.14869999907387, "prefill_cuda_event_ms": null, "kv_decode_ms": 977.5454999999056, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 995.6941999989795, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 73.31568266650024, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 348, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553857.4575253, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.57599999841477, "prefill_cuda_event_ms": 44.44316864013672, "kv_decode_ms": 449.22029999906954, "kv_decode_cuda_event_ms": 449.1181945800781, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 44.57599999841477, "cuda_event_ms": 44.44316864013672, "tokens_total": 17, "tokens_per_s": 381.3711414349551, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 349, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553857.4575253, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.57599999841477, "prefill_cuda_event_ms": 44.44316864013672, "kv_decode_ms": 449.22029999906954, "kv_decode_cuda_event_ms": 449.1181945800781, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 449.22029999906954, "cuda_event_ms": 449.1181945800781, "tokens_total": 64, "tokens_per_s": 142.46907363743927, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 116, "sample_idx": 350, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553857.4575253, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 44.57599999841477, "prefill_cuda_event_ms": 44.44316864013672, "kv_decode_ms": 449.22029999906954, "kv_decode_cuda_event_ms": 449.1181945800781, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 493.7962999974843, "cuda_event_ms": 493.56136322021484, "tokens_total": 81, "tokens_per_s": 164.03525097375712, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 351, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553857.9535286, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.845999999815831, "prefill_cuda_event_ms": 4.804031848907471, "kv_decode_ms": 259.71499999832304, "kv_decode_cuda_event_ms": 259.6843566894531, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.845999999815831, "cuda_event_ms": 4.804031848907471, "tokens_total": 17, "tokens_per_s": 3508.0478746690205, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 352, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553857.9535286, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.845999999815831, "prefill_cuda_event_ms": 4.804031848907471, "kv_decode_ms": 259.71499999832304, "kv_decode_cuda_event_ms": 259.6843566894531, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 259.71499999832304, "cuda_event_ms": 259.6843566894531, "tokens_total": 64, "tokens_per_s": 246.42396473216118, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 117, "sample_idx": 353, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553857.9535286, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.845999999815831, "prefill_cuda_event_ms": 4.804031848907471, "kv_decode_ms": 259.71499999832304, "kv_decode_cuda_event_ms": 259.6843566894531, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 264.56099999813887, "cuda_event_ms": 264.4883885383606, "tokens_total": 81, "tokens_per_s": 306.16757572193114, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 354, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553858.2188265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.0526000013633166, "prefill_cuda_event_ms": 5.006336212158203, "kv_decode_ms": 286.73300000082236, "kv_decode_cuda_event_ms": 286.70361328125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 5.0526000013633166, "cuda_event_ms": 5.006336212158203, "tokens_total": 17, "tokens_per_s": 3364.604361202743, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 355, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553858.2188265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.0526000013633166, "prefill_cuda_event_ms": 5.006336212158203, "kv_decode_ms": 286.73300000082236, "kv_decode_cuda_event_ms": 286.70361328125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 286.73300000082236, "cuda_event_ms": 286.70361328125, "tokens_total": 64, "tokens_per_s": 223.2041655470994, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 118, "sample_idx": 356, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553858.2188265, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 5.0526000013633166, "prefill_cuda_event_ms": 5.006336212158203, "kv_decode_ms": 286.73300000082236, "kv_decode_cuda_event_ms": 286.70361328125, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 291.7856000021857, "cuda_event_ms": 291.7099494934082, "tokens_total": 81, "tokens_per_s": 277.6010879200113, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 357, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553858.5114293, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.987700000128825, "prefill_cuda_event_ms": 4.936160087585449, "kv_decode_ms": 261.6260999984661, "kv_decode_cuda_event_ms": 261.6002502441406, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 4.987700000128825, "cuda_event_ms": 4.936160087585449, "tokens_total": 17, "tokens_per_s": 3408.38462609237, "gen_tokens": 0}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 358, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553858.5114293, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.987700000128825, "prefill_cuda_event_ms": 4.936160087585449, "kv_decode_ms": 261.6260999984661, "kv_decode_cuda_event_ms": 261.6002502441406, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 261.6260999984661, "cuda_event_ms": 261.6002502441406, "tokens_total": 64, "tokens_per_s": 244.62391175947366, "gen_tokens": 64}
{"task_idx": 29, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-100m", "model_kind": "hf", "params_millions": 96.088, "params_millions_config": 96.088, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 119, "sample_idx": 359, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553858.5114293, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.987700000128825, "prefill_cuda_event_ms": 4.936160087585449, "kv_decode_ms": 261.6260999984661, "kv_decode_cuda_event_ms": 261.6002502441406, "gpu_peak_mb": 633.97998046875, "params_millions_measured": 96.08832, "latency_ms": 266.6137999985949, "cuda_event_ms": 266.5364103317261, "tokens_total": 81, "tokens_per_s": 303.8102303797736, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 120, "sample_idx": 360, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553858.7786942, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 2228.1392, "prefill_cuda_event_ms": null, "kv_decode_ms": 1467.07, "kv_decode_ms_equiv": 2470.854736842105, "kv_decode_ms_per_token": 38.60710526315789, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 38, "gen_tokens_equiv": 64, "ollama_wall_ms": 16049.775999999838, "ollama_total_duration_ms": 15948.0496, "ollama_load_ms": 12135.2847, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2228.1392, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 37.69961948517399, "gen_tokens": 0}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 120, "sample_idx": 361, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553858.7786942, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 2228.1392, "prefill_cuda_event_ms": null, "kv_decode_ms": 1467.07, "kv_decode_ms_equiv": 2470.854736842105, "kv_decode_ms_per_token": 38.60710526315789, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 38, "gen_tokens_equiv": 64, "ollama_wall_ms": 16049.775999999838, "ollama_total_duration_ms": 15948.0496, "ollama_load_ms": 12135.2847, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2470.854736842105, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.90196786792723, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 120, "sample_idx": 362, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553858.7786942, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 2228.1392, "prefill_cuda_event_ms": null, "kv_decode_ms": 1467.07, "kv_decode_ms_equiv": 2470.854736842105, "kv_decode_ms_per_token": 38.60710526315789, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 38, "gen_tokens_equiv": 64, "ollama_wall_ms": 16049.775999999838, "ollama_total_duration_ms": 15948.0496, "ollama_load_ms": 12135.2847, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 4698.9939368421055, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 31.496103631804505, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 121, "sample_idx": 363, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553874.8291821, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 47.8134, "prefill_cuda_event_ms": null, "kv_decode_ms": 1308.4239, "kv_decode_ms_equiv": 2462.9155764705883, "kv_decode_ms_per_token": 38.48305588235294, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1601.6169000013178, "ollama_total_duration_ms": 1580.5543, "ollama_load_ms": 217.9424, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 47.8134, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1756.8296753629736, "gen_tokens": 0}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 121, "sample_idx": 364, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553874.8291821, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 47.8134, "prefill_cuda_event_ms": null, "kv_decode_ms": 1308.4239, "kv_decode_ms_equiv": 2462.9155764705883, "kv_decode_ms_per_token": 38.48305588235294, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1601.6169000013178, "ollama_total_duration_ms": 1580.5543, "ollama_load_ms": 217.9424, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2462.9155764705883, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 25.985462356656736, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 121, "sample_idx": 365, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553874.8291821, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 47.8134, "prefill_cuda_event_ms": null, "kv_decode_ms": 1308.4239, "kv_decode_ms_equiv": 2462.9155764705883, "kv_decode_ms_per_token": 38.48305588235294, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1601.6169000013178, "ollama_total_duration_ms": 1580.5543, "ollama_load_ms": 217.9424, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2510.7289764705883, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 58.947023508705556, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 122, "sample_idx": 366, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553876.431447, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 48.1459, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.7254, "kv_decode_ms_equiv": 2664.8948705882353, "kv_decode_ms_per_token": 41.63898235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1706.5332000001945, "ollama_total_duration_ms": 1684.4373, "ollama_load_ms": 217.0835, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 48.1459, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1744.6968485374664, "gen_tokens": 0}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 122, "sample_idx": 367, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553876.431447, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 48.1459, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.7254, "kv_decode_ms_equiv": 2664.8948705882353, "kv_decode_ms_per_token": 41.63898235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1706.5332000001945, "ollama_total_duration_ms": 1684.4373, "ollama_load_ms": 217.0835, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2664.8948705882353, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.01595676675717, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 122, "sample_idx": 368, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553876.431447, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 48.1459, "prefill_cuda_event_ms": null, "kv_decode_ms": 1415.7254, "kv_decode_ms_equiv": 2664.8948705882353, "kv_decode_ms_per_token": 41.63898235294118, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1706.5332000001945, "ollama_total_duration_ms": 1684.4373, "ollama_load_ms": 217.0835, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2713.0407705882353, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 54.551336494626646, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 123, "sample_idx": 369, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553878.1381683, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 54.0194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.7629, "kv_decode_ms_equiv": 2845.6713411764704, "kv_decode_ms_per_token": 44.46361470588235, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1819.708900000478, "ollama_total_duration_ms": 1793.959, "ollama_load_ms": 224.3706, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 54.0194, "cuda_event_ms": null, "tokens_total": 84, "tokens_per_s": 1554.996908518051, "gen_tokens": 0}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 123, "sample_idx": 370, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553878.1381683, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 54.0194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.7629, "kv_decode_ms_equiv": 2845.6713411764704, "kv_decode_ms_per_token": 44.46361470588235, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1819.708900000478, "ollama_total_duration_ms": 1793.959, "ollama_load_ms": 224.3706, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2845.6713411764704, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 22.490299239384697, "gen_tokens": 64}
{"task_idx": 30, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 123, "sample_idx": 371, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553878.1381683, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 84, "prefill_ms": 54.0194, "prefill_cuda_event_ms": null, "kv_decode_ms": 1511.7629, "kv_decode_ms_equiv": 2845.6713411764704, "kv_decode_ms_per_token": 44.46361470588235, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 1819.708900000478, "ollama_total_duration_ms": 1793.959, "ollama_load_ms": 224.3706, "ollama_done_reason": "stop", "params_millions_measured": 20900.0, "latency_ms": 2899.6907411764705, "cuda_event_ms": null, "tokens_total": 148, "tokens_per_s": 51.039925705992026, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 124, "sample_idx": 372, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553879.958301, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33619999907387, "prefill_cuda_event_ms": 10.21782398223877, "kv_decode_ms": 380.7937999990827, "kv_decode_cuda_event_ms": 380.0924072265625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 10.33619999907387, "cuda_event_ms": 10.21782398223877, "tokens_total": 9, "tokens_per_s": 870.7261857168404, "gen_tokens": 0}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 124, "sample_idx": 373, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553879.958301, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33619999907387, "prefill_cuda_event_ms": 10.21782398223877, "kv_decode_ms": 380.7937999990827, "kv_decode_cuda_event_ms": 380.0924072265625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 380.7937999990827, "cuda_event_ms": 380.0924072265625, "tokens_total": 64, "tokens_per_s": 168.06996332438757, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 124, "sample_idx": 374, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553879.958301, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 10.33619999907387, "prefill_cuda_event_ms": 10.21782398223877, "kv_decode_ms": 380.7937999990827, "kv_decode_cuda_event_ms": 380.0924072265625, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 391.12999999815656, "cuda_event_ms": 390.31023120880127, "tokens_total": 73, "tokens_per_s": 186.63871347210406, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 125, "sample_idx": 375, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553880.3545094, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.002300000953255, "prefill_cuda_event_ms": 7.9358720779418945, "kv_decode_ms": 391.9616000002861, "kv_decode_cuda_event_ms": 391.9032287597656, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 8.002300000953255, "cuda_event_ms": 7.9358720779418945, "tokens_total": 9, "tokens_per_s": 1124.6766553275804, "gen_tokens": 0}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 125, "sample_idx": 376, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553880.3545094, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.002300000953255, "prefill_cuda_event_ms": 7.9358720779418945, "kv_decode_ms": 391.9616000002861, "kv_decode_cuda_event_ms": 391.9032287597656, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 391.9616000002861, "cuda_event_ms": 391.9032287597656, "tokens_total": 64, "tokens_per_s": 163.28130102528738, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 125, "sample_idx": 377, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553880.3545094, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 8.002300000953255, "prefill_cuda_event_ms": 7.9358720779418945, "kv_decode_ms": 391.9616000002861, "kv_decode_cuda_event_ms": 391.9032287597656, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 399.9639000012394, "cuda_event_ms": 399.8391008377075, "tokens_total": 73, "tokens_per_s": 182.5164721110425, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 126, "sample_idx": 378, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553880.7559335, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.341800000882358, "prefill_cuda_event_ms": 6.2600321769714355, "kv_decode_ms": 381.2778999999864, "kv_decode_cuda_event_ms": 381.2341613769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 6.341800000882358, "cuda_event_ms": 6.2600321769714355, "tokens_total": 9, "tokens_per_s": 1419.1554446289376, "gen_tokens": 0}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 126, "sample_idx": 379, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553880.7559335, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.341800000882358, "prefill_cuda_event_ms": 6.2600321769714355, "kv_decode_ms": 381.2778999999864, "kv_decode_cuda_event_ms": 381.2341613769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 381.2778999999864, "cuda_event_ms": 381.2341613769531, "tokens_total": 64, "tokens_per_s": 167.85656866029288, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 126, "sample_idx": 380, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553880.7559335, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.341800000882358, "prefill_cuda_event_ms": 6.2600321769714355, "kv_decode_ms": 381.2778999999864, "kv_decode_cuda_event_ms": 381.2341613769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 387.61970000086876, "cuda_event_ms": 387.49419355392456, "tokens_total": 73, "tokens_per_s": 188.32892136244982, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 127, "sample_idx": 381, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553881.1447525, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.638144016265869, "kv_decode_ms": 378.40760000108276, "kv_decode_cuda_event_ms": 378.3669738769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 6.702200000290759, "cuda_event_ms": 6.638144016265869, "tokens_total": 9, "tokens_per_s": 1342.842648624266, "gen_tokens": 0}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 127, "sample_idx": 382, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553881.1447525, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.638144016265869, "kv_decode_ms": 378.40760000108276, "kv_decode_cuda_event_ms": 378.3669738769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 378.40760000108276, "cuda_event_ms": 378.3669738769531, "tokens_total": 64, "tokens_per_s": 169.12979548988147, "gen_tokens": 64}
{"task_idx": 31, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 127, "sample_idx": 383, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553881.1447525, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 6.702200000290759, "prefill_cuda_event_ms": 6.638144016265869, "kv_decode_ms": 378.40760000108276, "kv_decode_cuda_event_ms": 378.3669738769531, "gpu_peak_mb": 632.3837890625, "params_millions_measured": 74.824704, "latency_ms": 385.1098000013735, "cuda_event_ms": 385.005117893219, "tokens_total": 73, "tokens_per_s": 189.55632912935386, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 128, "sample_idx": 384, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553881.5307913, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 521.4259000003949, "prefill_cuda_event_ms": null, "kv_decode_ms": 1132.9650999996375, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.63460000080522, "params_millions_measured": 74.824704, "latency_ms": 521.4259000003949, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 32.602906760072955, "gen_tokens": 0}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 128, "sample_idx": 385, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553881.5307913, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 521.4259000003949, "prefill_cuda_event_ms": null, "kv_decode_ms": 1132.9650999996375, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.63460000080522, "params_millions_measured": 74.824704, "latency_ms": 1132.9650999996375, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 56.48894215719485, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 128, "sample_idx": 386, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553881.5307913, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 521.4259000003949, "prefill_cuda_event_ms": null, "kv_decode_ms": 1132.9650999996375, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "hf_load_ms": 178.63460000080522, "params_millions_measured": 74.824704, "latency_ms": 1654.3910000000324, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 48.960614510111824, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 129, "sample_idx": 387, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553883.364921, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.416700000074343, "prefill_cuda_event_ms": null, "kv_decode_ms": 1045.9480000008625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 24.416700000074343, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 696.2447832814524, "gen_tokens": 0}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 129, "sample_idx": 388, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553883.364921, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.416700000074343, "prefill_cuda_event_ms": null, "kv_decode_ms": 1045.9480000008625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1045.9480000008625, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.1885103274228, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 129, "sample_idx": 389, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553883.364921, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 24.416700000074343, "prefill_cuda_event_ms": null, "kv_decode_ms": 1045.9480000008625, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1070.3647000009369, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 75.67514137931595, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 130, "sample_idx": 390, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553884.4359837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.485200000024633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.818600000217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 26.485200000024633, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 641.8679111346786, "gen_tokens": 0}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 130, "sample_idx": 391, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553884.4359837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.485200000024633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.818600000217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1039.818600000217, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 61.54919713879579, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 130, "sample_idx": 392, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553884.4359837, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.485200000024633, "prefill_cuda_event_ms": null, "kv_decode_ms": 1039.818600000217, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1066.3038000002416, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 75.96334177931435, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 131, "sample_idx": 393, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553885.5029376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.330100001156097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.4671000011003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 25.330100001156097, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 671.1382899879629, "gen_tokens": 0}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 131, "sample_idx": 394, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553885.5029376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.330100001156097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.4671000011003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1056.4671000011003, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 60.57926460741971, "gen_tokens": 64}
{"task_idx": 32, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 131, "sample_idx": 395, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553885.5029376, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 25.330100001156097, "prefill_cuda_event_ms": null, "kv_decode_ms": 1056.4671000011003, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1081.7972000022564, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 74.87540178494736, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 132, "sample_idx": 396, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553886.5856023, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.74320000084117, "prefill_cuda_event_ms": 12.631135940551758, "kv_decode_ms": 427.9640000004292, "kv_decode_cuda_event_ms": 427.8425598144531, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 12.74320000084117, "cuda_event_ms": 12.631135940551758, "tokens_total": 9, "tokens_per_s": 706.2590243742479, "gen_tokens": 0}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 132, "sample_idx": 397, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553886.5856023, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.74320000084117, "prefill_cuda_event_ms": 12.631135940551758, "kv_decode_ms": 427.9640000004292, "kv_decode_cuda_event_ms": 427.8425598144531, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 427.9640000004292, "cuda_event_ms": 427.8425598144531, "tokens_total": 64, "tokens_per_s": 149.54528885592202, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 132, "sample_idx": 398, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553886.5856023, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 12.74320000084117, "prefill_cuda_event_ms": 12.631135940551758, "kv_decode_ms": 427.9640000004292, "kv_decode_cuda_event_ms": 427.8425598144531, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 440.7072000012704, "cuda_event_ms": 440.4736957550049, "tokens_total": 73, "tokens_per_s": 165.64285766102657, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 133, "sample_idx": 399, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553887.030611, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.445000000487198, "prefill_cuda_event_ms": 4.409599781036377, "kv_decode_ms": 269.4028000005346, "kv_decode_cuda_event_ms": 269.3785705566406, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 4.445000000487198, "cuda_event_ms": 4.409599781036377, "tokens_total": 9, "tokens_per_s": 2024.7469064147463, "gen_tokens": 0}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 133, "sample_idx": 400, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553887.030611, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.445000000487198, "prefill_cuda_event_ms": 4.409599781036377, "kv_decode_ms": 269.4028000005346, "kv_decode_cuda_event_ms": 269.3785705566406, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 269.4028000005346, "cuda_event_ms": 269.3785705566406, "tokens_total": 64, "tokens_per_s": 237.56249007015887, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 133, "sample_idx": 401, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553887.030611, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.445000000487198, "prefill_cuda_event_ms": 4.409599781036377, "kv_decode_ms": 269.4028000005346, "kv_decode_cuda_event_ms": 269.3785705566406, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 273.8478000010218, "cuda_event_ms": 273.788170337677, "tokens_total": 73, "tokens_per_s": 266.57143128309815, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 134, "sample_idx": 402, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553887.305097, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.629400000339956, "prefill_cuda_event_ms": 4.592832088470459, "kv_decode_ms": 259.7946000005322, "kv_decode_cuda_event_ms": 259.7652587890625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 4.629400000339956, "cuda_event_ms": 4.592832088470459, "tokens_total": 9, "tokens_per_s": 1944.0964270400252, "gen_tokens": 0}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 134, "sample_idx": 403, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553887.305097, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.629400000339956, "prefill_cuda_event_ms": 4.592832088470459, "kv_decode_ms": 259.7946000005322, "kv_decode_cuda_event_ms": 259.7652587890625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 259.7946000005322, "cuda_event_ms": 259.7652587890625, "tokens_total": 64, "tokens_per_s": 246.3484614378778, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 134, "sample_idx": 404, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553887.305097, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.629400000339956, "prefill_cuda_event_ms": 4.592832088470459, "kv_decode_ms": 259.7946000005322, "kv_decode_cuda_event_ms": 259.7652587890625, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 264.4240000008722, "cuda_event_ms": 264.35809087753296, "tokens_total": 73, "tokens_per_s": 276.0717635303876, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 135, "sample_idx": 405, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553887.570135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.113799999890034, "prefill_cuda_event_ms": 5.057600021362305, "kv_decode_ms": 266.0378000000492, "kv_decode_cuda_event_ms": 265.97784423828125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 5.113799999890034, "cuda_event_ms": 5.057600021362305, "tokens_total": 9, "tokens_per_s": 1759.9436818400277, "gen_tokens": 0}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 135, "sample_idx": 406, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553887.570135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.113799999890034, "prefill_cuda_event_ms": 5.057600021362305, "kv_decode_ms": 266.0378000000492, "kv_decode_cuda_event_ms": 265.97784423828125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 266.0378000000492, "cuda_event_ms": 265.97784423828125, "tokens_total": 64, "tokens_per_s": 240.5673178773399, "gen_tokens": 64}
{"task_idx": 33, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 135, "sample_idx": 407, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553887.570135, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.113799999890034, "prefill_cuda_event_ms": 5.057600021362305, "kv_decode_ms": 266.0378000000492, "kv_decode_cuda_event_ms": 265.97784423828125, "gpu_peak_mb": 632.44970703125, "params_millions_measured": 51.475968, "latency_ms": 271.1515999999392, "cuda_event_ms": 271.03544425964355, "tokens_total": 73, "tokens_per_s": 269.2220883078557, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 136, "sample_idx": 408, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553887.842162, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.670500000051106, "prefill_cuda_event_ms": null, "kv_decode_ms": 177.95189999924332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 20.670500000051106, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 48.378123412473215, "gen_tokens": 0}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 136, "sample_idx": 409, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553887.842162, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.670500000051106, "prefill_cuda_event_ms": null, "kv_decode_ms": 177.95189999924332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 177.95189999924332, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 359.64774751082814, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 136, "sample_idx": 410, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553887.842162, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.670500000051106, "prefill_cuda_event_ms": null, "kv_decode_ms": 177.95189999924332, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 198.62239999929443, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 327.25412642396276, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 137, "sample_idx": 411, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553888.042086, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.1980000001349254, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.8352000003215, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.1980000001349254, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 454.9590536572404, "gen_tokens": 0}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 137, "sample_idx": 412, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553888.042086, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.1980000001349254, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.8352000003215, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 178.8352000003215, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 357.87138102501603, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 137, "sample_idx": 413, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553888.042086, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.1980000001349254, "prefill_cuda_event_ms": null, "kv_decode_ms": 178.8352000003215, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 181.03320000045642, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 359.05016317358434, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 138, "sample_idx": 414, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553888.2235854, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.158600000257138, "prefill_cuda_event_ms": null, "kv_decode_ms": 187.05770000087796, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.158600000257138, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 463.26322610992185, "gen_tokens": 0}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 138, "sample_idx": 415, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553888.2235854, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.158600000257138, "prefill_cuda_event_ms": null, "kv_decode_ms": 187.05770000087796, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 187.05770000087796, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 342.14041977261354, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 138, "sample_idx": 416, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553888.2235854, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.158600000257138, "prefill_cuda_event_ms": null, "kv_decode_ms": 187.05770000087796, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 189.2163000011351, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 343.52220183784414, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 139, "sample_idx": 417, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553888.4132462, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.564399999755551, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.45359999996435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 2.564399999755551, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 389.9547652844034, "gen_tokens": 0}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 139, "sample_idx": 418, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553888.4132462, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.564399999755551, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.45359999996435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 180.45359999996435, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 354.6618077999699, "gen_tokens": 64}
{"task_idx": 34, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 139, "sample_idx": 419, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553888.4132462, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.564399999755551, "prefill_cuda_event_ms": null, "kv_decode_ms": 180.45359999996435, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 183.0179999997199, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 355.1563234222835, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 140, "sample_idx": 420, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553888.5967362, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 305.1911, "prefill_cuda_event_ms": null, "kv_decode_ms": 2623.2135, "kv_decode_ms_equiv": 2623.2135, "kv_decode_ms_per_token": 40.9877109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3191.608699999051, "ollama_total_duration_ms": 3188.8542, "ollama_load_ms": 243.7476, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 305.1911, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 324.38691691861266, "gen_tokens": 0}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 140, "sample_idx": 421, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553888.5967362, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 305.1911, "prefill_cuda_event_ms": null, "kv_decode_ms": 2623.2135, "kv_decode_ms_equiv": 2623.2135, "kv_decode_ms_per_token": 40.9877109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3191.608699999051, "ollama_total_duration_ms": 3188.8542, "ollama_load_ms": 243.7476, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2623.2135, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.397556660942772, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 140, "sample_idx": 422, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553888.5967362, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 305.1911, "prefill_cuda_event_ms": null, "kv_decode_ms": 2623.2135, "kv_decode_ms_equiv": 2623.2135, "kv_decode_ms_per_token": 40.9877109375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3191.608699999051, "ollama_total_duration_ms": 3188.8542, "ollama_load_ms": 243.7476, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2928.4046, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 55.6617074020441, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 141, "sample_idx": 423, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553891.7885754, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.7666, "prefill_cuda_event_ms": null, "kv_decode_ms": 2898.6608, "kv_decode_ms_equiv": 2898.6608, "kv_decode_ms_per_token": 45.291575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3236.3400000012916, "ollama_total_duration_ms": 3233.4021, "ollama_load_ms": 269.4159, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 48.7666, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 2030.077963196122, "gen_tokens": 0}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 141, "sample_idx": 424, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553891.7885754, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.7666, "prefill_cuda_event_ms": null, "kv_decode_ms": 2898.6608, "kv_decode_ms_equiv": 2898.6608, "kv_decode_ms_per_token": 45.291575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3236.3400000012916, "ollama_total_duration_ms": 3233.4021, "ollama_load_ms": 269.4159, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2898.6608, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 22.07916152176205, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 141, "sample_idx": 425, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553891.7885754, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 48.7666, "prefill_cuda_event_ms": null, "kv_decode_ms": 2898.6608, "kv_decode_ms_equiv": 2898.6608, "kv_decode_ms_per_token": 45.291575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3236.3400000012916, "ollama_total_duration_ms": 3233.4021, "ollama_load_ms": 269.4159, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2947.4274, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 55.30246478674928, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 142, "sample_idx": 426, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553895.0252466, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.6637, "prefill_cuda_event_ms": null, "kv_decode_ms": 3147.1232, "kv_decode_ms_equiv": 3147.1232, "kv_decode_ms_per_token": 49.1738, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3440.34659999852, "ollama_total_duration_ms": 3436.7203, "ollama_load_ms": 222.8294, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 53.6637, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1844.8224777642988, "gen_tokens": 0}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 142, "sample_idx": 427, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553895.0252466, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.6637, "prefill_cuda_event_ms": null, "kv_decode_ms": 3147.1232, "kv_decode_ms_equiv": 3147.1232, "kv_decode_ms_per_token": 49.1738, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3440.34659999852, "ollama_total_duration_ms": 3436.7203, "ollama_load_ms": 222.8294, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3147.1232, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.33603260272747, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 142, "sample_idx": 428, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553895.0252466, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 53.6637, "prefill_cuda_event_ms": null, "kv_decode_ms": 3147.1232, "kv_decode_ms_equiv": 3147.1232, "kv_decode_ms_per_token": 49.1738, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3440.34659999852, "ollama_total_duration_ms": 3436.7203, "ollama_load_ms": 222.8294, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3200.7869, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 50.924977229818076, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 143, "sample_idx": 429, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553898.4658227, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.7952, "prefill_cuda_event_ms": null, "kv_decode_ms": 3110.6182, "kv_decode_ms_equiv": 3110.6182, "kv_decode_ms_per_token": 48.603409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3428.8708999993105, "ollama_total_duration_ms": 3401.696, "ollama_load_ms": 226.5253, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 55.7952, "cuda_event_ms": null, "tokens_total": 99, "tokens_per_s": 1774.3461803165862, "gen_tokens": 0}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 143, "sample_idx": 430, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553898.4658227, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.7952, "prefill_cuda_event_ms": null, "kv_decode_ms": 3110.6182, "kv_decode_ms_equiv": 3110.6182, "kv_decode_ms_per_token": 48.603409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3428.8708999993105, "ollama_total_duration_ms": 3401.696, "ollama_load_ms": 226.5253, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3110.6182, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 20.574688336871432, "gen_tokens": 64}
{"task_idx": 35, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 143, "sample_idx": 431, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553898.4658227, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 99, "prefill_ms": 55.7952, "prefill_cuda_event_ms": null, "kv_decode_ms": 3110.6182, "kv_decode_ms_equiv": 3110.6182, "kv_decode_ms_per_token": 48.603409375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3428.8708999993105, "ollama_total_duration_ms": 3401.696, "ollama_load_ms": 226.5253, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3166.4134, "cuda_event_ms": null, "tokens_total": 163, "tokens_per_s": 51.477801350891205, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 144, "sample_idx": 432, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553901.8949084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 73.99860000077751, "prefill_cuda_event_ms": 73.90278625488281, "kv_decode_ms": 251.05729999995674, "kv_decode_cuda_event_ms": 251.01107788085938, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 73.99860000077751, "cuda_event_ms": 73.90278625488281, "tokens_total": 9, "tokens_per_s": 121.62392261347425, "gen_tokens": 0}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 144, "sample_idx": 433, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553901.8949084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 73.99860000077751, "prefill_cuda_event_ms": 73.90278625488281, "kv_decode_ms": 251.05729999995674, "kv_decode_cuda_event_ms": 251.01107788085938, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 251.05729999995674, "cuda_event_ms": 251.01107788085938, "tokens_total": 64, "tokens_per_s": 254.92188436668056, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 144, "sample_idx": 434, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553901.8949084, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 73.99860000077751, "prefill_cuda_event_ms": 73.90278625488281, "kv_decode_ms": 251.05729999995674, "kv_decode_cuda_event_ms": 251.01107788085938, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 325.05590000073425, "cuda_event_ms": 324.9138641357422, "tokens_total": 73, "tokens_per_s": 224.57675741260226, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 145, "sample_idx": 435, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553902.225976, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.863799998725881, "prefill_cuda_event_ms": 3.810688018798828, "kv_decode_ms": 223.09539999878325, "kv_decode_cuda_event_ms": 223.0179901123047, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 3.863799998725881, "cuda_event_ms": 3.810688018798828, "tokens_total": 9, "tokens_per_s": 2329.3131122128016, "gen_tokens": 0}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 145, "sample_idx": 436, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553902.225976, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.863799998725881, "prefill_cuda_event_ms": 3.810688018798828, "kv_decode_ms": 223.09539999878325, "kv_decode_cuda_event_ms": 223.0179901123047, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 223.09539999878325, "cuda_event_ms": 223.0179901123047, "tokens_total": 64, "tokens_per_s": 286.8727907448968, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 145, "sample_idx": 437, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553902.225976, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 3.863799998725881, "prefill_cuda_event_ms": 3.810688018798828, "kv_decode_ms": 223.09539999878325, "kv_decode_cuda_event_ms": 223.0179901123047, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 226.95919999750913, "cuda_event_ms": 226.82867813110352, "tokens_total": 73, "tokens_per_s": 321.6437139397794, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 146, "sample_idx": 438, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553902.4543567, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.180399999517249, "prefill_cuda_event_ms": 5.114175796508789, "kv_decode_ms": 229.38350000003993, "kv_decode_cuda_event_ms": 229.30841064453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 5.180399999517249, "cuda_event_ms": 5.114175796508789, "tokens_total": 9, "tokens_per_s": 1737.3175818158234, "gen_tokens": 0}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 146, "sample_idx": 439, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553902.4543567, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.180399999517249, "prefill_cuda_event_ms": 5.114175796508789, "kv_decode_ms": 229.38350000003993, "kv_decode_cuda_event_ms": 229.30841064453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 229.38350000003993, "cuda_event_ms": 229.30841064453125, "tokens_total": 64, "tokens_per_s": 279.00873428118786, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 146, "sample_idx": 440, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553902.4543567, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 5.180399999517249, "prefill_cuda_event_ms": 5.114175796508789, "kv_decode_ms": 229.38350000003993, "kv_decode_cuda_event_ms": 229.30841064453125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 234.56389999955718, "cuda_event_ms": 234.42258644104004, "tokens_total": 73, "tokens_per_s": 311.21583500333094, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 147, "sample_idx": 441, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553902.6899498, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.807100000107312, "prefill_cuda_event_ms": 4.737120151519775, "kv_decode_ms": 229.50540000056208, "kv_decode_cuda_event_ms": 229.45782470703125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 4.807100000107312, "cuda_event_ms": 4.737120151519775, "tokens_total": 9, "tokens_per_s": 1872.2306587753715, "gen_tokens": 0}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 147, "sample_idx": 442, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553902.6899498, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.807100000107312, "prefill_cuda_event_ms": 4.737120151519775, "kv_decode_ms": 229.50540000056208, "kv_decode_cuda_event_ms": 229.45782470703125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 229.50540000056208, "cuda_event_ms": 229.45782470703125, "tokens_total": 64, "tokens_per_s": 278.86054097133774, "gen_tokens": 64}
{"task_idx": 36, "scenario": "short", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 147, "sample_idx": 443, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553902.6899498, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 4.807100000107312, "prefill_cuda_event_ms": 4.737120151519775, "kv_decode_ms": 229.50540000056208, "kv_decode_cuda_event_ms": 229.45782470703125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 0.102714, "latency_ms": 234.3125000006694, "cuda_event_ms": 234.19494485855103, "tokens_total": 73, "tokens_per_s": 311.54974659820306, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 148, "sample_idx": 444, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553902.9259162, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.953299999120645, "prefill_cuda_event_ms": null, "kv_decode_ms": 162.79369999938353, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 8.953299999120645, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 1898.7412464308877, "gen_tokens": 0}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 148, "sample_idx": 445, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553902.9259162, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.953299999120645, "prefill_cuda_event_ms": null, "kv_decode_ms": 162.79369999938353, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 162.79369999938353, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 393.1356066005156, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 148, "sample_idx": 446, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553902.9259162, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 8.953299999120645, "prefill_cuda_event_ms": null, "kv_decode_ms": 162.79369999938353, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 171.74699999850418, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 471.62395850119924, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 149, "sample_idx": 447, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553903.1029804, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.9540999989403645, "prefill_cuda_event_ms": null, "kv_decode_ms": 165.83519999949203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.9540999989403645, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3431.501181574078, "gen_tokens": 0}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 149, "sample_idx": 448, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553903.1029804, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.9540999989403645, "prefill_cuda_event_ms": null, "kv_decode_ms": 165.83519999949203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 165.83519999949203, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 385.9253041585625, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 149, "sample_idx": 449, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553903.1029804, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.9540999989403645, "prefill_cuda_event_ms": null, "kv_decode_ms": 165.83519999949203, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 170.7892999984324, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 474.26858708797016, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 150, "sample_idx": 450, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553903.2745528, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.4903000014310237, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.86169999896083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 3.4903000014310237, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 4870.641490138382, "gen_tokens": 0}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 150, "sample_idx": 451, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553903.2745528, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.4903000014310237, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.86169999896083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 163.86169999896083, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 390.57327002225577, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 150, "sample_idx": 452, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553903.2745528, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 3.4903000014310237, "prefill_cuda_event_ms": null, "kv_decode_ms": 163.86169999896083, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 167.35200000039185, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 484.00975189905313, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 151, "sample_idx": 453, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553903.442776, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.706399999122368, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.50080000059097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 4.706399999122368, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 3612.1026693800136, "gen_tokens": 0}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 151, "sample_idx": 454, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553903.442776, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.706399999122368, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.50080000059097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 160.50080000059097, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 398.7519065310849, "gen_tokens": 64}
{"task_idx": 37, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 151, "sample_idx": 455, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553903.442776, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.706399999122368, "prefill_cuda_event_ms": null, "kv_decode_ms": 160.50080000059097, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 0.102714, "latency_ms": 165.20719999971334, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 490.2934012569703, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 152, "sample_idx": 456, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553903.6086605, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 25.25709999827086, "prefill_cuda_event_ms": 25.140064239501953, "kv_decode_ms": 206.70700000118813, "kv_decode_cuda_event_ms": 206.62579345703125, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 25.25709999827086, "cuda_event_ms": 25.140064239501953, "tokens_total": 1, "tokens_per_s": 39.59282736610543, "gen_tokens": 0}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 152, "sample_idx": 457, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553903.6086605, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 25.25709999827086, "prefill_cuda_event_ms": 25.140064239501953, "kv_decode_ms": 206.70700000118813, "kv_decode_cuda_event_ms": 206.62579345703125, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 206.70700000118813, "cuda_event_ms": 206.62579345703125, "tokens_total": 64, "tokens_per_s": 309.6169941009842, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 152, "sample_idx": 458, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553903.6086605, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 25.25709999827086, "prefill_cuda_event_ms": 25.140064239501953, "kv_decode_ms": 206.70700000118813, "kv_decode_cuda_event_ms": 206.62579345703125, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 231.964099999459, "cuda_event_ms": 231.7658576965332, "tokens_total": 65, "tokens_per_s": 280.2157747692492, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 153, "sample_idx": 459, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553903.842474, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8575999988097465, "prefill_cuda_event_ms": 3.783519983291626, "kv_decode_ms": 228.01239999898826, "kv_decode_cuda_event_ms": 227.97415161132812, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 3.8575999988097465, "cuda_event_ms": 3.783519983291626, "tokens_total": 1, "tokens_per_s": 259.22853595721375, "gen_tokens": 0}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 153, "sample_idx": 460, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553903.842474, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8575999988097465, "prefill_cuda_event_ms": 3.783519983291626, "kv_decode_ms": 228.01239999898826, "kv_decode_cuda_event_ms": 227.97415161132812, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 228.01239999898826, "cuda_event_ms": 227.97415161132812, "tokens_total": 64, "tokens_per_s": 280.68648898166936, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 153, "sample_idx": 461, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553903.842474, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 3.8575999988097465, "prefill_cuda_event_ms": 3.783519983291626, "kv_decode_ms": 228.01239999898826, "kv_decode_cuda_event_ms": 227.97415161132812, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 231.869999997798, "cuda_event_ms": 231.75767159461975, "tokens_total": 65, "tokens_per_s": 280.3294949782951, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 154, "sample_idx": 462, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553904.0753615, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.776499999934458, "prefill_cuda_event_ms": 4.682559967041016, "kv_decode_ms": 194.51929999922868, "kv_decode_cuda_event_ms": 194.47398376464844, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 4.776499999934458, "cuda_event_ms": 4.682559967041016, "tokens_total": 1, "tokens_per_s": 209.358316762006, "gen_tokens": 0}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 154, "sample_idx": 463, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553904.0753615, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.776499999934458, "prefill_cuda_event_ms": 4.682559967041016, "kv_decode_ms": 194.51929999922868, "kv_decode_cuda_event_ms": 194.47398376464844, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 194.51929999922868, "cuda_event_ms": 194.47398376464844, "tokens_total": 64, "tokens_per_s": 329.0161953094309, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 154, "sample_idx": 464, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553904.0753615, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.776499999934458, "prefill_cuda_event_ms": 4.682559967041016, "kv_decode_ms": 194.51929999922868, "kv_decode_cuda_event_ms": 194.47398376464844, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 199.29579999916314, "cuda_event_ms": 199.15654373168945, "tokens_total": 65, "tokens_per_s": 326.14836840652407, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 155, "sample_idx": 465, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553904.2757785, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.6777000002766727, "prefill_cuda_event_ms": 2.5748798847198486, "kv_decode_ms": 211.69079999890528, "kv_decode_cuda_event_ms": 211.6485137939453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 2.6777000002766727, "cuda_event_ms": 2.5748798847198486, "tokens_total": 1, "tokens_per_s": 373.4548305996472, "gen_tokens": 0}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 155, "sample_idx": 466, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553904.2757785, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.6777000002766727, "prefill_cuda_event_ms": 2.5748798847198486, "kv_decode_ms": 211.69079999890528, "kv_decode_cuda_event_ms": 211.6485137939453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 211.69079999890528, "cuda_event_ms": 211.6485137939453, "tokens_total": 64, "tokens_per_s": 302.3277346031616, "gen_tokens": 64}
{"task_idx": 38, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/tiny-gpt2", "model_kind": "hf", "params_millions": 0.103, "params_millions_config": 0.103, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 155, "sample_idx": 467, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553904.2757785, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 2.6777000002766727, "prefill_cuda_event_ms": 2.5748798847198486, "kv_decode_ms": 211.69079999890528, "kv_decode_cuda_event_ms": 211.6485137939453, "gpu_peak_mb": 630.53759765625, "params_millions_measured": 0.102714, "latency_ms": 214.36849999918195, "cuda_event_ms": 214.22339367866516, "tokens_total": 65, "tokens_per_s": 303.21619081277356, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 156, "sample_idx": 468, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553904.4912844, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.092099999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 1295.294500001546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 37.092099999426864, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 458.3186177181308, "gen_tokens": 0}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 156, "sample_idx": 469, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553904.4912844, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.092099999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 1295.294500001546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1295.294500001546, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.409613026167875, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 156, "sample_idx": 470, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553904.4912844, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 37.092099999426864, "prefill_cuda_event_ms": null, "kv_decode_ms": 1295.294500001546, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1332.386600000973, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 60.79316618760715, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 157, "sample_idx": 471, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553905.8242927, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.388699999501114, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.3363999990397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 32.388699999501114, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 524.8744160852968, "gen_tokens": 0}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 157, "sample_idx": 472, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553905.8242927, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.388699999501114, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.3363999990397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1172.3363999990397, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 54.59183899779315, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 157, "sample_idx": 473, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553905.8242927, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 32.388699999501114, "prefill_cuda_event_ms": null, "kv_decode_ms": 1172.3363999990397, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1204.7250999985408, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 67.23525557830422, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 158, "sample_idx": 474, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553907.0294514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.254899999912595, "prefill_cuda_event_ms": null, "kv_decode_ms": 1229.0763999990304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 26.254899999912595, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 647.4981812940287, "gen_tokens": 0}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 158, "sample_idx": 475, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553907.0294514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.254899999912595, "prefill_cuda_event_ms": null, "kv_decode_ms": 1229.0763999990304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1229.0763999990304, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 52.07162060881691, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 158, "sample_idx": 476, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553907.0294514, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 26.254899999912595, "prefill_cuda_event_ms": null, "kv_decode_ms": 1229.0763999990304, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1255.331299998943, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 64.52479915068493, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 159, "sample_idx": 477, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553908.2853317, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.948800000260235, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.8516999994026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.948800000260235, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 549.2943183534436, "gen_tokens": 0}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 159, "sample_idx": 478, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553908.2853317, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.948800000260235, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.8516999994026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1280.8516999994026, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 49.96675259128738, "gen_tokens": 64}
{"task_idx": 39, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 159, "sample_idx": 479, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553908.2853317, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 30.948800000260235, "prefill_cuda_event_ms": null, "kv_decode_ms": 1280.8516999994026, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1311.8004999996629, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 61.74719402837613, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 160, "sample_idx": 480, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553909.5977676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 21.7307, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.819, "kv_decode_ms_equiv": 720.819, "kv_decode_ms_per_token": 11.262796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6274.3525999994745, "ollama_total_duration_ms": 6271.149, "ollama_load_ms": 5460.5248, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 21.7307, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 2116.82090314624, "gen_tokens": 0}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 160, "sample_idx": 481, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553909.5977676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 21.7307, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.819, "kv_decode_ms_equiv": 720.819, "kv_decode_ms_per_token": 11.262796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6274.3525999994745, "ollama_total_duration_ms": 6271.149, "ollama_load_ms": 5460.5248, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 720.819, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 88.78789266098701, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 160, "sample_idx": 482, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553909.5977676, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 21.7307, "prefill_cuda_event_ms": null, "kv_decode_ms": 720.819, "kv_decode_ms_equiv": 720.819, "kv_decode_ms_per_token": 11.262796875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 6274.3525999994745, "ollama_total_duration_ms": 6271.149, "ollama_load_ms": 5460.5248, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 742.5496999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 148.13823236343643, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 161, "sample_idx": 483, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553915.8722641, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.6679, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.8845, "kv_decode_ms_equiv": 713.8845, "kv_decode_ms_per_token": 11.1544453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 959.0664000006655, "ollama_total_duration_ms": 935.3574, "ollama_load_ms": 132.4664, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.6679, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3942.4403705893947, "gen_tokens": 0}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 161, "sample_idx": 484, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553915.8722641, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.6679, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.8845, "kv_decode_ms_equiv": 713.8845, "kv_decode_ms_per_token": 11.1544453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 959.0664000006655, "ollama_total_duration_ms": 935.3574, "ollama_load_ms": 132.4664, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 713.8845, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.65035660530519, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 161, "sample_idx": 485, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553915.8722641, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.6679, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.8845, "kv_decode_ms_equiv": 713.8845, "kv_decode_ms_per_token": 11.1544453125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 959.0664000006655, "ollama_total_duration_ms": 935.3574, "ollama_load_ms": 132.4664, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 725.5524, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 151.60862261636788, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 162, "sample_idx": 486, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553916.8315074, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9963, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.0681, "kv_decode_ms_equiv": 719.0681, "kv_decode_ms_per_token": 11.2354390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 941.4123999995354, "ollama_total_duration_ms": 915.5764, "ollama_load_ms": 130.9508, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.9963, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 3834.5156423230496, "gen_tokens": 0}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 162, "sample_idx": 487, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553916.8315074, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9963, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.0681, "kv_decode_ms_equiv": 719.0681, "kv_decode_ms_per_token": 11.2354390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 941.4123999995354, "ollama_total_duration_ms": 915.5764, "ollama_load_ms": 130.9508, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 719.0681, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.00408737364376, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 162, "sample_idx": 488, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553916.8315074, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.9963, "prefill_cuda_event_ms": null, "kv_decode_ms": 719.0681, "kv_decode_ms_equiv": 719.0681, "kv_decode_ms_per_token": 11.2354390625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 941.4123999995354, "ollama_total_duration_ms": 915.5764, "ollama_load_ms": 130.9508, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 731.0644, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 150.46554038194174, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 163, "sample_idx": 489, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553917.7731905, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.415, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.6591, "kv_decode_ms_equiv": 713.6591, "kv_decode_ms_per_token": 11.1509234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 933.5654000005889, "ollama_total_duration_ms": 906.8005, "ollama_load_ms": 125.0655, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 11.415, "cuda_event_ms": null, "tokens_total": 46, "tokens_per_s": 4029.785370127026, "gen_tokens": 0}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 163, "sample_idx": 490, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553917.7731905, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.415, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.6591, "kv_decode_ms_equiv": 713.6591, "kv_decode_ms_per_token": 11.1509234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 933.5654000005889, "ollama_total_duration_ms": 906.8005, "ollama_load_ms": 125.0655, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 713.6591, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 89.67867151137007, "gen_tokens": 64}
{"task_idx": 40, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "qwen2.5:7b", "model_kind": "ollama", "params_millions": 7600.0, "params_millions_config": 7600.0, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 163, "sample_idx": 491, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553917.7731905, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 46, "prefill_ms": 11.415, "prefill_cuda_event_ms": null, "kv_decode_ms": 713.6591, "kv_decode_ms_equiv": 713.6591, "kv_decode_ms_per_token": 11.1509234375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 933.5654000005889, "ollama_total_duration_ms": 906.8005, "ollama_load_ms": 125.0655, "ollama_done_reason": "length", "params_millions_measured": 7600.0, "latency_ms": 725.0740999999999, "cuda_event_ms": null, "tokens_total": 110, "tokens_per_s": 151.70863226255082, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 164, "sample_idx": 492, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553918.7070503, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 127.54599999971106, "prefill_cuda_event_ms": null, "kv_decode_ms": 880.1756000011665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 127.54599999971106, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 7.840308594564043, "gen_tokens": 0}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 164, "sample_idx": 493, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553918.7070503, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 127.54599999971106, "prefill_cuda_event_ms": null, "kv_decode_ms": 880.1756000011665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 880.1756000011665, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 72.7127632257872, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 164, "sample_idx": 494, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553918.7070503, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 127.54599999971106, "prefill_cuda_event_ms": null, "kv_decode_ms": 880.1756000011665, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 1007.7216000008775, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 64.50194180609347, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 165, "sample_idx": 495, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553919.7168076, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.4071999998705, "prefill_cuda_event_ms": null, "kv_decode_ms": 822.2283000013704, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 12.4071999998705, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 80.5983622421205, "gen_tokens": 0}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 165, "sample_idx": 496, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553919.7168076, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.4071999998705, "prefill_cuda_event_ms": null, "kv_decode_ms": 822.2283000013704, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 822.2283000013704, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 77.83726247307875, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 165, "sample_idx": 497, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553919.7168076, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.4071999998705, "prefill_cuda_event_ms": null, "kv_decode_ms": 822.2283000013704, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 834.6355000012409, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 77.87830735680829, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 166, "sample_idx": 498, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553920.5519545, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.285799999721348, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.3749000008247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 13.285799999721348, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 75.26833160374036, "gen_tokens": 0}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 166, "sample_idx": 499, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553920.5519545, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.285799999721348, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.3749000008247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 840.3749000008247, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.15648682503154, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 166, "sample_idx": 500, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553920.5519545, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 13.285799999721348, "prefill_cuda_event_ms": null, "kv_decode_ms": 840.3749000008247, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 853.6607000005461, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 76.14266417554236, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 167, "sample_idx": 501, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553921.4060798, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.92079999964335, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.4526000004407, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 11.92079999964335, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 83.88698745301643, "gen_tokens": 0}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 167, "sample_idx": 502, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553921.4060798, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.92079999964335, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.4526000004407, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 809.4526000004407, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.06577852732224, "gen_tokens": 64}
{"task_idx": 41, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 167, "sample_idx": 503, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553921.4060798, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.92079999964335, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.4526000004407, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 821.3734000000841, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 79.13574995244957, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 168, "sample_idx": 504, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553922.227969, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.349599998880876, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3729999998613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 35.349599998880876, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 254.59976917093627, "gen_tokens": 0}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 168, "sample_idx": 505, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553922.227969, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.349599998880876, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3729999998613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2067.3729999998613, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 30.957161576553574, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 168, "sample_idx": 506, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553922.227969, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.349599998880876, "prefill_cuda_event_ms": null, "kv_decode_ms": 2067.3729999998613, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2102.722599998742, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 34.7168951339771, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 169, "sample_idx": 507, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553924.3314958, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.81010000016249, "prefill_cuda_event_ms": null, "kv_decode_ms": 1953.7816000010935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 35.81010000016249, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 251.32574329474537, "gen_tokens": 0}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 169, "sample_idx": 508, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553924.3314958, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.81010000016249, "prefill_cuda_event_ms": null, "kv_decode_ms": 1953.7816000010935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1953.7816000010935, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.756987782034685, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 169, "sample_idx": 509, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553924.3314958, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.81010000016249, "prefill_cuda_event_ms": null, "kv_decode_ms": 1953.7816000010935, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1989.591700001256, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 36.690945182347676, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 170, "sample_idx": 510, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553926.3222027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 40.24780000145256, "prefill_cuda_event_ms": null, "kv_decode_ms": 1917.779999999766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 40.24780000145256, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 223.61470688274107, "gen_tokens": 0}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 170, "sample_idx": 511, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553926.3222027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 40.24780000145256, "prefill_cuda_event_ms": null, "kv_decode_ms": 1917.779999999766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1917.779999999766, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.3719196153927, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 170, "sample_idx": 512, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553926.3222027, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 40.24780000145256, "prefill_cuda_event_ms": null, "kv_decode_ms": 1917.779999999766, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1958.0278000012186, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 37.28241243559186, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 171, "sample_idx": 513, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553928.2812257, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.23690000110946, "prefill_cuda_event_ms": null, "kv_decode_ms": 1840.855800001009, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 35.23690000110946, "cuda_event_ms": null, "tokens_total": 9, "tokens_per_s": 255.414068766453, "gen_tokens": 0}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 171, "sample_idx": 514, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553928.2812257, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.23690000110946, "prefill_cuda_event_ms": null, "kv_decode_ms": 1840.855800001009, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1840.855800001009, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 34.766438522759316, "gen_tokens": 64}
{"task_idx": 42, "scenario": "short", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 171, "sample_idx": 515, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553928.2812257, "batch_size": 1, "seq_len": 9, "prompt_tokens_raw": 9, "gen_tokens_raw": 64, "prefill_ms": 35.23690000110946, "prefill_cuda_event_ms": null, "kv_decode_ms": 1840.855800001009, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1876.0927000021184, "cuda_event_ms": null, "tokens_total": 73, "tokens_per_s": 38.910657239867504, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 172, "sample_idx": 516, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553930.1579325, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 1033.1005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2448.5972, "kv_decode_ms_equiv": 2448.5972, "kv_decode_ms_per_token": 38.25933125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14145.50539999982, "ollama_total_duration_ms": 14034.8378, "ollama_load_ms": 10479.4424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 1033.1005, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 89.052323563874, "gen_tokens": 0}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 172, "sample_idx": 517, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553930.1579325, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 1033.1005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2448.5972, "kv_decode_ms_equiv": 2448.5972, "kv_decode_ms_per_token": 38.25933125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14145.50539999982, "ollama_total_duration_ms": 14034.8378, "ollama_load_ms": 10479.4424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2448.5972, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 26.137414516360632, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 172, "sample_idx": 518, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553930.1579325, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 1033.1005, "prefill_cuda_event_ms": null, "kv_decode_ms": 2448.5972, "kv_decode_ms_equiv": 2448.5972, "kv_decode_ms_per_token": 38.25933125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 14145.50539999982, "ollama_total_duration_ms": 14034.8378, "ollama_load_ms": 10479.4424, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3481.6977, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 44.80572796426295, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 173, "sample_idx": 519, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553944.3035789, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.4252, "prefill_cuda_event_ms": null, "kv_decode_ms": 2611.0058, "kv_decode_ms_equiv": 2611.0058, "kv_decode_ms_per_token": 40.796965625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2898.376499999358, "ollama_total_duration_ms": 2896.3292, "ollama_load_ms": 219.5108, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 50.4252, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1824.4845831052728, "gen_tokens": 0}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 173, "sample_idx": 520, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553944.3035789, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.4252, "prefill_cuda_event_ms": null, "kv_decode_ms": 2611.0058, "kv_decode_ms_equiv": 2611.0058, "kv_decode_ms_per_token": 40.796965625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2898.376499999358, "ollama_total_duration_ms": 2896.3292, "ollama_load_ms": 219.5108, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2611.0058, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 24.511626898722323, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 173, "sample_idx": 521, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553944.3035789, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.4252, "prefill_cuda_event_ms": null, "kv_decode_ms": 2611.0058, "kv_decode_ms_equiv": 2611.0058, "kv_decode_ms_per_token": 40.796965625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 2898.376499999358, "ollama_total_duration_ms": 2896.3292, "ollama_load_ms": 219.5108, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2661.431, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 58.615083389349564, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 174, "sample_idx": 522, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553947.202105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.8811, "prefill_cuda_event_ms": null, "kv_decode_ms": 2985.6532, "kv_decode_ms_equiv": 2985.6532, "kv_decode_ms_per_token": 46.65083125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3296.270999999251, "ollama_total_duration_ms": 3277.7598, "ollama_load_ms": 223.6157, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 50.8811, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1808.1370096165372, "gen_tokens": 0}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 174, "sample_idx": 523, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553947.202105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.8811, "prefill_cuda_event_ms": null, "kv_decode_ms": 2985.6532, "kv_decode_ms_equiv": 2985.6532, "kv_decode_ms_per_token": 46.65083125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3296.270999999251, "ollama_total_duration_ms": 3277.7598, "ollama_load_ms": 223.6157, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 2985.6532, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 21.435845261599706, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 174, "sample_idx": 524, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553947.202105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 50.8811, "prefill_cuda_event_ms": null, "kv_decode_ms": 2985.6532, "kv_decode_ms_equiv": 2985.6532, "kv_decode_ms_per_token": 46.65083125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3296.270999999251, "ollama_total_duration_ms": 3277.7598, "ollama_load_ms": 223.6157, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3036.5343000000003, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 51.37435793167229, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 175, "sample_idx": 525, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553950.499424, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 62.9294, "prefill_cuda_event_ms": null, "kv_decode_ms": 3292.9925, "kv_decode_ms_equiv": 3292.9925, "kv_decode_ms_per_token": 51.4530078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3621.658500000194, "ollama_total_duration_ms": 3618.3307, "ollama_load_ms": 237.36, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 62.9294, "cuda_event_ms": null, "tokens_total": 92, "tokens_per_s": 1461.9557790158497, "gen_tokens": 0}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 175, "sample_idx": 526, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553950.499424, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 62.9294, "prefill_cuda_event_ms": null, "kv_decode_ms": 3292.9925, "kv_decode_ms_equiv": 3292.9925, "kv_decode_ms_per_token": 51.4530078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3621.658500000194, "ollama_total_duration_ms": 3618.3307, "ollama_load_ms": 237.36, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3292.9925, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 19.43520976740761, "gen_tokens": 64}
{"task_idx": 43, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gpt-oss-20b:latest", "model_kind": "ollama", "params_millions": 20900.0, "params_millions_config": 20900.0, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 175, "sample_idx": 527, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553950.499424, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 92, "prefill_ms": 62.9294, "prefill_cuda_event_ms": null, "kv_decode_ms": 3292.9925, "kv_decode_ms_equiv": 3292.9925, "kv_decode_ms_per_token": 51.4530078125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 3621.658500000194, "ollama_total_duration_ms": 3618.3307, "ollama_load_ms": 237.36, "ollama_done_reason": "length", "params_millions_measured": 20900.0, "latency_ms": 3355.9219, "cuda_event_ms": null, "tokens_total": 156, "tokens_per_s": 46.48499120316239, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 176, "sample_idx": 528, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553954.1212723, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 10.300799998731236, "prefill_cuda_event_ms": 10.196191787719727, "kv_decode_ms": 491.85530000067956, "kv_decode_cuda_event_ms": 491.7729187011719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 10.300799998731236, "cuda_event_ms": 10.196191787719727, "tokens_total": 1, "tokens_per_s": 97.07983847110627, "gen_tokens": 0}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 176, "sample_idx": 529, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553954.1212723, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 10.300799998731236, "prefill_cuda_event_ms": 10.196191787719727, "kv_decode_ms": 491.85530000067956, "kv_decode_cuda_event_ms": 491.7729187011719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 491.85530000067956, "cuda_event_ms": 491.7729187011719, "tokens_total": 64, "tokens_per_s": 130.1195697188006, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 176, "sample_idx": 530, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553954.1212723, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 10.300799998731236, "prefill_cuda_event_ms": 10.196191787719727, "kv_decode_ms": 491.85530000067956, "kv_decode_cuda_event_ms": 491.7729187011719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 502.1560999994108, "cuda_event_ms": 501.9691104888916, "tokens_total": 65, "tokens_per_s": 129.44182097972376, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 177, "sample_idx": 531, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553954.6257393, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.858700000928366, "prefill_cuda_event_ms": 8.76972770690918, "kv_decode_ms": 481.48650000075577, "kv_decode_cuda_event_ms": 481.4242858886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 8.858700000928366, "cuda_event_ms": 8.76972770690918, "tokens_total": 1, "tokens_per_s": 112.88338016810626, "gen_tokens": 0}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 177, "sample_idx": 532, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553954.6257393, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.858700000928366, "prefill_cuda_event_ms": 8.76972770690918, "kv_decode_ms": 481.48650000075577, "kv_decode_cuda_event_ms": 481.4242858886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 481.48650000075577, "cuda_event_ms": 481.4242858886719, "tokens_total": 64, "tokens_per_s": 132.92169146985336, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 177, "sample_idx": 533, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553954.6257393, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.858700000928366, "prefill_cuda_event_ms": 8.76972770690918, "kv_decode_ms": 481.48650000075577, "kv_decode_cuda_event_ms": 481.4242858886719, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 490.34520000168413, "cuda_event_ms": 490.19401359558105, "tokens_total": 65, "tokens_per_s": 132.55967428614935, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 178, "sample_idx": 534, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553955.116984, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.299799999804236, "prefill_cuda_event_ms": 8.236063957214355, "kv_decode_ms": 471.9717000007222, "kv_decode_cuda_event_ms": 471.91143798828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 8.299799999804236, "cuda_event_ms": 8.236063957214355, "tokens_total": 1, "tokens_per_s": 120.48483096262399, "gen_tokens": 0}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 178, "sample_idx": 535, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553955.116984, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.299799999804236, "prefill_cuda_event_ms": 8.236063957214355, "kv_decode_ms": 471.9717000007222, "kv_decode_cuda_event_ms": 471.91143798828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 471.9717000007222, "cuda_event_ms": 471.91143798828125, "tokens_total": 64, "tokens_per_s": 135.60135067399608, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 178, "sample_idx": 536, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553955.116984, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.299799999804236, "prefill_cuda_event_ms": 8.236063957214355, "kv_decode_ms": 471.9717000007222, "kv_decode_cuda_event_ms": 471.91143798828125, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 480.27150000052643, "cuda_event_ms": 480.1475019454956, "tokens_total": 65, "tokens_per_s": 135.34011491402, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 179, "sample_idx": 537, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553955.598135, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.172099998773774, "prefill_cuda_event_ms": 8.108192443847656, "kv_decode_ms": 478.855499999554, "kv_decode_cuda_event_ms": 478.7978210449219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 8.172099998773774, "cuda_event_ms": 8.108192443847656, "tokens_total": 1, "tokens_per_s": 122.36756771821811, "gen_tokens": 0}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 179, "sample_idx": 538, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553955.598135, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.172099998773774, "prefill_cuda_event_ms": 8.108192443847656, "kv_decode_ms": 478.855499999554, "kv_decode_cuda_event_ms": 478.7978210449219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 478.855499999554, "cuda_event_ms": 478.7978210449219, "tokens_total": 64, "tokens_per_s": 133.65200984443032, "gen_tokens": 64}
{"task_idx": 44, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 179, "sample_idx": 539, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553955.598135, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.172099998773774, "prefill_cuda_event_ms": 8.108192443847656, "kv_decode_ms": 478.855499999554, "kv_decode_cuda_event_ms": 478.7978210449219, "gpu_peak_mb": 631.30419921875, "params_millions_measured": 45.1712, "latency_ms": 487.0275999983278, "cuda_event_ms": 486.90601348876953, "tokens_total": 65, "tokens_per_s": 133.4626620754618, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 180, "sample_idx": 540, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553956.0859435, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 10.5256, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.9431, "kv_decode_ms_equiv": 267.18701176470586, "kv_decode_ms_per_token": 4.174797058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 3190.338000000338, "ollama_total_duration_ms": 3139.2537, "ollama_load_ms": 2943.9146, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 10.5256, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 1710.1162879075775, "gen_tokens": 0}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 180, "sample_idx": 541, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553956.0859435, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 10.5256, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.9431, "kv_decode_ms_equiv": 267.18701176470586, "kv_decode_ms_per_token": 4.174797058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 3190.338000000338, "ollama_total_duration_ms": 3139.2537, "ollama_load_ms": 2943.9146, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 267.18701176470586, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.53260144381798, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 180, "sample_idx": 542, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553956.0859435, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 10.5256, "prefill_cuda_event_ms": null, "kv_decode_ms": 141.9431, "kv_decode_ms_equiv": 267.18701176470586, "kv_decode_ms_per_token": 4.174797058823529, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 3190.338000000338, "ollama_total_duration_ms": 3139.2537, "ollama_load_ms": 2943.9146, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 277.71261176470585, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 295.2692694758679, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 181, "sample_idx": 543, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553959.2769337, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.9742, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0267, "kv_decode_ms_equiv": 259.81496470588235, "kv_decode_ms_per_token": 4.059608823529412, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.65970000019297, "ollama_total_duration_ms": 324.8888, "ollama_load_ms": 160.8493, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.9742, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3618.6723493225045, "gen_tokens": 0}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 181, "sample_idx": 544, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553959.2769337, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.9742, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0267, "kv_decode_ms_equiv": 259.81496470588235, "kv_decode_ms_per_token": 4.059608823529412, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.65970000019297, "ollama_total_duration_ms": 324.8888, "ollama_load_ms": 160.8493, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 259.81496470588235, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 246.3291522582225, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 181, "sample_idx": 545, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553959.2769337, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.9742, "prefill_cuda_event_ms": null, "kv_decode_ms": 138.0267, "kv_decode_ms_equiv": 259.81496470588235, "kv_decode_ms_per_token": 4.059608823529412, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 344.65970000019297, "ollama_total_duration_ms": 324.8888, "ollama_load_ms": 160.8493, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 264.78916470588234, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 309.68034545931084, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 182, "sample_idx": 546, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553959.6217802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.5617, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.4128, "kv_decode_ms_equiv": 258.6593882352941, "kv_decode_ms_per_token": 4.04155294117647, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 372.4974000015209, "ollama_total_duration_ms": 341.5525, "ollama_load_ms": 167.6585, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.5617, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3945.8973628252625, "gen_tokens": 0}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 182, "sample_idx": 547, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553959.6217802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.5617, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.4128, "kv_decode_ms_equiv": 258.6593882352941, "kv_decode_ms_per_token": 4.04155294117647, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 372.4974000015209, "ollama_total_duration_ms": 341.5525, "ollama_load_ms": 167.6585, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 258.6593882352941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 247.42964265337727, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 182, "sample_idx": 548, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553959.6217802, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.5617, "prefill_cuda_event_ms": null, "kv_decode_ms": 137.4128, "kv_decode_ms_equiv": 258.6593882352941, "kv_decode_ms_per_token": 4.04155294117647, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 372.4974000015209, "ollama_total_duration_ms": 341.5525, "ollama_load_ms": 167.6585, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 263.2210882352941, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 311.5251918064406, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 183, "sample_idx": 549, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553959.9944115, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.8808, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.596, "kv_decode_ms_equiv": 262.7689411764706, "kv_decode_ms_per_token": 4.105764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 371.16430000060063, "ollama_total_duration_ms": 335.6479, "ollama_load_ms": 160.2435, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 4.8808, "cuda_event_ms": null, "tokens_total": 18, "tokens_per_s": 3687.920013112605, "gen_tokens": 0}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 183, "sample_idx": 550, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553959.9944115, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.8808, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.596, "kv_decode_ms_equiv": 262.7689411764706, "kv_decode_ms_per_token": 4.105764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 371.16430000060063, "ollama_total_duration_ms": 335.6479, "ollama_load_ms": 160.2435, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 262.7689411764706, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 243.55998739218887, "gen_tokens": 64}
{"task_idx": 45, "scenario": "short", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Summarize RLHF in one sentence.", "prompt_chars": 31, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 183, "sample_idx": 551, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553959.9944115, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 18, "prefill_ms": 4.8808, "prefill_cuda_event_ms": null, "kv_decode_ms": 139.596, "kv_decode_ms_equiv": 262.7689411764706, "kv_decode_ms_per_token": 4.105764705882353, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 34, "gen_tokens_equiv": 64, "ollama_wall_ms": 371.16430000060063, "ollama_total_duration_ms": 335.6479, "ollama_load_ms": 160.2435, "ollama_done_reason": "stop", "params_millions_measured": 999.89, "latency_ms": 267.6497411764706, "cuda_event_ms": null, "tokens_total": 82, "tokens_per_s": 306.37055593464817, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 184, "sample_idx": 552, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553960.3660047, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.62669999981881, "prefill_cuda_event_ms": 22.48236846923828, "kv_decode_ms": 529.204700000264, "kv_decode_cuda_event_ms": 529.0966796875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 22.62669999981881, "cuda_event_ms": 22.48236846923828, "tokens_total": 17, "tokens_per_s": 751.3247623443159, "gen_tokens": 0}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 184, "sample_idx": 553, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553960.3660047, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.62669999981881, "prefill_cuda_event_ms": 22.48236846923828, "kv_decode_ms": 529.204700000264, "kv_decode_cuda_event_ms": 529.0966796875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 529.204700000264, "cuda_event_ms": 529.0966796875, "tokens_total": 64, "tokens_per_s": 120.936189720099, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 184, "sample_idx": 554, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553960.3660047, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 22.62669999981881, "prefill_cuda_event_ms": 22.48236846923828, "kv_decode_ms": 529.204700000264, "kv_decode_cuda_event_ms": 529.0966796875, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 551.8314000000828, "cuda_event_ms": 551.5790481567383, "tokens_total": 81, "tokens_per_s": 146.78396336270072, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 185, "sample_idx": 555, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553960.9195926, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.174299999358482, "prefill_cuda_event_ms": 9.09062385559082, "kv_decode_ms": 525.8088999999018, "kv_decode_cuda_event_ms": 525.7100830078125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 9.174299999358482, "cuda_event_ms": 9.09062385559082, "tokens_total": 17, "tokens_per_s": 1853.0024090327038, "gen_tokens": 0}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 185, "sample_idx": 556, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553960.9195926, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.174299999358482, "prefill_cuda_event_ms": 9.09062385559082, "kv_decode_ms": 525.8088999999018, "kv_decode_cuda_event_ms": 525.7100830078125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 525.8088999999018, "cuda_event_ms": 525.7100830078125, "tokens_total": 64, "tokens_per_s": 121.71722464190309, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 185, "sample_idx": 557, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553960.9195926, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.174299999358482, "prefill_cuda_event_ms": 9.09062385559082, "kv_decode_ms": 525.8088999999018, "kv_decode_cuda_event_ms": 525.7100830078125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 534.9831999992603, "cuda_event_ms": 534.8007068634033, "tokens_total": 81, "tokens_per_s": 151.40662361007224, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 186, "sample_idx": 558, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553961.4577742, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.358100000099512, "prefill_cuda_event_ms": 9.17193603515625, "kv_decode_ms": 547.6964000008593, "kv_decode_cuda_event_ms": 547.6300659179688, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 9.358100000099512, "cuda_event_ms": 9.17193603515625, "tokens_total": 17, "tokens_per_s": 1816.608072132081, "gen_tokens": 0}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 186, "sample_idx": 559, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553961.4577742, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.358100000099512, "prefill_cuda_event_ms": 9.17193603515625, "kv_decode_ms": 547.6964000008593, "kv_decode_cuda_event_ms": 547.6300659179688, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 547.6964000008593, "cuda_event_ms": 547.6300659179688, "tokens_total": 64, "tokens_per_s": 116.85305946852962, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 186, "sample_idx": 560, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553961.4577742, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 9.358100000099512, "prefill_cuda_event_ms": 9.17193603515625, "kv_decode_ms": 547.6964000008593, "kv_decode_cuda_event_ms": 547.6300659179688, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 557.0545000009588, "cuda_event_ms": 556.802001953125, "tokens_total": 81, "tokens_per_s": 145.40767555034665, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 187, "sample_idx": 561, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553962.0184257, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.482899999260553, "prefill_cuda_event_ms": 10.367391586303711, "kv_decode_ms": 565.1443999995536, "kv_decode_cuda_event_ms": 565.0821533203125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 10.482899999260553, "cuda_event_ms": 10.367391586303711, "tokens_total": 17, "tokens_per_s": 1621.6886549713488, "gen_tokens": 0}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 187, "sample_idx": 562, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553962.0184257, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.482899999260553, "prefill_cuda_event_ms": 10.367391586303711, "kv_decode_ms": 565.1443999995536, "kv_decode_cuda_event_ms": 565.0821533203125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 565.1443999995536, "cuda_event_ms": 565.0821533203125, "tokens_total": 64, "tokens_per_s": 113.2453935667602, "gen_tokens": 64}
{"task_idx": 46, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-45m", "model_kind": "hf", "params_millions": 45.171, "params_millions_config": 45.171, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 187, "sample_idx": 563, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553962.0184257, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 10.482899999260553, "prefill_cuda_event_ms": 10.367391586303711, "kv_decode_ms": 565.1443999995536, "kv_decode_cuda_event_ms": 565.0821533203125, "gpu_peak_mb": 633.025390625, "params_millions_measured": 45.1712, "latency_ms": 575.6272999988141, "cuda_event_ms": 575.4495449066162, "tokens_total": 81, "tokens_per_s": 140.7160501250842, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 188, "sample_idx": 564, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553962.5957956, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 35.417200000665616, "prefill_cuda_event_ms": null, "kv_decode_ms": 1600.929600001109, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 35.417200000665616, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 28.234868933207775, "gen_tokens": 0}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 188, "sample_idx": 565, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553962.5957956, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 35.417200000665616, "prefill_cuda_event_ms": null, "kv_decode_ms": 1600.929600001109, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1600.929600001109, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 39.976773494571944, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 188, "sample_idx": 566, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553962.5957956, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 35.417200000665616, "prefill_cuda_event_ms": null, "kv_decode_ms": 1600.929600001109, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1636.3468000017747, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 39.72263092391509, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 189, "sample_idx": 567, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553964.2326114, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.000500000605825, "prefill_cuda_event_ms": null, "kv_decode_ms": 1695.6176000003325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 20.000500000605825, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 49.99875002973473, "gen_tokens": 0}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 189, "sample_idx": 568, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553964.2326114, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.000500000605825, "prefill_cuda_event_ms": null, "kv_decode_ms": 1695.6176000003325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1695.6176000003325, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 37.744359341391274, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 189, "sample_idx": 569, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553964.2326114, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 20.000500000605825, "prefill_cuda_event_ms": null, "kv_decode_ms": 1695.6176000003325, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1715.6181000009383, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 37.88721977225843, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 190, "sample_idx": 570, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553965.948721, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 30.256399999416317, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.5513999989053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 30.256399999416317, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 33.05085866194561, "gen_tokens": 0}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 190, "sample_idx": 571, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553965.948721, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 30.256399999416317, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.5513999989053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1573.5513999989053, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 40.67232884800873, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 190, "sample_idx": 572, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553965.948721, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 30.256399999416317, "prefill_cuda_event_ms": null, "kv_decode_ms": 1573.5513999989053, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 1603.8077999983216, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 40.52854712395589, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 191, "sample_idx": 573, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553967.552931, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 28.996600000027684, "prefill_cuda_event_ms": null, "kv_decode_ms": 2027.1341000006942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 28.996600000027684, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 34.4868019008796, "gen_tokens": 0}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 191, "sample_idx": 574, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553967.552931, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 28.996600000027684, "prefill_cuda_event_ms": null, "kv_decode_ms": 2027.1341000006942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2027.1341000006942, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.571665633752637, "gen_tokens": 64}
{"task_idx": 47, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 191, "sample_idx": 575, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553967.552931, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 28.996600000027684, "prefill_cuda_event_ms": null, "kv_decode_ms": 2027.1341000006942, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 51.475968, "latency_ms": 2056.130700000722, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.612776366783095, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 192, "sample_idx": 576, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553969.6095357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 116.0903, "prefill_cuda_event_ms": null, "kv_decode_ms": 517.9066, "kv_decode_ms_equiv": 517.9066, "kv_decode_ms_per_token": 8.092290625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 866.6085000004387, "ollama_total_duration_ms": 828.1734, "ollama_load_ms": 151.424, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 116.0903, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 223.96358696635292, "gen_tokens": 0}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 192, "sample_idx": 577, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553969.6095357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 116.0903, "prefill_cuda_event_ms": null, "kv_decode_ms": 517.9066, "kv_decode_ms_equiv": 517.9066, "kv_decode_ms_per_token": 8.092290625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 866.6085000004387, "ollama_total_duration_ms": 828.1734, "ollama_load_ms": 151.424, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 517.9066, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 123.57440511474464, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 192, "sample_idx": 578, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553969.6095357, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 116.0903, "prefill_cuda_event_ms": null, "kv_decode_ms": 517.9066, "kv_decode_ms_equiv": 517.9066, "kv_decode_ms_per_token": 8.092290625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 866.6085000004387, "ollama_total_duration_ms": 828.1734, "ollama_load_ms": 151.424, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 633.9969, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 141.95653007136156, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 193, "sample_idx": 579, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553970.4763753, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.4185, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6022, "kv_decode_ms_equiv": 263.6022, "kv_decode_ms_per_token": 4.118784375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 508.0289999987144, "ollama_total_duration_ms": 462.8834, "ollama_load_ms": 151.983, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.4185, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 5884.349892497455, "gen_tokens": 0}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 193, "sample_idx": 580, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553970.4763753, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.4185, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6022, "kv_decode_ms_equiv": 263.6022, "kv_decode_ms_per_token": 4.118784375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 508.0289999987144, "ollama_total_duration_ms": 462.8834, "ollama_load_ms": 151.983, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 263.6022, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 242.79008293557487, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 193, "sample_idx": 581, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553970.4763753, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.4185, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.6022, "kv_decode_ms_equiv": 263.6022, "kv_decode_ms_per_token": 4.118784375, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 508.0289999987144, "ollama_total_duration_ms": 462.8834, "ollama_load_ms": 151.983, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 268.0207, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 335.7949591206948, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 194, "sample_idx": 582, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553970.9845474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.8184, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4244, "kv_decode_ms_equiv": 263.4244, "kv_decode_ms_per_token": 4.11600625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.15450000013516, "ollama_total_duration_ms": 453.1926, "ollama_load_ms": 145.047, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 3.8184, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 6809.13471611146, "gen_tokens": 0}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 194, "sample_idx": 583, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553970.9845474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.8184, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4244, "kv_decode_ms_equiv": 263.4244, "kv_decode_ms_per_token": 4.11600625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.15450000013516, "ollama_total_duration_ms": 453.1926, "ollama_load_ms": 145.047, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 263.4244, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 242.95395567001387, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 194, "sample_idx": 584, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553970.9845474, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 3.8184, "prefill_cuda_event_ms": null, "kv_decode_ms": 263.4244, "kv_decode_ms_equiv": 263.4244, "kv_decode_ms_per_token": 4.11600625, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 485.15450000013516, "ollama_total_duration_ms": 453.1926, "ollama_load_ms": 145.047, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 267.2428, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 336.77240322283706, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 195, "sample_idx": 585, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553971.4698467, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2116, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.8528, "kv_decode_ms_equiv": 266.8528, "kv_decode_ms_per_token": 4.169575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 461.9863999996596, "ollama_total_duration_ms": 446.6936, "ollama_load_ms": 134.8628, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.2116, "cuda_event_ms": null, "tokens_total": 26, "tokens_per_s": 6173.425776427012, "gen_tokens": 0}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 195, "sample_idx": 586, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553971.4698467, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2116, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.8528, "kv_decode_ms_equiv": 266.8528, "kv_decode_ms_per_token": 4.169575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 461.9863999996596, "ollama_total_duration_ms": 446.6936, "ollama_load_ms": 134.8628, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 266.8528, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 239.8325968474005, "gen_tokens": 64}
{"task_idx": 48, "scenario": "medium", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 195, "sample_idx": 587, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553971.4698467, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 26, "prefill_ms": 4.2116, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.8528, "kv_decode_ms_equiv": 266.8528, "kv_decode_ms_per_token": 4.169575, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 461.9863999996596, "ollama_total_duration_ms": 446.6936, "ollama_load_ms": 134.8628, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 271.0644, "cuda_event_ms": null, "tokens_total": 90, "tokens_per_s": 332.0244192892907, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 196, "sample_idx": 588, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553971.9320636, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 38.73649999877671, "prefill_cuda_event_ms": null, "kv_decode_ms": 1948.037199999817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 38.73649999877671, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.81544538178668, "gen_tokens": 0}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 196, "sample_idx": 589, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553971.9320636, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 38.73649999877671, "prefill_cuda_event_ms": null, "kv_decode_ms": 1948.037199999817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1948.037199999817, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 32.85358205685498, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 196, "sample_idx": 590, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553971.9320636, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 38.73649999877671, "prefill_cuda_event_ms": null, "kv_decode_ms": 1948.037199999817, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1986.7736999985937, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 32.716358184148504, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 197, "sample_idx": 591, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553973.9193482, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.28099999956612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.120299999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 22.28099999956612, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 44.88128899149379, "gen_tokens": 0}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 197, "sample_idx": 592, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553973.9193482, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.28099999956612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.120299999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2253.120299999864, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 28.40505231789171, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 197, "sample_idx": 593, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553973.9193482, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 22.28099999956612, "prefill_cuda_event_ms": null, "kv_decode_ms": 2253.120299999864, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2275.40129999943, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 28.566389585879325, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 198, "sample_idx": 594, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553976.1952877, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 39.53739999997197, "prefill_cuda_event_ms": null, "kv_decode_ms": 2044.9284999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 39.53739999997197, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 25.292507853341622, "gen_tokens": 0}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 198, "sample_idx": 595, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553976.1952877, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 39.53739999997197, "prefill_cuda_event_ms": null, "kv_decode_ms": 2044.9284999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2044.9284999995143, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 31.296937765802177, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 198, "sample_idx": 596, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553976.1952877, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 39.53739999997197, "prefill_cuda_event_ms": null, "kv_decode_ms": 2044.9284999995143, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 2084.4658999994863, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 31.183047897313177, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 199, "sample_idx": 597, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553978.2822149, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 23.684100000537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1883.943000000727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 23.684100000537, "cuda_event_ms": null, "tokens_total": 1, "tokens_per_s": 42.22241925922144, "gen_tokens": 0}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 199, "sample_idx": 598, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553978.2822149, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 23.684100000537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1883.943000000727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1883.943000000727, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 33.97130380270279, "gen_tokens": 64}
{"task_idx": 49, "scenario": "micro", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 199, "sample_idx": 599, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553978.2822149, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 23.684100000537, "prefill_cuda_event_ms": null, "kv_decode_ms": 1883.943000000727, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 74.824704, "latency_ms": 1907.627100001264, "cuda_event_ms": null, "tokens_total": 65, "tokens_per_s": 34.07374533521616, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 200, "sample_idx": 600, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553980.190652, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 8.5149, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2184, "kv_decode_ms_equiv": 135.08887272727273, "kv_decode_ms_per_token": 2.1107636363636364, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 1328.1182999999146, "ollama_total_duration_ms": 1278.0936, "ollama_load_ms": 1211.8144, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 8.5149, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 1174.4119132344479, "gen_tokens": 0}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 200, "sample_idx": 601, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553980.190652, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 8.5149, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2184, "kv_decode_ms_equiv": 135.08887272727273, "kv_decode_ms_per_token": 2.1107636363636364, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 1328.1182999999146, "ollama_total_duration_ms": 1278.0936, "ollama_load_ms": 1211.8144, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 135.08887272727273, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 473.7621886090342, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 200, "sample_idx": 602, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553980.190652, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 8.5149, "prefill_cuda_event_ms": null, "kv_decode_ms": 23.2184, "kv_decode_ms_equiv": 135.08887272727273, "kv_decode_ms_per_token": 2.1107636363636364, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 1328.1182999999146, "ollama_total_duration_ms": 1278.0936, "ollama_load_ms": 1211.8144, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 143.60377272727274, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 515.306795877419, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 201, "sample_idx": 603, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553981.5193884, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 3.1635, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7523, "kv_decode_ms_equiv": 126.55883636363637, "kv_decode_ms_per_token": 1.9774818181818183, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 214.54140000059851, "ollama_total_duration_ms": 179.5575, "ollama_load_ms": 144.7523, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 3.1635, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 3161.05579263474, "gen_tokens": 0}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 201, "sample_idx": 604, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553981.5193884, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 3.1635, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7523, "kv_decode_ms_equiv": 126.55883636363637, "kv_decode_ms_per_token": 1.9774818181818183, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 214.54140000059851, "ollama_total_duration_ms": 179.5575, "ollama_load_ms": 144.7523, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 126.55883636363637, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 505.69365078635354, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 201, "sample_idx": 605, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553981.5193884, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 3.1635, "prefill_cuda_event_ms": null, "kv_decode_ms": 21.7523, "kv_decode_ms_equiv": 126.55883636363637, "kv_decode_ms_per_token": 1.9774818181818183, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 214.54140000059851, "ollama_total_duration_ms": 179.5575, "ollama_load_ms": 144.7523, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 129.72233636363637, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 570.4491768677673, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 202, "sample_idx": 606, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553981.7349634, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.4161, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.7864, "kv_decode_ms_equiv": 120.93905454545455, "kv_decode_ms_per_token": 1.8896727272727274, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 219.04250000079628, "ollama_total_duration_ms": 181.0611, "ollama_load_ms": 144.9699, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.4161, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4138.90153553247, "gen_tokens": 0}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 202, "sample_idx": 607, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553981.7349634, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.4161, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.7864, "kv_decode_ms_equiv": 120.93905454545455, "kv_decode_ms_per_token": 1.8896727272727274, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 219.04250000079628, "ollama_total_duration_ms": 181.0611, "ollama_load_ms": 144.9699, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 120.93905454545455, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 529.1921641073009, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 202, "sample_idx": 608, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553981.7349634, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.4161, "prefill_cuda_event_ms": null, "kv_decode_ms": 20.7864, "kv_decode_ms_equiv": 120.93905454545455, "kv_decode_ms_per_token": 1.8896727272727274, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 219.04250000079628, "ollama_total_duration_ms": 181.0611, "ollama_load_ms": 144.9699, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 123.35515454545455, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 599.8938615307891, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 203, "sample_idx": 609, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553981.9541326, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.1907, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.8582, "kv_decode_ms_equiv": 115.53861818181818, "kv_decode_ms_per_token": 1.805290909090909, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 206.007500000851, "ollama_total_duration_ms": 174.2934, "ollama_load_ms": 143.1581, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 2.1907, "cuda_event_ms": null, "tokens_total": 10, "tokens_per_s": 4564.75099283334, "gen_tokens": 0}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 203, "sample_idx": 610, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553981.9541326, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.1907, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.8582, "kv_decode_ms_equiv": 115.53861818181818, "kv_decode_ms_per_token": 1.805290909090909, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 206.007500000851, "ollama_total_duration_ms": 174.2934, "ollama_load_ms": 143.1581, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 115.53861818181818, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 553.927344875165, "gen_tokens": 64}
{"task_idx": 50, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:270m", "model_kind": "ollama", "params_millions": 268.1, "params_millions_config": 268.1, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 203, "sample_idx": 611, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553981.9541326, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 10, "prefill_ms": 2.1907, "prefill_cuda_event_ms": null, "kv_decode_ms": 19.8582, "kv_decode_ms_equiv": 115.53861818181818, "kv_decode_ms_per_token": 1.805290909090909, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 11, "gen_tokens_equiv": 64, "ollama_wall_ms": 206.007500000851, "ollama_total_duration_ms": 174.2934, "ollama_load_ms": 143.1581, "ollama_done_reason": "stop", "params_millions_measured": 268.1, "latency_ms": 117.72931818181819, "cuda_event_ms": null, "tokens_total": 74, "tokens_per_s": 628.5605076359677, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 204, "sample_idx": 612, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553982.1603909, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 93.3581, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.1056, "kv_decode_ms_equiv": 266.1056, "kv_decode_ms_per_token": 4.1579, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 598.9709999994375, "ollama_total_duration_ms": 540.3864, "ollama_load_ms": 141.179, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 93.3581, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 117.82587691908897, "gen_tokens": 0}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 204, "sample_idx": 613, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553982.1603909, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 93.3581, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.1056, "kv_decode_ms_equiv": 266.1056, "kv_decode_ms_per_token": 4.1579, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 598.9709999994375, "ollama_total_duration_ms": 540.3864, "ollama_load_ms": 141.179, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 266.1056, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 240.50602467591813, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 204, "sample_idx": 614, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553982.1603909, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 93.3581, "prefill_cuda_event_ms": null, "kv_decode_ms": 266.1056, "kv_decode_ms_equiv": 266.1056, "kv_decode_ms_per_token": 4.1579, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 598.9709999994375, "ollama_total_duration_ms": 540.3864, "ollama_load_ms": 141.179, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 359.46369999999996, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 208.64415516782367, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 205, "sample_idx": 615, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553982.7595105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.0936, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.7502, "kv_decode_ms_equiv": 250.7502, "kv_decode_ms_per_token": 3.917971875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.86650000006193, "ollama_total_duration_ms": 448.2184, "ollama_load_ms": 152.7436, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.0936, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2687.1213601719755, "gen_tokens": 0}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 205, "sample_idx": 616, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553982.7595105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.0936, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.7502, "kv_decode_ms_equiv": 250.7502, "kv_decode_ms_per_token": 3.917971875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.86650000006193, "ollama_total_duration_ms": 448.2184, "ollama_load_ms": 152.7436, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 250.7502, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 255.23409353212875, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 205, "sample_idx": 617, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553982.7595105, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.0936, "prefill_cuda_event_ms": null, "kv_decode_ms": 250.7502, "kv_decode_ms_equiv": 250.7502, "kv_decode_ms_per_token": 3.917971875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 489.86650000006193, "ollama_total_duration_ms": 448.2184, "ollama_load_ms": 152.7436, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 254.84380000000002, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 294.2979189605554, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 206, "sample_idx": 618, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553983.2495298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 5.053, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.2941, "kv_decode_ms_equiv": 259.2941, "kv_decode_ms_per_token": 4.0514703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 533.0687000005128, "ollama_total_duration_ms": 492.3763, "ollama_load_ms": 190.9425, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 5.053, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2176.9245992479714, "gen_tokens": 0}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 206, "sample_idx": 619, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553983.2495298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 5.053, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.2941, "kv_decode_ms_equiv": 259.2941, "kv_decode_ms_per_token": 4.0514703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 533.0687000005128, "ollama_total_duration_ms": 492.3763, "ollama_load_ms": 190.9425, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 259.2941, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 246.82397324119597, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 206, "sample_idx": 620, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553983.2495298, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 5.053, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.2941, "kv_decode_ms_equiv": 259.2941, "kv_decode_ms_per_token": 4.0514703125, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 533.0687000005128, "ollama_total_duration_ms": 492.3763, "ollama_load_ms": 190.9425, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 264.3471, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 283.7178845540579, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 207, "sample_idx": 621, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553983.7827628, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.4579, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.3502, "kv_decode_ms_equiv": 259.3502, "kv_decode_ms_per_token": 4.052346875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.9470999993937, "ollama_total_duration_ms": 446.9128, "ollama_load_ms": 144.0642, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 4.4579, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 2467.529554274434, "gen_tokens": 0}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 207, "sample_idx": 622, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553983.7827628, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.4579, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.3502, "kv_decode_ms_equiv": 259.3502, "kv_decode_ms_per_token": 4.052346875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.9470999993937, "ollama_total_duration_ms": 446.9128, "ollama_load_ms": 144.0642, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 259.3502, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 246.7705827872892, "gen_tokens": 64}
{"task_idx": 51, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "gemma3:1b-it-qat", "model_kind": "ollama", "params_millions": 999.9, "params_millions_config": 999.9, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 207, "sample_idx": 623, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553983.7827628, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 4.4579, "prefill_cuda_event_ms": null, "kv_decode_ms": 259.3502, "kv_decode_ms_equiv": 259.3502, "kv_decode_ms_per_token": 4.052346875, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 64, "gen_tokens_equiv": 64, "ollama_wall_ms": 500.9470999993937, "ollama_total_duration_ms": 446.9128, "ollama_load_ms": 144.0642, "ollama_done_reason": "length", "params_millions_measured": 999.89, "latency_ms": 263.80809999999997, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 284.2975632666321, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 208, "sample_idx": 624, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553984.2842946, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 19.3729, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.3541, "kv_decode_ms_equiv": 693.46624, "kv_decode_ms_per_token": 10.83541, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 5173.920299999736, "ollama_total_duration_ms": 5169.4132, "ollama_load_ms": 5029.3616, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 19.3729, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 567.8034780543956, "gen_tokens": 0}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 208, "sample_idx": 625, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553984.2842946, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 19.3729, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.3541, "kv_decode_ms_equiv": 693.46624, "kv_decode_ms_per_token": 10.83541, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 5173.920299999736, "ollama_total_duration_ms": 5169.4132, "ollama_load_ms": 5029.3616, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 693.46624, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 92.29000102441901, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 208, "sample_idx": 626, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553984.2842946, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 19.3729, "prefill_cuda_event_ms": null, "kv_decode_ms": 108.3541, "kv_decode_ms_equiv": 693.46624, "kv_decode_ms_per_token": 10.83541, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 5173.920299999736, "ollama_total_duration_ms": 5169.4132, "ollama_load_ms": 5029.3616, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 712.8391399999999, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 105.21307794630918, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 209, "sample_idx": 627, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553989.4583461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4413, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.3018, "kv_decode_ms_equiv": 661.13152, "kv_decode_ms_per_token": 10.33018, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 260.4772000013327, "ollama_total_duration_ms": 258.3019, "ollama_load_ms": 134.5949, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.4413, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 961.4292082193457, "gen_tokens": 0}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 209, "sample_idx": 628, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553989.4583461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4413, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.3018, "kv_decode_ms_equiv": 661.13152, "kv_decode_ms_per_token": 10.33018, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 260.4772000013327, "ollama_total_duration_ms": 258.3019, "ollama_load_ms": 134.5949, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 661.13152, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 96.80373430085439, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 209, "sample_idx": 629, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553989.4583461, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4413, "prefill_cuda_event_ms": null, "kv_decode_ms": 103.3018, "kv_decode_ms_equiv": 661.13152, "kv_decode_ms_per_token": 10.33018, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 260.4772000013327, "ollama_total_duration_ms": 258.3019, "ollama_load_ms": 134.5949, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 672.57282, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 111.51208875791323, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 210, "sample_idx": 630, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553989.7190073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.6711, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.5849, "kv_decode_ms_equiv": 656.54336, "kv_decode_ms_per_token": 10.25849, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 254.6898999989935, "ollama_total_duration_ms": 251.8387, "ollama_load_ms": 128.2182, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.6711, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 942.4989932397119, "gen_tokens": 0}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 210, "sample_idx": 631, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553989.7190073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.6711, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.5849, "kv_decode_ms_equiv": 656.54336, "kv_decode_ms_per_token": 10.25849, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 254.6898999989935, "ollama_total_duration_ms": 251.8387, "ollama_load_ms": 128.2182, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 656.54336, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 97.48023344566305, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 210, "sample_idx": 632, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553989.7190073, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.6711, "prefill_cuda_event_ms": null, "kv_decode_ms": 102.5849, "kv_decode_ms_equiv": 656.54336, "kv_decode_ms_per_token": 10.25849, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 254.6898999989935, "ollama_total_duration_ms": 251.8387, "ollama_load_ms": 128.2182, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 668.21446, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 112.23941487288376, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 211, "sample_idx": 633, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553989.973814, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4993, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.3227, "kv_decode_ms_equiv": 648.46528, "kv_decode_ms_per_token": 10.13227, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 277.0762999989529, "ollama_total_duration_ms": 253.7752, "ollama_load_ms": 129.5955, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 11.4993, "cuda_event_ms": null, "tokens_total": 11, "tokens_per_s": 956.5799657370449, "gen_tokens": 0}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 211, "sample_idx": 634, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553989.973814, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4993, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.3227, "kv_decode_ms_equiv": 648.46528, "kv_decode_ms_per_token": 10.13227, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 277.0762999989529, "ollama_total_duration_ms": 253.7752, "ollama_load_ms": 129.5955, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 648.46528, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 98.69456696278326, "gen_tokens": 64}
{"task_idx": 52, "scenario": "micro", "backend": "ollama", "backend_kind": "ollama", "model": "llama3.1:8b-instruct-q4_0", "model_kind": "ollama", "params_millions": 8000.0, "params_millions_config": 8000.0, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 211, "sample_idx": 635, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553989.973814, "batch_size": 1, "seq_len": null, "prompt_tokens_raw": 11, "prefill_ms": 11.4993, "prefill_cuda_event_ms": null, "kv_decode_ms": 101.3227, "kv_decode_ms_equiv": 648.46528, "kv_decode_ms_per_token": 10.13227, "kv_decode_cuda_event_ms": null, "gen_tokens_raw": 10, "gen_tokens_equiv": 64, "ollama_wall_ms": 277.0762999989529, "ollama_total_duration_ms": 253.7752, "ollama_load_ms": 129.5955, "ollama_done_reason": "stop", "params_millions_measured": 8000.0, "latency_ms": 659.96458, "cuda_event_ms": null, "tokens_total": 75, "tokens_per_s": 113.64246244851506, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 212, "sample_idx": 636, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553990.251008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.585000000399305, "prefill_cuda_event_ms": 12.508511543273926, "kv_decode_ms": 329.75660000010976, "kv_decode_cuda_event_ms": 329.7003479003906, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 12.585000000399305, "cuda_event_ms": 12.508511543273926, "tokens_total": 17, "tokens_per_s": 1350.8144616178477, "gen_tokens": 0}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 212, "sample_idx": 637, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553990.251008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.585000000399305, "prefill_cuda_event_ms": 12.508511543273926, "kv_decode_ms": 329.75660000010976, "kv_decode_cuda_event_ms": 329.7003479003906, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 329.75660000010976, "cuda_event_ms": 329.7003479003906, "tokens_total": 64, "tokens_per_s": 194.08254451913533, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 212, "sample_idx": 638, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553990.251008, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 12.585000000399305, "prefill_cuda_event_ms": 12.508511543273926, "kv_decode_ms": 329.75660000010976, "kv_decode_cuda_event_ms": 329.7003479003906, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 342.34160000050906, "cuda_event_ms": 342.20885944366455, "tokens_total": 81, "tokens_per_s": 236.60577621848924, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 213, "sample_idx": 639, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553990.6175814, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.015100001197425, "prefill_cuda_event_ms": 3.9611198902130127, "kv_decode_ms": 300.0415999995312, "kv_decode_cuda_event_ms": 300.01434326171875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 4.015100001197425, "cuda_event_ms": 3.9611198902130127, "tokens_total": 17, "tokens_per_s": 4234.016586119918, "gen_tokens": 0}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 213, "sample_idx": 640, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553990.6175814, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.015100001197425, "prefill_cuda_event_ms": 3.9611198902130127, "kv_decode_ms": 300.0415999995312, "kv_decode_cuda_event_ms": 300.01434326171875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 300.0415999995312, "cuda_event_ms": 300.01434326171875, "tokens_total": 64, "tokens_per_s": 213.30375521294383, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 213, "sample_idx": 641, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553990.6175814, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 4.015100001197425, "prefill_cuda_event_ms": 3.9611198902130127, "kv_decode_ms": 300.0415999995312, "kv_decode_cuda_event_ms": 300.01434326171875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 304.0567000007286, "cuda_event_ms": 303.97546315193176, "tokens_total": 81, "tokens_per_s": 266.3976817475356, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 214, "sample_idx": 642, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553990.9226487, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.546700000399142, "prefill_cuda_event_ms": 6.492191791534424, "kv_decode_ms": 318.0960999998206, "kv_decode_cuda_event_ms": 318.03802490234375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 6.546700000399142, "cuda_event_ms": 6.492191791534424, "tokens_total": 17, "tokens_per_s": 2596.728122407249, "gen_tokens": 0}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 214, "sample_idx": 643, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553990.9226487, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.546700000399142, "prefill_cuda_event_ms": 6.492191791534424, "kv_decode_ms": 318.0960999998206, "kv_decode_cuda_event_ms": 318.03802490234375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 318.0960999998206, "cuda_event_ms": 318.03802490234375, "tokens_total": 64, "tokens_per_s": 201.19705963083516, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 214, "sample_idx": 644, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553990.9226487, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 6.546700000399142, "prefill_cuda_event_ms": 6.492191791534424, "kv_decode_ms": 318.0960999998206, "kv_decode_cuda_event_ms": 318.03802490234375, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 324.6428000002197, "cuda_event_ms": 324.5302166938782, "tokens_total": 81, "tokens_per_s": 249.50499441215138, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 215, "sample_idx": 645, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553991.2482727, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.061800000883522, "prefill_cuda_event_ms": 7.000864028930664, "kv_decode_ms": 304.30929999965883, "kv_decode_cuda_event_ms": 304.248046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 7.061800000883522, "cuda_event_ms": 7.000864028930664, "tokens_total": 17, "tokens_per_s": 2407.3182471711293, "gen_tokens": 0}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 215, "sample_idx": 646, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553991.2482727, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.061800000883522, "prefill_cuda_event_ms": 7.000864028930664, "kv_decode_ms": 304.30929999965883, "kv_decode_cuda_event_ms": 304.248046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 304.30929999965883, "cuda_event_ms": 304.248046875, "tokens_total": 64, "tokens_per_s": 210.31233682332993, "gen_tokens": 64}
{"task_idx": 53, "scenario": "medium", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-75m", "model_kind": "hf", "params_millions": 74.825, "params_millions_config": 74.825, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 215, "sample_idx": 647, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553991.2482727, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 7.061800000883522, "prefill_cuda_event_ms": 7.000864028930664, "kv_decode_ms": 304.30929999965883, "kv_decode_cuda_event_ms": 304.248046875, "gpu_peak_mb": 633.26806640625, "params_millions_measured": 74.824704, "latency_ms": 311.37110000054236, "cuda_event_ms": 311.24891090393066, "tokens_total": 81, "tokens_per_s": 260.1397496423365, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 216, "sample_idx": 648, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553991.5610635, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.41480000114825, "prefill_cuda_event_ms": 11.30742359161377, "kv_decode_ms": 754.102500000954, "kv_decode_cuda_event_ms": 754.03369140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 11.41480000114825, "cuda_event_ms": 11.30742359161377, "tokens_total": 1, "tokens_per_s": 87.60556469665757, "gen_tokens": 0}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 216, "sample_idx": 649, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553991.5610635, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.41480000114825, "prefill_cuda_event_ms": 11.30742359161377, "kv_decode_ms": 754.102500000954, "kv_decode_cuda_event_ms": 754.03369140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 754.102500000954, "cuda_event_ms": 754.03369140625, "tokens_total": 64, "tokens_per_s": 84.86909935972767, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 216, "sample_idx": 650, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553991.5610635, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 11.41480000114825, "prefill_cuda_event_ms": 11.30742359161377, "kv_decode_ms": 754.102500000954, "kv_decode_cuda_event_ms": 754.03369140625, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 765.5173000021023, "cuda_event_ms": 765.3411149978638, "tokens_total": 65, "tokens_per_s": 84.90990340756701, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 217, "sample_idx": 651, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553992.329823, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.135500001022592, "prefill_cuda_event_ms": 12.02723217010498, "kv_decode_ms": 657.8916000016761, "kv_decode_cuda_event_ms": 657.8411254882812, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 12.135500001022592, "cuda_event_ms": 12.02723217010498, "tokens_total": 1, "tokens_per_s": 82.40286761284953, "gen_tokens": 0}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 217, "sample_idx": 652, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553992.329823, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.135500001022592, "prefill_cuda_event_ms": 12.02723217010498, "kv_decode_ms": 657.8916000016761, "kv_decode_cuda_event_ms": 657.8411254882812, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 657.8916000016761, "cuda_event_ms": 657.8411254882812, "tokens_total": 64, "tokens_per_s": 97.28046383300371, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 217, "sample_idx": 653, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553992.329823, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 12.135500001022592, "prefill_cuda_event_ms": 12.02723217010498, "kv_decode_ms": 657.8916000016761, "kv_decode_cuda_event_ms": 657.8411254882812, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 670.0271000026987, "cuda_event_ms": 669.8683576583862, "tokens_total": 65, "tokens_per_s": 97.01100149492191, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 218, "sample_idx": 654, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553993.0012834, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 14.426200001253164, "prefill_cuda_event_ms": 14.323424339294434, "kv_decode_ms": 399.9349000005168, "kv_decode_cuda_event_ms": 399.8791809082031, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 14.426200001253164, "cuda_event_ms": 14.323424339294434, "tokens_total": 1, "tokens_per_s": 69.31832359964042, "gen_tokens": 0}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 218, "sample_idx": 655, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553993.0012834, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 14.426200001253164, "prefill_cuda_event_ms": 14.323424339294434, "kv_decode_ms": 399.9349000005168, "kv_decode_cuda_event_ms": 399.8791809082031, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 399.9349000005168, "cuda_event_ms": 399.8791809082031, "tokens_total": 64, "tokens_per_s": 160.02604423849306, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 218, "sample_idx": 656, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553993.0012834, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 14.426200001253164, "prefill_cuda_event_ms": 14.323424339294434, "kv_decode_ms": 399.9349000005168, "kv_decode_cuda_event_ms": 399.8791809082031, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 414.36110000176996, "cuda_event_ms": 414.20260524749756, "tokens_total": 65, "tokens_per_s": 156.86800715540707, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 219, "sample_idx": 657, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553993.4164546, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 6.464100000812323, "prefill_cuda_event_ms": 6.410272121429443, "kv_decode_ms": 395.2238000001671, "kv_decode_cuda_event_ms": 395.188232421875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 6.464100000812323, "cuda_event_ms": 6.410272121429443, "tokens_total": 1, "tokens_per_s": 154.7005770137116, "gen_tokens": 0}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 219, "sample_idx": 658, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553993.4164546, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 6.464100000812323, "prefill_cuda_event_ms": 6.410272121429443, "kv_decode_ms": 395.2238000001671, "kv_decode_cuda_event_ms": 395.188232421875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 395.2238000001671, "cuda_event_ms": 395.188232421875, "tokens_total": 64, "tokens_per_s": 161.93356776584037, "gen_tokens": 64}
{"task_idx": 54, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-5m", "model_kind": "hf", "params_millions": 5.037, "params_millions_config": 5.037, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 219, "sample_idx": 659, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553993.4164546, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 6.464100000812323, "prefill_cuda_event_ms": 6.410272121429443, "kv_decode_ms": 395.2238000001671, "kv_decode_cuda_event_ms": 395.188232421875, "gpu_peak_mb": 630.7822265625, "params_millions_measured": 5.03672, "latency_ms": 401.6879000009794, "cuda_event_ms": 401.59850454330444, "tokens_total": 65, "tokens_per_s": 161.81717198810696, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 220, "sample_idx": 660, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553993.8187413, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.496699998431723, "prefill_cuda_event_ms": 8.443136215209961, "kv_decode_ms": 296.5186999990692, "kv_decode_cuda_event_ms": 296.4939880371094, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 8.496699998431723, "cuda_event_ms": 8.443136215209961, "tokens_total": 1, "tokens_per_s": 117.69275132517035, "gen_tokens": 0}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 220, "sample_idx": 661, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553993.8187413, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.496699998431723, "prefill_cuda_event_ms": 8.443136215209961, "kv_decode_ms": 296.5186999990692, "kv_decode_cuda_event_ms": 296.4939880371094, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 296.5186999990692, "cuda_event_ms": 296.4939880371094, "tokens_total": 64, "tokens_per_s": 215.83798930792864, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 220, "sample_idx": 662, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553993.8187413, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 8.496699998431723, "prefill_cuda_event_ms": 8.443136215209961, "kv_decode_ms": 296.5186999990692, "kv_decode_cuda_event_ms": 296.4939880371094, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 305.01539999750094, "cuda_event_ms": 304.93712425231934, "tokens_total": 65, "tokens_per_s": 213.103994095159, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 221, "sample_idx": 663, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553994.1278977, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.140400000324007, "prefill_cuda_event_ms": 4.101920127868652, "kv_decode_ms": 249.8838000010437, "kv_decode_cuda_event_ms": 249.85498046875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.140400000324007, "cuda_event_ms": 4.101920127868652, "tokens_total": 1, "tokens_per_s": 241.5225581880362, "gen_tokens": 0}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 221, "sample_idx": 664, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553994.1278977, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.140400000324007, "prefill_cuda_event_ms": 4.101920127868652, "kv_decode_ms": 249.8838000010437, "kv_decode_cuda_event_ms": 249.85498046875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 249.8838000010437, "cuda_event_ms": 249.85498046875, "tokens_total": 64, "tokens_per_s": 256.1190441306427, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 221, "sample_idx": 665, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553994.1278977, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.140400000324007, "prefill_cuda_event_ms": 4.101920127868652, "kv_decode_ms": 249.8838000010437, "kv_decode_cuda_event_ms": 249.85498046875, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 254.02420000136772, "cuda_event_ms": 253.95690059661865, "tokens_total": 65, "tokens_per_s": 255.88113258362796, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 222, "sample_idx": 666, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553994.382523, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.774799999722745, "prefill_cuda_event_ms": 4.723167896270752, "kv_decode_ms": 264.1955999988568, "kv_decode_cuda_event_ms": 264.158203125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.774799999722745, "cuda_event_ms": 4.723167896270752, "tokens_total": 1, "tokens_per_s": 209.43285583858304, "gen_tokens": 0}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 222, "sample_idx": 667, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553994.382523, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.774799999722745, "prefill_cuda_event_ms": 4.723167896270752, "kv_decode_ms": 264.1955999988568, "kv_decode_cuda_event_ms": 264.158203125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 264.1955999988568, "cuda_event_ms": 264.158203125, "tokens_total": 64, "tokens_per_s": 242.24476107958247, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 222, "sample_idx": 668, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553994.382523, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.774799999722745, "prefill_cuda_event_ms": 4.723167896270752, "kv_decode_ms": 264.1955999988568, "kv_decode_cuda_event_ms": 264.158203125, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 268.9703999985795, "cuda_event_ms": 268.88137102127075, "tokens_total": 65, "tokens_per_s": 241.66227956809848, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 223, "sample_idx": 669, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553994.652175, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.651799999919604, "prefill_cuda_event_ms": 4.599264144897461, "kv_decode_ms": 255.56159999905503, "kv_decode_cuda_event_ms": 255.5351104736328, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 4.651799999919604, "cuda_event_ms": 4.599264144897461, "tokens_total": 1, "tokens_per_s": 214.97054903849755, "gen_tokens": 0}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 223, "sample_idx": 670, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553994.652175, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.651799999919604, "prefill_cuda_event_ms": 4.599264144897461, "kv_decode_ms": 255.56159999905503, "kv_decode_cuda_event_ms": 255.5351104736328, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 255.56159999905503, "cuda_event_ms": 255.5351104736328, "tokens_total": 64, "tokens_per_s": 250.42885942268575, "gen_tokens": 64}
{"task_idx": 55, "scenario": "micro", "backend": "hf_gpu_fp16", "backend_kind": "hf", "model": "models/gpt2-50m", "model_kind": "hf", "params_millions": 51.476, "params_millions_config": 51.476, "prompt": "Hello", "prompt_chars": 5, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 223, "sample_idx": 671, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553994.652175, "batch_size": 1, "seq_len": 1, "prompt_tokens_raw": 1, "gen_tokens_raw": 64, "prefill_ms": 4.651799999919604, "prefill_cuda_event_ms": 4.599264144897461, "kv_decode_ms": 255.56159999905503, "kv_decode_cuda_event_ms": 255.5351104736328, "gpu_peak_mb": 631.55810546875, "params_millions_measured": 51.475968, "latency_ms": 260.21339999897464, "cuda_event_ms": 260.1343746185303, "tokens_total": 65, "tokens_per_s": 249.7949759707076, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 224, "sample_idx": 672, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553994.9131567, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.06070000040927, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.7453999998834, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.06070000040927, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 891.8874962427916, "gen_tokens": 0}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 224, "sample_idx": 673, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553994.9131567, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.06070000040927, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.7453999998834, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 809.7453999998834, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 79.03718872624557, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 224, "sample_idx": 674, "is_warmup": true, "warmup_idx": 0, "rep": null, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553994.9131567, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.06070000040927, "prefill_cuda_event_ms": null, "kv_decode_ms": 809.7453999998834, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 828.8061000002926, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 97.7309409281271, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 225, "sample_idx": 675, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553995.7655406, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.51900000090245, "prefill_cuda_event_ms": null, "kv_decode_ms": 833.8151999996626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 17.51900000090245, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 970.3750213553446, "gen_tokens": 0}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 225, "sample_idx": 676, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553995.7655406, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.51900000090245, "prefill_cuda_event_ms": null, "kv_decode_ms": 833.8151999996626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 833.8151999996626, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 76.75561683215405, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 225, "sample_idx": 677, "is_warmup": false, "warmup_idx": null, "rep": 0, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553995.7655406, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 17.51900000090245, "prefill_cuda_event_ms": null, "kv_decode_ms": 833.8151999996626, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 851.334200000565, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 95.14477393243011, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 226, "sample_idx": 678, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553996.6177797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 21.236500000668457, "prefill_cuda_event_ms": null, "kv_decode_ms": 873.0967000010423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 21.236500000668457, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 800.5085583530664, "gen_tokens": 0}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 226, "sample_idx": 679, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553996.6177797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 21.236500000668457, "prefill_cuda_event_ms": null, "kv_decode_ms": 873.0967000010423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 873.0967000010423, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 73.30230431511606, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 226, "sample_idx": 680, "is_warmup": false, "warmup_idx": null, "rep": 1, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553996.6177797, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 21.236500000668457, "prefill_cuda_event_ms": null, "kv_decode_ms": 873.0967000010423, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 894.3332000017108, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 90.57027067746681, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 227, "sample_idx": 681, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "prefill", "status": "ok", "error": "", "started_at_unix": 1766553997.5126836, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.73790000010922, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.3007999992842, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 19.73790000010922, "cuda_event_ms": null, "tokens_total": 17, "tokens_per_s": 861.2871683363443, "gen_tokens": 0}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 227, "sample_idx": 682, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "kv_decode", "status": "ok", "error": "", "started_at_unix": 1766553997.5126836, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.73790000010922, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.3007999992842, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 852.3007999992842, "cuda_event_ms": null, "tokens_total": 64, "tokens_per_s": 75.0908599405911, "gen_tokens": 64}
{"task_idx": 56, "scenario": "medium", "backend": "hf_cpu_fp32", "backend_kind": "hf", "model": "models/gpt2-25m", "model_kind": "hf", "params_millions": 25.016, "params_millions_config": 25.016, "prompt": "Explain how backpressure works in an inference service and when to enable queueing.", "prompt_chars": 83, "gen_tokens_target": 64, "batch_size_config": 1, "call_idx": 227, "sample_idx": 683, "is_warmup": false, "warmup_idx": null, "rep": 2, "mode": "e2e_kv", "status": "ok", "error": "", "started_at_unix": 1766553997.5126836, "batch_size": 1, "seq_len": 17, "prompt_tokens_raw": 17, "gen_tokens_raw": 64, "prefill_ms": 19.73790000010922, "prefill_cuda_event_ms": null, "kv_decode_ms": 852.3007999992842, "kv_decode_cuda_event_ms": null, "gpu_peak_mb": null, "params_millions_measured": 25.016064, "latency_ms": 872.0386999993934, "cuda_event_ms": null, "tokens_total": 81, "tokens_per_s": 92.88578591759327, "gen_tokens": 64}
